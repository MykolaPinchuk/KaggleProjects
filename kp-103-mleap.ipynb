{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Define a problem, describe business context.\n2. Load data, preclean it.\n3. EDA: target, features.\n4. Descibe train/test split strategy. Show main results and discuss them.\n5. Run evth for a few periods. Show feature importance and error analysis.","metadata":{}},{"cell_type":"markdown","source":"### 1. Business problem\n\n#### Objective:\n\n#### Metric:\n\n#### Summary of results:\n","metadata":{}},{"cell_type":"markdown","source":"### 2. Load data and preclean it","metadata":{"execution":{"iopub.status.busy":"2022-09-26T15:43:15.362648Z","iopub.execute_input":"2022-09-26T15:43:15.363102Z","iopub.status.idle":"2022-09-26T15:43:15.385837Z","shell.execute_reply.started":"2022-09-26T15:43:15.363009Z","shell.execute_reply":"2022-09-26T15:43:15.384918Z"}}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle, shap\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, KFold, PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\nfrom optuna.integration import TFKerasPruningCallback\nfrom optuna.trial import TrialState\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\noptuna.logging.set_verbosity(optuna.logging.WARNING)\npd.set_option('display.max_columns', 150)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:01:04.674524Z","iopub.execute_input":"2022-09-27T14:01:04.674913Z","iopub.status.idle":"2022-09-27T14:01:15.262625Z","shell.execute_reply.started":"2022-09-27T14:01:04.674832Z","shell.execute_reply":"2022-09-27T14:01:15.261645Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:01:15.266602Z","iopub.execute_input":"2022-09-27T14:01:15.267735Z","iopub.status.idle":"2022-09-27T14:01:15.278149Z","shell.execute_reply.started":"2022-09-27T14:01:15.267696Z","shell.execute_reply":"2022-09-27T14:01:15.277179Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:01:15.279691Z","iopub.execute_input":"2022-09-27T14:01:15.280436Z","iopub.status.idle":"2022-09-27T14:01:15.311805Z","shell.execute_reply.started":"2022-09-27T14:01:15.280369Z","shell.execute_reply":"2022-09-27T14:01:15.310582Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Functions for Optuna NNs\n\ncv_nn_regularizer = 0.075\n\ndef create_snnn4_model_hyperpars(neurons_base=4, l1_reg_rate=0.1, l2_reg_rate=0.1):\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\ndef create_snnn4_model(trial):\n\n    # hyperparameter space\n    neurons_base = trial.suggest_int(\"neurons_base\", 4, 24, 2)\n    l2_reg_rate = trial.suggest_float('l2_regularizer', 0, 0.9)\n    l1_reg_rate = trial.suggest_float('l1_regularizer', 0, 0.9)\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\n\ndef create_snnn6_model_hyperpars(neurons_base=4, l1_reg_rate=0.1, l2_reg_rate=0.1):\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\ndef create_snnn6_model(trial):\n\n    # hyperparameter space\n    neurons_base = trial.suggest_int(\"neurons_base\", 4, 24, 2)\n    l2_reg_rate = trial.suggest_float('l2_regularizer', 0, 0.9)\n    l1_reg_rate = trial.suggest_float('l1_regularizer', 0, 0.9)\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\n\ndef objective_nn4(trial):\n    \n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_snnn4_model(trial)\n\n        callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=40),\n        TFKerasPruningCallback(trial, \"val_loss\"),\n    ]\n\n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n        history = model.fit(X_train, y_train, \n                                validation_data=(X_val, y_val),\n                                batch_size=2048, \n                                epochs=500, \n                                verbose=0, \n                                callbacks=callbacks)\n\n        # Evaluate the model accuracy on the validation set.\n        loss_train = model.evaluate(X_train, y_train, verbose=0)\n        loss_val = model.evaluate(X_val_extra, y_val_extra, verbose=0)\n        overfit = max(loss_val[1]-loss_train[1], 0)\n        return loss_val[1] + cv_nn_regularizer*overfit\n    \n    \ndef objective_nn6(trial):\n    \n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_snnn6_model(trial)\n\n        callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=40),\n        TFKerasPruningCallback(trial, \"val_loss\"),\n    ]\n\n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n        history = model.fit(X_train, y_train, \n                                validation_data=(X_val, y_val),\n                                batch_size=2048, \n                                epochs=500, \n                                verbose=0, \n                                callbacks=callbacks)\n\n        # Evaluate the model accuracy on the validation set.\n        loss_train = model.evaluate(X_train, y_train, verbose=0)\n        loss_val = model.evaluate(X_val_extra, y_val_extra, verbose=0)\n        overfit = max(loss_val[1]-loss_train[1], 0)\n        return loss_val[1] + cv_nn_regularizer*overfit\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:01:15.315465Z","iopub.execute_input":"2022-09-27T14:01:15.316148Z","iopub.status.idle":"2022-09-27T14:01:15.345554Z","shell.execute_reply.started":"2022-09-27T14:01:15.316105Z","shell.execute_reply":"2022-09-27T14:01:15.344581Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"time0 = time.time()\n\nmin_prd = 350\nwindows_width = 5*12\ncv_xgb_regularizer=0.2\noptuna_xgb_trials = 80\noptuna_nn_trials = 100\n\n\nwith open('../input/mleap-46-preprocessed/MLEAP_46_v7.pkl', 'rb') as pickled_one:\n    df = pickle.load(pickled_one)\ndf = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+10))]\ndf_cnt = df.count()\nempty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\ndf.drop(columns=empty_cols, inplace=True)\n#display(df.shape, df.head(), df.year.describe(), df.count())\n\nfeatures_miss_dummies = ['amhd', 'BAspr']\nfor col in features_miss_dummies:\n    if col in df.columns:\n        df[col+'_miss'] = df[col].isnull().astype(int)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:01:15.346636Z","iopub.execute_input":"2022-09-27T14:01:15.347571Z","iopub.status.idle":"2022-09-27T14:01:21.660367Z","shell.execute_reply.started":"2022-09-27T14:01:15.347536Z","shell.execute_reply":"2022-09-27T14:01:21.659356Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### 3. EDA","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Train/test split strategy","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Main results","metadata":{}},{"cell_type":"code","source":"results_df = pd.read_csv('../input/mleap-v49-results/temp_models_reg005_1.csv')\nresults_df","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:01:21.661643Z","iopub.execute_input":"2022-09-27T14:01:21.661988Z","iopub.status.idle":"2022-09-27T14:01:21.709202Z","shell.execute_reply.started":"2022-09-27T14:01:21.661954Z","shell.execute_reply":"2022-09-27T14:01:21.708168Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    Unnamed: 0  min_prd  xgbf_train  xgbf_val  xgbf_test  xgbgs_train  \\\n0            0      100    0.110633 -0.010679   0.005897     0.032282   \n1            1      125    0.066434  0.004399   0.015985     0.040227   \n2            2      150    0.064235  0.028400  -0.007106     0.108114   \n3            3      175    0.073734  0.019790  -0.011655     0.093350   \n4            4      200    0.056176  0.048308   0.034120     0.111365   \n5            5      225    0.064410 -0.005404   0.012234     0.083176   \n6            6      250    0.061997 -0.017393   0.027061     0.018820   \n7            7      275    0.084432  0.128375   0.097800     0.109516   \n8            8      300    0.109596  0.041026   0.083551     0.097602   \n9            9      325    0.111924  0.042263  -0.038793     0.133787   \n10          10      350    0.064108  0.020667   0.031485     0.055273   \n11          11      375    0.045452  0.020881   0.013222     0.063410   \n12          12      400    0.041836  0.023006   0.023431     0.041640   \n13          13      425    0.043315  0.023428   0.021350     0.042503   \n14          14      450    0.044173  0.004679   0.008707     0.057491   \n15          15      475    0.042778  0.002243   0.002169     0.038150   \n16          16      500    0.042942  0.039946   0.045640     0.092373   \n17          17      525    0.055984  0.008138   0.037555     0.068378   \n18          18      550    0.079068 -0.007977  -0.002293     0.036324   \n19          19      575    0.061489  0.046520   0.011930     0.084962   \n20          20      600    0.053373 -0.018035  -0.004766     0.014890   \n21          21      625         NaN       NaN        NaN          NaN   \n22          22      650         NaN       NaN        NaN          NaN   \n\n    xgbgs_val  xgbgs_test  xgbo_train  xgbo_val  xgbo_test  nn4_train  \\\n0    0.009775    0.016553    0.142414  0.097214   0.017316   0.092751   \n1    0.053275    0.020740    0.081517  0.077877   0.018348   0.020937   \n2    0.136225   -0.040268    0.102913  0.141299  -0.040073   0.039123   \n3    0.065665    0.000453    0.017994  0.013669  -0.002192   0.057370   \n4    0.141292    0.040536    0.057042  0.089245   0.040346   0.041166   \n5    0.067360    0.026466    0.070134  0.033220   0.020088   0.055780   \n6   -0.001580    0.013451    0.189037  0.155622   0.031786   0.048662   \n7    0.192547    0.108320    0.102921  0.176972   0.108397   0.073269   \n8    0.058866    0.086605    0.098751  0.054814   0.084547   0.098849   \n9    0.091224   -0.039374    0.112554  0.067039  -0.034580   0.106888   \n10   0.033387    0.033121    0.059194  0.035949   0.034516   0.058240   \n11   0.054559    0.020181    0.059622  0.051217   0.016368   0.035642   \n12   0.041122    0.028316    0.033655  0.031300   0.027806   0.029551   \n13   0.047076    0.017911    0.034322  0.037631   0.016773   0.031853   \n14   0.063234    0.021913    0.031195  0.029232   0.012268   0.034023   \n15   0.026061    0.009934    0.022383  0.010773   0.004530   0.026574   \n16   0.106835    0.065450    0.056338  0.078596   0.058214   0.033054   \n17   0.034044    0.045484    0.049435  0.033478   0.042042   0.038853   \n18   0.007323    0.014142    0.059759  0.017997   0.009193   0.065788   \n19   0.089735    0.022645    0.049855  0.059204   0.020475   0.049639   \n20  -0.000217    0.005112    0.036381 -0.004054   0.002889   0.011194   \n21        NaN         NaN         NaN       NaN        NaN        NaN   \n22        NaN         NaN         NaN       NaN        NaN        NaN   \n\n     nn4_val  nn4_test  nn6_train   nn6_val  nn6_test  nn4opt_train  \\\n0  -0.004731  0.004806   0.077033  0.003310  0.007623  2.368052e-02   \n1   0.023310  0.014654   0.039157  0.018810  0.009496 -1.010410e-06   \n2   0.041242 -0.044962   0.046364  0.044968 -0.055987  4.366676e-02   \n3   0.021523 -0.006702   0.064954  0.017231 -0.010337 -4.252701e-06   \n4   0.071331  0.022324   0.041645  0.073893  0.025533  3.709691e-02   \n5   0.008508  0.012525   0.052652  0.008472  0.013793 -5.871936e-07   \n6  -0.007640  0.024796   0.048832 -0.016709 -0.007211 -1.719546e-09   \n7   0.144908  0.110993   0.070322  0.143302  0.112889  7.233643e-02   \n8   0.040880  0.086885   0.095496  0.042473  0.086713  1.023098e-01   \n9   0.049632 -0.052411   0.104488  0.049346 -0.055748  9.684091e-02   \n10  0.030566  0.022642   0.055391  0.030115  0.026270  5.876430e-02   \n11  0.024271  0.005288   0.034158  0.025277  0.002064  3.371769e-02   \n12  0.025455  0.034004   0.025434  0.024934  0.036392  2.552223e-02   \n13  0.030089  0.025648   0.029592  0.030158  0.021560  3.133441e-02   \n14  0.031850  0.015509   0.035800  0.034246  0.007337 -1.435706e-07   \n15  0.011256  0.011812   0.033069  0.012727 -0.002586 -3.848525e-07   \n16  0.044643  0.043081   0.035159  0.043644  0.044818  2.918796e-02   \n17  0.012892  0.031937   0.042841  0.010648  0.029456  3.686924e-02   \n18  0.005629  0.016282   0.063776  0.008980  0.021159 -9.343118e-07   \n19  0.055342  0.008340   0.045969  0.055721  0.019505  3.947695e-02   \n20 -0.003839  0.004517   0.031915 -0.025365 -0.014393 -1.116158e-07   \n21       NaN       NaN        NaN       NaN       NaN           NaN   \n22       NaN       NaN        NaN       NaN       NaN           NaN   \n\n    nn4opt_val   nn4opt_test  nn6opt_train    nn6opt_val  nn6opt_test  \n0    -0.006378  4.413288e-03  2.373109e-02 -1.185371e-03     0.006157  \n1    -0.000038 -1.945688e-05 -7.328776e-07 -3.910448e-05    -0.000020  \n2     0.040878 -4.501329e-02  5.041909e-02  4.294926e-02    -0.043427  \n3    -0.000010 -4.954723e-07 -7.191610e-09 -6.917556e-07    -0.000008  \n4     0.065155  2.521314e-02  3.844909e-02  6.439698e-02     0.024244  \n5    -0.000169 -1.014969e-04 -5.182621e-06 -1.285523e-04    -0.000073  \n6    -0.000007 -2.775066e-06 -8.389908e-07 -2.850800e-06    -0.000007  \n7     0.144175  1.100276e-01  7.211387e-02  1.423667e-01     0.106699  \n8     0.039196  8.664547e-02  9.784490e-02  3.844309e-02     0.087651  \n9     0.041965 -6.858033e-02  1.009017e-01  4.594199e-02    -0.066538  \n10    0.030567  2.703620e-02  5.610774e-02  2.959755e-02     0.028619  \n11    0.022140  5.191548e-03  3.490645e-02  2.254672e-02    -0.003187  \n12    0.021759  3.121626e-02  3.332331e-02  2.432268e-02     0.030985  \n13    0.028021  2.200129e-02  2.661165e-02  3.152557e-02     0.012151  \n14   -0.000024 -1.837915e-05 -1.779765e-08 -2.865202e-05    -0.000022  \n15   -0.000045 -3.566047e-05 -3.725332e-07 -4.539190e-05    -0.000036  \n16    0.039344  4.251399e-02  5.253141e-02  5.219970e-02     0.037235  \n17    0.013139  3.232509e-02 -1.155932e-06 -8.184362e-05    -0.000014  \n18   -0.000007 -1.136215e-05 -8.872024e-08 -9.620223e-06    -0.000015  \n19    0.050070  1.513632e-02  5.294881e-02  5.275299e-02     0.013744  \n20   -0.000010 -2.508825e-05 -3.862022e-12 -1.229114e-05    -0.000029  \n21         NaN           NaN           NaN           NaN          NaN  \n22         NaN           NaN           NaN           NaN          NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n      <th>nn4_train</th>\n      <th>nn4_val</th>\n      <th>nn4_test</th>\n      <th>nn6_train</th>\n      <th>nn6_val</th>\n      <th>nn6_test</th>\n      <th>nn4opt_train</th>\n      <th>nn4opt_val</th>\n      <th>nn4opt_test</th>\n      <th>nn6opt_train</th>\n      <th>nn6opt_val</th>\n      <th>nn6opt_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>100</td>\n      <td>0.110633</td>\n      <td>-0.010679</td>\n      <td>0.005897</td>\n      <td>0.032282</td>\n      <td>0.009775</td>\n      <td>0.016553</td>\n      <td>0.142414</td>\n      <td>0.097214</td>\n      <td>0.017316</td>\n      <td>0.092751</td>\n      <td>-0.004731</td>\n      <td>0.004806</td>\n      <td>0.077033</td>\n      <td>0.003310</td>\n      <td>0.007623</td>\n      <td>2.368052e-02</td>\n      <td>-0.006378</td>\n      <td>4.413288e-03</td>\n      <td>2.373109e-02</td>\n      <td>-1.185371e-03</td>\n      <td>0.006157</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>125</td>\n      <td>0.066434</td>\n      <td>0.004399</td>\n      <td>0.015985</td>\n      <td>0.040227</td>\n      <td>0.053275</td>\n      <td>0.020740</td>\n      <td>0.081517</td>\n      <td>0.077877</td>\n      <td>0.018348</td>\n      <td>0.020937</td>\n      <td>0.023310</td>\n      <td>0.014654</td>\n      <td>0.039157</td>\n      <td>0.018810</td>\n      <td>0.009496</td>\n      <td>-1.010410e-06</td>\n      <td>-0.000038</td>\n      <td>-1.945688e-05</td>\n      <td>-7.328776e-07</td>\n      <td>-3.910448e-05</td>\n      <td>-0.000020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>150</td>\n      <td>0.064235</td>\n      <td>0.028400</td>\n      <td>-0.007106</td>\n      <td>0.108114</td>\n      <td>0.136225</td>\n      <td>-0.040268</td>\n      <td>0.102913</td>\n      <td>0.141299</td>\n      <td>-0.040073</td>\n      <td>0.039123</td>\n      <td>0.041242</td>\n      <td>-0.044962</td>\n      <td>0.046364</td>\n      <td>0.044968</td>\n      <td>-0.055987</td>\n      <td>4.366676e-02</td>\n      <td>0.040878</td>\n      <td>-4.501329e-02</td>\n      <td>5.041909e-02</td>\n      <td>4.294926e-02</td>\n      <td>-0.043427</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>175</td>\n      <td>0.073734</td>\n      <td>0.019790</td>\n      <td>-0.011655</td>\n      <td>0.093350</td>\n      <td>0.065665</td>\n      <td>0.000453</td>\n      <td>0.017994</td>\n      <td>0.013669</td>\n      <td>-0.002192</td>\n      <td>0.057370</td>\n      <td>0.021523</td>\n      <td>-0.006702</td>\n      <td>0.064954</td>\n      <td>0.017231</td>\n      <td>-0.010337</td>\n      <td>-4.252701e-06</td>\n      <td>-0.000010</td>\n      <td>-4.954723e-07</td>\n      <td>-7.191610e-09</td>\n      <td>-6.917556e-07</td>\n      <td>-0.000008</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>200</td>\n      <td>0.056176</td>\n      <td>0.048308</td>\n      <td>0.034120</td>\n      <td>0.111365</td>\n      <td>0.141292</td>\n      <td>0.040536</td>\n      <td>0.057042</td>\n      <td>0.089245</td>\n      <td>0.040346</td>\n      <td>0.041166</td>\n      <td>0.071331</td>\n      <td>0.022324</td>\n      <td>0.041645</td>\n      <td>0.073893</td>\n      <td>0.025533</td>\n      <td>3.709691e-02</td>\n      <td>0.065155</td>\n      <td>2.521314e-02</td>\n      <td>3.844909e-02</td>\n      <td>6.439698e-02</td>\n      <td>0.024244</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>225</td>\n      <td>0.064410</td>\n      <td>-0.005404</td>\n      <td>0.012234</td>\n      <td>0.083176</td>\n      <td>0.067360</td>\n      <td>0.026466</td>\n      <td>0.070134</td>\n      <td>0.033220</td>\n      <td>0.020088</td>\n      <td>0.055780</td>\n      <td>0.008508</td>\n      <td>0.012525</td>\n      <td>0.052652</td>\n      <td>0.008472</td>\n      <td>0.013793</td>\n      <td>-5.871936e-07</td>\n      <td>-0.000169</td>\n      <td>-1.014969e-04</td>\n      <td>-5.182621e-06</td>\n      <td>-1.285523e-04</td>\n      <td>-0.000073</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>250</td>\n      <td>0.061997</td>\n      <td>-0.017393</td>\n      <td>0.027061</td>\n      <td>0.018820</td>\n      <td>-0.001580</td>\n      <td>0.013451</td>\n      <td>0.189037</td>\n      <td>0.155622</td>\n      <td>0.031786</td>\n      <td>0.048662</td>\n      <td>-0.007640</td>\n      <td>0.024796</td>\n      <td>0.048832</td>\n      <td>-0.016709</td>\n      <td>-0.007211</td>\n      <td>-1.719546e-09</td>\n      <td>-0.000007</td>\n      <td>-2.775066e-06</td>\n      <td>-8.389908e-07</td>\n      <td>-2.850800e-06</td>\n      <td>-0.000007</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>275</td>\n      <td>0.084432</td>\n      <td>0.128375</td>\n      <td>0.097800</td>\n      <td>0.109516</td>\n      <td>0.192547</td>\n      <td>0.108320</td>\n      <td>0.102921</td>\n      <td>0.176972</td>\n      <td>0.108397</td>\n      <td>0.073269</td>\n      <td>0.144908</td>\n      <td>0.110993</td>\n      <td>0.070322</td>\n      <td>0.143302</td>\n      <td>0.112889</td>\n      <td>7.233643e-02</td>\n      <td>0.144175</td>\n      <td>1.100276e-01</td>\n      <td>7.211387e-02</td>\n      <td>1.423667e-01</td>\n      <td>0.106699</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>300</td>\n      <td>0.109596</td>\n      <td>0.041026</td>\n      <td>0.083551</td>\n      <td>0.097602</td>\n      <td>0.058866</td>\n      <td>0.086605</td>\n      <td>0.098751</td>\n      <td>0.054814</td>\n      <td>0.084547</td>\n      <td>0.098849</td>\n      <td>0.040880</td>\n      <td>0.086885</td>\n      <td>0.095496</td>\n      <td>0.042473</td>\n      <td>0.086713</td>\n      <td>1.023098e-01</td>\n      <td>0.039196</td>\n      <td>8.664547e-02</td>\n      <td>9.784490e-02</td>\n      <td>3.844309e-02</td>\n      <td>0.087651</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>325</td>\n      <td>0.111924</td>\n      <td>0.042263</td>\n      <td>-0.038793</td>\n      <td>0.133787</td>\n      <td>0.091224</td>\n      <td>-0.039374</td>\n      <td>0.112554</td>\n      <td>0.067039</td>\n      <td>-0.034580</td>\n      <td>0.106888</td>\n      <td>0.049632</td>\n      <td>-0.052411</td>\n      <td>0.104488</td>\n      <td>0.049346</td>\n      <td>-0.055748</td>\n      <td>9.684091e-02</td>\n      <td>0.041965</td>\n      <td>-6.858033e-02</td>\n      <td>1.009017e-01</td>\n      <td>4.594199e-02</td>\n      <td>-0.066538</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>350</td>\n      <td>0.064108</td>\n      <td>0.020667</td>\n      <td>0.031485</td>\n      <td>0.055273</td>\n      <td>0.033387</td>\n      <td>0.033121</td>\n      <td>0.059194</td>\n      <td>0.035949</td>\n      <td>0.034516</td>\n      <td>0.058240</td>\n      <td>0.030566</td>\n      <td>0.022642</td>\n      <td>0.055391</td>\n      <td>0.030115</td>\n      <td>0.026270</td>\n      <td>5.876430e-02</td>\n      <td>0.030567</td>\n      <td>2.703620e-02</td>\n      <td>5.610774e-02</td>\n      <td>2.959755e-02</td>\n      <td>0.028619</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>375</td>\n      <td>0.045452</td>\n      <td>0.020881</td>\n      <td>0.013222</td>\n      <td>0.063410</td>\n      <td>0.054559</td>\n      <td>0.020181</td>\n      <td>0.059622</td>\n      <td>0.051217</td>\n      <td>0.016368</td>\n      <td>0.035642</td>\n      <td>0.024271</td>\n      <td>0.005288</td>\n      <td>0.034158</td>\n      <td>0.025277</td>\n      <td>0.002064</td>\n      <td>3.371769e-02</td>\n      <td>0.022140</td>\n      <td>5.191548e-03</td>\n      <td>3.490645e-02</td>\n      <td>2.254672e-02</td>\n      <td>-0.003187</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>400</td>\n      <td>0.041836</td>\n      <td>0.023006</td>\n      <td>0.023431</td>\n      <td>0.041640</td>\n      <td>0.041122</td>\n      <td>0.028316</td>\n      <td>0.033655</td>\n      <td>0.031300</td>\n      <td>0.027806</td>\n      <td>0.029551</td>\n      <td>0.025455</td>\n      <td>0.034004</td>\n      <td>0.025434</td>\n      <td>0.024934</td>\n      <td>0.036392</td>\n      <td>2.552223e-02</td>\n      <td>0.021759</td>\n      <td>3.121626e-02</td>\n      <td>3.332331e-02</td>\n      <td>2.432268e-02</td>\n      <td>0.030985</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>425</td>\n      <td>0.043315</td>\n      <td>0.023428</td>\n      <td>0.021350</td>\n      <td>0.042503</td>\n      <td>0.047076</td>\n      <td>0.017911</td>\n      <td>0.034322</td>\n      <td>0.037631</td>\n      <td>0.016773</td>\n      <td>0.031853</td>\n      <td>0.030089</td>\n      <td>0.025648</td>\n      <td>0.029592</td>\n      <td>0.030158</td>\n      <td>0.021560</td>\n      <td>3.133441e-02</td>\n      <td>0.028021</td>\n      <td>2.200129e-02</td>\n      <td>2.661165e-02</td>\n      <td>3.152557e-02</td>\n      <td>0.012151</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>450</td>\n      <td>0.044173</td>\n      <td>0.004679</td>\n      <td>0.008707</td>\n      <td>0.057491</td>\n      <td>0.063234</td>\n      <td>0.021913</td>\n      <td>0.031195</td>\n      <td>0.029232</td>\n      <td>0.012268</td>\n      <td>0.034023</td>\n      <td>0.031850</td>\n      <td>0.015509</td>\n      <td>0.035800</td>\n      <td>0.034246</td>\n      <td>0.007337</td>\n      <td>-1.435706e-07</td>\n      <td>-0.000024</td>\n      <td>-1.837915e-05</td>\n      <td>-1.779765e-08</td>\n      <td>-2.865202e-05</td>\n      <td>-0.000022</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>475</td>\n      <td>0.042778</td>\n      <td>0.002243</td>\n      <td>0.002169</td>\n      <td>0.038150</td>\n      <td>0.026061</td>\n      <td>0.009934</td>\n      <td>0.022383</td>\n      <td>0.010773</td>\n      <td>0.004530</td>\n      <td>0.026574</td>\n      <td>0.011256</td>\n      <td>0.011812</td>\n      <td>0.033069</td>\n      <td>0.012727</td>\n      <td>-0.002586</td>\n      <td>-3.848525e-07</td>\n      <td>-0.000045</td>\n      <td>-3.566047e-05</td>\n      <td>-3.725332e-07</td>\n      <td>-4.539190e-05</td>\n      <td>-0.000036</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>500</td>\n      <td>0.042942</td>\n      <td>0.039946</td>\n      <td>0.045640</td>\n      <td>0.092373</td>\n      <td>0.106835</td>\n      <td>0.065450</td>\n      <td>0.056338</td>\n      <td>0.078596</td>\n      <td>0.058214</td>\n      <td>0.033054</td>\n      <td>0.044643</td>\n      <td>0.043081</td>\n      <td>0.035159</td>\n      <td>0.043644</td>\n      <td>0.044818</td>\n      <td>2.918796e-02</td>\n      <td>0.039344</td>\n      <td>4.251399e-02</td>\n      <td>5.253141e-02</td>\n      <td>5.219970e-02</td>\n      <td>0.037235</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>525</td>\n      <td>0.055984</td>\n      <td>0.008138</td>\n      <td>0.037555</td>\n      <td>0.068378</td>\n      <td>0.034044</td>\n      <td>0.045484</td>\n      <td>0.049435</td>\n      <td>0.033478</td>\n      <td>0.042042</td>\n      <td>0.038853</td>\n      <td>0.012892</td>\n      <td>0.031937</td>\n      <td>0.042841</td>\n      <td>0.010648</td>\n      <td>0.029456</td>\n      <td>3.686924e-02</td>\n      <td>0.013139</td>\n      <td>3.232509e-02</td>\n      <td>-1.155932e-06</td>\n      <td>-8.184362e-05</td>\n      <td>-0.000014</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>550</td>\n      <td>0.079068</td>\n      <td>-0.007977</td>\n      <td>-0.002293</td>\n      <td>0.036324</td>\n      <td>0.007323</td>\n      <td>0.014142</td>\n      <td>0.059759</td>\n      <td>0.017997</td>\n      <td>0.009193</td>\n      <td>0.065788</td>\n      <td>0.005629</td>\n      <td>0.016282</td>\n      <td>0.063776</td>\n      <td>0.008980</td>\n      <td>0.021159</td>\n      <td>-9.343118e-07</td>\n      <td>-0.000007</td>\n      <td>-1.136215e-05</td>\n      <td>-8.872024e-08</td>\n      <td>-9.620223e-06</td>\n      <td>-0.000015</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>575</td>\n      <td>0.061489</td>\n      <td>0.046520</td>\n      <td>0.011930</td>\n      <td>0.084962</td>\n      <td>0.089735</td>\n      <td>0.022645</td>\n      <td>0.049855</td>\n      <td>0.059204</td>\n      <td>0.020475</td>\n      <td>0.049639</td>\n      <td>0.055342</td>\n      <td>0.008340</td>\n      <td>0.045969</td>\n      <td>0.055721</td>\n      <td>0.019505</td>\n      <td>3.947695e-02</td>\n      <td>0.050070</td>\n      <td>1.513632e-02</td>\n      <td>5.294881e-02</td>\n      <td>5.275299e-02</td>\n      <td>0.013744</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>600</td>\n      <td>0.053373</td>\n      <td>-0.018035</td>\n      <td>-0.004766</td>\n      <td>0.014890</td>\n      <td>-0.000217</td>\n      <td>0.005112</td>\n      <td>0.036381</td>\n      <td>-0.004054</td>\n      <td>0.002889</td>\n      <td>0.011194</td>\n      <td>-0.003839</td>\n      <td>0.004517</td>\n      <td>0.031915</td>\n      <td>-0.025365</td>\n      <td>-0.014393</td>\n      <td>-1.116158e-07</td>\n      <td>-0.000010</td>\n      <td>-2.508825e-05</td>\n      <td>-3.862022e-12</td>\n      <td>-1.229114e-05</td>\n      <td>-0.000029</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>625</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Fit the model(s) for one window and explore results","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_cols = ['PERMNO', 'year', 'prd']\ndf.reset_index(inplace=True, drop=True)\nX = df.copy()\ny = X.pop('RET')\n\ntrain_indx = X.prd<(min_prd+windows_width-1)\nval_indx = X['prd'].isin(range(min_prd+windows_width-1, min_prd+windows_width+2))\nval_indx_extra = X['prd'].isin(range(min_prd+windows_width+5, min_prd+windows_width+8))\ntest_indx = X['prd'].isin(range(min_prd+windows_width+2, min_prd+windows_width+5))\n\nX_train = X[train_indx]\nX_val = X[val_indx]\nX_val_extra = X[val_indx_extra]\nX_test = X[test_indx]\ny_train = y[train_indx]\ny_val = y[val_indx]\ny_val_extra = y[val_indx_extra]\ny_test = y[test_indx]\n\n#display(X_train.head(3), X_train.tail(3), y_train.head(3), y_train.tail(3))\n#display(X_train.prd.describe(), X_val.prd.describe(), X_test.prd.describe())\ndisplay(X_train.shape, X_val.shape, X_test.shape)\n\nX_train.drop(columns=temp_cols, inplace=True)\nX_val.drop(columns=temp_cols, inplace=True)\nX_val_extra.drop(columns=temp_cols, inplace=True)\nX_test.drop(columns=temp_cols, inplace=True)\n\n#display(X_train.tail())\ncol_cat = ['ind']\ncol_num = [x for x in X_train.columns if x not in col_cat]\nfor col in col_num:\n    X_train[col] = X_train[col].fillna(X_train[col].median())\n    X_val[col] = X_val[col].fillna(X_train[col].median())\n    X_val_extra[col] = X_val_extra[col].fillna(X_train[col].median())\n    X_test[col] = X_test[col].fillna(X_train[col].median())\nfor col in col_cat:\n    X_train[col] = X_train[col].fillna(value=-1000)\n    X_val[col] = X_val[col].fillna(value=-1000)\n    X_val_extra[col] = X_val_extra[col].fillna(value=-1000)\n    X_test[col] = X_test[col].fillna(value=-1000)\n\n#display(X_train.tail())\nfeature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                        (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                        remainder=\"passthrough\")\n\nprint('Number of features before transformation: ', X_train.shape)\ntrain_index, val_index, val_index_extra, test_index = X_train.index, X_val.index, X_val_extra.index, X_test.index\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_val = pd.DataFrame(feature_transformer.transform(X_val), columns=feature_transformer.get_feature_names_out())\nX_val_extra = pd.DataFrame(feature_transformer.transform(X_val_extra), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ')\nprint('Number of features after transformation: ', X_train.shape, X_val.shape, X_val_extra.shape, X_test.shape)\nX_train.index = train_index\nX_val.index = val_index\nX_val_extra.index = val_index_extra\nX_test.index = test_index\n#display(X_train.tail())\n\nX = pd.concat([X_train, X_val])\ny = pd.concat([y_train, y_val])\n#display(X,y)\n\nX_ = pd.concat([X_train, X_val, X_val_extra])\ny_ = pd.concat([y_train, y_val, y_val_extra])\n#display(X,y, X_,y_)\n\n\nresults = pd.DataFrame(columns = ['min_prd', 'xgbf_train', 'xgbf_val', 'xgbf_test', \n                                  'xgbgs_train', 'xgbgs_val', 'xgbgs_test', \n                                  'xgbo_train', 'xgbo_val', 'xgbo_test',\n                                  'nn4_train', 'nn4_val', 'nn4_test',\n                                 'nn6_train', 'nn6_val', 'nn6_test',\n                                 'nn4opt_train', 'nn4opt_val', 'nn4opt_test',\n                                 'nn6opt_train', 'nn6opt_val', 'nn6opt_test'])\n\nresults['min_prd'] = [min_prd]\n\n\n### Modeling part ###\n\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=400, max_depth=4, eta=0.02, colsample_bytree=0.4, subsample=0.6)\nxgb1.fit(X_train, y_train)\nprint('fixed XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\nprint('XGB val:', mean_absolute_error(y_val, xgb1.predict(X_val)), r2_score(y_val, xgb1.predict(X_val)))\nprint('XGB val extra:', mean_absolute_error(y_val_extra, xgb1.predict(X_val_extra)), r2_score(y_val_extra, xgb1.predict(X_val_extra)))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbf_train':'xgbf_test'] = \\\n[r2_score(y_train, xgb1.predict(X_train)), \nr2_score(y_val, xgb1.predict(X_val)),\nr2_score(y_test, xgb1.predict(X_test))]\n\ntime1 = time.time()\n\n# Create a list where train data indices are -1 and validation data indices are 0\nsplit_index = [-1 if x in X_train.index else 0 for x in X.index]\npds = PredefinedSplit(test_fold = split_index)\n\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[400, 600, 800], 'max_depth':[2,3,4,5], 'eta':[0.006, 0.012, 0.02], \n              'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbgs = GridSearchCV(estimator = xgb, cv=pds, param_grid=param_grid)\n\n# Fit with all data\nxgbgs.fit(X_, y_)\n\nprint('gs XGB', xgbgs.best_params_, xgbgs.best_score_, time.time()-time1)\nprint('XGB train:', mean_absolute_error(y_train, xgbgs.predict(X_train)), r2_score(y_train, xgbgs.predict(X_train)))\nprint('XGB validation:', mean_absolute_error(y_val, xgbgs.predict(X_val)), r2_score(y_val, xgbgs.predict(X_val)))\nprint('XGB validation extra:', mean_absolute_error(y_val_extra, xgbgs.predict(X_val_extra)), r2_score(y_val_extra, xgbgs.predict(X_val_extra)))\nprint('XGB test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_score(y_test, xgbgs.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbgs_train':'xgbgs_test'] = \\\n[r2_score(y_train, xgbgs.predict(X_train)), \nr2_score(y_val, xgbgs.predict(X_val)),\nr2_score(y_test, xgbgs.predict(X_test))]\n\ntime1 = time.time()\ndef objective_xgb(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    params = {\n    \"tree_method\": 'gpu_hist',\n    \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n    \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 1500),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n    \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.0005, 0.03),\n    \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.05, 0.95),\n    \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 0.95),\n    \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 50.0),\n    \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 500.0),\n    \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 100.0),\n    \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n    model = XGBRegressor(**params, njobs=-1)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose = False)\n\n    score_train = r2_score(y_train, model.predict(X_train))\n    score_val = r2_score(y_val, model.predict(X_val))\n    score_val_extra = r2_score(y_val_extra, model.predict(X_val_extra)) \n    score_val = (score_val+score_val_extra)/2\n    overfit = np.abs(score_train-score_val)\n\n    return score_val-cv_xgb_regularizer*overfit\n\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective_xgb, n_trials=optuna_xgb_trials)\nprint('Total time for hypermarameter optimization, XGB: ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X, y)\nprint('Optuna XGB train: \\n', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), '\\nvalidation \\n',\n      mean_absolute_error(y_val, optuna_xgb.predict(X_val)), r2_score(y_val, optuna_xgb.predict(X_val)),\n      mean_absolute_error(y_val_extra, optuna_xgb.predict(X_val_extra)), r2_score(y_val_extra, optuna_xgb.predict(X_val_extra)), '\\ntest \\n',\n      mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbo_train':'xgbo_test'] = \\\n[r2_score(y_train, optuna_xgb.predict(X_train)), \nr2_score(y_val, optuna_xgb.predict(X_val)),\nr2_score(y_test, optuna_xgb.predict(X_test))]\n\n###########\n### NNs ###\n###########\n\nneurons_base = 8\nl2_reg_rate = 0.5\n\nmodel_snn6 = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn6.count_params())\n\nearly_stopping50 = EarlyStopping(patience=50, restore_best_weights=True)\noptimizer_adam = tf.keras.optimizers.Adam()\n\nmodel_snn6.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n\ntime1 = time.time()\nhistory = model_snn6.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=0, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nresults.loc[results.min_prd==min_prd,'nn6_train':'nn6_test'] = \\\n[r2_score(y_train, model_snn6.predict(X_train)), \nr2_score(y_val, model_snn6.predict(X_val)),\nr2_score(y_test, model_snn6.predict(X_test))]\n\n\n\nneurons_base = 8\nl2_reg_rate = 0.3\n\nmodel_snn4 = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn4.count_params())\n\ntime1 = time.time()\nmodel_snn4.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_snn4.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=0, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nresults.loc[results.min_prd==min_prd,'nn4_train':'nn4_test'] = \\\n[r2_score(y_train, model_snn4.predict(X_train)), \nr2_score(y_val, model_snn4.predict(X_val)),\nr2_score(y_test, model_snn4.predict(X_test))]\n\n\n\n# try optuna, using this kaggle notebook: https://www.kaggle.com/code/mistag/keras-model-tuning-with-optuna\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"minimize\", \n                            sampler=optuna.samplers.TPESampler(), \n                            pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective_nn4, n_trials=optuna_nn_trials)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\ntemp = study.best_params\ndisplay('Optuna NN4', study.best_params, time.time()-time1)\n\noptimal_hyperpars = list(temp.values())\ndisplay(optimal_hyperpars)\nprint('Time for hyperparameter optimization: ',time.time()-time1, optimal_hyperpars)\n\noptuna_nn = create_snnn4_model_hyperpars(neurons_base=optimal_hyperpars[0], \n                                         l2_reg_rate=optimal_hyperpars[1],\n                                        l1_reg_rate=optimal_hyperpars[2])\nhistory = optuna_nn.fit(X_train, y_train, \n                        validation_data=(X_val, y_val),\n                        batch_size=2048, \n                        epochs=1000,\n                        verbose=0, \n                        callbacks=[early_stopping50])\n\nresults.loc[results.min_prd==min_prd,'nn4opt_train':'nn4opt_test'] = \\\n[r2_score(y_train, optuna_nn.predict(X_train)), \nr2_score(y_val, optuna_nn.predict(X_val)),\nr2_score(y_test, optuna_nn.predict(X_test))]\n\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"minimize\", \n                            sampler=optuna.samplers.TPESampler(), \n                            pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective_nn6, n_trials=optuna_nn_trials)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\ntemp = study.best_params\ndisplay('Optuna NN6', study.best_params, time.time()-time1)\n\noptimal_hyperpars = list(temp.values())\ndisplay(optimal_hyperpars)\nprint('Time for hyperparameter optimization: ', time.time()-time1, optimal_hyperpars)\n\noptuna_nn = create_snnn6_model_hyperpars(neurons_base=optimal_hyperpars[0], \n                                         l2_reg_rate=optimal_hyperpars[1],\n                                        l1_reg_rate=optimal_hyperpars[2])\nhistory = optuna_nn.fit(X_train, y_train, \n                        validation_data=(X_val, y_val),\n                        batch_size=2048, \n                        epochs=1000,\n                        verbose=0, \n                        callbacks=[early_stopping50])\n\nresults.loc[results.min_prd==min_prd,'nn6opt_train':'nn6opt_test'] = \\\n[r2_score(y_train, optuna_nn.predict(X_train)), \nr2_score(y_val, optuna_nn.predict(X_val)),\nr2_score(y_test, optuna_nn.predict(X_test))]\n\n\nprint('total time for the script: ', time.time()-time0)\ndisplay(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:01:21.711132Z","iopub.execute_input":"2022-09-27T14:01:21.711513Z","iopub.status.idle":"2022-09-27T14:50:09.333255Z","shell.execute_reply.started":"2022-09-27T14:01:21.711477Z","shell.execute_reply":"2022-09-27T14:50:09.332333Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"(140202, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6690, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6591, 47)"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (140202, 44)\ntime to do feature proprocessing: \nNumber of features after transformation:  (140202, 92) (6690, 92) (6578, 92) (6591, 92)\nmae of a constant model 11.353653074247589\nR2 of a constant model 0.0\nfixed XGB train: 11.441539050192986 0.042941848174661734\nXGB val: 8.950909435524022 0.03994560353123788\nXGB val extra: 9.183649080170062 0.01697029425043617\nXGB test: 9.082367943805426 0.04563991575175996\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 5, 'n_estimators': 800, 'subsample': 0.6} 0.04624691315299856 82.26884770393372\nXGB train: 11.179420490631308 0.09196886457440279\nXGB validation: 8.615551994699656 0.10818405087001537\nXGB validation extra: 8.7943048588114 0.09521941067138817\nXGB test: 8.931430949262754 0.06743689427662403\nTotal time for hypermarameter optimization, XGB:  442.7427830696106\n        n_estimators : 955\n           max_depth : 4\n       learning_rate : 0.028345218279641926\n    colsample_bytree : 0.5731081850280325\n           subsample : 0.8875386930701449\n               alpha : 0.3656540246478755\n              lambda : 83.81577041806717\n               gamma : 0.045107114992369225\n    min_child_weight : 0.5620178559900786\nbest objective value : 0.0317020749930671\nOptuna XGB train: \n 11.29119329697234 0.06693500027554411 \nvalidation \n 8.689759441613488 0.08815389732891565 9.064471419718902 0.033703210092623115 \ntest \n 8.975077907434716 0.059120951467871885\n","output_type":"stream"},{"name":"stderr","text":"2022-09-27 14:10:30.349585: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 14:10:30.350797: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 14:10:30.351567: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 14:10:30.352444: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-09-27 14:10:30.352755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 14:10:30.353459: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 14:10:30.354094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 14:10:36.777383: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 14:10:36.778298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 14:10:36.779034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 14:10:36.779669: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14605 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"4025\n","output_type":"stream"},{"name":"stderr","text":"2022-09-27 14:10:37.508893: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Minimum Validation Loss: 157.0379\n3681\nMinimum Validation Loss: 156.6260\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'Optuna NN4'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"{'neurons_base': 10,\n 'l2_regularizer': 0.2349964482470355,\n 'l1_regularizer': 0.031745992453638794}"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1168.40722990036"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"[10, 0.2349964482470355, 0.031745992453638794]"},"metadata":{}},{"name":"stdout","text":"Time for hyperparameter optimization:  1168.4146835803986 [10, 0.2349964482470355, 0.031745992453638794]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'Optuna NN6'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"{'neurons_base': 4,\n 'l2_regularizer': 0.47516552721399063,\n 'l1_regularizer': 0.038579222282080855}"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"931.1994078159332"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"[4, 0.47516552721399063, 0.038579222282080855]"},"metadata":{}},{"name":"stdout","text":"Time for hyperparameter optimization:  931.2087378501892 [4, 0.47516552721399063, 0.038579222282080855]\ntotal time for the script:  2933.761435031891\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Empty DataFrame\nColumns: [min_prd, xgbf_train, xgbf_val, xgbf_test, xgbgs_train, xgbgs_val, xgbgs_test, xgbo_train, xgbo_val, xgbo_test, nn4_train, nn4_val, nn4_test, nn6_train, nn6_val, nn6_test, nn4opt_train, nn4opt_val, nn4opt_test, nn6opt_train, nn6opt_val, nn6opt_test]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n      <th>nn4_train</th>\n      <th>nn4_val</th>\n      <th>nn4_test</th>\n      <th>nn6_train</th>\n      <th>nn6_val</th>\n      <th>nn6_test</th>\n      <th>nn4opt_train</th>\n      <th>nn4opt_val</th>\n      <th>nn4opt_test</th>\n      <th>nn6opt_train</th>\n      <th>nn6opt_val</th>\n      <th>nn6opt_test</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"### Feature Importance","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:50:09.334790Z","iopub.execute_input":"2022-09-27T14:50:09.335077Z","iopub.status.idle":"2022-09-27T14:50:09.339374Z","shell.execute_reply.started":"2022-09-27T14:50:09.335050Z","shell.execute_reply":"2022-09-27T14:50:09.338355Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"### Error Analysis","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:50:09.341035Z","iopub.execute_input":"2022-09-27T14:50:09.341360Z","iopub.status.idle":"2022-09-27T14:50:09.348501Z","shell.execute_reply.started":"2022-09-27T14:50:09.341327Z","shell.execute_reply":"2022-09-27T14:50:09.347299Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.min_prd = min_prd","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:53:10.696059Z","iopub.execute_input":"2022-09-27T14:53:10.696444Z","iopub.status.idle":"2022-09-27T14:53:10.701705Z","shell.execute_reply.started":"2022-09-27T14:53:10.696389Z","shell.execute_reply":"2022-09-27T14:53:10.700489Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"results.min_prd","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:53:31.276587Z","iopub.execute_input":"2022-09-27T14:53:31.276949Z","iopub.status.idle":"2022-09-27T14:53:31.284668Z","shell.execute_reply.started":"2022-09-27T14:53:31.276918Z","shell.execute_reply":"2022-09-27T14:53:31.283630Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"Series([], Name: min_prd, dtype: int64)"},"metadata":{}}]},{"cell_type":"code","source":"results['min_prd'] = [min_prd]\nresults","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:54:47.825680Z","iopub.execute_input":"2022-09-27T14:54:47.826036Z","iopub.status.idle":"2022-09-27T14:54:47.846653Z","shell.execute_reply.started":"2022-09-27T14:54:47.826006Z","shell.execute_reply":"2022-09-27T14:54:47.845544Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"   min_prd xgbf_train xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      500        NaN      NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train xgbo_val xgbo_test nn4_train nn4_val nn4_test nn6_train nn6_val  \\\n0        NaN      NaN       NaN       NaN     NaN      NaN       NaN     NaN   \n\n  nn6_test nn4opt_train nn4opt_val nn4opt_test nn6opt_train nn6opt_val  \\\n0      NaN          NaN        NaN         NaN          NaN        NaN   \n\n  nn6opt_test  \n0         NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n      <th>nn4_train</th>\n      <th>nn4_val</th>\n      <th>nn4_test</th>\n      <th>nn6_train</th>\n      <th>nn6_val</th>\n      <th>nn6_test</th>\n      <th>nn4opt_train</th>\n      <th>nn4opt_val</th>\n      <th>nn4opt_test</th>\n      <th>nn6opt_train</th>\n      <th>nn6opt_val</th>\n      <th>nn6opt_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>500</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general point:\n# compared to NN, xgb is harder to regularize\n# in NN, you can simply shrink coefficient towards constant prediction.\n# in xgb, you can not do that. the only way to regularize is via hyperparameters.\n# in other words, by tweaking hyperpars, in NN you can approach R^2=0.0 prediction from a constant model arbitrarily close\n# in xgb, you can not do that.\n# by setting eta as low as 0.1% you can bring r2 down to 0.1%, but lowering eta further actyally increases abs(r2).\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:50:09.352759Z","iopub.execute_input":"2022-09-27T14:50:09.353140Z","iopub.status.idle":"2022-09-27T14:50:09.357695Z","shell.execute_reply.started":"2022-09-27T14:50:09.353115Z","shell.execute_reply":"2022-09-27T14:50:09.356641Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print('total time for the script: ', time.time()-time0)\ndisplay(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:50:09.359258Z","iopub.execute_input":"2022-09-27T14:50:09.359688Z","iopub.status.idle":"2022-09-27T14:50:09.377046Z","shell.execute_reply.started":"2022-09-27T14:50:09.359654Z","shell.execute_reply":"2022-09-27T14:50:09.376037Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"total time for the script:  2934.0078518390656\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Empty DataFrame\nColumns: [min_prd, xgbf_train, xgbf_val, xgbf_test, xgbgs_train, xgbgs_val, xgbgs_test, xgbo_train, xgbo_val, xgbo_test, nn4_train, nn4_val, nn4_test, nn6_train, nn6_val, nn6_test, nn4opt_train, nn4opt_val, nn4opt_test, nn6opt_train, nn6opt_val, nn6opt_test]\nIndex: []","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n      <th>nn4_train</th>\n      <th>nn4_val</th>\n      <th>nn4_test</th>\n      <th>nn6_train</th>\n      <th>nn6_val</th>\n      <th>nn6_test</th>\n      <th>nn4opt_train</th>\n      <th>nn4opt_val</th>\n      <th>nn4opt_test</th>\n      <th>nn6opt_train</th>\n      <th>nn6opt_val</th>\n      <th>nn6opt_test</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":" \n# def objective_nn(trial):\n    \n#     tf.keras.backend.clear_session()\n    \n#     with strategy.scope():\n#         # Generate our trial model.\n#         model = create_snnn_model(trial)\n\n#         callbacks = [\n#         tf.keras.callbacks.EarlyStopping(patience=40),\n#         TFKerasPruningCallback(trial, \"val_loss\"),\n#     ]\n\n#         # Fit the model on the training data.\n#         # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n#         history = model.fit(X_train, y_train, \n#                                 validation_data=(X_val, y_val),\n#                                 batch_size=2048, \n#                                 epochs=500, \n#                                 verbose=1, \n#                                 callbacks=callbacks)\n\n#         # Evaluate the model accuracy on the validation set.\n#         score = model.evaluate(X_val, y_val, verbose=0)\n#         return score[1]\n\n# trials = 50\n\n# study = optuna.create_study(direction=\"minimize\", \n#                             sampler=optuna.samplers.TPESampler(), \n#                             pruner=optuna.pruners.HyperbandPruner())\n# study.optimize(objective_nn, n_trials=trials)\n# pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n# complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n# temp = study.best_params\n# display(study.best_params, time.time()-time1)\n\n# optimal_hyperpars = list(temp.values())\n# display(optimal_hyperpars)\n# print(time.time()-time1, optimal_hyperpars)\n\n# optuna_nn = create_snnn_model_hyperpars(neurons_base=optimal_hyperpars[0], l2_reg_rate=optimal_hyperpars[1])\n# history = optuna_nn.fit(X_train, y_train, \n#                         validation_data=(X_val, y_val),\n#                         batch_size=2048, \n#                         epochs=1000,\n#                         verbose=1, \n#                         callbacks=[early_stopping50])\n\n# results.loc[results.min_prd==min_prd,'nn4opt_train':'nn4opt_test'] = \\\n# [r2_score(y_train, optuna_nn.predict(X_train)), \n# r2_score(y_val, optuna_nn.predict(X_val)),\n# r2_score(y_test, optuna_nn.predict(X_test))]","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:50:09.378655Z","iopub.execute_input":"2022-09-27T14:50:09.378988Z","iopub.status.idle":"2022-09-27T14:50:09.385009Z","shell.execute_reply.started":"2022-09-27T14:50:09.378955Z","shell.execute_reply":"2022-09-27T14:50:09.383978Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# # try optuna for NN:\n\n# def objective(trial):\n\n#     n_layers = trial.suggest_int('n_layers', 1, 3)\n#     model = tf.keras.Sequential()\n#     for i in range(n_layers):\n#         num_hidden = trial.suggest_int(f'n_units_l{i}', 4, 128, log=True)\n#         model.add(tf.keras.layers.Dense(num_hidden, activation='relu'))\n#     model.add(tf.keras.layers.Dense(1))\n#     display(model.summary())\n#     return accuracy\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=10)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:50:09.386743Z","iopub.execute_input":"2022-09-27T14:50:09.387180Z","iopub.status.idle":"2022-09-27T14:50:09.396293Z","shell.execute_reply.started":"2022-09-27T14:50:09.387147Z","shell.execute_reply":"2022-09-27T14:50:09.395431Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# on nns:\n# - try classic regularizers (l1, l2 etc)\n# - try different architecture (not snnn)\n# classic architecture:\n# He initialization, elu activation, batch norm, l2 reg, adam.\n\n# - try exotic architecture, e.g., wide'n'deep\n# \n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:50:09.397867Z","iopub.execute_input":"2022-09-27T14:50:09.398232Z","iopub.status.idle":"2022-09-27T14:50:09.405234Z","shell.execute_reply.started":"2022-09-27T14:50:09.398200Z","shell.execute_reply":"2022-09-27T14:50:09.404274Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# usually self-norm seems better: it overfits less and runs faster\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:50:09.406681Z","iopub.execute_input":"2022-09-27T14:50:09.407218Z","iopub.status.idle":"2022-09-27T14:50:09.416409Z","shell.execute_reply.started":"2022-09-27T14:50:09.407181Z","shell.execute_reply":"2022-09-27T14:50:09.415444Z"},"trusted":true},"execution_count":15,"outputs":[]}]}
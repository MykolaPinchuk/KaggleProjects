{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Load libraries and data.\n2. pEDA. Look at feature distribution, fix them if they do not look right.\n3. Train-test split. Most likely couple years into test set. 2015-2018?. Impute missing values.\n4. Transform numerical features, add ohe for inds.\n5. Fit classic models: ols as a baseline, then xgb.\n6. Fir DL.\n\n\nNotes:\nideally, I want to use time-based cross-validation.\nsince I have panel data, it is not a trivial task.\nneed to find some solution online.\ne.g., https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8.\n\nfor now, will try to do siple for loop.\n","metadata":{}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\npd.set_option('display.max_columns', 110)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:42:50.384774Z","iopub.execute_input":"2022-08-25T23:42:50.385442Z","iopub.status.idle":"2022-08-25T23:42:50.397290Z","shell.execute_reply.started":"2022-08-25T23:42:50.385406Z","shell.execute_reply":"2022-08-25T23:42:50.396300Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:42:50.431882Z","iopub.execute_input":"2022-08-25T23:42:50.432161Z","iopub.status.idle":"2022-08-25T23:42:50.442808Z","shell.execute_reply.started":"2022-08-25T23:42:50.432117Z","shell.execute_reply":"2022-08-25T23:42:50.441663Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:42:50.480643Z","iopub.execute_input":"2022-08-25T23:42:50.481407Z","iopub.status.idle":"2022-08-25T23:42:50.488466Z","shell.execute_reply.started":"2022-08-25T23:42:50.481373Z","shell.execute_reply":"2022-08-25T23:42:50.487178Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"# for loop to see appx performance over the whole sample with some rolling window\n\ntime0 = time.time()\n\n#min_prd_list = range(100, 676, 25)\nmin_prd_list = [125]\nwindows_width = 3*12\ncv_regularizer=0.05\noptuna_trials = 40\n\nresults = pd.DataFrame(columns = ['min_prd', 'xgbf', 'xgbgs', 'xgbo'])\nresults.min_prd = min_prd_list\n\nfor min_prd in min_prd_list:\n    \n    \n    with open('../input/kaggle-46pkl/IMLEAP_v4.pkl', 'rb') as pickled_one:\n        df = pickle.load(pickled_one)\n    df = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+2))]\n    df_cnt = df.count()\n    empty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\n    df.drop(columns=empty_cols, inplace=True)\n    display(df.shape, df.head(), df.year.describe(), df.count())\n    \n    df = df[(df.RET>-50)&(df.RET<75)]\n    meanret = df.groupby('prd').RET.mean().to_frame().reset_index().rename(columns={'RET':'mRET'})\n    df = pd.merge(df, meanret, on='prd', how='left')\n    df.RET = df.RET-df.mRET\n    df.drop(columns='mRET', inplace=True)\n\n    features_miss_dummies = ['amhd', 'BAspr']\n    for col in features_miss_dummies:\n        if col in df.columns:\n            df[col+'_miss'] = df[col].isnull().astype(int)\n\n    df.reset_index(inplace=True, drop=True)\n    temp_cols = ['PERMNO', 'year']\n    train = df[df.prd<(min_prd+windows_width)]\n    test = df[df.prd==(min_prd+windows_width)]\n    train.drop(columns=temp_cols, inplace=True)\n    test.drop(columns=temp_cols, inplace=True)\n\n    col_ignore = ['RET', 'prd']\n    col_cat = ['ind']\n    col_num = [x for x in train.columns if x not in col_ignore+col_cat]\n    for col in col_num:\n        train[col] = train[col].fillna(train[col].median())\n        test[col] = test[col].fillna(train[col].median())\n    for col in col_cat:\n        train[col] = train[col].fillna(value=-1000)\n        test[col] = test[col].fillna(value=-1000)\n\n    X_train = train.copy()\n    y_train = X_train.pop('RET')\n    X_test = test.copy()\n    y_test = X_test.pop('RET')\n    y_train.reset_index(inplace=True, drop=True)\n\n    feature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                            (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                            remainder=\"passthrough\")\n\n    print('Number of features before transformation: ', X_train.shape)\n    X_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\n    X_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\n    print('time to do feature proprocessing: ')\n    print('Number of features after transformation: ', X_train.shape)\n    \n    X_train0 = X_train.copy()\n    y_train0 = y_train.copy()\n    \n    X_train.drop(columns=['remainder__prd'], inplace=True)\n    X_test.drop(columns=['remainder__prd'], inplace=True)\n\n    print('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n    print('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\n    xgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=300, max_depth=5, eta=0.03, colsample_bytree=0.6)\n    xgb1.fit(X_train, y_train)\n    print('XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\n\n    time1 = time.time()\n    xgb = XGBRegressor(tree_method = 'gpu_hist')\n    param_grid = {'n_estimators':[400, 700], 'max_depth':[2,3,4], 'eta':[0.006, 0.012, 0.02], 'subsample':[0.6], 'colsample_bytree':[0.6]}\n    xgbm = GridSearchCV(xgb, param_grid, cv=2, verbose=2, scoring='r2')\n    xgbm.fit(X_train, y_train)\n    print('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)\n    print('XGB train:', mean_absolute_error(y_train, xgbm.predict(X_train)), r2_score(y_train, xgbm.predict(X_train)), time.time()-time1)\n\n    time1 = time.time()\n    def objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n        params = {\n        \"tree_method\": 'gpu_hist',\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1000),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.001, 0.05),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 0.95),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 0.95),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 30.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 200.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 10.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n        temp_out = []\n\n        for i in range(cv_runs):\n\n            X = X_train\n            y = y_train\n            model = XGBRegressor(**params, njobs=-1)\n            rkf = KFold(n_splits=n_splits, shuffle=True)\n            X_values = X.values\n            y_values = y.values\n            y_pred = np.zeros_like(y_values)\n            y_pred_train = np.zeros_like(y_values)\n            for train_index, test_index in rkf.split(X_values):\n                X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n                y_A, y_B = y_values[train_index], y_values[test_index]\n                model.fit(X_A, y_A, eval_set=[(X_B, y_B)], verbose = False)\n                y_pred[test_index] = model.predict(X_B)\n                y_pred_train[train_index] = model.predict(X_A)\n\n            score_train = r2_score(y_train, y_pred_train)\n            score_test = r2_score(y_train, y_pred) \n            overfit = (score_train-score_test)\n            temp_out.append(score_test-cv_regularizer*overfit)\n\n        return (np.mean(temp_out))\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=optuna_trials)\n    print('Total time for hypermarameter optimization ', time.time()-time1)\n    hp = study.best_params\n    for key, value in hp.items():\n        print(f\"{key:>20s} : {value}\")\n    print(f\"{'best objective value':>20s} : {study.best_value}\")\n    optuna_hyperpars = study.best_params\n    optuna_hyperpars['tree_method']='gpu_hist'\n    optuna_xgb = XGBRegressor(**optuna_hyperpars)\n    optuna_xgb.fit(X_train, y_train)\n    print('Optuna XGB train:', \n          mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), time.time()-time1)\n\n    # Evaluate performance of XGB models:\n    r2_xgb1 = r2_score(y_test, xgb1.predict(X_test))\n    r2_xgbgs = r2_score(y_test, xgbm.predict(X_test))\n    r2_xgbo = r2_score(y_test, optuna_xgb.predict(X_test))\n\n    print('Min_prd: ', min_prd)\n    print('Constant guess: ', mean_absolute_error(y_test, np.ones(len(y_test))*y_test.mean()), \n          r2_score(y_test, np.ones(len(y_test))*y_test.mean()))\n    print('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_xgb1)\n    print('XGB GS test:', mean_absolute_error(y_test, xgbm.predict(X_test)), r2_xgbgs)\n    print('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_xgbo)\n\n    results.loc[results.min_prd==min_prd,'xgbf':'xgbo'] = r2_xgb1, r2_xgbgs, r2_xgbo\n    \nprint(time.time()-time0, results)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:42:50.528254Z","iopub.execute_input":"2022-08-25T23:42:50.530111Z","iopub.status.idle":"2022-08-25T23:46:18.131791Z","shell.execute_reply.started":"2022-08-25T23:42:50.530082Z","shell.execute_reply":"2022-08-25T23:46:18.130900Z"},"trusted":true},"execution_count":85,"outputs":[{"output_type":"display_data","data":{"text/plain":"(40682, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    PERMNO  prd     mom482     mom242  year      RET   ind       bm        op  \\\n49   10006  124  57.022356  27.230111  1968 -10.5198  25.0 -0.22327  0.183384   \n50   10006  125  45.576895  32.149862  1968  -6.2596  25.0 -0.22327  0.183384   \n51   10006  126  16.380849  21.566933  1968   4.3219  25.0 -0.22327  0.183384   \n52   10006  127  32.954723  53.105785  1968   9.7618  25.0 -0.22327  0.183384   \n53   10006  128  34.099293  39.415200  1968   3.9450  25.0 -0.22327  0.183384   \n\n          gp       inv    mom11     mom122      amhd  ivol_capm  ivol_ff5  \\\n49  0.269118  0.100395   3.4619  12.086336  1.536049   1.377650  1.078594   \n50  0.269118  0.100395 -10.5198  24.581595  1.493226   2.217285  1.948001   \n51  0.269118  0.100395  -6.2596  13.253947  1.469247   4.800074  4.663338   \n52  0.269118  0.100395   4.3219  11.098127  1.375395   1.477982  1.203894   \n53  0.269118  0.100395   9.7618  21.666549  1.246353   1.734429  1.621548   \n\n     beta_bw     MAX     vol1m     vol6m    vol12m      size       lbm  \\\n49  0.854703  4.9820  1.945352  2.653852  2.174071  5.867580 -0.149515   \n50  0.794842  2.7293  2.301335  2.797800  2.238881  5.751293 -0.149515   \n51  0.777908  7.2477  4.297812  3.251701  2.512787  5.691229 -0.149515   \n52  0.787439  4.4095  1.764988  3.249921  2.531631  5.737749 -0.149515   \n53  0.750551  5.4695  1.618184  3.253052  2.540064  5.824760 -0.149515   \n\n         lop       lgp      linv      llme    l1amhd      l1MAX    l3amhd  \\\n49  0.173745  0.242714  0.119169  5.740148  1.573985  21.135115  1.636271   \n50  0.173745  0.242714  0.119169  5.660875  1.536049   4.982000  1.641999   \n51  0.173745  0.242714  0.119169  5.648296  1.493226   2.729300  1.573985   \n52  0.173745  0.242714  0.119169  5.606947  1.469247   7.247700  1.536049   \n53  0.173745  0.242714  0.119169  5.549943  1.375395   4.409500  1.493226   \n\n        l3MAX    l6amhd   l6MAX   l12amhd     l12MAX  l12mom122  l12ivol_capm  \\\n49   4.208600  1.653423  2.5316  1.833293  21.135115   6.066114      1.906794   \n50   2.202200  1.585251  1.7344  1.792446   4.982000  27.909142      1.503199   \n51  21.135115  1.624355  2.1833  1.744677   2.729300  25.117139      1.051638   \n52   4.982000  1.636271  4.2086  1.727351   7.247700  26.865911      1.174851   \n53   2.729300  1.641999  2.2022  1.738743   4.409500  20.035787      1.903329   \n\n    l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  \n49     1.545510    1.069805  1.668624   1.814489  \n50     1.346875    1.007441  1.599164   1.760787  \n51     0.849621    0.989661  1.421761   1.677152  \n52     1.018713    1.014681  1.442506   1.555119  \n53     1.637691    0.996995  1.592287   1.605068  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>49</th>\n      <td>10006</td>\n      <td>124</td>\n      <td>57.022356</td>\n      <td>27.230111</td>\n      <td>1968</td>\n      <td>-10.5198</td>\n      <td>25.0</td>\n      <td>-0.22327</td>\n      <td>0.183384</td>\n      <td>0.269118</td>\n      <td>0.100395</td>\n      <td>3.4619</td>\n      <td>12.086336</td>\n      <td>1.536049</td>\n      <td>1.377650</td>\n      <td>1.078594</td>\n      <td>0.854703</td>\n      <td>4.9820</td>\n      <td>1.945352</td>\n      <td>2.653852</td>\n      <td>2.174071</td>\n      <td>5.867580</td>\n      <td>-0.149515</td>\n      <td>0.173745</td>\n      <td>0.242714</td>\n      <td>0.119169</td>\n      <td>5.740148</td>\n      <td>1.573985</td>\n      <td>21.135115</td>\n      <td>1.636271</td>\n      <td>4.208600</td>\n      <td>1.653423</td>\n      <td>2.5316</td>\n      <td>1.833293</td>\n      <td>21.135115</td>\n      <td>6.066114</td>\n      <td>1.906794</td>\n      <td>1.545510</td>\n      <td>1.069805</td>\n      <td>1.668624</td>\n      <td>1.814489</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>10006</td>\n      <td>125</td>\n      <td>45.576895</td>\n      <td>32.149862</td>\n      <td>1968</td>\n      <td>-6.2596</td>\n      <td>25.0</td>\n      <td>-0.22327</td>\n      <td>0.183384</td>\n      <td>0.269118</td>\n      <td>0.100395</td>\n      <td>-10.5198</td>\n      <td>24.581595</td>\n      <td>1.493226</td>\n      <td>2.217285</td>\n      <td>1.948001</td>\n      <td>0.794842</td>\n      <td>2.7293</td>\n      <td>2.301335</td>\n      <td>2.797800</td>\n      <td>2.238881</td>\n      <td>5.751293</td>\n      <td>-0.149515</td>\n      <td>0.173745</td>\n      <td>0.242714</td>\n      <td>0.119169</td>\n      <td>5.660875</td>\n      <td>1.536049</td>\n      <td>4.982000</td>\n      <td>1.641999</td>\n      <td>2.202200</td>\n      <td>1.585251</td>\n      <td>1.7344</td>\n      <td>1.792446</td>\n      <td>4.982000</td>\n      <td>27.909142</td>\n      <td>1.503199</td>\n      <td>1.346875</td>\n      <td>1.007441</td>\n      <td>1.599164</td>\n      <td>1.760787</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>10006</td>\n      <td>126</td>\n      <td>16.380849</td>\n      <td>21.566933</td>\n      <td>1968</td>\n      <td>4.3219</td>\n      <td>25.0</td>\n      <td>-0.22327</td>\n      <td>0.183384</td>\n      <td>0.269118</td>\n      <td>0.100395</td>\n      <td>-6.2596</td>\n      <td>13.253947</td>\n      <td>1.469247</td>\n      <td>4.800074</td>\n      <td>4.663338</td>\n      <td>0.777908</td>\n      <td>7.2477</td>\n      <td>4.297812</td>\n      <td>3.251701</td>\n      <td>2.512787</td>\n      <td>5.691229</td>\n      <td>-0.149515</td>\n      <td>0.173745</td>\n      <td>0.242714</td>\n      <td>0.119169</td>\n      <td>5.648296</td>\n      <td>1.493226</td>\n      <td>2.729300</td>\n      <td>1.573985</td>\n      <td>21.135115</td>\n      <td>1.624355</td>\n      <td>2.1833</td>\n      <td>1.744677</td>\n      <td>2.729300</td>\n      <td>25.117139</td>\n      <td>1.051638</td>\n      <td>0.849621</td>\n      <td>0.989661</td>\n      <td>1.421761</td>\n      <td>1.677152</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>10006</td>\n      <td>127</td>\n      <td>32.954723</td>\n      <td>53.105785</td>\n      <td>1968</td>\n      <td>9.7618</td>\n      <td>25.0</td>\n      <td>-0.22327</td>\n      <td>0.183384</td>\n      <td>0.269118</td>\n      <td>0.100395</td>\n      <td>4.3219</td>\n      <td>11.098127</td>\n      <td>1.375395</td>\n      <td>1.477982</td>\n      <td>1.203894</td>\n      <td>0.787439</td>\n      <td>4.4095</td>\n      <td>1.764988</td>\n      <td>3.249921</td>\n      <td>2.531631</td>\n      <td>5.737749</td>\n      <td>-0.149515</td>\n      <td>0.173745</td>\n      <td>0.242714</td>\n      <td>0.119169</td>\n      <td>5.606947</td>\n      <td>1.469247</td>\n      <td>7.247700</td>\n      <td>1.536049</td>\n      <td>4.982000</td>\n      <td>1.636271</td>\n      <td>4.2086</td>\n      <td>1.727351</td>\n      <td>7.247700</td>\n      <td>26.865911</td>\n      <td>1.174851</td>\n      <td>1.018713</td>\n      <td>1.014681</td>\n      <td>1.442506</td>\n      <td>1.555119</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>10006</td>\n      <td>128</td>\n      <td>34.099293</td>\n      <td>39.415200</td>\n      <td>1968</td>\n      <td>3.9450</td>\n      <td>25.0</td>\n      <td>-0.22327</td>\n      <td>0.183384</td>\n      <td>0.269118</td>\n      <td>0.100395</td>\n      <td>9.7618</td>\n      <td>21.666549</td>\n      <td>1.246353</td>\n      <td>1.734429</td>\n      <td>1.621548</td>\n      <td>0.750551</td>\n      <td>5.4695</td>\n      <td>1.618184</td>\n      <td>3.253052</td>\n      <td>2.540064</td>\n      <td>5.824760</td>\n      <td>-0.149515</td>\n      <td>0.173745</td>\n      <td>0.242714</td>\n      <td>0.119169</td>\n      <td>5.549943</td>\n      <td>1.375395</td>\n      <td>4.409500</td>\n      <td>1.493226</td>\n      <td>2.729300</td>\n      <td>1.641999</td>\n      <td>2.2022</td>\n      <td>1.738743</td>\n      <td>4.409500</td>\n      <td>20.035787</td>\n      <td>1.903329</td>\n      <td>1.637691</td>\n      <td>0.996995</td>\n      <td>1.592287</td>\n      <td>1.605068</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    40682.000000\nmean      1969.798437\nstd          0.978257\nmin       1968.000000\n25%       1969.000000\n50%       1970.000000\n75%       1971.000000\nmax       1971.000000\nName: year, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"PERMNO          40682\nprd             40682\nmom482          35826\nmom242          40286\nyear            40682\nRET             40682\nind             40682\nbm              40682\nop              40682\ngp              40682\ninv             40678\nmom11           40682\nmom122          40682\namhd            37268\nivol_capm       40681\nivol_ff5        40681\nbeta_bw         40682\nMAX             40682\nvol1m           40681\nvol6m           40682\nvol12m          40678\nsize            40682\nlbm             40682\nlop             40682\nlgp             40682\nlinv            40682\nllme            40682\nl1amhd          37313\nl1MAX           40682\nl3amhd          37398\nl3MAX           40682\nl6amhd          37547\nl6MAX           40682\nl12amhd         37988\nl12MAX          40682\nl12mom122       40497\nl12ivol_capm    40673\nl12ivol_ff5     40673\nl12beta_bw      40680\nl12vol6m        40637\nl12vol12m       40296\ndtype: int64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (38133, 39)\ntime to do feature proprocessing: \nNumber of features after transformation:  (38133, 83)\nmae of a constant model 7.1541100803200415\nR2 of a constant model 0.0\nXGB train: 6.700516343240393 0.15021333381905422\nFitting 2 folds for each of 18 candidates, totalling 36 fits\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.5s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.5s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=700, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=700, subsample=0.6; total time=   1.1s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.6s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.7s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.2s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.1s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.5s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.5s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=700, subsample=0.6; total time=   0.8s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=700, subsample=0.6; total time=   0.8s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.6s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.7s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.2s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.1s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=700, subsample=0.6; total time=   0.8s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=700, subsample=0.6; total time=   0.8s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.6s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.7s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.2s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.0s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\nXGB {'colsample_bytree': 0.6, 'eta': 0.006, 'max_depth': 4, 'n_estimators': 400, 'subsample': 0.6} 0.008323971099045424 34.43299317359924\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-08-25 23:43:27,754]\u001b[0m A new study created in memory with name: no-name-9eca9f6b-4114-4810-966c-63febb37fdcb\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"XGB train: 7.036265023358253 0.043365857443366185 34.7956383228302\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-08-25 23:43:29,951]\u001b[0m Trial 0 finished with value: 0.0007022830264557848 and parameters: {'n_estimators': 513, 'max_depth': 3, 'learning_rate': 0.04698510052906226, 'colsample_bytree': 0.1366656960202447, 'subsample': 0.32336772226011057, 'alpha': 0.17971187437917585, 'lambda': 0.22115195905035534, 'gamma': 0.00017744203861825796, 'min_child_weight': 0.4370743069880602}. Best is trial 0 with value: 0.0007022830264557848.\u001b[0m\n\u001b[32m[I 2022-08-25 23:43:34,340]\u001b[0m Trial 1 finished with value: 0.006557888855663246 and parameters: {'n_estimators': 500, 'max_depth': 5, 'learning_rate': 0.02926680509604762, 'colsample_bytree': 0.8627513865328104, 'subsample': 0.6211393036294219, 'alpha': 28.357384199786, 'lambda': 1.740805446283207, 'gamma': 1.1401796048386874e-06, 'min_child_weight': 12.238196946818995}. Best is trial 1 with value: 0.006557888855663246.\u001b[0m\n\u001b[32m[I 2022-08-25 23:43:38,209]\u001b[0m Trial 2 finished with value: 0.01131830000324923 and parameters: {'n_estimators': 664, 'max_depth': 4, 'learning_rate': 0.028049260301494156, 'colsample_bytree': 0.7089439782206047, 'subsample': 0.5557813191701123, 'alpha': 0.9393877473782347, 'lambda': 163.92876685450133, 'gamma': 2.5279245301135446e-08, 'min_child_weight': 3.18968379385004}. Best is trial 2 with value: 0.01131830000324923.\u001b[0m\n\u001b[32m[I 2022-08-25 23:43:45,163]\u001b[0m Trial 3 finished with value: 0.00773940689559736 and parameters: {'n_estimators': 826, 'max_depth': 5, 'learning_rate': 0.021073512598841226, 'colsample_bytree': 0.6985139534321009, 'subsample': 0.6000510977012272, 'alpha': 5.234969057153901, 'lambda': 34.782307942726234, 'gamma': 0.19576384139450886, 'min_child_weight': 15.201302823293663}. Best is trial 2 with value: 0.01131830000324923.\u001b[0m\n\u001b[32m[I 2022-08-25 23:43:50,629]\u001b[0m Trial 4 finished with value: -0.0029137056127462764 and parameters: {'n_estimators': 957, 'max_depth': 4, 'learning_rate': 0.03885419815726593, 'colsample_bytree': 0.859070906860766, 'subsample': 0.8098026326496977, 'alpha': 0.40602197926853345, 'lambda': 12.950346720015837, 'gamma': 0.00014463432633783188, 'min_child_weight': 6.264918700176474}. Best is trial 2 with value: 0.01131830000324923.\u001b[0m\n\u001b[32m[I 2022-08-25 23:43:53,046]\u001b[0m Trial 5 finished with value: 0.018110645723461515 and parameters: {'n_estimators': 521, 'max_depth': 3, 'learning_rate': 0.01371639198493807, 'colsample_bytree': 0.7628416931633023, 'subsample': 0.7813919852433608, 'alpha': 4.19935530987331, 'lambda': 0.41056788724037613, 'gamma': 2.2210605480539143e-07, 'min_child_weight': 42.31074274940658}. Best is trial 5 with value: 0.018110645723461515.\u001b[0m\n\u001b[32m[I 2022-08-25 23:43:55,887]\u001b[0m Trial 6 finished with value: 0.014645118095069801 and parameters: {'n_estimators': 675, 'max_depth': 3, 'learning_rate': 0.021448383985761932, 'colsample_bytree': 0.3821263955628691, 'subsample': 0.834404806691319, 'alpha': 0.822714293988161, 'lambda': 0.6214316276532089, 'gamma': 5.55591730559736e-06, 'min_child_weight': 0.3966951689155642}. Best is trial 5 with value: 0.018110645723461515.\u001b[0m\n\u001b[32m[I 2022-08-25 23:43:59,253]\u001b[0m Trial 7 finished with value: 0.0193811388452319 and parameters: {'n_estimators': 524, 'max_depth': 4, 'learning_rate': 0.011869354210555523, 'colsample_bytree': 0.26176733361406057, 'subsample': 0.3884791444510701, 'alpha': 1.8173394072895284, 'lambda': 47.19289925153245, 'gamma': 0.4179957519293914, 'min_child_weight': 0.5220229413085891}. Best is trial 7 with value: 0.0193811388452319.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:04,151]\u001b[0m Trial 8 finished with value: 0.006345333030377573 and parameters: {'n_estimators': 622, 'max_depth': 5, 'learning_rate': 0.0013863857461072246, 'colsample_bytree': 0.14762628270723474, 'subsample': 0.4104069531637826, 'alpha': 15.921681324221616, 'lambda': 1.689296882336185, 'gamma': 2.697536514028694e-07, 'min_child_weight': 2.384103206672129}. Best is trial 7 with value: 0.0193811388452319.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:10,777]\u001b[0m Trial 9 finished with value: -0.03295829597002874 and parameters: {'n_estimators': 792, 'max_depth': 5, 'learning_rate': 0.04608111408614176, 'colsample_bytree': 0.9161522822191118, 'subsample': 0.6505504365083337, 'alpha': 1.7632535737999822, 'lambda': 1.0635261586515454, 'gamma': 0.004730976115647422, 'min_child_weight': 9.412087739054687}. Best is trial 7 with value: 0.0193811388452319.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:13,827]\u001b[0m Trial 10 finished with value: 0.010977690414206395 and parameters: {'n_estimators': 909, 'max_depth': 2, 'learning_rate': 0.0027044263801280256, 'colsample_bytree': 0.39586113344829166, 'subsample': 0.47343902833638957, 'alpha': 0.12071457188054895, 'lambda': 192.76906778188518, 'gamma': 3.8185802781352054, 'min_child_weight': 0.12197834716227189}. Best is trial 7 with value: 0.0193811388452319.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:16,021]\u001b[0m Trial 11 finished with value: 0.014887795967448767 and parameters: {'n_estimators': 564, 'max_depth': 2, 'learning_rate': 0.012034321730679018, 'colsample_bytree': 0.549119299060981, 'subsample': 0.9481707378179607, 'alpha': 4.564615659507498, 'lambda': 0.1282907989221017, 'gamma': 4.4076086093333207e-10, 'min_child_weight': 46.68695782428612}. Best is trial 7 with value: 0.0193811388452319.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:18,573]\u001b[0m Trial 12 finished with value: 0.012354721640006533 and parameters: {'n_estimators': 593, 'max_depth': 3, 'learning_rate': 0.011586975305250536, 'colsample_bytree': 0.3593227083731225, 'subsample': 0.7562894012767581, 'alpha': 3.994942093878591, 'lambda': 10.970520721877433, 'gamma': 2.533295303583115e-09, 'min_child_weight': 0.7569841716674612}. Best is trial 7 with value: 0.0193811388452319.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:22,644]\u001b[0m Trial 13 finished with value: 0.019209006465425747 and parameters: {'n_estimators': 717, 'max_depth': 4, 'learning_rate': 0.012724439822668365, 'colsample_bytree': 0.558521808473128, 'subsample': 0.7230040995408429, 'alpha': 2.0601798366645534, 'lambda': 49.6127208039531, 'gamma': 0.025126019827163466, 'min_child_weight': 0.9861017106308904}. Best is trial 7 with value: 0.0193811388452319.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:26,789]\u001b[0m Trial 14 finished with value: 0.01809991132574052 and parameters: {'n_estimators': 709, 'max_depth': 4, 'learning_rate': 0.008029279558820959, 'colsample_bytree': 0.5504547105478815, 'subsample': 0.30264525822078214, 'alpha': 1.8543998684800798, 'lambda': 48.22225407143697, 'gamma': 0.018668517466597954, 'min_child_weight': 0.9318590397880135}. Best is trial 7 with value: 0.0193811388452319.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:30,917]\u001b[0m Trial 15 finished with value: 0.016558218593744012 and parameters: {'n_estimators': 758, 'max_depth': 4, 'learning_rate': 0.018733873443384605, 'colsample_bytree': 0.2965568619574619, 'subsample': 0.5083930593360254, 'alpha': 0.6137343101019112, 'lambda': 51.41617934238483, 'gamma': 7.883018848976538, 'min_child_weight': 0.13782386148340686}. Best is trial 7 with value: 0.0193811388452319.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:35,537]\u001b[0m Trial 16 finished with value: 0.0024750131368930627 and parameters: {'n_estimators': 860, 'max_depth': 4, 'learning_rate': 0.03222437753538538, 'colsample_bytree': 0.2618488608764681, 'subsample': 0.7296700177493796, 'alpha': 10.86742621410867, 'lambda': 5.810011176276041, 'gamma': 0.08462928864099006, 'min_child_weight': 1.108982976065024}. Best is trial 7 with value: 0.0193811388452319.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:39,661]\u001b[0m Trial 17 finished with value: 0.01980335148320294 and parameters: {'n_estimators': 710, 'max_depth': 4, 'learning_rate': 0.006452126589226301, 'colsample_bytree': 0.47958106223513586, 'subsample': 0.686459031246848, 'alpha': 2.1541508150725894, 'lambda': 22.145753301638255, 'gamma': 0.0014693446864708507, 'min_child_weight': 0.30240725213688674}. Best is trial 17 with value: 0.01980335148320294.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:42,389]\u001b[0m Trial 18 finished with value: 0.014223990941272531 and parameters: {'n_estimators': 639, 'max_depth': 3, 'learning_rate': 0.005705597918116362, 'colsample_bytree': 0.44031814100943445, 'subsample': 0.3981232899073247, 'alpha': 0.29054047919726617, 'lambda': 17.791616949818522, 'gamma': 0.0016308060391670733, 'min_child_weight': 0.25886258261758055}. Best is trial 17 with value: 0.01980335148320294.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:47,251]\u001b[0m Trial 19 finished with value: 0.011521428757374148 and parameters: {'n_estimators': 585, 'max_depth': 5, 'learning_rate': 0.017374497480226878, 'colsample_bytree': 0.4769783344047964, 'subsample': 0.6590910670557882, 'alpha': 9.863804859039114, 'lambda': 4.266935406966546, 'gamma': 0.0006689825587133985, 'min_child_weight': 0.23118104305092144}. Best is trial 17 with value: 0.01980335148320294.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:52,670]\u001b[0m Trial 20 finished with value: 0.017537885652394208 and parameters: {'n_estimators': 980, 'max_depth': 4, 'learning_rate': 0.0063019975000900294, 'colsample_bytree': 0.2423426912146382, 'subsample': 0.8935636528761626, 'alpha': 0.9775713609673574, 'lambda': 85.4646050741424, 'gamma': 0.5863686265854493, 'min_child_weight': 0.4695699796487945}. Best is trial 17 with value: 0.01980335148320294.\u001b[0m\n\u001b[32m[I 2022-08-25 23:44:56,745]\u001b[0m Trial 21 finished with value: 0.01886999900834675 and parameters: {'n_estimators': 719, 'max_depth': 4, 'learning_rate': 0.014045793945503862, 'colsample_bytree': 0.5952697268697826, 'subsample': 0.7175264229788819, 'alpha': 2.4355046380637027, 'lambda': 22.085735644640586, 'gamma': 0.011885052777679397, 'min_child_weight': 1.1970011969776582}. Best is trial 17 with value: 0.01980335148320294.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:02,064]\u001b[0m Trial 22 finished with value: 0.020023938347974685 and parameters: {'n_estimators': 758, 'max_depth': 4, 'learning_rate': 0.009909937365740609, 'colsample_bytree': 0.6238065960655288, 'subsample': 0.6896259535812675, 'alpha': 2.492784865275193, 'lambda': 82.6754134901234, 'gamma': 0.5596066145279218, 'min_child_weight': 1.6941497050272925}. Best is trial 22 with value: 0.020023938347974685.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:06,511]\u001b[0m Trial 23 finished with value: 0.0191953449123588 and parameters: {'n_estimators': 785, 'max_depth': 4, 'learning_rate': 0.006810229378752258, 'colsample_bytree': 0.6433164194910822, 'subsample': 0.5546041794748332, 'alpha': 1.2164449999409086, 'lambda': 100.1017882924123, 'gamma': 0.8202281426315247, 'min_child_weight': 3.908700122003604}. Best is trial 22 with value: 0.020023938347974685.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:10,172]\u001b[0m Trial 24 finished with value: 0.01719096607978006 and parameters: {'n_estimators': 842, 'max_depth': 3, 'learning_rate': 0.023378460894532122, 'colsample_bytree': 0.47440012680002264, 'subsample': 0.68360686234621, 'alpha': 3.2617402199998864, 'lambda': 6.873880859946064, 'gamma': 1.2560491854203348, 'min_child_weight': 1.72789945763566}. Best is trial 22 with value: 0.020023938347974685.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:15,030]\u001b[0m Trial 25 finished with value: 0.01683438442505342 and parameters: {'n_estimators': 905, 'max_depth': 4, 'learning_rate': 0.008883906808210534, 'colsample_bytree': 0.3154000346338054, 'subsample': 0.5814565308873948, 'alpha': 5.965558728525551, 'lambda': 28.412969173386514, 'gamma': 3.4145602308558035e-05, 'min_child_weight': 0.20074767222940781}. Best is trial 22 with value: 0.020023938347974685.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:21,149]\u001b[0m Trial 26 finished with value: 0.013654126267526441 and parameters: {'n_estimators': 756, 'max_depth': 5, 'learning_rate': 0.017026216024276838, 'colsample_bytree': 0.21604676895371422, 'subsample': 0.4942942601045478, 'alpha': 1.3231500477361937, 'lambda': 96.06148823451954, 'gamma': 0.09584869236631176, 'min_child_weight': 0.6414010214486628}. Best is trial 22 with value: 0.020023938347974685.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:24,355]\u001b[0m Trial 27 finished with value: 0.008641840556540246 and parameters: {'n_estimators': 554, 'max_depth': 4, 'learning_rate': 0.0012186852931074582, 'colsample_bytree': 0.6297563855293343, 'subsample': 0.8603472694507288, 'alpha': 0.5535825896190691, 'lambda': 75.13229533048472, 'gamma': 0.0009830624907228888, 'min_child_weight': 1.733333820967979}. Best is trial 22 with value: 0.020023938347974685.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:27,329]\u001b[0m Trial 28 finished with value: 0.019015962730001543 and parameters: {'n_estimators': 683, 'max_depth': 3, 'learning_rate': 0.009611271605990152, 'colsample_bytree': 0.758345623928325, 'subsample': 0.3995243072361724, 'alpha': 2.8314394762033346, 'lambda': 10.885402328748555, 'gamma': 0.11852268594993819, 'min_child_weight': 0.32915645797143134}. Best is trial 22 with value: 0.020023938347974685.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:30,021]\u001b[0m Trial 29 finished with value: 0.01494699810109732 and parameters: {'n_estimators': 805, 'max_depth': 2, 'learning_rate': 0.03543704447138515, 'colsample_bytree': 0.11805805631705901, 'subsample': 0.685433843998101, 'alpha': 6.601348666704201, 'lambda': 26.86287057725271, 'gamma': 8.149048065274889e-05, 'min_child_weight': 0.5501600511322582}. Best is trial 22 with value: 0.020023938347974685.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:36,784]\u001b[0m Trial 30 finished with value: 0.014656674890564398 and parameters: {'n_estimators': 888, 'max_depth': 5, 'learning_rate': 0.004209020923778167, 'colsample_bytree': 0.17589216980121342, 'subsample': 0.5331226032254142, 'alpha': 0.23138821907916504, 'lambda': 3.3037011984260873, 'gamma': 1.0267141218858864e-05, 'min_child_weight': 0.1047883050627559}. Best is trial 22 with value: 0.020023938347974685.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:40,882]\u001b[0m Trial 31 finished with value: 0.020427281486879268 and parameters: {'n_estimators': 724, 'max_depth': 4, 'learning_rate': 0.015486649918972657, 'colsample_bytree': 0.5294280826747957, 'subsample': 0.7187605091747378, 'alpha': 2.1667201034037014, 'lambda': 39.631817324995325, 'gamma': 0.02061392327243693, 'min_child_weight': 1.4616877940515698}. Best is trial 31 with value: 0.020427281486879268.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:45,191]\u001b[0m Trial 32 finished with value: 0.01645157348751005 and parameters: {'n_estimators': 734, 'max_depth': 4, 'learning_rate': 0.01544451600027591, 'colsample_bytree': 0.5026645159226063, 'subsample': 0.6312172111904271, 'alpha': 2.812810912859354, 'lambda': 145.8731809994464, 'gamma': 0.005004958134870777, 'min_child_weight': 1.4700137585819657}. Best is trial 31 with value: 0.020427281486879268.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:48,831]\u001b[0m Trial 33 finished with value: 0.018335315699263253 and parameters: {'n_estimators': 644, 'max_depth': 4, 'learning_rate': 0.010028942809785776, 'colsample_bytree': 0.4254573358712652, 'subsample': 0.342707213305264, 'alpha': 1.3910652928761633, 'lambda': 41.41558399414634, 'gamma': 2.1158867407977686, 'min_child_weight': 3.315695291659952}. Best is trial 31 with value: 0.020427281486879268.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:52,747]\u001b[0m Trial 34 finished with value: 0.012587848783867542 and parameters: {'n_estimators': 688, 'max_depth': 4, 'learning_rate': 0.026382211872283717, 'colsample_bytree': 0.6721862877202348, 'subsample': 0.7739454646511765, 'alpha': 7.7838721483640905, 'lambda': 15.995058407460903, 'gamma': 0.2355386304956796, 'min_child_weight': 4.616915366125715}. Best is trial 31 with value: 0.020427281486879268.\u001b[0m\n\u001b[32m[I 2022-08-25 23:45:57,372]\u001b[0m Trial 35 finished with value: 0.01699801214391536 and parameters: {'n_estimators': 513, 'max_depth': 5, 'learning_rate': 0.018858592176785517, 'colsample_bytree': 0.7711971124276552, 'subsample': 0.5862910826363631, 'alpha': 0.770080434513332, 'lambda': 66.0380797782782, 'gamma': 0.00045212549506432493, 'min_child_weight': 2.227825729807237}. Best is trial 31 with value: 0.020427281486879268.\u001b[0m\n\u001b[32m[I 2022-08-25 23:46:02,197]\u001b[0m Trial 36 finished with value: 0.014083686432490132 and parameters: {'n_estimators': 763, 'max_depth': 4, 'learning_rate': 0.02241875294487272, 'colsample_bytree': 0.723593147044751, 'subsample': 0.6877689020852531, 'alpha': 1.4674330986464026, 'lambda': 141.5817366435047, 'gamma': 0.03834371255496499, 'min_child_weight': 0.344903548160775}. Best is trial 31 with value: 0.020427281486879268.\u001b[0m\n\u001b[32m[I 2022-08-25 23:46:07,109]\u001b[0m Trial 37 finished with value: 0.014953283379882967 and parameters: {'n_estimators': 813, 'max_depth': 4, 'learning_rate': 0.015735586833938418, 'colsample_bytree': 0.6080903783543758, 'subsample': 0.6201696417176048, 'alpha': 3.80237481013571, 'lambda': 33.61161951933553, 'gamma': 0.4172349454827498, 'min_child_weight': 0.1817106792858549}. Best is trial 31 with value: 0.020427281486879268.\u001b[0m\n\u001b[32m[I 2022-08-25 23:46:09,744]\u001b[0m Trial 38 finished with value: 0.015321919478697512 and parameters: {'n_estimators': 622, 'max_depth': 3, 'learning_rate': 0.005144650308740073, 'colsample_bytree': 0.336055369788472, 'subsample': 0.8052825761955745, 'alpha': 2.267113628047515, 'lambda': 7.299026686290155, 'gamma': 0.004692113119962353, 'min_child_weight': 0.7171746704752112}. Best is trial 31 with value: 0.020427281486879268.\u001b[0m\n\u001b[32m[I 2022-08-25 23:46:15,121]\u001b[0m Trial 39 finished with value: 0.0216817800698473 and parameters: {'n_estimators': 658, 'max_depth': 5, 'learning_rate': 0.010489766383071643, 'colsample_bytree': 0.5230960308617957, 'subsample': 0.7491435614259916, 'alpha': 25.041485791983742, 'lambda': 118.76939235347685, 'gamma': 8.78786166241555, 'min_child_weight': 6.313201660765905}. Best is trial 39 with value: 0.0216817800698473.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  167.36829662322998\n        n_estimators : 658\n           max_depth : 5\n       learning_rate : 0.010489766383071643\n    colsample_bytree : 0.5230960308617957\n           subsample : 0.7491435614259916\n               alpha : 25.041485791983742\n              lambda : 118.76939235347685\n               gamma : 8.78786166241555\n    min_child_weight : 6.313201660765905\nbest objective value : 0.0216817800698473\nOptuna XGB train: 6.887444697503428 0.08431824162461499 170.29664659500122\nMin_prd:  125\nConstant guess:  6.801798451895731 0.0\nXGB test: 6.690736270893462 0.001886781638528956\nXGB GS test: 6.7089180299557585 0.006943571377436508\nOptuna XGB test: 6.683955382866307 0.00471540944205473\n207.5646300315857    min_prd      xgbf     xgbgs      xgbo\n0      125  0.001887  0.006944  0.004715\n","output_type":"stream"}]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:46:18.135521Z","iopub.execute_input":"2022-08-25T23:46:18.137546Z","iopub.status.idle":"2022-08-25T23:46:18.148455Z","shell.execute_reply.started":"2022-08-25T23:46:18.137512Z","shell.execute_reply":"2022-08-25T23:46:18.147301Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"   min_prd      xgbf     xgbgs      xgbo\n0      125  0.001887  0.006944  0.004715","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf</th>\n      <th>xgbgs</th>\n      <th>xgbo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>125</td>\n      <td>0.001887</td>\n      <td>0.006944</td>\n      <td>0.004715</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('Total time for a script: ', time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:46:18.149640Z","iopub.execute_input":"2022-08-25T23:46:18.150251Z","iopub.status.idle":"2022-08-25T23:46:18.159620Z","shell.execute_reply.started":"2022-08-25T23:46:18.150217Z","shell.execute_reply":"2022-08-25T23:46:18.158371Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stdout","text":"Total time for a script:  207.5944380760193\n","output_type":"stream"}]},{"cell_type":"code","source":"results.iloc[:,1:].mean()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:46:18.161911Z","iopub.execute_input":"2022-08-25T23:46:18.163640Z","iopub.status.idle":"2022-08-25T23:46:18.174732Z","shell.execute_reply.started":"2022-08-25T23:46:18.163602Z","shell.execute_reply":"2022-08-25T23:46:18.173374Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"xgbf     0.001887\nxgbgs    0.006944\nxgbo     0.004715\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# 3yr window, trials=20, cv_reg=0.03: 0.88%. runs 1 hr.\n# 3yr, t=40, cv_reg=0.04: 0.96%.\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:29:26.858448Z","iopub.execute_input":"2022-08-25T23:29:26.858766Z","iopub.status.idle":"2022-08-25T23:29:26.863897Z","shell.execute_reply.started":"2022-08-25T23:29:26.858736Z","shell.execute_reply":"2022-08-25T23:29:26.862771Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"results0","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:29:26.865542Z","iopub.execute_input":"2022-08-25T23:29:26.866347Z","iopub.status.idle":"2022-08-25T23:29:26.883159Z","shell.execute_reply.started":"2022-08-25T23:29:26.866267Z","shell.execute_reply":"2022-08-25T23:29:26.882047Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"    min_prd      xgbf     xgbgs      xgbo\n0       100 -0.033926 -0.019374  -0.02375\n1       125  0.001887  0.006944  0.009033\n2       150  0.009361  0.018258  0.018259\n3       175 -0.008671 -0.002783 -0.000479\n4       200  0.012099   0.01089  0.009292\n5       225  0.024659  0.024404  0.025087\n6       250 -0.004759  0.003918  0.006247\n7       275  0.013317  0.017266  0.022251\n8       300 -0.009404 -0.000589 -0.000884\n9       325  -0.00607  0.004755  0.009447\n10      350 -0.014595  -0.00526 -0.000838\n11      375  0.001776  0.006979  0.007666\n12      400  0.006133  0.007229  0.004144\n13      425  0.009275  0.007466  0.007247\n14      450  0.012836  0.009185  0.009102\n15      475  0.016533  0.018099  0.013429\n16      500  0.049542  0.054596  0.057267\n17      525  0.026679  0.028794  0.030998\n18      550 -0.002261  0.001317 -0.000554\n19      575 -0.014905 -0.005435 -0.008113\n20      600  0.006236  0.007251  0.006108\n21      625 -0.020183 -0.013108 -0.013691\n22      650   0.04702  0.040816  0.047552\n23      675 -0.025128 -0.010631 -0.005623","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf</th>\n      <th>xgbgs</th>\n      <th>xgbo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>-0.033926</td>\n      <td>-0.019374</td>\n      <td>-0.02375</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>125</td>\n      <td>0.001887</td>\n      <td>0.006944</td>\n      <td>0.009033</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>150</td>\n      <td>0.009361</td>\n      <td>0.018258</td>\n      <td>0.018259</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>175</td>\n      <td>-0.008671</td>\n      <td>-0.002783</td>\n      <td>-0.000479</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200</td>\n      <td>0.012099</td>\n      <td>0.01089</td>\n      <td>0.009292</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>225</td>\n      <td>0.024659</td>\n      <td>0.024404</td>\n      <td>0.025087</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>250</td>\n      <td>-0.004759</td>\n      <td>0.003918</td>\n      <td>0.006247</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>275</td>\n      <td>0.013317</td>\n      <td>0.017266</td>\n      <td>0.022251</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>300</td>\n      <td>-0.009404</td>\n      <td>-0.000589</td>\n      <td>-0.000884</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>325</td>\n      <td>-0.00607</td>\n      <td>0.004755</td>\n      <td>0.009447</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>350</td>\n      <td>-0.014595</td>\n      <td>-0.00526</td>\n      <td>-0.000838</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>375</td>\n      <td>0.001776</td>\n      <td>0.006979</td>\n      <td>0.007666</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>400</td>\n      <td>0.006133</td>\n      <td>0.007229</td>\n      <td>0.004144</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>425</td>\n      <td>0.009275</td>\n      <td>0.007466</td>\n      <td>0.007247</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>450</td>\n      <td>0.012836</td>\n      <td>0.009185</td>\n      <td>0.009102</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>475</td>\n      <td>0.016533</td>\n      <td>0.018099</td>\n      <td>0.013429</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>500</td>\n      <td>0.049542</td>\n      <td>0.054596</td>\n      <td>0.057267</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>525</td>\n      <td>0.026679</td>\n      <td>0.028794</td>\n      <td>0.030998</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>550</td>\n      <td>-0.002261</td>\n      <td>0.001317</td>\n      <td>-0.000554</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>575</td>\n      <td>-0.014905</td>\n      <td>-0.005435</td>\n      <td>-0.008113</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>600</td>\n      <td>0.006236</td>\n      <td>0.007251</td>\n      <td>0.006108</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>625</td>\n      <td>-0.020183</td>\n      <td>-0.013108</td>\n      <td>-0.013691</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>650</td>\n      <td>0.04702</td>\n      <td>0.040816</td>\n      <td>0.047552</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>675</td>\n      <td>-0.025128</td>\n      <td>-0.010631</td>\n      <td>-0.005623</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# create validation set \nval_indx = X_train0.remainder__prd == min_prd+windows_width-1\nX_val = X_train0[val_indx]\nX_val.drop(columns='remainder__prd', inplace=True)\ny_val = y_train0[val_indx]\ndisplay(X_val, y_val)\ntrain_indx = X_train0.remainder__prd < min_prd+windows_width-1\nX_train = X_train0[train_indx]\nX_train.drop(columns='remainder__prd', inplace=True)\ny_train = y_train0[train_indx]\ndisplay(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T00:39:17.491796Z","iopub.execute_input":"2022-08-26T00:39:17.492789Z","iopub.status.idle":"2022-08-26T00:39:17.716128Z","shell.execute_reply.started":"2022-08-26T00:39:17.492751Z","shell.execute_reply":"2022-08-26T00:39:17.715162Z"},"trusted":true},"execution_count":132,"outputs":[{"output_type":"display_data","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n36       -0.612479     0.093461  0.719846 -0.253931 -0.987864 -0.395222   \n73       -1.098087    -1.056204  1.111671 -1.729057 -0.621714 -0.126246   \n98       -1.017334    -0.761409  1.163306 -0.309223 -0.278237 -0.481448   \n135      -1.100278     0.032686  1.092450 -0.504720 -0.889660 -0.659544   \n172      -0.760740     0.149421  1.087237 -0.416474 -0.963139 -0.602710   \n...            ...          ...       ...       ...       ...       ...   \n38054    -0.258451    -0.233632  0.019828  0.976392  2.291757  0.425113   \n38056    -0.258451    -0.233632 -0.186698  1.540608  2.291757 -0.698786   \n38093    -0.437714    -0.345022  0.606545  0.148160 -0.155946 -0.649777   \n38095    -0.258451    -0.675738  1.313995 -0.315314 -0.270157 -0.627896   \n38132     0.003116    -0.392549 -0.370949 -0.051501  0.347610  1.701109   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n36      -0.839787     1.311527  -0.698449       -1.107615      -1.085305   \n73       0.660355    -0.027070   1.122396        0.783425       0.489108   \n98      -0.022911     0.048652   0.852550       -0.339425      -0.179673   \n135     -1.199866     1.311926  -0.833087       -0.445394      -0.390569   \n172     -0.484662     1.991333  -1.277050       -0.865144      -0.816650   \n...           ...          ...        ...             ...            ...   \n38054    1.780885     2.386609   0.034506        0.764370       0.974170   \n38056    0.382980     0.095169   1.190230       -0.216883      -0.317184   \n38093   -0.933260     0.896756   0.037911        0.427254       0.575023   \n38095    0.020639     0.898530   0.034506       -0.020632       0.096871   \n38132   -0.494689     1.615490  -0.272370       -1.272487      -1.177594   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  num__size  \\\n36        -0.948009 -1.107851   -1.061303   -1.091215    -1.263500   0.728270   \n73         1.326108  0.371155    0.568497    0.759708     1.631631  -0.854679   \n98        -0.820050 -0.097148   -0.476122   -0.802269    -0.445159  -0.254341   \n135        0.285349 -0.821593   -0.403480   -0.736434    -0.756806   0.622390   \n172        0.219649 -0.739330   -0.826352   -0.666444    -0.748998   1.354701   \n...             ...       ...         ...         ...          ...        ...   \n38054      0.139572  0.694449    0.610883    0.382628     0.546210  -0.887824   \n38056     -0.743497  0.073921    0.066257   -0.497630    -0.741324  -0.563841   \n38093      0.032663  0.324143    0.237936   -0.819750    -0.842935   0.388448   \n38095     -0.536320  0.534275   -0.119711    0.422353     0.381169  -1.448784   \n38132     -0.848577 -1.107851   -1.252635   -1.201416    -1.129014   0.758103   \n\n       num__lbm  num__lop  num__lgp  num__linv  num__llme  num__l1amhd  \\\n36     0.542291 -0.049710 -0.915318  -0.071219   0.559899    -0.568393   \n73     0.452220 -0.846728 -0.549495   1.558847  -1.028437     1.177818   \n98     0.442542  0.724077  0.114054  -0.820425  -0.272731     0.854642   \n135    1.436114 -0.562715 -0.848914  -0.316655   0.449012    -0.691659   \n172    0.958646 -0.268558 -0.894380  -0.716941   1.088137    -1.165634   \n...         ...       ...       ...        ...        ...          ...   \n38054 -0.313723  1.176995  2.242091  -0.463802  -1.444274     0.035808   \n38056 -0.767156  1.712398  2.242091  -0.450405  -0.640354     1.221661   \n38093  0.233137  0.349421 -0.002480  -0.344220   0.269507     0.076310   \n38095  0.737779  0.907250  0.061940   0.175870  -1.691655     0.035808   \n38132 -0.639719  0.408728  0.798166   1.616227   0.446894    -0.035893   \n\n       num__l1MAX  num__l3amhd  num__l3MAX  num__l6amhd  num__l6MAX  \\\n36      -1.113677    -0.409735    0.517731    -0.304907   -0.783420   \n73      -0.535184     1.247437    3.053172     1.301634    0.122334   \n98       0.216873     0.845819   -0.845943     0.993837    1.109063   \n135     -0.941756    -0.533623   -0.683832    -0.445939   -0.706069   \n172      0.158669    -1.071988   -0.215578    -0.998049   -0.794888   \n...           ...          ...         ...          ...         ...   \n38054    0.602841     0.035425    0.413269     0.038960    0.961130   \n38056   -0.898580     1.286012   -0.777083     1.334271   -0.495719   \n38093   -1.043675     0.198068    0.354022     0.265541   -0.833966   \n38095   -0.917892     0.035425   -0.124776     0.038960    3.304932   \n38132   -0.604270     0.258139   -1.088977     0.348058   -1.011094   \n\n       num__l12amhd  num__l12MAX  num__l12mom122  num__l12ivol_capm  \\\n36        -0.352128    -1.113677       -0.411241          -0.760507   \n73         0.986915    -0.535184       -1.636807           1.061128   \n98         0.993492     0.216873       -1.113882          -0.611153   \n135       -0.499549    -0.941756       -0.972298           0.549475   \n172       -1.066169     0.158669       -1.147174           0.641499   \n...             ...          ...             ...                ...   \n38054      0.054105     0.602841       -0.431152          -0.840754   \n38056      1.148370    -0.898580       -0.747516          -1.003954   \n38093      0.096022    -1.043675       -0.906791           0.016794   \n38095      0.054105    -0.917892       -1.511419          -0.271934   \n38132      0.301280    -0.604270       -1.245630           1.331538   \n\n       num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  num__l12vol12m  \\\n36            -0.692327        -0.586912      -0.825575       -1.116346   \n73             0.793389         1.587190       2.139835        1.972031   \n98            -0.402149        -1.157941      -0.417698       -0.258807   \n135            0.642682         0.364889       0.145648       -0.157740   \n172            0.811720         0.080118      -0.033113       -0.319384   \n...                 ...              ...            ...             ...   \n38054         -0.900806        -1.529243       0.276725       -0.210009   \n38056         -0.850037        -0.687256      -0.007706       -0.210009   \n38093         -0.018488        -0.554340      -0.401027       -0.708172   \n38095         -0.147834        -0.414281       0.584731        0.329961   \n38132          0.363086        -0.144910       0.399951        0.050476   \n\n       num__amhd_miss  cat__ind_1.0  cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  \\\n36          -0.300695           0.0           0.0           0.0           0.0   \n73          -0.300695           0.0           0.0           0.0           0.0   \n98          -0.300695           0.0           0.0           0.0           0.0   \n135         -0.300695           0.0           0.0           0.0           0.0   \n172         -0.300695           0.0           0.0           0.0           0.0   \n...               ...           ...           ...           ...           ...   \n38054        3.325624           0.0           0.0           0.0           0.0   \n38056       -0.300695           0.0           0.0           0.0           0.0   \n38093       -0.300695           0.0           0.0           0.0           0.0   \n38095        3.325624           0.0           0.0           0.0           0.0   \n38132       -0.300695           0.0           0.0           0.0           0.0   \n\n       cat__ind_5.0  cat__ind_6.0  cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  \\\n36              0.0           0.0           0.0           0.0           0.0   \n73              0.0           0.0           0.0           0.0           0.0   \n98              0.0           0.0           0.0           0.0           0.0   \n135             0.0           0.0           0.0           0.0           0.0   \n172             0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n38054           0.0           0.0           0.0           0.0           1.0   \n38056           0.0           0.0           0.0           0.0           1.0   \n38093           0.0           0.0           0.0           0.0           0.0   \n38095           0.0           0.0           0.0           0.0           1.0   \n38132           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_10.0  cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            1.0   \n172              0.0            0.0            0.0            1.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_15.0  cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            1.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_19.0  cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            1.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_23.0  cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  \\\n36               0.0            0.0            1.0            0.0   \n73               1.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_27.0  cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_31.0  cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_35.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            1.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_47.0  cat__ind_48.0  \n36               0.0            0.0            0.0            0.0  \n73               0.0            0.0            0.0            0.0  \n98               0.0            0.0            0.0            0.0  \n135              0.0            0.0            0.0            0.0  \n172              0.0            0.0            0.0            0.0  \n...              ...            ...            ...            ...  \n38054            0.0            0.0            0.0            0.0  \n38056            0.0            0.0            0.0            0.0  \n38093            0.0            0.0            0.0            0.0  \n38095            0.0            0.0            0.0            0.0  \n38132            0.0            0.0            0.0            0.0  \n\n[1264 rows x 82 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36</th>\n      <td>-0.612479</td>\n      <td>0.093461</td>\n      <td>0.719846</td>\n      <td>-0.253931</td>\n      <td>-0.987864</td>\n      <td>-0.395222</td>\n      <td>-0.839787</td>\n      <td>1.311527</td>\n      <td>-0.698449</td>\n      <td>-1.107615</td>\n      <td>-1.085305</td>\n      <td>-0.948009</td>\n      <td>-1.107851</td>\n      <td>-1.061303</td>\n      <td>-1.091215</td>\n      <td>-1.263500</td>\n      <td>0.728270</td>\n      <td>0.542291</td>\n      <td>-0.049710</td>\n      <td>-0.915318</td>\n      <td>-0.071219</td>\n      <td>0.559899</td>\n      <td>-0.568393</td>\n      <td>-1.113677</td>\n      <td>-0.409735</td>\n      <td>0.517731</td>\n      <td>-0.304907</td>\n      <td>-0.783420</td>\n      <td>-0.352128</td>\n      <td>-1.113677</td>\n      <td>-0.411241</td>\n      <td>-0.760507</td>\n      <td>-0.692327</td>\n      <td>-0.586912</td>\n      <td>-0.825575</td>\n      <td>-1.116346</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>-1.098087</td>\n      <td>-1.056204</td>\n      <td>1.111671</td>\n      <td>-1.729057</td>\n      <td>-0.621714</td>\n      <td>-0.126246</td>\n      <td>0.660355</td>\n      <td>-0.027070</td>\n      <td>1.122396</td>\n      <td>0.783425</td>\n      <td>0.489108</td>\n      <td>1.326108</td>\n      <td>0.371155</td>\n      <td>0.568497</td>\n      <td>0.759708</td>\n      <td>1.631631</td>\n      <td>-0.854679</td>\n      <td>0.452220</td>\n      <td>-0.846728</td>\n      <td>-0.549495</td>\n      <td>1.558847</td>\n      <td>-1.028437</td>\n      <td>1.177818</td>\n      <td>-0.535184</td>\n      <td>1.247437</td>\n      <td>3.053172</td>\n      <td>1.301634</td>\n      <td>0.122334</td>\n      <td>0.986915</td>\n      <td>-0.535184</td>\n      <td>-1.636807</td>\n      <td>1.061128</td>\n      <td>0.793389</td>\n      <td>1.587190</td>\n      <td>2.139835</td>\n      <td>1.972031</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>-1.017334</td>\n      <td>-0.761409</td>\n      <td>1.163306</td>\n      <td>-0.309223</td>\n      <td>-0.278237</td>\n      <td>-0.481448</td>\n      <td>-0.022911</td>\n      <td>0.048652</td>\n      <td>0.852550</td>\n      <td>-0.339425</td>\n      <td>-0.179673</td>\n      <td>-0.820050</td>\n      <td>-0.097148</td>\n      <td>-0.476122</td>\n      <td>-0.802269</td>\n      <td>-0.445159</td>\n      <td>-0.254341</td>\n      <td>0.442542</td>\n      <td>0.724077</td>\n      <td>0.114054</td>\n      <td>-0.820425</td>\n      <td>-0.272731</td>\n      <td>0.854642</td>\n      <td>0.216873</td>\n      <td>0.845819</td>\n      <td>-0.845943</td>\n      <td>0.993837</td>\n      <td>1.109063</td>\n      <td>0.993492</td>\n      <td>0.216873</td>\n      <td>-1.113882</td>\n      <td>-0.611153</td>\n      <td>-0.402149</td>\n      <td>-1.157941</td>\n      <td>-0.417698</td>\n      <td>-0.258807</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>-1.100278</td>\n      <td>0.032686</td>\n      <td>1.092450</td>\n      <td>-0.504720</td>\n      <td>-0.889660</td>\n      <td>-0.659544</td>\n      <td>-1.199866</td>\n      <td>1.311926</td>\n      <td>-0.833087</td>\n      <td>-0.445394</td>\n      <td>-0.390569</td>\n      <td>0.285349</td>\n      <td>-0.821593</td>\n      <td>-0.403480</td>\n      <td>-0.736434</td>\n      <td>-0.756806</td>\n      <td>0.622390</td>\n      <td>1.436114</td>\n      <td>-0.562715</td>\n      <td>-0.848914</td>\n      <td>-0.316655</td>\n      <td>0.449012</td>\n      <td>-0.691659</td>\n      <td>-0.941756</td>\n      <td>-0.533623</td>\n      <td>-0.683832</td>\n      <td>-0.445939</td>\n      <td>-0.706069</td>\n      <td>-0.499549</td>\n      <td>-0.941756</td>\n      <td>-0.972298</td>\n      <td>0.549475</td>\n      <td>0.642682</td>\n      <td>0.364889</td>\n      <td>0.145648</td>\n      <td>-0.157740</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>-0.760740</td>\n      <td>0.149421</td>\n      <td>1.087237</td>\n      <td>-0.416474</td>\n      <td>-0.963139</td>\n      <td>-0.602710</td>\n      <td>-0.484662</td>\n      <td>1.991333</td>\n      <td>-1.277050</td>\n      <td>-0.865144</td>\n      <td>-0.816650</td>\n      <td>0.219649</td>\n      <td>-0.739330</td>\n      <td>-0.826352</td>\n      <td>-0.666444</td>\n      <td>-0.748998</td>\n      <td>1.354701</td>\n      <td>0.958646</td>\n      <td>-0.268558</td>\n      <td>-0.894380</td>\n      <td>-0.716941</td>\n      <td>1.088137</td>\n      <td>-1.165634</td>\n      <td>0.158669</td>\n      <td>-1.071988</td>\n      <td>-0.215578</td>\n      <td>-0.998049</td>\n      <td>-0.794888</td>\n      <td>-1.066169</td>\n      <td>0.158669</td>\n      <td>-1.147174</td>\n      <td>0.641499</td>\n      <td>0.811720</td>\n      <td>0.080118</td>\n      <td>-0.033113</td>\n      <td>-0.319384</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>38054</th>\n      <td>-0.258451</td>\n      <td>-0.233632</td>\n      <td>0.019828</td>\n      <td>0.976392</td>\n      <td>2.291757</td>\n      <td>0.425113</td>\n      <td>1.780885</td>\n      <td>2.386609</td>\n      <td>0.034506</td>\n      <td>0.764370</td>\n      <td>0.974170</td>\n      <td>0.139572</td>\n      <td>0.694449</td>\n      <td>0.610883</td>\n      <td>0.382628</td>\n      <td>0.546210</td>\n      <td>-0.887824</td>\n      <td>-0.313723</td>\n      <td>1.176995</td>\n      <td>2.242091</td>\n      <td>-0.463802</td>\n      <td>-1.444274</td>\n      <td>0.035808</td>\n      <td>0.602841</td>\n      <td>0.035425</td>\n      <td>0.413269</td>\n      <td>0.038960</td>\n      <td>0.961130</td>\n      <td>0.054105</td>\n      <td>0.602841</td>\n      <td>-0.431152</td>\n      <td>-0.840754</td>\n      <td>-0.900806</td>\n      <td>-1.529243</td>\n      <td>0.276725</td>\n      <td>-0.210009</td>\n      <td>3.325624</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38056</th>\n      <td>-0.258451</td>\n      <td>-0.233632</td>\n      <td>-0.186698</td>\n      <td>1.540608</td>\n      <td>2.291757</td>\n      <td>-0.698786</td>\n      <td>0.382980</td>\n      <td>0.095169</td>\n      <td>1.190230</td>\n      <td>-0.216883</td>\n      <td>-0.317184</td>\n      <td>-0.743497</td>\n      <td>0.073921</td>\n      <td>0.066257</td>\n      <td>-0.497630</td>\n      <td>-0.741324</td>\n      <td>-0.563841</td>\n      <td>-0.767156</td>\n      <td>1.712398</td>\n      <td>2.242091</td>\n      <td>-0.450405</td>\n      <td>-0.640354</td>\n      <td>1.221661</td>\n      <td>-0.898580</td>\n      <td>1.286012</td>\n      <td>-0.777083</td>\n      <td>1.334271</td>\n      <td>-0.495719</td>\n      <td>1.148370</td>\n      <td>-0.898580</td>\n      <td>-0.747516</td>\n      <td>-1.003954</td>\n      <td>-0.850037</td>\n      <td>-0.687256</td>\n      <td>-0.007706</td>\n      <td>-0.210009</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38093</th>\n      <td>-0.437714</td>\n      <td>-0.345022</td>\n      <td>0.606545</td>\n      <td>0.148160</td>\n      <td>-0.155946</td>\n      <td>-0.649777</td>\n      <td>-0.933260</td>\n      <td>0.896756</td>\n      <td>0.037911</td>\n      <td>0.427254</td>\n      <td>0.575023</td>\n      <td>0.032663</td>\n      <td>0.324143</td>\n      <td>0.237936</td>\n      <td>-0.819750</td>\n      <td>-0.842935</td>\n      <td>0.388448</td>\n      <td>0.233137</td>\n      <td>0.349421</td>\n      <td>-0.002480</td>\n      <td>-0.344220</td>\n      <td>0.269507</td>\n      <td>0.076310</td>\n      <td>-1.043675</td>\n      <td>0.198068</td>\n      <td>0.354022</td>\n      <td>0.265541</td>\n      <td>-0.833966</td>\n      <td>0.096022</td>\n      <td>-1.043675</td>\n      <td>-0.906791</td>\n      <td>0.016794</td>\n      <td>-0.018488</td>\n      <td>-0.554340</td>\n      <td>-0.401027</td>\n      <td>-0.708172</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38095</th>\n      <td>-0.258451</td>\n      <td>-0.675738</td>\n      <td>1.313995</td>\n      <td>-0.315314</td>\n      <td>-0.270157</td>\n      <td>-0.627896</td>\n      <td>0.020639</td>\n      <td>0.898530</td>\n      <td>0.034506</td>\n      <td>-0.020632</td>\n      <td>0.096871</td>\n      <td>-0.536320</td>\n      <td>0.534275</td>\n      <td>-0.119711</td>\n      <td>0.422353</td>\n      <td>0.381169</td>\n      <td>-1.448784</td>\n      <td>0.737779</td>\n      <td>0.907250</td>\n      <td>0.061940</td>\n      <td>0.175870</td>\n      <td>-1.691655</td>\n      <td>0.035808</td>\n      <td>-0.917892</td>\n      <td>0.035425</td>\n      <td>-0.124776</td>\n      <td>0.038960</td>\n      <td>3.304932</td>\n      <td>0.054105</td>\n      <td>-0.917892</td>\n      <td>-1.511419</td>\n      <td>-0.271934</td>\n      <td>-0.147834</td>\n      <td>-0.414281</td>\n      <td>0.584731</td>\n      <td>0.329961</td>\n      <td>3.325624</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38132</th>\n      <td>0.003116</td>\n      <td>-0.392549</td>\n      <td>-0.370949</td>\n      <td>-0.051501</td>\n      <td>0.347610</td>\n      <td>1.701109</td>\n      <td>-0.494689</td>\n      <td>1.615490</td>\n      <td>-0.272370</td>\n      <td>-1.272487</td>\n      <td>-1.177594</td>\n      <td>-0.848577</td>\n      <td>-1.107851</td>\n      <td>-1.252635</td>\n      <td>-1.201416</td>\n      <td>-1.129014</td>\n      <td>0.758103</td>\n      <td>-0.639719</td>\n      <td>0.408728</td>\n      <td>0.798166</td>\n      <td>1.616227</td>\n      <td>0.446894</td>\n      <td>-0.035893</td>\n      <td>-0.604270</td>\n      <td>0.258139</td>\n      <td>-1.088977</td>\n      <td>0.348058</td>\n      <td>-1.011094</td>\n      <td>0.301280</td>\n      <td>-0.604270</td>\n      <td>-1.245630</td>\n      <td>1.331538</td>\n      <td>0.363086</td>\n      <td>-0.144910</td>\n      <td>0.399951</td>\n      <td>0.050476</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1264 rows  82 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"36       11.636391\n73      -14.450009\n98        8.057591\n135       7.106191\n172       5.680091\n           ...    \n38054     3.070491\n38056     4.701091\n38093     3.527791\n38095   -11.022609\n38132    -4.772609\nName: RET, Length: 1264, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n0         0.223712     0.411154  0.517180  0.607708 -0.658944 -0.306924   \n1         0.086569     0.493729  0.517180  0.607708 -0.658944 -0.306924   \n2        -0.263268     0.316102  0.517180  0.607708 -0.658944 -0.306924   \n3        -0.064674     0.845461  0.517180  0.607708 -0.658944 -0.306924   \n4        -0.050959     0.615673  0.517180  0.607708 -0.658944 -0.306924   \n...            ...          ...       ...       ...       ...       ...   \n38127    -0.126171    -0.573528 -0.696348  0.515807  0.843280  1.629851   \n38128    -0.061265    -0.411097 -0.696348  0.515807  0.843280  1.629851   \n38129    -0.225563    -0.518573 -0.696348  0.515807  0.843280  1.629851   \n38130     0.049301    -0.254967 -0.696348  0.515807  0.843280  1.629851   \n38131     0.117787    -0.151727 -0.370949 -0.051501  0.347610  1.701109   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n0        0.378862     0.401231  -1.035214       -0.744129      -0.843638   \n1       -0.918057     0.728623  -1.061755       -0.018037       0.006479   \n2       -0.522888     0.431824  -1.076616        2.215483       2.661567   \n3        0.458634     0.375338  -1.134783       -0.657365      -0.721118   \n4        0.963230     0.652245  -1.214758       -0.435597      -0.312731   \n...           ...          ...        ...             ...            ...   \n38127   -0.277997    -0.094913   0.246105       -0.404615      -0.267449   \n38128    0.633474    -0.228509   0.239233       -1.193447      -1.234102   \n38129   -0.415622     0.336865   0.219605       -1.088828      -1.079392   \n38130    1.660382     0.737432   0.104408       -0.977658      -0.867269   \n38131    0.398740     1.996235  -0.045837       -0.737308      -0.720663   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  num__size  \\\n0         -0.665956 -0.141294   -0.399484    0.074478    -0.405186   0.859184   \n1         -0.834701 -0.750279   -0.116693    0.207577    -0.341607   0.792355   \n2         -0.882438  0.471206    1.469296    0.627268    -0.072902   0.757836   \n3         -0.855571 -0.296061   -0.542764    0.625622    -0.054416   0.784571   \n4         -0.959557 -0.009505   -0.659384    0.628517    -0.046143   0.834576   \n...             ...       ...         ...         ...          ...        ...   \n38127     -0.175701 -0.870226   -0.585051   -0.994533    -0.252118   0.670639   \n38128     -0.152716 -0.930376   -1.241484   -1.153020    -0.259118   0.706867   \n38129     -0.188757 -1.062516   -0.950018   -1.197305    -0.330178   0.678458   \n38130     -0.737408 -0.731842   -0.758071   -1.134158    -0.677850   0.768156   \n38131     -0.770298 -0.596566   -0.805469   -1.163331    -0.788939   0.790951   \n\n       num__lbm  num__lop  num__lgp  num__linv  num__llme  num__l1amhd  \\\n0      0.593453  0.376156 -0.827453  -0.241889   0.781835    -1.007636   \n1      0.593453  0.376156 -0.827453  -0.241889   0.734604    -1.031290   \n2      0.593453  0.376156 -0.827453  -0.241889   0.727109    -1.057992   \n3      0.593453  0.376156 -0.827453  -0.241889   0.702473    -1.072944   \n4      0.593453  0.376156 -0.827453  -0.241889   0.668511    -1.131465   \n...         ...       ...       ...        ...        ...          ...   \n38127 -1.174157  0.360269  1.044217  -0.155434   0.651907     0.272377   \n38128 -1.174157  0.360269  1.044217  -0.155434   0.666802     0.257825   \n38129 -1.174157  0.360269  1.044217  -0.155434   0.575572     0.250911   \n38130 -1.174157  0.360269  1.044217  -0.155434   0.465228     0.231164   \n38131 -0.639719  0.408728  0.798166   1.616227   0.370368     0.115266   \n\n       num__l1MAX  num__l3amhd  num__l3MAX  num__l6amhd  num__l6MAX  \\\n0        4.200045    -0.956844   -0.387515    -0.921897   -0.820376   \n1       -0.150671    -0.953228   -0.921780    -0.965768   -1.034487   \n2       -0.757418    -0.996169    4.119681    -0.940603   -0.913922   \n3        0.459578    -1.020119   -0.181574    -0.932934   -0.369970   \n4       -0.304869    -1.047156   -0.781423    -0.929248   -0.908846   \n...           ...          ...         ...          ...         ...   \n38127   -1.001927     0.302923   -1.133632     0.297189    0.455183   \n38128   -0.876925     0.297320   -0.709048     0.300987   -0.257812   \n38129   -0.936854     0.299867   -1.023153     0.332168   -0.780063   \n38130   -1.068508     0.285134   -0.899572     0.351173   -1.122527   \n38131   -0.739049     0.278133   -0.958819     0.345462   -0.694279   \n\n       num__l12amhd  num__l12MAX  num__l12mom122  num__l12ivol_capm  \\\n0         -0.768271     4.200045       -0.021914          -0.256914   \n1         -0.795208    -0.150671        0.496798          -0.620918   \n2         -0.826711    -0.757418        0.430496          -1.028182   \n3         -0.838137     0.459578        0.472025          -0.917057   \n4         -0.830624    -0.304869        0.309828          -0.260039   \n...             ...          ...             ...                ...   \n38127     -0.388469    -1.001927       -0.650163          -0.777400   \n38128     -0.373562    -0.876925       -0.678496          -0.988693   \n38129     -0.316877    -0.936854       -0.559808          -0.730223   \n38130     -0.059970    -1.068508       -0.969757           1.122606   \n38131      0.132807    -0.739049       -1.145215           0.251409   \n\n       num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  num__l12vol12m  \\\n0             -0.356260        -0.051503      -0.783741       -0.658663   \n1             -0.559273        -0.221874      -0.853985       -0.716271   \n2             -1.067485        -0.270448      -1.033392       -0.805989   \n3             -0.894666        -0.202096      -1.012412       -0.936897   \n4             -0.262048        -0.250413      -0.860940       -0.883314   \n...                 ...              ...            ...             ...   \n38127         -0.696552        -0.681413      -0.487075       -0.650639   \n38128         -1.001747        -0.694511      -0.695724       -0.714824   \n38129         -0.577867        -0.407545      -0.777324       -0.637199   \n38130          1.039763        -0.136708      -0.158711       -0.254123   \n38131          0.161753        -0.281873      -0.037081       -0.181759   \n\n       num__amhd_miss  cat__ind_1.0  cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  \\\n0           -0.300695           0.0           0.0           0.0           0.0   \n1           -0.300695           0.0           0.0           0.0           0.0   \n2           -0.300695           0.0           0.0           0.0           0.0   \n3           -0.300695           0.0           0.0           0.0           0.0   \n4           -0.300695           0.0           0.0           0.0           0.0   \n...               ...           ...           ...           ...           ...   \n38127       -0.300695           0.0           0.0           0.0           0.0   \n38128       -0.300695           0.0           0.0           0.0           0.0   \n38129       -0.300695           0.0           0.0           0.0           0.0   \n38130       -0.300695           0.0           0.0           0.0           0.0   \n38131       -0.300695           0.0           0.0           0.0           0.0   \n\n       cat__ind_5.0  cat__ind_6.0  cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  \\\n0               0.0           0.0           0.0           0.0           0.0   \n1               0.0           0.0           0.0           0.0           0.0   \n2               0.0           0.0           0.0           0.0           0.0   \n3               0.0           0.0           0.0           0.0           0.0   \n4               0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n38127           0.0           0.0           0.0           0.0           0.0   \n38128           0.0           0.0           0.0           0.0           0.0   \n38129           0.0           0.0           0.0           0.0           0.0   \n38130           0.0           0.0           0.0           0.0           0.0   \n38131           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_10.0  cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38127            0.0            0.0            0.0            0.0   \n38128            0.0            0.0            0.0            0.0   \n38129            0.0            0.0            0.0            0.0   \n38130            0.0            0.0            0.0            0.0   \n38131            0.0            0.0            0.0            0.0   \n\n       cat__ind_15.0  cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38127            0.0            0.0            0.0            0.0   \n38128            0.0            0.0            0.0            0.0   \n38129            0.0            0.0            0.0            0.0   \n38130            0.0            0.0            0.0            0.0   \n38131            0.0            0.0            0.0            0.0   \n\n       cat__ind_19.0  cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38127            0.0            0.0            0.0            0.0   \n38128            0.0            0.0            0.0            0.0   \n38129            0.0            0.0            0.0            0.0   \n38130            0.0            0.0            0.0            0.0   \n38131            0.0            0.0            0.0            0.0   \n\n       cat__ind_23.0  cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  \\\n0                0.0            0.0            1.0            0.0   \n1                0.0            0.0            1.0            0.0   \n2                0.0            0.0            1.0            0.0   \n3                0.0            0.0            1.0            0.0   \n4                0.0            0.0            1.0            0.0   \n...              ...            ...            ...            ...   \n38127            0.0            0.0            0.0            0.0   \n38128            0.0            0.0            0.0            0.0   \n38129            0.0            0.0            0.0            0.0   \n38130            0.0            0.0            0.0            0.0   \n38131            0.0            0.0            0.0            0.0   \n\n       cat__ind_27.0  cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38127            0.0            0.0            0.0            0.0   \n38128            0.0            0.0            0.0            0.0   \n38129            0.0            0.0            0.0            0.0   \n38130            0.0            0.0            0.0            0.0   \n38131            0.0            0.0            0.0            0.0   \n\n       cat__ind_31.0  cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38127            0.0            0.0            0.0            0.0   \n38128            0.0            0.0            0.0            0.0   \n38129            0.0            0.0            0.0            0.0   \n38130            0.0            0.0            0.0            0.0   \n38131            0.0            0.0            0.0            0.0   \n\n       cat__ind_35.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38127            0.0            0.0            0.0            0.0   \n38128            0.0            0.0            0.0            0.0   \n38129            0.0            0.0            0.0            0.0   \n38130            0.0            0.0            0.0            0.0   \n38131            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38127            0.0            0.0            0.0            1.0   \n38128            0.0            0.0            0.0            1.0   \n38129            0.0            0.0            0.0            1.0   \n38130            0.0            0.0            0.0            1.0   \n38131            0.0            0.0            0.0            1.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_47.0  cat__ind_48.0  \n0                0.0            0.0            0.0            0.0  \n1                0.0            0.0            0.0            0.0  \n2                0.0            0.0            0.0            0.0  \n3                0.0            0.0            0.0            0.0  \n4                0.0            0.0            0.0            0.0  \n...              ...            ...            ...            ...  \n38127            0.0            0.0            0.0            0.0  \n38128            0.0            0.0            0.0            0.0  \n38129            0.0            0.0            0.0            0.0  \n38130            0.0            0.0            0.0            0.0  \n38131            0.0            0.0            0.0            0.0  \n\n[36869 rows x 82 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.223712</td>\n      <td>0.411154</td>\n      <td>0.517180</td>\n      <td>0.607708</td>\n      <td>-0.658944</td>\n      <td>-0.306924</td>\n      <td>0.378862</td>\n      <td>0.401231</td>\n      <td>-1.035214</td>\n      <td>-0.744129</td>\n      <td>-0.843638</td>\n      <td>-0.665956</td>\n      <td>-0.141294</td>\n      <td>-0.399484</td>\n      <td>0.074478</td>\n      <td>-0.405186</td>\n      <td>0.859184</td>\n      <td>0.593453</td>\n      <td>0.376156</td>\n      <td>-0.827453</td>\n      <td>-0.241889</td>\n      <td>0.781835</td>\n      <td>-1.007636</td>\n      <td>4.200045</td>\n      <td>-0.956844</td>\n      <td>-0.387515</td>\n      <td>-0.921897</td>\n      <td>-0.820376</td>\n      <td>-0.768271</td>\n      <td>4.200045</td>\n      <td>-0.021914</td>\n      <td>-0.256914</td>\n      <td>-0.356260</td>\n      <td>-0.051503</td>\n      <td>-0.783741</td>\n      <td>-0.658663</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.086569</td>\n      <td>0.493729</td>\n      <td>0.517180</td>\n      <td>0.607708</td>\n      <td>-0.658944</td>\n      <td>-0.306924</td>\n      <td>-0.918057</td>\n      <td>0.728623</td>\n      <td>-1.061755</td>\n      <td>-0.018037</td>\n      <td>0.006479</td>\n      <td>-0.834701</td>\n      <td>-0.750279</td>\n      <td>-0.116693</td>\n      <td>0.207577</td>\n      <td>-0.341607</td>\n      <td>0.792355</td>\n      <td>0.593453</td>\n      <td>0.376156</td>\n      <td>-0.827453</td>\n      <td>-0.241889</td>\n      <td>0.734604</td>\n      <td>-1.031290</td>\n      <td>-0.150671</td>\n      <td>-0.953228</td>\n      <td>-0.921780</td>\n      <td>-0.965768</td>\n      <td>-1.034487</td>\n      <td>-0.795208</td>\n      <td>-0.150671</td>\n      <td>0.496798</td>\n      <td>-0.620918</td>\n      <td>-0.559273</td>\n      <td>-0.221874</td>\n      <td>-0.853985</td>\n      <td>-0.716271</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.263268</td>\n      <td>0.316102</td>\n      <td>0.517180</td>\n      <td>0.607708</td>\n      <td>-0.658944</td>\n      <td>-0.306924</td>\n      <td>-0.522888</td>\n      <td>0.431824</td>\n      <td>-1.076616</td>\n      <td>2.215483</td>\n      <td>2.661567</td>\n      <td>-0.882438</td>\n      <td>0.471206</td>\n      <td>1.469296</td>\n      <td>0.627268</td>\n      <td>-0.072902</td>\n      <td>0.757836</td>\n      <td>0.593453</td>\n      <td>0.376156</td>\n      <td>-0.827453</td>\n      <td>-0.241889</td>\n      <td>0.727109</td>\n      <td>-1.057992</td>\n      <td>-0.757418</td>\n      <td>-0.996169</td>\n      <td>4.119681</td>\n      <td>-0.940603</td>\n      <td>-0.913922</td>\n      <td>-0.826711</td>\n      <td>-0.757418</td>\n      <td>0.430496</td>\n      <td>-1.028182</td>\n      <td>-1.067485</td>\n      <td>-0.270448</td>\n      <td>-1.033392</td>\n      <td>-0.805989</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.064674</td>\n      <td>0.845461</td>\n      <td>0.517180</td>\n      <td>0.607708</td>\n      <td>-0.658944</td>\n      <td>-0.306924</td>\n      <td>0.458634</td>\n      <td>0.375338</td>\n      <td>-1.134783</td>\n      <td>-0.657365</td>\n      <td>-0.721118</td>\n      <td>-0.855571</td>\n      <td>-0.296061</td>\n      <td>-0.542764</td>\n      <td>0.625622</td>\n      <td>-0.054416</td>\n      <td>0.784571</td>\n      <td>0.593453</td>\n      <td>0.376156</td>\n      <td>-0.827453</td>\n      <td>-0.241889</td>\n      <td>0.702473</td>\n      <td>-1.072944</td>\n      <td>0.459578</td>\n      <td>-1.020119</td>\n      <td>-0.181574</td>\n      <td>-0.932934</td>\n      <td>-0.369970</td>\n      <td>-0.838137</td>\n      <td>0.459578</td>\n      <td>0.472025</td>\n      <td>-0.917057</td>\n      <td>-0.894666</td>\n      <td>-0.202096</td>\n      <td>-1.012412</td>\n      <td>-0.936897</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.050959</td>\n      <td>0.615673</td>\n      <td>0.517180</td>\n      <td>0.607708</td>\n      <td>-0.658944</td>\n      <td>-0.306924</td>\n      <td>0.963230</td>\n      <td>0.652245</td>\n      <td>-1.214758</td>\n      <td>-0.435597</td>\n      <td>-0.312731</td>\n      <td>-0.959557</td>\n      <td>-0.009505</td>\n      <td>-0.659384</td>\n      <td>0.628517</td>\n      <td>-0.046143</td>\n      <td>0.834576</td>\n      <td>0.593453</td>\n      <td>0.376156</td>\n      <td>-0.827453</td>\n      <td>-0.241889</td>\n      <td>0.668511</td>\n      <td>-1.131465</td>\n      <td>-0.304869</td>\n      <td>-1.047156</td>\n      <td>-0.781423</td>\n      <td>-0.929248</td>\n      <td>-0.908846</td>\n      <td>-0.830624</td>\n      <td>-0.304869</td>\n      <td>0.309828</td>\n      <td>-0.260039</td>\n      <td>-0.262048</td>\n      <td>-0.250413</td>\n      <td>-0.860940</td>\n      <td>-0.883314</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>38127</th>\n      <td>-0.126171</td>\n      <td>-0.573528</td>\n      <td>-0.696348</td>\n      <td>0.515807</td>\n      <td>0.843280</td>\n      <td>1.629851</td>\n      <td>-0.277997</td>\n      <td>-0.094913</td>\n      <td>0.246105</td>\n      <td>-0.404615</td>\n      <td>-0.267449</td>\n      <td>-0.175701</td>\n      <td>-0.870226</td>\n      <td>-0.585051</td>\n      <td>-0.994533</td>\n      <td>-0.252118</td>\n      <td>0.670639</td>\n      <td>-1.174157</td>\n      <td>0.360269</td>\n      <td>1.044217</td>\n      <td>-0.155434</td>\n      <td>0.651907</td>\n      <td>0.272377</td>\n      <td>-1.001927</td>\n      <td>0.302923</td>\n      <td>-1.133632</td>\n      <td>0.297189</td>\n      <td>0.455183</td>\n      <td>-0.388469</td>\n      <td>-1.001927</td>\n      <td>-0.650163</td>\n      <td>-0.777400</td>\n      <td>-0.696552</td>\n      <td>-0.681413</td>\n      <td>-0.487075</td>\n      <td>-0.650639</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38128</th>\n      <td>-0.061265</td>\n      <td>-0.411097</td>\n      <td>-0.696348</td>\n      <td>0.515807</td>\n      <td>0.843280</td>\n      <td>1.629851</td>\n      <td>0.633474</td>\n      <td>-0.228509</td>\n      <td>0.239233</td>\n      <td>-1.193447</td>\n      <td>-1.234102</td>\n      <td>-0.152716</td>\n      <td>-0.930376</td>\n      <td>-1.241484</td>\n      <td>-1.153020</td>\n      <td>-0.259118</td>\n      <td>0.706867</td>\n      <td>-1.174157</td>\n      <td>0.360269</td>\n      <td>1.044217</td>\n      <td>-0.155434</td>\n      <td>0.666802</td>\n      <td>0.257825</td>\n      <td>-0.876925</td>\n      <td>0.297320</td>\n      <td>-0.709048</td>\n      <td>0.300987</td>\n      <td>-0.257812</td>\n      <td>-0.373562</td>\n      <td>-0.876925</td>\n      <td>-0.678496</td>\n      <td>-0.988693</td>\n      <td>-1.001747</td>\n      <td>-0.694511</td>\n      <td>-0.695724</td>\n      <td>-0.714824</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38129</th>\n      <td>-0.225563</td>\n      <td>-0.518573</td>\n      <td>-0.696348</td>\n      <td>0.515807</td>\n      <td>0.843280</td>\n      <td>1.629851</td>\n      <td>-0.415622</td>\n      <td>0.336865</td>\n      <td>0.219605</td>\n      <td>-1.088828</td>\n      <td>-1.079392</td>\n      <td>-0.188757</td>\n      <td>-1.062516</td>\n      <td>-0.950018</td>\n      <td>-1.197305</td>\n      <td>-0.330178</td>\n      <td>0.678458</td>\n      <td>-1.174157</td>\n      <td>0.360269</td>\n      <td>1.044217</td>\n      <td>-0.155434</td>\n      <td>0.575572</td>\n      <td>0.250911</td>\n      <td>-0.936854</td>\n      <td>0.299867</td>\n      <td>-1.023153</td>\n      <td>0.332168</td>\n      <td>-0.780063</td>\n      <td>-0.316877</td>\n      <td>-0.936854</td>\n      <td>-0.559808</td>\n      <td>-0.730223</td>\n      <td>-0.577867</td>\n      <td>-0.407545</td>\n      <td>-0.777324</td>\n      <td>-0.637199</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38130</th>\n      <td>0.049301</td>\n      <td>-0.254967</td>\n      <td>-0.696348</td>\n      <td>0.515807</td>\n      <td>0.843280</td>\n      <td>1.629851</td>\n      <td>1.660382</td>\n      <td>0.737432</td>\n      <td>0.104408</td>\n      <td>-0.977658</td>\n      <td>-0.867269</td>\n      <td>-0.737408</td>\n      <td>-0.731842</td>\n      <td>-0.758071</td>\n      <td>-1.134158</td>\n      <td>-0.677850</td>\n      <td>0.768156</td>\n      <td>-1.174157</td>\n      <td>0.360269</td>\n      <td>1.044217</td>\n      <td>-0.155434</td>\n      <td>0.465228</td>\n      <td>0.231164</td>\n      <td>-1.068508</td>\n      <td>0.285134</td>\n      <td>-0.899572</td>\n      <td>0.351173</td>\n      <td>-1.122527</td>\n      <td>-0.059970</td>\n      <td>-1.068508</td>\n      <td>-0.969757</td>\n      <td>1.122606</td>\n      <td>1.039763</td>\n      <td>-0.136708</td>\n      <td>-0.158711</td>\n      <td>-0.254123</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38131</th>\n      <td>0.117787</td>\n      <td>-0.151727</td>\n      <td>-0.370949</td>\n      <td>-0.051501</td>\n      <td>0.347610</td>\n      <td>1.701109</td>\n      <td>0.398740</td>\n      <td>1.996235</td>\n      <td>-0.045837</td>\n      <td>-0.737308</td>\n      <td>-0.720663</td>\n      <td>-0.770298</td>\n      <td>-0.596566</td>\n      <td>-0.805469</td>\n      <td>-1.163331</td>\n      <td>-0.788939</td>\n      <td>0.790951</td>\n      <td>-0.639719</td>\n      <td>0.408728</td>\n      <td>0.798166</td>\n      <td>1.616227</td>\n      <td>0.370368</td>\n      <td>0.115266</td>\n      <td>-0.739049</td>\n      <td>0.278133</td>\n      <td>-0.958819</td>\n      <td>0.345462</td>\n      <td>-0.694279</td>\n      <td>0.132807</td>\n      <td>-0.739049</td>\n      <td>-1.145215</td>\n      <td>0.251409</td>\n      <td>0.161753</td>\n      <td>-0.281873</td>\n      <td>-0.037081</td>\n      <td>-0.181759</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>36869 rows  82 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"0       -13.789167\n1       -12.370234\n2         3.534886\n3         3.438924\n4         4.611103\n           ...    \n38127     0.887722\n38128    -8.021688\n38129    22.542507\n38130     6.212760\n38131    -0.103652\nName: RET, Length: 36869, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"### try SnNN  \n\nneurons_base = 4\ndropout_rate = 0.00\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*32, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train.shape[1:]),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*32, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*16, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*16, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*16, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*16, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"), \n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"), \n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"), \n    tf.keras.layers.AlphaDropout(dropout_rate),\n    Dense(1)])\n\n# tf.keras.layers.AlphaDropout(0.4) if needed\nprint(model_snn.count_params())\n#model_snn.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-26T01:41:46.474534Z","iopub.execute_input":"2022-08-26T01:41:46.474897Z","iopub.status.idle":"2022-08-26T01:41:46.705898Z","shell.execute_reply.started":"2022-08-26T01:41:46.474866Z","shell.execute_reply":"2022-08-26T01:41:46.704827Z"},"trusted":true},"execution_count":249,"outputs":[{"name":"stdout","text":"54917\nModel: \"sequential_69\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_1355 (Dense)           (None, 128)               10624     \n_________________________________________________________________\nalpha_dropout_358 (AlphaDrop (None, 128)               0         \n_________________________________________________________________\ndense_1356 (Dense)           (None, 128)               16512     \n_________________________________________________________________\nalpha_dropout_359 (AlphaDrop (None, 128)               0         \n_________________________________________________________________\ndense_1357 (Dense)           (None, 64)                8256      \n_________________________________________________________________\nalpha_dropout_360 (AlphaDrop (None, 64)                0         \n_________________________________________________________________\ndense_1358 (Dense)           (None, 64)                4160      \n_________________________________________________________________\nalpha_dropout_361 (AlphaDrop (None, 64)                0         \n_________________________________________________________________\ndense_1359 (Dense)           (None, 64)                4160      \n_________________________________________________________________\nalpha_dropout_362 (AlphaDrop (None, 64)                0         \n_________________________________________________________________\ndense_1360 (Dense)           (None, 64)                4160      \n_________________________________________________________________\nalpha_dropout_363 (AlphaDrop (None, 64)                0         \n_________________________________________________________________\ndense_1361 (Dense)           (None, 32)                2080      \n_________________________________________________________________\nalpha_dropout_364 (AlphaDrop (None, 32)                0         \n_________________________________________________________________\ndense_1362 (Dense)           (None, 32)                1056      \n_________________________________________________________________\nalpha_dropout_365 (AlphaDrop (None, 32)                0         \n_________________________________________________________________\ndense_1363 (Dense)           (None, 32)                1056      \n_________________________________________________________________\nalpha_dropout_366 (AlphaDrop (None, 32)                0         \n_________________________________________________________________\ndense_1364 (Dense)           (None, 32)                1056      \n_________________________________________________________________\nalpha_dropout_367 (AlphaDrop (None, 32)                0         \n_________________________________________________________________\ndense_1365 (Dense)           (None, 16)                528       \n_________________________________________________________________\nalpha_dropout_368 (AlphaDrop (None, 16)                0         \n_________________________________________________________________\ndense_1366 (Dense)           (None, 16)                272       \n_________________________________________________________________\nalpha_dropout_369 (AlphaDrop (None, 16)                0         \n_________________________________________________________________\ndense_1367 (Dense)           (None, 16)                272       \n_________________________________________________________________\nalpha_dropout_370 (AlphaDrop (None, 16)                0         \n_________________________________________________________________\ndense_1368 (Dense)           (None, 16)                272       \n_________________________________________________________________\nalpha_dropout_371 (AlphaDrop (None, 16)                0         \n_________________________________________________________________\ndense_1369 (Dense)           (None, 8)                 136       \n_________________________________________________________________\nalpha_dropout_372 (AlphaDrop (None, 8)                 0         \n_________________________________________________________________\ndense_1370 (Dense)           (None, 8)                 72        \n_________________________________________________________________\nalpha_dropout_373 (AlphaDrop (None, 8)                 0         \n_________________________________________________________________\ndense_1371 (Dense)           (None, 8)                 72        \n_________________________________________________________________\nalpha_dropout_374 (AlphaDrop (None, 8)                 0         \n_________________________________________________________________\ndense_1372 (Dense)           (None, 8)                 72        \n_________________________________________________________________\nalpha_dropout_375 (AlphaDrop (None, 8)                 0         \n_________________________________________________________________\ndense_1373 (Dense)           (None, 4)                 36        \n_________________________________________________________________\nalpha_dropout_376 (AlphaDrop (None, 4)                 0         \n_________________________________________________________________\ndense_1374 (Dense)           (None, 4)                 20        \n_________________________________________________________________\nalpha_dropout_377 (AlphaDrop (None, 4)                 0         \n_________________________________________________________________\ndense_1375 (Dense)           (None, 4)                 20        \n_________________________________________________________________\nalpha_dropout_378 (AlphaDrop (None, 4)                 0         \n_________________________________________________________________\ndense_1376 (Dense)           (None, 4)                 20        \n_________________________________________________________________\nalpha_dropout_379 (AlphaDrop (None, 4)                 0         \n_________________________________________________________________\ndense_1377 (Dense)           (None, 1)                 5         \n=================================================================\nTotal params: 54,917\nTrainable params: 54,917\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"neurons_base = 16\ndropout_rate = 0.05\n# n_b=8 was ok with small overfit.\n# n_b=32 starts clearly overfitting. \n# 128 fits clearly slower than 64 and becomes somewhat unstable. regularization could make it work, but i see no reason to go wider.\n# 64 seems to have nice balance of flexibility and runtime, but its variance may be too large. dropout makes variance vene worse.\n# 6 hidden layers is probably most this architecture can hold\n\n# in this framework the optimal model seems to have width of 16 or 32, somehow regularized. try l1/l2?\n# w32 can take at most 0.03 dropout.\n# w16 looks good w/o dropout.\n\n# more general point:\n# main drawback of dropout is in incresing variance\n# for textbook problems with high s/n ratio (e.g., mnist) this may be ok.\n# for application like this with very low s/n ratio dropout may be a bad idea.\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*32, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train.shape[1:]),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*16, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    Dense(1)])\n\nprint(model_snn.count_params())","metadata":{"execution":{"iopub.status.busy":"2022-08-26T03:07:17.535651Z","iopub.execute_input":"2022-08-26T03:07:17.536005Z","iopub.status.idle":"2022-08-26T03:07:17.609987Z","shell.execute_reply.started":"2022-08-26T03:07:17.535974Z","shell.execute_reply":"2022-08-26T03:07:17.608898Z"},"trusted":true},"execution_count":533,"outputs":[{"name":"stdout","text":"217601\n","output_type":"stream"}]},{"cell_type":"code","source":"neurons_base = 16\ndropout_rate = 0.05\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*27, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train.shape[1:]),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*9, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*3, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    Dense(1)])\n\nprint(model_snn.count_params())\n\n# similar problem as before: model seems ok in terms of flexibility and variance, but adding dropout breaks it before i can fix overfitting.\n# the solution is to either use smaller models or to use laternative regularizers (which do not increase variance.)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T03:24:25.611364Z","iopub.execute_input":"2022-08-26T03:24:25.611882Z","iopub.status.idle":"2022-08-26T03:24:25.687915Z","shell.execute_reply.started":"2022-08-26T03:24:25.611849Z","shell.execute_reply":"2022-08-26T03:24:25.686825Z"},"trusted":true},"execution_count":620,"outputs":[{"name":"stdout","text":"105969\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping50 = EarlyStopping(patience=50, restore_best_weights=True)\ntime1 = time.time()\noptimizer_adam = tf.keras.optimizers.Adam()\nmodel_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_snn.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=2, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nprint([r2_score(y_train, model_snn.predict(X_train)), \n       r2_score(y_val, model_snn.predict(X_val)),\n       r2_score(y_test, model_snn.predict(X_test))])\nprint(time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-26T03:24:25.828633Z","iopub.execute_input":"2022-08-26T03:24:25.828972Z","iopub.status.idle":"2022-08-26T03:24:33.208843Z","shell.execute_reply.started":"2022-08-26T03:24:25.828944Z","shell.execute_reply":"2022-08-26T03:24:33.207959Z"},"trusted":true},"execution_count":621,"outputs":[{"name":"stdout","text":"Epoch 1/1000\n19/19 - 1s - loss: 93.4900 - mean_squared_error: 93.4900 - val_loss: 101.1570 - val_mean_squared_error: 101.1570\nEpoch 2/1000\n19/19 - 0s - loss: 92.9929 - mean_squared_error: 92.9929 - val_loss: 98.8929 - val_mean_squared_error: 98.8929\nEpoch 3/1000\n19/19 - 0s - loss: 93.0172 - mean_squared_error: 93.0172 - val_loss: 99.9819 - val_mean_squared_error: 99.9819\nEpoch 4/1000\n19/19 - 0s - loss: 92.8174 - mean_squared_error: 92.8174 - val_loss: 100.5657 - val_mean_squared_error: 100.5657\nEpoch 5/1000\n19/19 - 0s - loss: 92.6527 - mean_squared_error: 92.6527 - val_loss: 100.6618 - val_mean_squared_error: 100.6618\nEpoch 6/1000\n19/19 - 0s - loss: 92.7658 - mean_squared_error: 92.7658 - val_loss: 101.9710 - val_mean_squared_error: 101.9710\nEpoch 7/1000\n19/19 - 0s - loss: 92.6335 - mean_squared_error: 92.6335 - val_loss: 100.8951 - val_mean_squared_error: 100.8951\nEpoch 8/1000\n19/19 - 0s - loss: 92.5593 - mean_squared_error: 92.5593 - val_loss: 100.1218 - val_mean_squared_error: 100.1218\nEpoch 9/1000\n19/19 - 0s - loss: 92.4623 - mean_squared_error: 92.4623 - val_loss: 99.5566 - val_mean_squared_error: 99.5566\nEpoch 10/1000\n19/19 - 0s - loss: 92.3034 - mean_squared_error: 92.3034 - val_loss: 99.8197 - val_mean_squared_error: 99.8197\nEpoch 11/1000\n19/19 - 0s - loss: 92.4351 - mean_squared_error: 92.4351 - val_loss: 99.9211 - val_mean_squared_error: 99.9211\nEpoch 12/1000\n19/19 - 0s - loss: 92.4685 - mean_squared_error: 92.4685 - val_loss: 100.0559 - val_mean_squared_error: 100.0559\nEpoch 13/1000\n19/19 - 0s - loss: 92.4976 - mean_squared_error: 92.4976 - val_loss: 98.4774 - val_mean_squared_error: 98.4774\nEpoch 14/1000\n19/19 - 0s - loss: 92.4031 - mean_squared_error: 92.4031 - val_loss: 97.0386 - val_mean_squared_error: 97.0386\nEpoch 15/1000\n19/19 - 0s - loss: 92.3587 - mean_squared_error: 92.3587 - val_loss: 96.2059 - val_mean_squared_error: 96.2059\nEpoch 16/1000\n19/19 - 0s - loss: 92.3845 - mean_squared_error: 92.3845 - val_loss: 97.6543 - val_mean_squared_error: 97.6543\nEpoch 17/1000\n19/19 - 0s - loss: 92.2587 - mean_squared_error: 92.2587 - val_loss: 98.2645 - val_mean_squared_error: 98.2645\nEpoch 18/1000\n19/19 - 0s - loss: 92.4192 - mean_squared_error: 92.4192 - val_loss: 98.5150 - val_mean_squared_error: 98.5150\nEpoch 19/1000\n19/19 - 0s - loss: 92.3333 - mean_squared_error: 92.3333 - val_loss: 98.2889 - val_mean_squared_error: 98.2889\nEpoch 20/1000\n19/19 - 0s - loss: 92.1457 - mean_squared_error: 92.1457 - val_loss: 98.7730 - val_mean_squared_error: 98.7730\nEpoch 21/1000\n19/19 - 0s - loss: 92.0632 - mean_squared_error: 92.0632 - val_loss: 98.8365 - val_mean_squared_error: 98.8365\nEpoch 22/1000\n19/19 - 0s - loss: 92.1027 - mean_squared_error: 92.1027 - val_loss: 98.4907 - val_mean_squared_error: 98.4907\nEpoch 23/1000\n19/19 - 0s - loss: 91.8507 - mean_squared_error: 91.8507 - val_loss: 99.1450 - val_mean_squared_error: 99.1450\nEpoch 24/1000\n19/19 - 0s - loss: 91.9226 - mean_squared_error: 91.9226 - val_loss: 97.4268 - val_mean_squared_error: 97.4268\nEpoch 25/1000\n19/19 - 0s - loss: 91.9841 - mean_squared_error: 91.9841 - val_loss: 98.5098 - val_mean_squared_error: 98.5098\nEpoch 26/1000\n19/19 - 0s - loss: 91.8306 - mean_squared_error: 91.8306 - val_loss: 97.5739 - val_mean_squared_error: 97.5739\nEpoch 27/1000\n19/19 - 0s - loss: 91.9066 - mean_squared_error: 91.9066 - val_loss: 97.8510 - val_mean_squared_error: 97.8510\nEpoch 28/1000\n19/19 - 0s - loss: 91.8649 - mean_squared_error: 91.8649 - val_loss: 97.7166 - val_mean_squared_error: 97.7166\nEpoch 29/1000\n19/19 - 0s - loss: 91.8256 - mean_squared_error: 91.8256 - val_loss: 98.1233 - val_mean_squared_error: 98.1233\nEpoch 30/1000\n19/19 - 0s - loss: 91.8328 - mean_squared_error: 91.8328 - val_loss: 99.7863 - val_mean_squared_error: 99.7863\nEpoch 31/1000\n19/19 - 0s - loss: 92.0066 - mean_squared_error: 92.0066 - val_loss: 99.2473 - val_mean_squared_error: 99.2473\nEpoch 32/1000\n19/19 - 0s - loss: 91.8749 - mean_squared_error: 91.8749 - val_loss: 99.2222 - val_mean_squared_error: 99.2222\nEpoch 33/1000\n19/19 - 0s - loss: 91.8968 - mean_squared_error: 91.8968 - val_loss: 99.1090 - val_mean_squared_error: 99.1090\nEpoch 34/1000\n19/19 - 0s - loss: 91.8650 - mean_squared_error: 91.8650 - val_loss: 97.8041 - val_mean_squared_error: 97.8041\nEpoch 35/1000\n19/19 - 0s - loss: 91.9661 - mean_squared_error: 91.9661 - val_loss: 97.5195 - val_mean_squared_error: 97.5195\nEpoch 36/1000\n19/19 - 0s - loss: 91.9011 - mean_squared_error: 91.9011 - val_loss: 96.7107 - val_mean_squared_error: 96.7107\nEpoch 37/1000\n19/19 - 0s - loss: 91.8304 - mean_squared_error: 91.8304 - val_loss: 97.2574 - val_mean_squared_error: 97.2574\nEpoch 38/1000\n19/19 - 0s - loss: 91.8302 - mean_squared_error: 91.8302 - val_loss: 98.0305 - val_mean_squared_error: 98.0305\nEpoch 39/1000\n19/19 - 0s - loss: 91.7999 - mean_squared_error: 91.7999 - val_loss: 100.9005 - val_mean_squared_error: 100.9005\nEpoch 40/1000\n19/19 - 0s - loss: 91.5800 - mean_squared_error: 91.5800 - val_loss: 100.5909 - val_mean_squared_error: 100.5909\nEpoch 41/1000\n19/19 - 0s - loss: 91.7488 - mean_squared_error: 91.7488 - val_loss: 99.7978 - val_mean_squared_error: 99.7978\nEpoch 42/1000\n19/19 - 0s - loss: 91.6552 - mean_squared_error: 91.6552 - val_loss: 99.2869 - val_mean_squared_error: 99.2869\nEpoch 43/1000\n19/19 - 0s - loss: 91.7415 - mean_squared_error: 91.7415 - val_loss: 99.6637 - val_mean_squared_error: 99.6637\nEpoch 44/1000\n19/19 - 0s - loss: 91.6001 - mean_squared_error: 91.6001 - val_loss: 99.2904 - val_mean_squared_error: 99.2904\nEpoch 45/1000\n19/19 - 0s - loss: 91.6853 - mean_squared_error: 91.6853 - val_loss: 98.4525 - val_mean_squared_error: 98.4525\nEpoch 46/1000\n19/19 - 0s - loss: 91.6434 - mean_squared_error: 91.6434 - val_loss: 99.6453 - val_mean_squared_error: 99.6453\nEpoch 47/1000\n19/19 - 0s - loss: 91.7109 - mean_squared_error: 91.7109 - val_loss: 99.2224 - val_mean_squared_error: 99.2224\nEpoch 48/1000\n19/19 - 0s - loss: 91.7854 - mean_squared_error: 91.7854 - val_loss: 98.2533 - val_mean_squared_error: 98.2533\nEpoch 49/1000\n19/19 - 0s - loss: 91.6976 - mean_squared_error: 91.6976 - val_loss: 98.5005 - val_mean_squared_error: 98.5005\nEpoch 50/1000\n19/19 - 0s - loss: 91.6366 - mean_squared_error: 91.6366 - val_loss: 98.3596 - val_mean_squared_error: 98.3596\nEpoch 51/1000\n19/19 - 0s - loss: 91.6720 - mean_squared_error: 91.6720 - val_loss: 98.9345 - val_mean_squared_error: 98.9345\nEpoch 52/1000\n19/19 - 0s - loss: 91.6877 - mean_squared_error: 91.6877 - val_loss: 101.2403 - val_mean_squared_error: 101.2403\nEpoch 53/1000\n19/19 - 0s - loss: 91.5429 - mean_squared_error: 91.5429 - val_loss: 99.7437 - val_mean_squared_error: 99.7437\nEpoch 54/1000\n19/19 - 0s - loss: 91.5559 - mean_squared_error: 91.5559 - val_loss: 98.5031 - val_mean_squared_error: 98.5031\nEpoch 55/1000\n19/19 - 0s - loss: 91.5445 - mean_squared_error: 91.5445 - val_loss: 100.6325 - val_mean_squared_error: 100.6325\nEpoch 56/1000\n19/19 - 0s - loss: 91.4858 - mean_squared_error: 91.4858 - val_loss: 99.7744 - val_mean_squared_error: 99.7744\nEpoch 57/1000\n19/19 - 0s - loss: 91.4912 - mean_squared_error: 91.4912 - val_loss: 98.7508 - val_mean_squared_error: 98.7508\nEpoch 58/1000\n19/19 - 0s - loss: 91.4497 - mean_squared_error: 91.4497 - val_loss: 97.7598 - val_mean_squared_error: 97.7598\nEpoch 59/1000\n19/19 - 0s - loss: 91.5600 - mean_squared_error: 91.5600 - val_loss: 97.9404 - val_mean_squared_error: 97.9404\nEpoch 60/1000\n19/19 - 0s - loss: 91.6082 - mean_squared_error: 91.6082 - val_loss: 97.9450 - val_mean_squared_error: 97.9450\nEpoch 61/1000\n19/19 - 0s - loss: 91.5259 - mean_squared_error: 91.5259 - val_loss: 98.4774 - val_mean_squared_error: 98.4774\nEpoch 62/1000\n19/19 - 0s - loss: 91.4856 - mean_squared_error: 91.4856 - val_loss: 98.5466 - val_mean_squared_error: 98.5466\nEpoch 63/1000\n19/19 - 0s - loss: 91.4937 - mean_squared_error: 91.4937 - val_loss: 99.0356 - val_mean_squared_error: 99.0356\nEpoch 64/1000\n19/19 - 0s - loss: 91.4378 - mean_squared_error: 91.4378 - val_loss: 98.6926 - val_mean_squared_error: 98.6926\nEpoch 65/1000\n19/19 - 0s - loss: 91.3951 - mean_squared_error: 91.3951 - val_loss: 98.5815 - val_mean_squared_error: 98.5815\nMinimum Validation Loss: 96.2059\n[-0.014347137759581408, 0.012646640240421614, -0.01723054271991442]\n7.273446798324585\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val","metadata":{"execution":{"iopub.status.busy":"2022-08-26T01:13:16.368464Z","iopub.execute_input":"2022-08-26T01:13:16.368916Z","iopub.status.idle":"2022-08-26T01:13:16.475652Z","shell.execute_reply.started":"2022-08-26T01:13:16.368876Z","shell.execute_reply":"2022-08-26T01:13:16.474574Z"},"trusted":true},"execution_count":180,"outputs":[{"execution_count":180,"output_type":"execute_result","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n36       -0.612479     0.093461  0.719846 -0.253931 -0.987864 -0.395222   \n73       -1.098087    -1.056204  1.111671 -1.729057 -0.621714 -0.126246   \n98       -1.017334    -0.761409  1.163306 -0.309223 -0.278237 -0.481448   \n135      -1.100278     0.032686  1.092450 -0.504720 -0.889660 -0.659544   \n172      -0.760740     0.149421  1.087237 -0.416474 -0.963139 -0.602710   \n...            ...          ...       ...       ...       ...       ...   \n38054    -0.258451    -0.233632  0.019828  0.976392  2.291757  0.425113   \n38056    -0.258451    -0.233632 -0.186698  1.540608  2.291757 -0.698786   \n38093    -0.437714    -0.345022  0.606545  0.148160 -0.155946 -0.649777   \n38095    -0.258451    -0.675738  1.313995 -0.315314 -0.270157 -0.627896   \n38132     0.003116    -0.392549 -0.370949 -0.051501  0.347610  1.701109   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n36      -0.839787     1.311527  -0.698449       -1.107615      -1.085305   \n73       0.660355    -0.027070   1.122396        0.783425       0.489108   \n98      -0.022911     0.048652   0.852550       -0.339425      -0.179673   \n135     -1.199866     1.311926  -0.833087       -0.445394      -0.390569   \n172     -0.484662     1.991333  -1.277050       -0.865144      -0.816650   \n...           ...          ...        ...             ...            ...   \n38054    1.780885     2.386609   0.034506        0.764370       0.974170   \n38056    0.382980     0.095169   1.190230       -0.216883      -0.317184   \n38093   -0.933260     0.896756   0.037911        0.427254       0.575023   \n38095    0.020639     0.898530   0.034506       -0.020632       0.096871   \n38132   -0.494689     1.615490  -0.272370       -1.272487      -1.177594   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  num__size  \\\n36        -0.948009 -1.107851   -1.061303   -1.091215    -1.263500   0.728270   \n73         1.326108  0.371155    0.568497    0.759708     1.631631  -0.854679   \n98        -0.820050 -0.097148   -0.476122   -0.802269    -0.445159  -0.254341   \n135        0.285349 -0.821593   -0.403480   -0.736434    -0.756806   0.622390   \n172        0.219649 -0.739330   -0.826352   -0.666444    -0.748998   1.354701   \n...             ...       ...         ...         ...          ...        ...   \n38054      0.139572  0.694449    0.610883    0.382628     0.546210  -0.887824   \n38056     -0.743497  0.073921    0.066257   -0.497630    -0.741324  -0.563841   \n38093      0.032663  0.324143    0.237936   -0.819750    -0.842935   0.388448   \n38095     -0.536320  0.534275   -0.119711    0.422353     0.381169  -1.448784   \n38132     -0.848577 -1.107851   -1.252635   -1.201416    -1.129014   0.758103   \n\n       num__lbm  num__lop  num__lgp  num__linv  num__llme  num__l1amhd  \\\n36     0.542291 -0.049710 -0.915318  -0.071219   0.559899    -0.568393   \n73     0.452220 -0.846728 -0.549495   1.558847  -1.028437     1.177818   \n98     0.442542  0.724077  0.114054  -0.820425  -0.272731     0.854642   \n135    1.436114 -0.562715 -0.848914  -0.316655   0.449012    -0.691659   \n172    0.958646 -0.268558 -0.894380  -0.716941   1.088137    -1.165634   \n...         ...       ...       ...        ...        ...          ...   \n38054 -0.313723  1.176995  2.242091  -0.463802  -1.444274     0.035808   \n38056 -0.767156  1.712398  2.242091  -0.450405  -0.640354     1.221661   \n38093  0.233137  0.349421 -0.002480  -0.344220   0.269507     0.076310   \n38095  0.737779  0.907250  0.061940   0.175870  -1.691655     0.035808   \n38132 -0.639719  0.408728  0.798166   1.616227   0.446894    -0.035893   \n\n       num__l1MAX  num__l3amhd  num__l3MAX  num__l6amhd  num__l6MAX  \\\n36      -1.113677    -0.409735    0.517731    -0.304907   -0.783420   \n73      -0.535184     1.247437    3.053172     1.301634    0.122334   \n98       0.216873     0.845819   -0.845943     0.993837    1.109063   \n135     -0.941756    -0.533623   -0.683832    -0.445939   -0.706069   \n172      0.158669    -1.071988   -0.215578    -0.998049   -0.794888   \n...           ...          ...         ...          ...         ...   \n38054    0.602841     0.035425    0.413269     0.038960    0.961130   \n38056   -0.898580     1.286012   -0.777083     1.334271   -0.495719   \n38093   -1.043675     0.198068    0.354022     0.265541   -0.833966   \n38095   -0.917892     0.035425   -0.124776     0.038960    3.304932   \n38132   -0.604270     0.258139   -1.088977     0.348058   -1.011094   \n\n       num__l12amhd  num__l12MAX  num__l12mom122  num__l12ivol_capm  \\\n36        -0.352128    -1.113677       -0.411241          -0.760507   \n73         0.986915    -0.535184       -1.636807           1.061128   \n98         0.993492     0.216873       -1.113882          -0.611153   \n135       -0.499549    -0.941756       -0.972298           0.549475   \n172       -1.066169     0.158669       -1.147174           0.641499   \n...             ...          ...             ...                ...   \n38054      0.054105     0.602841       -0.431152          -0.840754   \n38056      1.148370    -0.898580       -0.747516          -1.003954   \n38093      0.096022    -1.043675       -0.906791           0.016794   \n38095      0.054105    -0.917892       -1.511419          -0.271934   \n38132      0.301280    -0.604270       -1.245630           1.331538   \n\n       num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  num__l12vol12m  \\\n36            -0.692327        -0.586912      -0.825575       -1.116346   \n73             0.793389         1.587190       2.139835        1.972031   \n98            -0.402149        -1.157941      -0.417698       -0.258807   \n135            0.642682         0.364889       0.145648       -0.157740   \n172            0.811720         0.080118      -0.033113       -0.319384   \n...                 ...              ...            ...             ...   \n38054         -0.900806        -1.529243       0.276725       -0.210009   \n38056         -0.850037        -0.687256      -0.007706       -0.210009   \n38093         -0.018488        -0.554340      -0.401027       -0.708172   \n38095         -0.147834        -0.414281       0.584731        0.329961   \n38132          0.363086        -0.144910       0.399951        0.050476   \n\n       num__amhd_miss  cat__ind_1.0  cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  \\\n36          -0.300695           0.0           0.0           0.0           0.0   \n73          -0.300695           0.0           0.0           0.0           0.0   \n98          -0.300695           0.0           0.0           0.0           0.0   \n135         -0.300695           0.0           0.0           0.0           0.0   \n172         -0.300695           0.0           0.0           0.0           0.0   \n...               ...           ...           ...           ...           ...   \n38054        3.325624           0.0           0.0           0.0           0.0   \n38056       -0.300695           0.0           0.0           0.0           0.0   \n38093       -0.300695           0.0           0.0           0.0           0.0   \n38095        3.325624           0.0           0.0           0.0           0.0   \n38132       -0.300695           0.0           0.0           0.0           0.0   \n\n       cat__ind_5.0  cat__ind_6.0  cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  \\\n36              0.0           0.0           0.0           0.0           0.0   \n73              0.0           0.0           0.0           0.0           0.0   \n98              0.0           0.0           0.0           0.0           0.0   \n135             0.0           0.0           0.0           0.0           0.0   \n172             0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n38054           0.0           0.0           0.0           0.0           1.0   \n38056           0.0           0.0           0.0           0.0           1.0   \n38093           0.0           0.0           0.0           0.0           0.0   \n38095           0.0           0.0           0.0           0.0           1.0   \n38132           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_10.0  cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            1.0   \n172              0.0            0.0            0.0            1.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_15.0  cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            1.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_19.0  cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            1.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_23.0  cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  \\\n36               0.0            0.0            1.0            0.0   \n73               1.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_27.0  cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_31.0  cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_35.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n36               0.0            0.0            0.0            0.0   \n73               0.0            0.0            0.0            0.0   \n98               0.0            0.0            0.0            0.0   \n135              0.0            0.0            0.0            0.0   \n172              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n38054            0.0            0.0            0.0            0.0   \n38056            0.0            0.0            0.0            0.0   \n38093            0.0            0.0            0.0            0.0   \n38095            0.0            0.0            0.0            0.0   \n38132            0.0            0.0            0.0            1.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_47.0  cat__ind_48.0  \n36               0.0            0.0            0.0            0.0  \n73               0.0            0.0            0.0            0.0  \n98               0.0            0.0            0.0            0.0  \n135              0.0            0.0            0.0            0.0  \n172              0.0            0.0            0.0            0.0  \n...              ...            ...            ...            ...  \n38054            0.0            0.0            0.0            0.0  \n38056            0.0            0.0            0.0            0.0  \n38093            0.0            0.0            0.0            0.0  \n38095            0.0            0.0            0.0            0.0  \n38132            0.0            0.0            0.0            0.0  \n\n[1264 rows x 82 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36</th>\n      <td>-0.612479</td>\n      <td>0.093461</td>\n      <td>0.719846</td>\n      <td>-0.253931</td>\n      <td>-0.987864</td>\n      <td>-0.395222</td>\n      <td>-0.839787</td>\n      <td>1.311527</td>\n      <td>-0.698449</td>\n      <td>-1.107615</td>\n      <td>-1.085305</td>\n      <td>-0.948009</td>\n      <td>-1.107851</td>\n      <td>-1.061303</td>\n      <td>-1.091215</td>\n      <td>-1.263500</td>\n      <td>0.728270</td>\n      <td>0.542291</td>\n      <td>-0.049710</td>\n      <td>-0.915318</td>\n      <td>-0.071219</td>\n      <td>0.559899</td>\n      <td>-0.568393</td>\n      <td>-1.113677</td>\n      <td>-0.409735</td>\n      <td>0.517731</td>\n      <td>-0.304907</td>\n      <td>-0.783420</td>\n      <td>-0.352128</td>\n      <td>-1.113677</td>\n      <td>-0.411241</td>\n      <td>-0.760507</td>\n      <td>-0.692327</td>\n      <td>-0.586912</td>\n      <td>-0.825575</td>\n      <td>-1.116346</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>-1.098087</td>\n      <td>-1.056204</td>\n      <td>1.111671</td>\n      <td>-1.729057</td>\n      <td>-0.621714</td>\n      <td>-0.126246</td>\n      <td>0.660355</td>\n      <td>-0.027070</td>\n      <td>1.122396</td>\n      <td>0.783425</td>\n      <td>0.489108</td>\n      <td>1.326108</td>\n      <td>0.371155</td>\n      <td>0.568497</td>\n      <td>0.759708</td>\n      <td>1.631631</td>\n      <td>-0.854679</td>\n      <td>0.452220</td>\n      <td>-0.846728</td>\n      <td>-0.549495</td>\n      <td>1.558847</td>\n      <td>-1.028437</td>\n      <td>1.177818</td>\n      <td>-0.535184</td>\n      <td>1.247437</td>\n      <td>3.053172</td>\n      <td>1.301634</td>\n      <td>0.122334</td>\n      <td>0.986915</td>\n      <td>-0.535184</td>\n      <td>-1.636807</td>\n      <td>1.061128</td>\n      <td>0.793389</td>\n      <td>1.587190</td>\n      <td>2.139835</td>\n      <td>1.972031</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>-1.017334</td>\n      <td>-0.761409</td>\n      <td>1.163306</td>\n      <td>-0.309223</td>\n      <td>-0.278237</td>\n      <td>-0.481448</td>\n      <td>-0.022911</td>\n      <td>0.048652</td>\n      <td>0.852550</td>\n      <td>-0.339425</td>\n      <td>-0.179673</td>\n      <td>-0.820050</td>\n      <td>-0.097148</td>\n      <td>-0.476122</td>\n      <td>-0.802269</td>\n      <td>-0.445159</td>\n      <td>-0.254341</td>\n      <td>0.442542</td>\n      <td>0.724077</td>\n      <td>0.114054</td>\n      <td>-0.820425</td>\n      <td>-0.272731</td>\n      <td>0.854642</td>\n      <td>0.216873</td>\n      <td>0.845819</td>\n      <td>-0.845943</td>\n      <td>0.993837</td>\n      <td>1.109063</td>\n      <td>0.993492</td>\n      <td>0.216873</td>\n      <td>-1.113882</td>\n      <td>-0.611153</td>\n      <td>-0.402149</td>\n      <td>-1.157941</td>\n      <td>-0.417698</td>\n      <td>-0.258807</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>135</th>\n      <td>-1.100278</td>\n      <td>0.032686</td>\n      <td>1.092450</td>\n      <td>-0.504720</td>\n      <td>-0.889660</td>\n      <td>-0.659544</td>\n      <td>-1.199866</td>\n      <td>1.311926</td>\n      <td>-0.833087</td>\n      <td>-0.445394</td>\n      <td>-0.390569</td>\n      <td>0.285349</td>\n      <td>-0.821593</td>\n      <td>-0.403480</td>\n      <td>-0.736434</td>\n      <td>-0.756806</td>\n      <td>0.622390</td>\n      <td>1.436114</td>\n      <td>-0.562715</td>\n      <td>-0.848914</td>\n      <td>-0.316655</td>\n      <td>0.449012</td>\n      <td>-0.691659</td>\n      <td>-0.941756</td>\n      <td>-0.533623</td>\n      <td>-0.683832</td>\n      <td>-0.445939</td>\n      <td>-0.706069</td>\n      <td>-0.499549</td>\n      <td>-0.941756</td>\n      <td>-0.972298</td>\n      <td>0.549475</td>\n      <td>0.642682</td>\n      <td>0.364889</td>\n      <td>0.145648</td>\n      <td>-0.157740</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>172</th>\n      <td>-0.760740</td>\n      <td>0.149421</td>\n      <td>1.087237</td>\n      <td>-0.416474</td>\n      <td>-0.963139</td>\n      <td>-0.602710</td>\n      <td>-0.484662</td>\n      <td>1.991333</td>\n      <td>-1.277050</td>\n      <td>-0.865144</td>\n      <td>-0.816650</td>\n      <td>0.219649</td>\n      <td>-0.739330</td>\n      <td>-0.826352</td>\n      <td>-0.666444</td>\n      <td>-0.748998</td>\n      <td>1.354701</td>\n      <td>0.958646</td>\n      <td>-0.268558</td>\n      <td>-0.894380</td>\n      <td>-0.716941</td>\n      <td>1.088137</td>\n      <td>-1.165634</td>\n      <td>0.158669</td>\n      <td>-1.071988</td>\n      <td>-0.215578</td>\n      <td>-0.998049</td>\n      <td>-0.794888</td>\n      <td>-1.066169</td>\n      <td>0.158669</td>\n      <td>-1.147174</td>\n      <td>0.641499</td>\n      <td>0.811720</td>\n      <td>0.080118</td>\n      <td>-0.033113</td>\n      <td>-0.319384</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>38054</th>\n      <td>-0.258451</td>\n      <td>-0.233632</td>\n      <td>0.019828</td>\n      <td>0.976392</td>\n      <td>2.291757</td>\n      <td>0.425113</td>\n      <td>1.780885</td>\n      <td>2.386609</td>\n      <td>0.034506</td>\n      <td>0.764370</td>\n      <td>0.974170</td>\n      <td>0.139572</td>\n      <td>0.694449</td>\n      <td>0.610883</td>\n      <td>0.382628</td>\n      <td>0.546210</td>\n      <td>-0.887824</td>\n      <td>-0.313723</td>\n      <td>1.176995</td>\n      <td>2.242091</td>\n      <td>-0.463802</td>\n      <td>-1.444274</td>\n      <td>0.035808</td>\n      <td>0.602841</td>\n      <td>0.035425</td>\n      <td>0.413269</td>\n      <td>0.038960</td>\n      <td>0.961130</td>\n      <td>0.054105</td>\n      <td>0.602841</td>\n      <td>-0.431152</td>\n      <td>-0.840754</td>\n      <td>-0.900806</td>\n      <td>-1.529243</td>\n      <td>0.276725</td>\n      <td>-0.210009</td>\n      <td>3.325624</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38056</th>\n      <td>-0.258451</td>\n      <td>-0.233632</td>\n      <td>-0.186698</td>\n      <td>1.540608</td>\n      <td>2.291757</td>\n      <td>-0.698786</td>\n      <td>0.382980</td>\n      <td>0.095169</td>\n      <td>1.190230</td>\n      <td>-0.216883</td>\n      <td>-0.317184</td>\n      <td>-0.743497</td>\n      <td>0.073921</td>\n      <td>0.066257</td>\n      <td>-0.497630</td>\n      <td>-0.741324</td>\n      <td>-0.563841</td>\n      <td>-0.767156</td>\n      <td>1.712398</td>\n      <td>2.242091</td>\n      <td>-0.450405</td>\n      <td>-0.640354</td>\n      <td>1.221661</td>\n      <td>-0.898580</td>\n      <td>1.286012</td>\n      <td>-0.777083</td>\n      <td>1.334271</td>\n      <td>-0.495719</td>\n      <td>1.148370</td>\n      <td>-0.898580</td>\n      <td>-0.747516</td>\n      <td>-1.003954</td>\n      <td>-0.850037</td>\n      <td>-0.687256</td>\n      <td>-0.007706</td>\n      <td>-0.210009</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38093</th>\n      <td>-0.437714</td>\n      <td>-0.345022</td>\n      <td>0.606545</td>\n      <td>0.148160</td>\n      <td>-0.155946</td>\n      <td>-0.649777</td>\n      <td>-0.933260</td>\n      <td>0.896756</td>\n      <td>0.037911</td>\n      <td>0.427254</td>\n      <td>0.575023</td>\n      <td>0.032663</td>\n      <td>0.324143</td>\n      <td>0.237936</td>\n      <td>-0.819750</td>\n      <td>-0.842935</td>\n      <td>0.388448</td>\n      <td>0.233137</td>\n      <td>0.349421</td>\n      <td>-0.002480</td>\n      <td>-0.344220</td>\n      <td>0.269507</td>\n      <td>0.076310</td>\n      <td>-1.043675</td>\n      <td>0.198068</td>\n      <td>0.354022</td>\n      <td>0.265541</td>\n      <td>-0.833966</td>\n      <td>0.096022</td>\n      <td>-1.043675</td>\n      <td>-0.906791</td>\n      <td>0.016794</td>\n      <td>-0.018488</td>\n      <td>-0.554340</td>\n      <td>-0.401027</td>\n      <td>-0.708172</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38095</th>\n      <td>-0.258451</td>\n      <td>-0.675738</td>\n      <td>1.313995</td>\n      <td>-0.315314</td>\n      <td>-0.270157</td>\n      <td>-0.627896</td>\n      <td>0.020639</td>\n      <td>0.898530</td>\n      <td>0.034506</td>\n      <td>-0.020632</td>\n      <td>0.096871</td>\n      <td>-0.536320</td>\n      <td>0.534275</td>\n      <td>-0.119711</td>\n      <td>0.422353</td>\n      <td>0.381169</td>\n      <td>-1.448784</td>\n      <td>0.737779</td>\n      <td>0.907250</td>\n      <td>0.061940</td>\n      <td>0.175870</td>\n      <td>-1.691655</td>\n      <td>0.035808</td>\n      <td>-0.917892</td>\n      <td>0.035425</td>\n      <td>-0.124776</td>\n      <td>0.038960</td>\n      <td>3.304932</td>\n      <td>0.054105</td>\n      <td>-0.917892</td>\n      <td>-1.511419</td>\n      <td>-0.271934</td>\n      <td>-0.147834</td>\n      <td>-0.414281</td>\n      <td>0.584731</td>\n      <td>0.329961</td>\n      <td>3.325624</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38132</th>\n      <td>0.003116</td>\n      <td>-0.392549</td>\n      <td>-0.370949</td>\n      <td>-0.051501</td>\n      <td>0.347610</td>\n      <td>1.701109</td>\n      <td>-0.494689</td>\n      <td>1.615490</td>\n      <td>-0.272370</td>\n      <td>-1.272487</td>\n      <td>-1.177594</td>\n      <td>-0.848577</td>\n      <td>-1.107851</td>\n      <td>-1.252635</td>\n      <td>-1.201416</td>\n      <td>-1.129014</td>\n      <td>0.758103</td>\n      <td>-0.639719</td>\n      <td>0.408728</td>\n      <td>0.798166</td>\n      <td>1.616227</td>\n      <td>0.446894</td>\n      <td>-0.035893</td>\n      <td>-0.604270</td>\n      <td>0.258139</td>\n      <td>-1.088977</td>\n      <td>0.348058</td>\n      <td>-1.011094</td>\n      <td>0.301280</td>\n      <td>-0.604270</td>\n      <td>-1.245630</td>\n      <td>1.331538</td>\n      <td>0.363086</td>\n      <td>-0.144910</td>\n      <td>0.399951</td>\n      <td>0.050476</td>\n      <td>-0.300695</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1264 rows  82 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
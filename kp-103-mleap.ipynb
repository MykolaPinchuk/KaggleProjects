{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Define a problem, describe business context.\n2. Load data, preclean it.\n3. EDA: target, features.\n4. Descibe train/test split strategy. Show main results and discuss them.\n5. Run evth for a few periods. Show feature importance and error analysis.","metadata":{}},{"cell_type":"markdown","source":"### 1. Business problem\n\n#### Objective:\n\n#### Metric:\n\n#### Summary of results:\n","metadata":{}},{"cell_type":"markdown","source":"### 2. Load data and preclean it","metadata":{"execution":{"iopub.status.busy":"2022-09-26T15:43:15.362648Z","iopub.execute_input":"2022-09-26T15:43:15.363102Z","iopub.status.idle":"2022-09-26T15:43:15.385837Z","shell.execute_reply.started":"2022-09-26T15:43:15.363009Z","shell.execute_reply":"2022-09-26T15:43:15.384918Z"}}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle, shap\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, KFold, PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\nfrom optuna.integration import TFKerasPruningCallback\nfrom optuna.trial import TrialState\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\noptuna.logging.set_verbosity(optuna.logging.WARNING)\npd.set_option('display.max_columns', 150)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:58:27.783032Z","iopub.execute_input":"2022-09-27T14:58:27.783625Z","iopub.status.idle":"2022-09-27T14:58:27.795895Z","shell.execute_reply.started":"2022-09-27T14:58:27.783573Z","shell.execute_reply":"2022-09-27T14:58:27.794682Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:58:27.798321Z","iopub.execute_input":"2022-09-27T14:58:27.798769Z","iopub.status.idle":"2022-09-27T14:58:27.811386Z","shell.execute_reply.started":"2022-09-27T14:58:27.798735Z","shell.execute_reply":"2022-09-27T14:58:27.810197Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:58:27.813168Z","iopub.execute_input":"2022-09-27T14:58:27.813602Z","iopub.status.idle":"2022-09-27T14:58:27.823886Z","shell.execute_reply.started":"2022-09-27T14:58:27.813555Z","shell.execute_reply":"2022-09-27T14:58:27.822772Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Functions for Optuna NNs\n\ncv_nn_regularizer = 0.075\n\ndef create_snnn4_model_hyperpars(neurons_base=4, l1_reg_rate=0.1, l2_reg_rate=0.1):\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\ndef create_snnn4_model(trial):\n\n    # hyperparameter space\n    neurons_base = trial.suggest_int(\"neurons_base\", 4, 24, 2)\n    l2_reg_rate = trial.suggest_float('l2_regularizer', 0, 0.9)\n    l1_reg_rate = trial.suggest_float('l1_regularizer', 0, 0.9)\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\n\ndef create_snnn6_model_hyperpars(neurons_base=4, l1_reg_rate=0.1, l2_reg_rate=0.1):\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\ndef create_snnn6_model(trial):\n\n    # hyperparameter space\n    neurons_base = trial.suggest_int(\"neurons_base\", 4, 24, 2)\n    l2_reg_rate = trial.suggest_float('l2_regularizer', 0, 0.9)\n    l1_reg_rate = trial.suggest_float('l1_regularizer', 0, 0.9)\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\n\ndef objective_nn4(trial):\n    \n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_snnn4_model(trial)\n\n        callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=40),\n        TFKerasPruningCallback(trial, \"val_loss\"),\n    ]\n\n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n        history = model.fit(X_train, y_train, \n                                validation_data=(X_val, y_val),\n                                batch_size=2048, \n                                epochs=500, \n                                verbose=0, \n                                callbacks=callbacks)\n\n        # Evaluate the model accuracy on the validation set.\n        loss_train = model.evaluate(X_train, y_train, verbose=0)\n        loss_val = model.evaluate(X_val_extra, y_val_extra, verbose=0)\n        overfit = max(loss_val[1]-loss_train[1], 0)\n        return loss_val[1] + cv_nn_regularizer*overfit\n    \n    \ndef objective_nn6(trial):\n    \n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_snnn6_model(trial)\n\n        callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=40),\n        TFKerasPruningCallback(trial, \"val_loss\"),\n    ]\n\n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n        history = model.fit(X_train, y_train, \n                                validation_data=(X_val, y_val),\n                                batch_size=2048, \n                                epochs=500, \n                                verbose=0, \n                                callbacks=callbacks)\n\n        # Evaluate the model accuracy on the validation set.\n        loss_train = model.evaluate(X_train, y_train, verbose=0)\n        loss_val = model.evaluate(X_val_extra, y_val_extra, verbose=0)\n        overfit = max(loss_val[1]-loss_train[1], 0)\n        return loss_val[1] + cv_nn_regularizer*overfit\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:58:27.981765Z","iopub.execute_input":"2022-09-27T14:58:27.982124Z","iopub.status.idle":"2022-09-27T14:58:28.020331Z","shell.execute_reply.started":"2022-09-27T14:58:27.982087Z","shell.execute_reply":"2022-09-27T14:58:28.018944Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"time0 = time.time()\n\nmin_prd = 350\nwindows_width = 5*12\ncv_xgb_regularizer=0.2\noptuna_xgb_trials = 80\noptuna_nn_trials = 100\n\n\nwith open('../input/mleap-46-preprocessed/MLEAP_46_v7.pkl', 'rb') as pickled_one:\n    df = pickle.load(pickled_one)\ndf = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+10))]\ndf_cnt = df.count()\nempty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\ndf.drop(columns=empty_cols, inplace=True)\n#display(df.shape, df.head(), df.year.describe(), df.count())\n\nfeatures_miss_dummies = ['amhd', 'BAspr']\nfor col in features_miss_dummies:\n    if col in df.columns:\n        df[col+'_miss'] = df[col].isnull().astype(int)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:58:28.024840Z","iopub.execute_input":"2022-09-27T14:58:28.025359Z","iopub.status.idle":"2022-09-27T14:58:28.841722Z","shell.execute_reply.started":"2022-09-27T14:58:28.025297Z","shell.execute_reply":"2022-09-27T14:58:28.840590Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### 3. EDA","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Train/test split strategy","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Main results","metadata":{}},{"cell_type":"code","source":"results_df = pd.read_csv('../input/mleap-v49-results/temp_models_reg005_1.csv')\nresults_df","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:58:28.843522Z","iopub.execute_input":"2022-09-27T14:58:28.843996Z","iopub.status.idle":"2022-09-27T14:58:28.887210Z","shell.execute_reply.started":"2022-09-27T14:58:28.843960Z","shell.execute_reply":"2022-09-27T14:58:28.886144Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"    Unnamed: 0  min_prd  xgbf_train  xgbf_val  xgbf_test  xgbgs_train  \\\n0            0      100    0.110633 -0.010679   0.005897     0.032282   \n1            1      125    0.066434  0.004399   0.015985     0.040227   \n2            2      150    0.064235  0.028400  -0.007106     0.108114   \n3            3      175    0.073734  0.019790  -0.011655     0.093350   \n4            4      200    0.056176  0.048308   0.034120     0.111365   \n5            5      225    0.064410 -0.005404   0.012234     0.083176   \n6            6      250    0.061997 -0.017393   0.027061     0.018820   \n7            7      275    0.084432  0.128375   0.097800     0.109516   \n8            8      300    0.109596  0.041026   0.083551     0.097602   \n9            9      325    0.111924  0.042263  -0.038793     0.133787   \n10          10      350    0.064108  0.020667   0.031485     0.055273   \n11          11      375    0.045452  0.020881   0.013222     0.063410   \n12          12      400    0.041836  0.023006   0.023431     0.041640   \n13          13      425    0.043315  0.023428   0.021350     0.042503   \n14          14      450    0.044173  0.004679   0.008707     0.057491   \n15          15      475    0.042778  0.002243   0.002169     0.038150   \n16          16      500    0.042942  0.039946   0.045640     0.092373   \n17          17      525    0.055984  0.008138   0.037555     0.068378   \n18          18      550    0.079068 -0.007977  -0.002293     0.036324   \n19          19      575    0.061489  0.046520   0.011930     0.084962   \n20          20      600    0.053373 -0.018035  -0.004766     0.014890   \n21          21      625         NaN       NaN        NaN          NaN   \n22          22      650         NaN       NaN        NaN          NaN   \n\n    xgbgs_val  xgbgs_test  xgbo_train  xgbo_val  xgbo_test  nn4_train  \\\n0    0.009775    0.016553    0.142414  0.097214   0.017316   0.092751   \n1    0.053275    0.020740    0.081517  0.077877   0.018348   0.020937   \n2    0.136225   -0.040268    0.102913  0.141299  -0.040073   0.039123   \n3    0.065665    0.000453    0.017994  0.013669  -0.002192   0.057370   \n4    0.141292    0.040536    0.057042  0.089245   0.040346   0.041166   \n5    0.067360    0.026466    0.070134  0.033220   0.020088   0.055780   \n6   -0.001580    0.013451    0.189037  0.155622   0.031786   0.048662   \n7    0.192547    0.108320    0.102921  0.176972   0.108397   0.073269   \n8    0.058866    0.086605    0.098751  0.054814   0.084547   0.098849   \n9    0.091224   -0.039374    0.112554  0.067039  -0.034580   0.106888   \n10   0.033387    0.033121    0.059194  0.035949   0.034516   0.058240   \n11   0.054559    0.020181    0.059622  0.051217   0.016368   0.035642   \n12   0.041122    0.028316    0.033655  0.031300   0.027806   0.029551   \n13   0.047076    0.017911    0.034322  0.037631   0.016773   0.031853   \n14   0.063234    0.021913    0.031195  0.029232   0.012268   0.034023   \n15   0.026061    0.009934    0.022383  0.010773   0.004530   0.026574   \n16   0.106835    0.065450    0.056338  0.078596   0.058214   0.033054   \n17   0.034044    0.045484    0.049435  0.033478   0.042042   0.038853   \n18   0.007323    0.014142    0.059759  0.017997   0.009193   0.065788   \n19   0.089735    0.022645    0.049855  0.059204   0.020475   0.049639   \n20  -0.000217    0.005112    0.036381 -0.004054   0.002889   0.011194   \n21        NaN         NaN         NaN       NaN        NaN        NaN   \n22        NaN         NaN         NaN       NaN        NaN        NaN   \n\n     nn4_val  nn4_test  nn6_train   nn6_val  nn6_test  nn4opt_train  \\\n0  -0.004731  0.004806   0.077033  0.003310  0.007623  2.368052e-02   \n1   0.023310  0.014654   0.039157  0.018810  0.009496 -1.010410e-06   \n2   0.041242 -0.044962   0.046364  0.044968 -0.055987  4.366676e-02   \n3   0.021523 -0.006702   0.064954  0.017231 -0.010337 -4.252701e-06   \n4   0.071331  0.022324   0.041645  0.073893  0.025533  3.709691e-02   \n5   0.008508  0.012525   0.052652  0.008472  0.013793 -5.871936e-07   \n6  -0.007640  0.024796   0.048832 -0.016709 -0.007211 -1.719546e-09   \n7   0.144908  0.110993   0.070322  0.143302  0.112889  7.233643e-02   \n8   0.040880  0.086885   0.095496  0.042473  0.086713  1.023098e-01   \n9   0.049632 -0.052411   0.104488  0.049346 -0.055748  9.684091e-02   \n10  0.030566  0.022642   0.055391  0.030115  0.026270  5.876430e-02   \n11  0.024271  0.005288   0.034158  0.025277  0.002064  3.371769e-02   \n12  0.025455  0.034004   0.025434  0.024934  0.036392  2.552223e-02   \n13  0.030089  0.025648   0.029592  0.030158  0.021560  3.133441e-02   \n14  0.031850  0.015509   0.035800  0.034246  0.007337 -1.435706e-07   \n15  0.011256  0.011812   0.033069  0.012727 -0.002586 -3.848525e-07   \n16  0.044643  0.043081   0.035159  0.043644  0.044818  2.918796e-02   \n17  0.012892  0.031937   0.042841  0.010648  0.029456  3.686924e-02   \n18  0.005629  0.016282   0.063776  0.008980  0.021159 -9.343118e-07   \n19  0.055342  0.008340   0.045969  0.055721  0.019505  3.947695e-02   \n20 -0.003839  0.004517   0.031915 -0.025365 -0.014393 -1.116158e-07   \n21       NaN       NaN        NaN       NaN       NaN           NaN   \n22       NaN       NaN        NaN       NaN       NaN           NaN   \n\n    nn4opt_val   nn4opt_test  nn6opt_train    nn6opt_val  nn6opt_test  \n0    -0.006378  4.413288e-03  2.373109e-02 -1.185371e-03     0.006157  \n1    -0.000038 -1.945688e-05 -7.328776e-07 -3.910448e-05    -0.000020  \n2     0.040878 -4.501329e-02  5.041909e-02  4.294926e-02    -0.043427  \n3    -0.000010 -4.954723e-07 -7.191610e-09 -6.917556e-07    -0.000008  \n4     0.065155  2.521314e-02  3.844909e-02  6.439698e-02     0.024244  \n5    -0.000169 -1.014969e-04 -5.182621e-06 -1.285523e-04    -0.000073  \n6    -0.000007 -2.775066e-06 -8.389908e-07 -2.850800e-06    -0.000007  \n7     0.144175  1.100276e-01  7.211387e-02  1.423667e-01     0.106699  \n8     0.039196  8.664547e-02  9.784490e-02  3.844309e-02     0.087651  \n9     0.041965 -6.858033e-02  1.009017e-01  4.594199e-02    -0.066538  \n10    0.030567  2.703620e-02  5.610774e-02  2.959755e-02     0.028619  \n11    0.022140  5.191548e-03  3.490645e-02  2.254672e-02    -0.003187  \n12    0.021759  3.121626e-02  3.332331e-02  2.432268e-02     0.030985  \n13    0.028021  2.200129e-02  2.661165e-02  3.152557e-02     0.012151  \n14   -0.000024 -1.837915e-05 -1.779765e-08 -2.865202e-05    -0.000022  \n15   -0.000045 -3.566047e-05 -3.725332e-07 -4.539190e-05    -0.000036  \n16    0.039344  4.251399e-02  5.253141e-02  5.219970e-02     0.037235  \n17    0.013139  3.232509e-02 -1.155932e-06 -8.184362e-05    -0.000014  \n18   -0.000007 -1.136215e-05 -8.872024e-08 -9.620223e-06    -0.000015  \n19    0.050070  1.513632e-02  5.294881e-02  5.275299e-02     0.013744  \n20   -0.000010 -2.508825e-05 -3.862022e-12 -1.229114e-05    -0.000029  \n21         NaN           NaN           NaN           NaN          NaN  \n22         NaN           NaN           NaN           NaN          NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n      <th>nn4_train</th>\n      <th>nn4_val</th>\n      <th>nn4_test</th>\n      <th>nn6_train</th>\n      <th>nn6_val</th>\n      <th>nn6_test</th>\n      <th>nn4opt_train</th>\n      <th>nn4opt_val</th>\n      <th>nn4opt_test</th>\n      <th>nn6opt_train</th>\n      <th>nn6opt_val</th>\n      <th>nn6opt_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>100</td>\n      <td>0.110633</td>\n      <td>-0.010679</td>\n      <td>0.005897</td>\n      <td>0.032282</td>\n      <td>0.009775</td>\n      <td>0.016553</td>\n      <td>0.142414</td>\n      <td>0.097214</td>\n      <td>0.017316</td>\n      <td>0.092751</td>\n      <td>-0.004731</td>\n      <td>0.004806</td>\n      <td>0.077033</td>\n      <td>0.003310</td>\n      <td>0.007623</td>\n      <td>2.368052e-02</td>\n      <td>-0.006378</td>\n      <td>4.413288e-03</td>\n      <td>2.373109e-02</td>\n      <td>-1.185371e-03</td>\n      <td>0.006157</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>125</td>\n      <td>0.066434</td>\n      <td>0.004399</td>\n      <td>0.015985</td>\n      <td>0.040227</td>\n      <td>0.053275</td>\n      <td>0.020740</td>\n      <td>0.081517</td>\n      <td>0.077877</td>\n      <td>0.018348</td>\n      <td>0.020937</td>\n      <td>0.023310</td>\n      <td>0.014654</td>\n      <td>0.039157</td>\n      <td>0.018810</td>\n      <td>0.009496</td>\n      <td>-1.010410e-06</td>\n      <td>-0.000038</td>\n      <td>-1.945688e-05</td>\n      <td>-7.328776e-07</td>\n      <td>-3.910448e-05</td>\n      <td>-0.000020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>150</td>\n      <td>0.064235</td>\n      <td>0.028400</td>\n      <td>-0.007106</td>\n      <td>0.108114</td>\n      <td>0.136225</td>\n      <td>-0.040268</td>\n      <td>0.102913</td>\n      <td>0.141299</td>\n      <td>-0.040073</td>\n      <td>0.039123</td>\n      <td>0.041242</td>\n      <td>-0.044962</td>\n      <td>0.046364</td>\n      <td>0.044968</td>\n      <td>-0.055987</td>\n      <td>4.366676e-02</td>\n      <td>0.040878</td>\n      <td>-4.501329e-02</td>\n      <td>5.041909e-02</td>\n      <td>4.294926e-02</td>\n      <td>-0.043427</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>175</td>\n      <td>0.073734</td>\n      <td>0.019790</td>\n      <td>-0.011655</td>\n      <td>0.093350</td>\n      <td>0.065665</td>\n      <td>0.000453</td>\n      <td>0.017994</td>\n      <td>0.013669</td>\n      <td>-0.002192</td>\n      <td>0.057370</td>\n      <td>0.021523</td>\n      <td>-0.006702</td>\n      <td>0.064954</td>\n      <td>0.017231</td>\n      <td>-0.010337</td>\n      <td>-4.252701e-06</td>\n      <td>-0.000010</td>\n      <td>-4.954723e-07</td>\n      <td>-7.191610e-09</td>\n      <td>-6.917556e-07</td>\n      <td>-0.000008</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>200</td>\n      <td>0.056176</td>\n      <td>0.048308</td>\n      <td>0.034120</td>\n      <td>0.111365</td>\n      <td>0.141292</td>\n      <td>0.040536</td>\n      <td>0.057042</td>\n      <td>0.089245</td>\n      <td>0.040346</td>\n      <td>0.041166</td>\n      <td>0.071331</td>\n      <td>0.022324</td>\n      <td>0.041645</td>\n      <td>0.073893</td>\n      <td>0.025533</td>\n      <td>3.709691e-02</td>\n      <td>0.065155</td>\n      <td>2.521314e-02</td>\n      <td>3.844909e-02</td>\n      <td>6.439698e-02</td>\n      <td>0.024244</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>225</td>\n      <td>0.064410</td>\n      <td>-0.005404</td>\n      <td>0.012234</td>\n      <td>0.083176</td>\n      <td>0.067360</td>\n      <td>0.026466</td>\n      <td>0.070134</td>\n      <td>0.033220</td>\n      <td>0.020088</td>\n      <td>0.055780</td>\n      <td>0.008508</td>\n      <td>0.012525</td>\n      <td>0.052652</td>\n      <td>0.008472</td>\n      <td>0.013793</td>\n      <td>-5.871936e-07</td>\n      <td>-0.000169</td>\n      <td>-1.014969e-04</td>\n      <td>-5.182621e-06</td>\n      <td>-1.285523e-04</td>\n      <td>-0.000073</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>250</td>\n      <td>0.061997</td>\n      <td>-0.017393</td>\n      <td>0.027061</td>\n      <td>0.018820</td>\n      <td>-0.001580</td>\n      <td>0.013451</td>\n      <td>0.189037</td>\n      <td>0.155622</td>\n      <td>0.031786</td>\n      <td>0.048662</td>\n      <td>-0.007640</td>\n      <td>0.024796</td>\n      <td>0.048832</td>\n      <td>-0.016709</td>\n      <td>-0.007211</td>\n      <td>-1.719546e-09</td>\n      <td>-0.000007</td>\n      <td>-2.775066e-06</td>\n      <td>-8.389908e-07</td>\n      <td>-2.850800e-06</td>\n      <td>-0.000007</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>275</td>\n      <td>0.084432</td>\n      <td>0.128375</td>\n      <td>0.097800</td>\n      <td>0.109516</td>\n      <td>0.192547</td>\n      <td>0.108320</td>\n      <td>0.102921</td>\n      <td>0.176972</td>\n      <td>0.108397</td>\n      <td>0.073269</td>\n      <td>0.144908</td>\n      <td>0.110993</td>\n      <td>0.070322</td>\n      <td>0.143302</td>\n      <td>0.112889</td>\n      <td>7.233643e-02</td>\n      <td>0.144175</td>\n      <td>1.100276e-01</td>\n      <td>7.211387e-02</td>\n      <td>1.423667e-01</td>\n      <td>0.106699</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>300</td>\n      <td>0.109596</td>\n      <td>0.041026</td>\n      <td>0.083551</td>\n      <td>0.097602</td>\n      <td>0.058866</td>\n      <td>0.086605</td>\n      <td>0.098751</td>\n      <td>0.054814</td>\n      <td>0.084547</td>\n      <td>0.098849</td>\n      <td>0.040880</td>\n      <td>0.086885</td>\n      <td>0.095496</td>\n      <td>0.042473</td>\n      <td>0.086713</td>\n      <td>1.023098e-01</td>\n      <td>0.039196</td>\n      <td>8.664547e-02</td>\n      <td>9.784490e-02</td>\n      <td>3.844309e-02</td>\n      <td>0.087651</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>325</td>\n      <td>0.111924</td>\n      <td>0.042263</td>\n      <td>-0.038793</td>\n      <td>0.133787</td>\n      <td>0.091224</td>\n      <td>-0.039374</td>\n      <td>0.112554</td>\n      <td>0.067039</td>\n      <td>-0.034580</td>\n      <td>0.106888</td>\n      <td>0.049632</td>\n      <td>-0.052411</td>\n      <td>0.104488</td>\n      <td>0.049346</td>\n      <td>-0.055748</td>\n      <td>9.684091e-02</td>\n      <td>0.041965</td>\n      <td>-6.858033e-02</td>\n      <td>1.009017e-01</td>\n      <td>4.594199e-02</td>\n      <td>-0.066538</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>350</td>\n      <td>0.064108</td>\n      <td>0.020667</td>\n      <td>0.031485</td>\n      <td>0.055273</td>\n      <td>0.033387</td>\n      <td>0.033121</td>\n      <td>0.059194</td>\n      <td>0.035949</td>\n      <td>0.034516</td>\n      <td>0.058240</td>\n      <td>0.030566</td>\n      <td>0.022642</td>\n      <td>0.055391</td>\n      <td>0.030115</td>\n      <td>0.026270</td>\n      <td>5.876430e-02</td>\n      <td>0.030567</td>\n      <td>2.703620e-02</td>\n      <td>5.610774e-02</td>\n      <td>2.959755e-02</td>\n      <td>0.028619</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>375</td>\n      <td>0.045452</td>\n      <td>0.020881</td>\n      <td>0.013222</td>\n      <td>0.063410</td>\n      <td>0.054559</td>\n      <td>0.020181</td>\n      <td>0.059622</td>\n      <td>0.051217</td>\n      <td>0.016368</td>\n      <td>0.035642</td>\n      <td>0.024271</td>\n      <td>0.005288</td>\n      <td>0.034158</td>\n      <td>0.025277</td>\n      <td>0.002064</td>\n      <td>3.371769e-02</td>\n      <td>0.022140</td>\n      <td>5.191548e-03</td>\n      <td>3.490645e-02</td>\n      <td>2.254672e-02</td>\n      <td>-0.003187</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>400</td>\n      <td>0.041836</td>\n      <td>0.023006</td>\n      <td>0.023431</td>\n      <td>0.041640</td>\n      <td>0.041122</td>\n      <td>0.028316</td>\n      <td>0.033655</td>\n      <td>0.031300</td>\n      <td>0.027806</td>\n      <td>0.029551</td>\n      <td>0.025455</td>\n      <td>0.034004</td>\n      <td>0.025434</td>\n      <td>0.024934</td>\n      <td>0.036392</td>\n      <td>2.552223e-02</td>\n      <td>0.021759</td>\n      <td>3.121626e-02</td>\n      <td>3.332331e-02</td>\n      <td>2.432268e-02</td>\n      <td>0.030985</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>425</td>\n      <td>0.043315</td>\n      <td>0.023428</td>\n      <td>0.021350</td>\n      <td>0.042503</td>\n      <td>0.047076</td>\n      <td>0.017911</td>\n      <td>0.034322</td>\n      <td>0.037631</td>\n      <td>0.016773</td>\n      <td>0.031853</td>\n      <td>0.030089</td>\n      <td>0.025648</td>\n      <td>0.029592</td>\n      <td>0.030158</td>\n      <td>0.021560</td>\n      <td>3.133441e-02</td>\n      <td>0.028021</td>\n      <td>2.200129e-02</td>\n      <td>2.661165e-02</td>\n      <td>3.152557e-02</td>\n      <td>0.012151</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>450</td>\n      <td>0.044173</td>\n      <td>0.004679</td>\n      <td>0.008707</td>\n      <td>0.057491</td>\n      <td>0.063234</td>\n      <td>0.021913</td>\n      <td>0.031195</td>\n      <td>0.029232</td>\n      <td>0.012268</td>\n      <td>0.034023</td>\n      <td>0.031850</td>\n      <td>0.015509</td>\n      <td>0.035800</td>\n      <td>0.034246</td>\n      <td>0.007337</td>\n      <td>-1.435706e-07</td>\n      <td>-0.000024</td>\n      <td>-1.837915e-05</td>\n      <td>-1.779765e-08</td>\n      <td>-2.865202e-05</td>\n      <td>-0.000022</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>475</td>\n      <td>0.042778</td>\n      <td>0.002243</td>\n      <td>0.002169</td>\n      <td>0.038150</td>\n      <td>0.026061</td>\n      <td>0.009934</td>\n      <td>0.022383</td>\n      <td>0.010773</td>\n      <td>0.004530</td>\n      <td>0.026574</td>\n      <td>0.011256</td>\n      <td>0.011812</td>\n      <td>0.033069</td>\n      <td>0.012727</td>\n      <td>-0.002586</td>\n      <td>-3.848525e-07</td>\n      <td>-0.000045</td>\n      <td>-3.566047e-05</td>\n      <td>-3.725332e-07</td>\n      <td>-4.539190e-05</td>\n      <td>-0.000036</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>500</td>\n      <td>0.042942</td>\n      <td>0.039946</td>\n      <td>0.045640</td>\n      <td>0.092373</td>\n      <td>0.106835</td>\n      <td>0.065450</td>\n      <td>0.056338</td>\n      <td>0.078596</td>\n      <td>0.058214</td>\n      <td>0.033054</td>\n      <td>0.044643</td>\n      <td>0.043081</td>\n      <td>0.035159</td>\n      <td>0.043644</td>\n      <td>0.044818</td>\n      <td>2.918796e-02</td>\n      <td>0.039344</td>\n      <td>4.251399e-02</td>\n      <td>5.253141e-02</td>\n      <td>5.219970e-02</td>\n      <td>0.037235</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>525</td>\n      <td>0.055984</td>\n      <td>0.008138</td>\n      <td>0.037555</td>\n      <td>0.068378</td>\n      <td>0.034044</td>\n      <td>0.045484</td>\n      <td>0.049435</td>\n      <td>0.033478</td>\n      <td>0.042042</td>\n      <td>0.038853</td>\n      <td>0.012892</td>\n      <td>0.031937</td>\n      <td>0.042841</td>\n      <td>0.010648</td>\n      <td>0.029456</td>\n      <td>3.686924e-02</td>\n      <td>0.013139</td>\n      <td>3.232509e-02</td>\n      <td>-1.155932e-06</td>\n      <td>-8.184362e-05</td>\n      <td>-0.000014</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>550</td>\n      <td>0.079068</td>\n      <td>-0.007977</td>\n      <td>-0.002293</td>\n      <td>0.036324</td>\n      <td>0.007323</td>\n      <td>0.014142</td>\n      <td>0.059759</td>\n      <td>0.017997</td>\n      <td>0.009193</td>\n      <td>0.065788</td>\n      <td>0.005629</td>\n      <td>0.016282</td>\n      <td>0.063776</td>\n      <td>0.008980</td>\n      <td>0.021159</td>\n      <td>-9.343118e-07</td>\n      <td>-0.000007</td>\n      <td>-1.136215e-05</td>\n      <td>-8.872024e-08</td>\n      <td>-9.620223e-06</td>\n      <td>-0.000015</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>575</td>\n      <td>0.061489</td>\n      <td>0.046520</td>\n      <td>0.011930</td>\n      <td>0.084962</td>\n      <td>0.089735</td>\n      <td>0.022645</td>\n      <td>0.049855</td>\n      <td>0.059204</td>\n      <td>0.020475</td>\n      <td>0.049639</td>\n      <td>0.055342</td>\n      <td>0.008340</td>\n      <td>0.045969</td>\n      <td>0.055721</td>\n      <td>0.019505</td>\n      <td>3.947695e-02</td>\n      <td>0.050070</td>\n      <td>1.513632e-02</td>\n      <td>5.294881e-02</td>\n      <td>5.275299e-02</td>\n      <td>0.013744</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>600</td>\n      <td>0.053373</td>\n      <td>-0.018035</td>\n      <td>-0.004766</td>\n      <td>0.014890</td>\n      <td>-0.000217</td>\n      <td>0.005112</td>\n      <td>0.036381</td>\n      <td>-0.004054</td>\n      <td>0.002889</td>\n      <td>0.011194</td>\n      <td>-0.003839</td>\n      <td>0.004517</td>\n      <td>0.031915</td>\n      <td>-0.025365</td>\n      <td>-0.014393</td>\n      <td>-1.116158e-07</td>\n      <td>-0.000010</td>\n      <td>-2.508825e-05</td>\n      <td>-3.862022e-12</td>\n      <td>-1.229114e-05</td>\n      <td>-0.000029</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>625</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Fit the model(s) for one window and explore results","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_cols = ['PERMNO', 'year', 'prd']\ndf.reset_index(inplace=True, drop=True)\nX = df.copy()\ny = X.pop('RET')\n\ntrain_indx = X.prd<(min_prd+windows_width-1)\nval_indx = X['prd'].isin(range(min_prd+windows_width-1, min_prd+windows_width+2))\nval_indx_extra = X['prd'].isin(range(min_prd+windows_width+5, min_prd+windows_width+8))\ntest_indx = X['prd'].isin(range(min_prd+windows_width+2, min_prd+windows_width+5))\n\nX_train = X[train_indx]\nX_val = X[val_indx]\nX_val_extra = X[val_indx_extra]\nX_test = X[test_indx]\ny_train = y[train_indx]\ny_val = y[val_indx]\ny_val_extra = y[val_indx_extra]\ny_test = y[test_indx]\n\n#display(X_train.head(3), X_train.tail(3), y_train.head(3), y_train.tail(3))\n#display(X_train.prd.describe(), X_val.prd.describe(), X_test.prd.describe())\ndisplay(X_train.shape, X_val.shape, X_test.shape)\n\nX_train.drop(columns=temp_cols, inplace=True)\nX_val.drop(columns=temp_cols, inplace=True)\nX_val_extra.drop(columns=temp_cols, inplace=True)\nX_test.drop(columns=temp_cols, inplace=True)\n\n#display(X_train.tail())\ncol_cat = ['ind']\ncol_num = [x for x in X_train.columns if x not in col_cat]\nfor col in col_num:\n    X_train[col] = X_train[col].fillna(X_train[col].median())\n    X_val[col] = X_val[col].fillna(X_train[col].median())\n    X_val_extra[col] = X_val_extra[col].fillna(X_train[col].median())\n    X_test[col] = X_test[col].fillna(X_train[col].median())\nfor col in col_cat:\n    X_train[col] = X_train[col].fillna(value=-1000)\n    X_val[col] = X_val[col].fillna(value=-1000)\n    X_val_extra[col] = X_val_extra[col].fillna(value=-1000)\n    X_test[col] = X_test[col].fillna(value=-1000)\n\n#display(X_train.tail())\nfeature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                        (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                        remainder=\"passthrough\")\n\nprint('Number of features before transformation: ', X_train.shape)\ntrain_index, val_index, val_index_extra, test_index = X_train.index, X_val.index, X_val_extra.index, X_test.index\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_val = pd.DataFrame(feature_transformer.transform(X_val), columns=feature_transformer.get_feature_names_out())\nX_val_extra = pd.DataFrame(feature_transformer.transform(X_val_extra), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ')\nprint('Number of features after transformation: ', X_train.shape, X_val.shape, X_val_extra.shape, X_test.shape)\nX_train.index = train_index\nX_val.index = val_index\nX_val_extra.index = val_index_extra\nX_test.index = test_index\n#display(X_train.tail())\n\nX = pd.concat([X_train, X_val])\ny = pd.concat([y_train, y_val])\n#display(X,y)\n\nX_ = pd.concat([X_train, X_val, X_val_extra])\ny_ = pd.concat([y_train, y_val, y_val_extra])\n#display(X,y, X_,y_)\n\n\nresults = pd.DataFrame(columns = ['min_prd', 'xgbf_train', 'xgbf_val', 'xgbf_test', \n                                  'xgbgs_train', 'xgbgs_val', 'xgbgs_test', \n                                  'xgbo_train', 'xgbo_val', 'xgbo_test',\n                                  'nn4_train', 'nn4_val', 'nn4_test',\n                                 'nn6_train', 'nn6_val', 'nn6_test',\n                                 'nn4opt_train', 'nn4opt_val', 'nn4opt_test',\n                                 'nn6opt_train', 'nn6opt_val', 'nn6opt_test'])\n\nresults['min_prd'] = [min_prd]\n\n\n### Modeling part ###\n\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=400, max_depth=4, eta=0.02, colsample_bytree=0.4, subsample=0.6)\nxgb1.fit(X_train, y_train)\nprint('fixed XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\nprint('XGB val:', mean_absolute_error(y_val, xgb1.predict(X_val)), r2_score(y_val, xgb1.predict(X_val)))\nprint('XGB val extra:', mean_absolute_error(y_val_extra, xgb1.predict(X_val_extra)), r2_score(y_val_extra, xgb1.predict(X_val_extra)))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbf_train':'xgbf_test'] = \\\n[r2_score(y_train, xgb1.predict(X_train)), \nr2_score(y_val, xgb1.predict(X_val)),\nr2_score(y_test, xgb1.predict(X_test))]\n\ntime1 = time.time()\n\n# Create a list where train data indices are -1 and validation data indices are 0\nsplit_index = [-1 if x in X_train.index else 0 for x in X.index]\npds = PredefinedSplit(test_fold = split_index)\n\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[400, 600, 800], 'max_depth':[2,3,4,5], 'eta':[0.006, 0.012, 0.02], \n              'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbgs = GridSearchCV(estimator = xgb, cv=pds, param_grid=param_grid)\n\n# Fit with all data\nxgbgs.fit(X_, y_)\n\nprint('gs XGB', xgbgs.best_params_, xgbgs.best_score_, time.time()-time1)\nprint('XGB train:', mean_absolute_error(y_train, xgbgs.predict(X_train)), r2_score(y_train, xgbgs.predict(X_train)))\nprint('XGB validation:', mean_absolute_error(y_val, xgbgs.predict(X_val)), r2_score(y_val, xgbgs.predict(X_val)))\nprint('XGB validation extra:', mean_absolute_error(y_val_extra, xgbgs.predict(X_val_extra)), r2_score(y_val_extra, xgbgs.predict(X_val_extra)))\nprint('XGB test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_score(y_test, xgbgs.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbgs_train':'xgbgs_test'] = \\\n[r2_score(y_train, xgbgs.predict(X_train)), \nr2_score(y_val, xgbgs.predict(X_val)),\nr2_score(y_test, xgbgs.predict(X_test))]\n\ntime1 = time.time()\ndef objective_xgb(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    params = {\n    \"tree_method\": 'gpu_hist',\n    \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n    \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 1500),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n    \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.0005, 0.03),\n    \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.05, 0.95),\n    \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 0.95),\n    \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 50.0),\n    \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 500.0),\n    \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 100.0),\n    \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n    model = XGBRegressor(**params, njobs=-1)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose = False)\n\n    score_train = r2_score(y_train, model.predict(X_train))\n    score_val = r2_score(y_val, model.predict(X_val))\n    score_val_extra = r2_score(y_val_extra, model.predict(X_val_extra)) \n    score_val = (score_val+score_val_extra)/2\n    overfit = np.abs(score_train-score_val)\n\n    return score_val-cv_xgb_regularizer*overfit\n\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective_xgb, n_trials=optuna_xgb_trials)\nprint('Total time for hypermarameter optimization, XGB: ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X, y)\nprint('Optuna XGB train: \\n', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), '\\nvalidation \\n',\n      mean_absolute_error(y_val, optuna_xgb.predict(X_val)), r2_score(y_val, optuna_xgb.predict(X_val)),\n      mean_absolute_error(y_val_extra, optuna_xgb.predict(X_val_extra)), r2_score(y_val_extra, optuna_xgb.predict(X_val_extra)), '\\ntest \\n',\n      mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbo_train':'xgbo_test'] = \\\n[r2_score(y_train, optuna_xgb.predict(X_train)), \nr2_score(y_val, optuna_xgb.predict(X_val)),\nr2_score(y_test, optuna_xgb.predict(X_test))]\n\n###########\n### NNs ###\n###########\n\nneurons_base = 8\nl2_reg_rate = 0.5\n\nmodel_snn6 = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn6.count_params())\n\nearly_stopping50 = EarlyStopping(patience=50, restore_best_weights=True)\noptimizer_adam = tf.keras.optimizers.Adam()\n\nmodel_snn6.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n\ntime1 = time.time()\nhistory = model_snn6.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=0, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nresults.loc[results.min_prd==min_prd,'nn6_train':'nn6_test'] = \\\n[r2_score(y_train, model_snn6.predict(X_train)), \nr2_score(y_val, model_snn6.predict(X_val)),\nr2_score(y_test, model_snn6.predict(X_test))]\n\n\n\nneurons_base = 8\nl2_reg_rate = 0.3\n\nmodel_snn4 = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn4.count_params())\n\ntime1 = time.time()\nmodel_snn4.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_snn4.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=0, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nresults.loc[results.min_prd==min_prd,'nn4_train':'nn4_test'] = \\\n[r2_score(y_train, model_snn4.predict(X_train)), \nr2_score(y_val, model_snn4.predict(X_val)),\nr2_score(y_test, model_snn4.predict(X_test))]\n\n\n\n# try optuna, using this kaggle notebook: https://www.kaggle.com/code/mistag/keras-model-tuning-with-optuna\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"minimize\", \n                            sampler=optuna.samplers.TPESampler(), \n                            pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective_nn4, n_trials=optuna_nn_trials)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\ntemp = study.best_params\ndisplay('Optuna NN4', study.best_params, time.time()-time1)\n\noptimal_hyperpars = list(temp.values())\ndisplay(optimal_hyperpars)\nprint('Time for hyperparameter optimization: ',time.time()-time1, optimal_hyperpars)\n\noptuna_nn = create_snnn4_model_hyperpars(neurons_base=optimal_hyperpars[0], \n                                         l2_reg_rate=optimal_hyperpars[1],\n                                        l1_reg_rate=optimal_hyperpars[2])\nhistory = optuna_nn.fit(X_train, y_train, \n                        validation_data=(X_val, y_val),\n                        batch_size=2048, \n                        epochs=1000,\n                        verbose=0, \n                        callbacks=[early_stopping50])\n\nresults.loc[results.min_prd==min_prd,'nn4opt_train':'nn4opt_test'] = \\\n[r2_score(y_train, optuna_nn.predict(X_train)), \nr2_score(y_val, optuna_nn.predict(X_val)),\nr2_score(y_test, optuna_nn.predict(X_test))]\n\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"minimize\", \n                            sampler=optuna.samplers.TPESampler(), \n                            pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective_nn6, n_trials=optuna_nn_trials)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\ntemp = study.best_params\ndisplay('Optuna NN6', study.best_params, time.time()-time1)\n\noptimal_hyperpars = list(temp.values())\ndisplay(optimal_hyperpars)\nprint('Time for hyperparameter optimization: ', time.time()-time1, optimal_hyperpars)\n\noptuna_nn = create_snnn6_model_hyperpars(neurons_base=optimal_hyperpars[0], \n                                         l2_reg_rate=optimal_hyperpars[1],\n                                        l1_reg_rate=optimal_hyperpars[2])\nhistory = optuna_nn.fit(X_train, y_train, \n                        validation_data=(X_val, y_val),\n                        batch_size=2048, \n                        epochs=1000,\n                        verbose=0, \n                        callbacks=[early_stopping50])\n\nresults.loc[results.min_prd==min_prd,'nn6opt_train':'nn6opt_test'] = \\\n[r2_score(y_train, optuna_nn.predict(X_train)), \nr2_score(y_val, optuna_nn.predict(X_val)),\nr2_score(y_test, optuna_nn.predict(X_test))]\n\n\nprint('total time for the script: ', time.time()-time0)\ndisplay(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T14:58:28.890638Z","iopub.execute_input":"2022-09-27T14:58:28.890989Z","iopub.status.idle":"2022-09-27T15:40:01.497251Z","shell.execute_reply.started":"2022-09-27T14:58:28.890953Z","shell.execute_reply":"2022-09-27T15:40:01.496156Z"},"trusted":true},"execution_count":32,"outputs":[{"output_type":"display_data","data":{"text/plain":"(146944, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(7507, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(7561, 41)"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (146944, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (146944, 86) (7507, 86) (7431, 86) (7561, 86)\nmae of a constant model 10.01529988807284\nR2 of a constant model 0.0\nfixed XGB train: 9.55921006173112 0.06410827554684584\nXGB val: 9.271005226317717 0.02066692926133007\nXGB val extra: 10.901656205339957 0.01356724415096211\nXGB test: 9.4228780692512 0.031484648991487285\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.012, 'max_depth': 3, 'n_estimators': 800, 'subsample': 0.6} 0.021518235581785672 78.80262470245361\nXGB train: 9.602715421644447 0.055272575410556635\nXGB validation: 9.214616030425278 0.03338672541856802\nXGB validation extra: 10.826646916675854 0.026527574479032645\nXGB test: 9.409131718392727 0.03312134954357382\nTotal time for hypermarameter optimization, XGB:  279.26213932037354\n        n_estimators : 1168\n           max_depth : 2\n       learning_rate : 0.016939671171658614\n    colsample_bytree : 0.5735325999904543\n           subsample : 0.2718152600589454\n               alpha : 6.542261722735922\n              lambda : 4.710167336615569\n               gamma : 2.0286882614384914e-06\n    min_child_weight : 35.165239659877805\nbest objective value : 0.012294413833636586\nOptuna XGB train: \n 9.588131840160969 0.05553882927352627 \nvalidation \n 9.210533517728669 0.03400591371502726 10.89171589013708 0.013840739321189122 \ntest \n 9.408266950999115 0.033074795111511235\n3833\nMinimum Validation Loss: 173.0644\n3489\nMinimum Validation Loss: 171.5836\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'Optuna NN4'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"{'neurons_base': 6,\n 'l2_regularizer': 0.3597316955065116,\n 'l1_regularizer': 0.04172337735181555}"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1130.4501633644104"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"[6, 0.3597316955065116, 0.04172337735181555]"},"metadata":{}},{"name":"stdout","text":"Time for hyperparameter optimization:  1130.4606156349182 [6, 0.3597316955065116, 0.04172337735181555]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'Optuna NN6'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"{'neurons_base': 24,\n 'l2_regularizer': 0.5811442495468524,\n 'l1_regularizer': 0.04799275515347461}"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"807.0093257427216"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"[24, 0.5811442495468524, 0.04799275515347461]"},"metadata":{}},{"name":"stdout","text":"Time for hyperparameter optimization:  807.0196619033813 [24, 0.5811442495468524, 0.04799275515347461]\ntotal time for the script:  2493.2662994861603\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      350   0.064108  0.020667  0.031485    0.055273  0.033387   0.033121   \n\n  xgbo_train  xgbo_val xgbo_test nn4_train   nn4_val  nn4_test nn6_train  \\\n0   0.055539  0.034006  0.033075  0.057466  0.029474  0.022955  0.057071   \n\n    nn6_val nn6_test nn4opt_train nn4opt_val nn4opt_test nn6opt_train  \\\n0  0.027439  0.02969     0.051402   0.027272    0.023739     0.051426   \n\n  nn6opt_val nn6opt_test  \n0   0.027512    0.029561  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n      <th>nn4_train</th>\n      <th>nn4_val</th>\n      <th>nn4_test</th>\n      <th>nn6_train</th>\n      <th>nn6_val</th>\n      <th>nn6_test</th>\n      <th>nn4opt_train</th>\n      <th>nn4opt_val</th>\n      <th>nn4opt_test</th>\n      <th>nn6opt_train</th>\n      <th>nn6opt_val</th>\n      <th>nn6opt_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>350</td>\n      <td>0.064108</td>\n      <td>0.020667</td>\n      <td>0.031485</td>\n      <td>0.055273</td>\n      <td>0.033387</td>\n      <td>0.033121</td>\n      <td>0.055539</td>\n      <td>0.034006</td>\n      <td>0.033075</td>\n      <td>0.057466</td>\n      <td>0.029474</td>\n      <td>0.022955</td>\n      <td>0.057071</td>\n      <td>0.027439</td>\n      <td>0.02969</td>\n      <td>0.051402</td>\n      <td>0.027272</td>\n      <td>0.023739</td>\n      <td>0.051426</td>\n      <td>0.027512</td>\n      <td>0.029561</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAXMAAAD1CAYAAACiJBXjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA4XElEQVR4nO3deXhU5dnH8e+ZmUwyyWRPJmEJgbCp7ApWFEGCiRCMoCitUrWRVkt9jUhfF6CvUhcoaltb21KoWrXuIkIRLJQoSy2KssgWNiGQQGYC2ddZz/vHA4FIIAEShknuz3V5Bc9MZu45SX7nOfd5zjmarus6QgghAprB3wUIIYS4cBLmQgjRBkiYCyFEGyBhLoQQbYCEuRBCtAEmf7xpXV0d27dvJz4+HqPR6I8ShBAi4Hi9Xo4ePUrfvn0JCQlp8FiTYV5YWMhjjz1GcXExmqYxceJE7r33Xl566SVycnIwGAzExsYyZ84cEhIS0HWd5557jjVr1hASEsJvfvMb+vTp0+A1t2/fzqRJk1r2UwohRDvx9ttvM3jw4AbLtKbmmRcVFXH06FH69OlDVVUVEyZM4M9//jOJiYlYrVYA3nzzTfbt28fTTz/NmjVr+Mc//sHf/vY3vv32W5577jk+/PDDBq958OBB0tPTefvtt0lMTGzhjymEEG2T3W5n0qRJrFy5kuTk5AaPNTkyt9ls2Gw2AKxWKykpKTgcDnr06FH/nNraWjRNAyAnJ4fx48ejaRoDBw6koqKCoqKi+tcA6lsriYmJdO7c+cI/oRBCtCONtafPqWdeUFBAbm4uAwYMAOD3v/89ixcvJjw8nDfffBMAh8PRYLSdmJiIw+FoEOZCCCFaVrNns1RXV5Odnc2MGTPq2yuPPPIIa9asITMzk7feeqvVihRCCHF2zQpzt9tNdnY2mZmZpKenn/Z4ZmYmK1euBCAhIQG73V7/mN1uJyEhoYXKFUII0Zgmw1zXdWbOnElKSgpZWVn1y/Py8ur/nZOTQ0pKCgCpqaksXrwYXdfZsmUL4eHh0mIRQohW1mTPfOPGjSxZsoRevXoxbtw4AKZNm8bChQs5cOAAmqbRqVMnfv3rXwMwYsQI1qxZQ1paGhaLhdmzZ7fuJxBCCNF0mA8ePJjdu3eftnzEiBGNPl/TNJ566qkLr0wIIUSzBdzp/NPe38IfVu31dxlCiAA1aNAgf5fQKvxyOv+F2O2opLzW7e8yhBDikhJwYR5qNlLj8vq7DCFEgNN1neeff55169ahaRpTpkwhIyODoqIiHnnkEaqqqvB6vcyaNYtBgwYxc+ZMtm/fjqZpTJgwgZ/85Cf+/ggNBFyYW8wmGZkL0QZ8tLGAD77Jb9HXnDg4iQlXNe+s8pUrV7Jr1y6WLFlCaWkpt99+O4MHD+aTTz5h2LBhTJkyBa/XS21tLbm5uTgcDj755BMAKioqWrTulhBwPXNLkIFal8ffZQghAtzGjRsZO3YsRqORuLg4hgwZwrZt2+jXrx+LFi3i5ZdfZs+ePVitVpKSksjPz+eZZ55h7dq19SdOXkoCbmQeajZR65Y2ixCBbsJVnZs9ir6YhgwZwltvvcWaNWt44oknyMrKYvz48SxZsoT//Oc/vPfee3z66afMmTPH36U2EHgjc7ORWumZCyEu0ODBg/n000/xer2UlJTwzTff0L9/fw4fPkxcXBwTJ07kjjvuYMeOHZSUlKDrOjfddBNTp05l586d/i7/NIE3Mg+SA6BCiAuXlpbG5s2bGTduHJqm8eijjxIfH8/HH3/Mq6++islkIjQ0lLlz51JUVMT06dPx+XyAOnHyUhNwYW4xG6l1e9F1vf6yu0II0VybN28G1AmOjz/+OI8//niDx2+99VZuvfXW077v448/vij1na+AbLPoOjg9Pn+XIoQQl4yAC/PQIHVRdmm1CCHESYEX5mbVGaqR6YlCCFEv4MLcYlYjc5nRIoQQJwVemEubRQghThNwYR56YmQuJw4JIUS9gAtzabMIIcTpAi7MTx4AlTAXQrS+s13/vKCggJtvvvkiVnNmARfmJ3vmMptFCCFOCMgzQAHqpGcuRGDb8i5sfqtlX3PQj2HgnWd9yosvvkiHDh2YNGkSAC+//DJGo5GvvvqKiooKPB4PDz/8MDfeeOM5vbXT6WTWrFls374do9HIE088wTXXXMPevXuZPn06brcbn8/Hyy+/jM1mY+rUqdjtdnw+H7/4xS/IyMg4748NARjmJw6ASptFCHE+MjIymD17dn2Yf/rpp7z66qvcc889WK1WSkpK+OEPf8ioUaPO6ZIhb7/9NgBLly7lu+++Y/LkyaxYsYL33nuPe+65h1tuuQWXy4XP52PNmjXYbDYWLFgAQGVl5QV/roALc5maKEQbMfDOJkfRreGKK66guLgYh8NBaWkpERERxMXFMWfOHL7++msMBgMOh4Njx44RHx/f7NfduHEjP/7xjwHo3r07HTt25MCBAwwcOJC//vWv2O120tPT6dq1K7169WLu3Lm88MILjBw5ksGDB1/w5wq4nrnBoBESZJCpiUKI8zZ69GhWrFjB8uXLycjIYOnSpZSUlLBo0SKWLFlCXFwcTqezRd4rMzOTefPmERISwv3338/69evp1q0bixYtolevXrz00kv86U9/uuD3CbgwBzU6l6mJQojzlZGRwfLly1mxYgWjR4+msrKS2NhYgoKC+PLLLzl8+PA5v+bgwYNZunQpAAcOHKCwsJCUlBTy8/NJSkrinnvuYdSoUezevRuHw4HFYmHcuHFMnjy5Ra6PHnBtFlDTE6XNIoQ4Xz179qS6uhqbzYbNZiMzM5MpU6aQmZlJ3759SUlJOefXvOuuu5g1axaZmZkYjUbmzJmD2Wzm008/ZcmSJZhMJuLi4njggQfYtm0bzz//PAaDAZPJxKxZsy74M2m6rusX/CrnqKCggFGjRpGTk0Pnzud+26gbf7eGXglW/jLpqlaoTgghLk1ny86AbLOEmuVuQ0IIcaqAbLOEyK3jhBAX0e7du3nssccaLDObzXz44Yd+quh0TYZ5YWEhjz32GMXFxWiaxsSJE7n33nuZO3cun3/+OUFBQXTp0oU5c+YQEREBwPz581m4cCEGg4Ff/epXXH/99S1adKjZSEm1q0VfUwghzqR3794sWbLE32WcVZNtlhNnMi1fvpz333+fd955h3379nHdddfxySefsHTpUrp27cr8+fMB2LdvH8uWLWPZsmW88sor/PrXv8brbdlRtLRZhBCioSbD3Gaz0adPHwCsVispKSk4HA6GDRuGyaQG9gMHDsRutwOQk5PD2LFjMZvNJCUlkZyczNatW1u0aEuQSaYmCiHEKc7pAGhBQQG5ubkMGDCgwfKPPvqI4cOHA+BwOEhMTKx/LCEhAYfD0QKlnqRG5nKhLSGEOKHZYV5dXU12djYzZszAarXWL583bx5Go5FbbrmlVQpsjMVslDNAhRDiFM2azeJ2u8nOziYzM5P09PT65YsWLWL16tW8/vrr9RekSUhIqG+5gBqpJyQktGjRliAjdW4fPp+OwdD8C+EIIURb1eTIXNd1Zs6cSUpKCllZWfXL165dyyuvvMK8efOwWCz1y1NTU1m2bBkul4v8/Hzy8vLo379/ixYtt44TQoiGmhyZb9y4kSVLltCrVy/GjRsHwLRp03j22WdxuVz1AT9gwACefvppevbsyZgxY8jIyMBoNPLkk09iNBpbtOhTL4MbFhyQU+WFEKJFNZmEgwcPZvfu3actHzFixBm/Z8qUKUyZMuXCKjuLkCC5QYUQQpwqQE/nl/uACiHEqQI0zOU+oEIIcaqADPMT9wGVE4eEEEIJyDCX2SxCCNFQQIa53AdUCCEaCswwlzaLEEI0EJBhfnI2ixwAFUIICNgwP95mkZ65EEIAARrmwSYDmgZ10mYRQgggQMNc0zQscus4IYSoF5BhDsevaS5tFiGEAAI4zC1mo8xmEUKI4wI2zEPl1nFCCFEvYMM8RNosQghRL2DDPDTISK3MMxdCCCCQw9wss1mEEOKEgA1zuamzEEKcFLBhbg02UVknbRYhhIAADvPI0CDKa93ouu7vUoQQwu8CNsyjLGZcHh91bp+/SxFCCL8L3DAPDQKgrNbl50qEEML/AjbMo0+EeY3bz5UIIYT/BWyYR1rMgIS5EEJAAId5fZulRtosQggR+GFeKyNzIYQI3DCXNosQQtQL2DAPCTJgNhlkNosQQtCMMC8sLOTuu+8mIyODsWPH8sYbbwDw6aefMnbsWC677DK2bdvW4Hvmz59PWloaN910E+vWrWuVwjVNI8oSRLmMzIUQAlNTTzAajTzxxBP06dOHqqoqJkyYwHXXXUevXr14+eWXeeqppxo8f9++fSxbtoxly5bhcDjIyspixYoVGI3GFi8+OtQsbRYhhKAZI3ObzUafPn0AsFqtpKSk4HA46N69OykpKac9Pycnh7Fjx2I2m0lKSiI5OZmtW7e2fOWoU/qlzSKEEOfYMy8oKCA3N5cBAwac8TkOh4PExMT6/09ISMDhcJx/hWcRZQmSkbkQQnAOYV5dXU12djYzZszAarW2Zk3NFhUqYS6EENDMMHe73WRnZ5OZmUl6evpZn5uQkIDdbq//f4fDQUJCwoVVeQZRoWZpswghBM0Ic13XmTlzJikpKWRlZTX5gqmpqSxbtgyXy0V+fj55eXn079+/RYr9vkhLEHVuH3VykwohRDvX5GyWjRs3smTJEnr16sW4ceMAmDZtGi6Xi2eeeYaSkhIeeOABLr/8cl599VV69uzJmDFjyMjIwGg08uSTT7bKTBY4eRZoea2bkKDWeQ8hhAgETYb54MGD2b17d6OPpaWlNbp8ypQpTJky5cIqa4ZTzwJNiAhp9fcTQohLVcCeAQqnXgZX+uZCiPYtoMM88niYl8qMFiFEOxfQYR4Vqtos5TKjRQjRzgV2mFvkbkNCCAEBHuahZiNBRk2uaS6EaPcCOsw1TSPSIhfbEkKIgA5zUHPNpWcuhGjvAj/M5WJbQgjRBsJcrmkuhBBtIcyD5KQhIUS7F/hhbgmS2SxCiHYv8MM8NIgalxenR66cKIRovwI+zGPCggE4ViWtFiFE+xXwYZ4cGwrAweJqP1cihBD+04bCvMbPlQghhP8EfJh3iLQQZNQkzIUQ7VrAh7nRoJEUEyptFiFEuxbwYQ7QNTaMPBmZCyHasTYR5l1iQjlUXI2u6/4uRQgh/KJNhHnX2FCqXV6ZniiEaLfaRJgnx4YBcKhE+uZCiPapjYS5mp6Yd0z65kKI9qlNhHnn6FAMmpw4JIRov9pEmJtNBjpGWThYIiNzIUT71CbCHGR6ohCifWszYZ4cq6YnCiFEe9Smwry0xk253HVICNEONRnmhYWF3H333WRkZDB27FjeeOMNAMrKysjKyiI9PZ2srCzKy8sB0HWdZ599lrS0NDIzM9mxY0frfoLjTkxPPCjTE4UQ7VCTYW40GnniiSdYvnw577//Pu+88w779u1jwYIFDB06lJUrVzJ06FAWLFgAwNq1a8nLy2PlypU888wzzJo1q7U/AwA9bFYAdhVWXpT3E0KIS0mTYW6z2ejTpw8AVquVlJQUHA4HOTk5jB8/HoDx48ezatUqgPrlmqYxcOBAKioqKCoqar1PcFy32DAiLUFsOlTa6u8lhBCXmnPqmRcUFJCbm8uAAQMoLi7GZrMBEB8fT3FxMQAOh4PExMT670lMTMThcLRgyY0zGDQGdYli40EJcyFE+9PsMK+uriY7O5sZM2ZgtVobPKZpGpqmtXhxjfrop/D57EYfurJLNHuLqiiXGzwLIdqZZoW52+0mOzubzMxM0tPTAYiNja1vnxQVFRETEwNAQkICdru9/nvtdjsJCQktV3Hxd1DwTaMPXdklGoAt+WUt935CCBEAmgxzXdeZOXMmKSkpZGVl1S9PTU1l8eLFACxevJhRo0Y1WK7rOlu2bCE8PLy+HdMiQiLBWdHoQwOSIjFosElaLUKIdsbU1BM2btzIkiVL6NWrF+PGjQNg2rRp3H///UydOpWFCxfSsWNHXnrpJQBGjBjBmjVrSEtLw2KxMHt24y2R8xYSCRWHG30oPCSIXgnhchBUCNHuNBnmgwcPZvfu3Y0+dmLO+ak0TeOpp5668MrOJCQS6srP+PCVydEs/fYIPp+OwXCR+vhCCOFngXcGaEjE2cO8SzSVdR72Ha26iEUJIYR/BWCYR4KnDtx1jT58ZZcoAJmiKIRoVwIwzKPU1zMcBO0WF0anKAsrdtgbfVwIIdqiAAzzSPX1DK0WTdMYP6gja/ccpaiy8dG7EEK0NQEc5o2PzAFuHdQZnw7/3HLkIhUlhBD+FcBhXnbGp/SwWRnQOZJFmxqfwiiEEG1NAIf5mWe0ANx2ZWd2Flawy37mEbwQQrQVgRfmwRHqaxNhnjmgIyaDJqNzIUS7EHhh3syReUyYmbQrEvjgm3xqXd6LUJgQQvhP4IW5OQw0Y5NhDnDfsG6U1bj5aFPBRShMCCH8J/DCXNOaPKX/hMHJ0fTvHMlrXxzA59MvQnFCCOEfgRfmcNYrJ55K0zQmD+vG/qPVrN7T+nc7EkIIfwncMG/GyBwgo18HEiNCWLB2P7ouo3MhRNvU5sM8yGhgyg3d+XJ/CR/JzBYhRBsVoGF+9isnft/d1yRzdbcYfv3PHRwpq23FwoQQwj8CNMybPzIHdbPnF28fgFfXefyjrXjlYKgQoo0J0DCPOqcwB+gSG8qvxl7Bur3H+PlbG2XuuRCiTQnQMI8Edw143ef0bXf9oAu/vqUPq3IdTHrlSw4cq26lAoUQ4uIK3DCHs1458UzuvbYrf7nrSnYWVjDqt6t56N3N5EmoCyECXICHedl5ffuYfh1Y+9hIfjY8hc9yHYz+w1pelxOLhBABLMDD/Nz65qeyhYcwfczl5PzyBoamxDJr6U4mv/G19NKFEAGp3Yb5CYmRIbz2kyE8Pa4Pq/cc5Sd/30CV03PBryuEEBeTyd8FnJdmXga3uTRN456hXYm0BDHtg29J+90aetisdI62MLR7HMN7xhEVam6R9xJCiNYQmGHegiPzU40b2IlISxBvf3WIokony7fZeXdDPgYNhveK546rkhjaPZbo0CA0TWvR9xZCiAshYf49N/S2cUNvGwBen863BWX8e6eDxZsP8+A7mwAIDzYxsEsUtw7qxOi+iYSaT67Goso64sKCMRgk7IUQF09ghrnZCpqhVcL8VEaDxpVdormySzT/m96brw4Us6uwkrziaj7fXcS0D75lxsfbGNYjjssSI1iV62CXvZIeNis/u74boy5PIDbMLKN4IUSrazLMp0+fzurVq4mNjeWTTz4BYNeuXTz11FPU1NTQqVMnXnzxRaxWKwDz589n4cKFGAwGfvWrX3H99de3fNUGg+qbN+MyuC3FaNC4tnsc13aPA0DXdb7OK2X5tkJW5TpYlVvEoC5RPHJjL1bssPP4R9uAbViCjHSOttA52kKXmFC626zEhJn5Jq+UzYdK6RYXxvU947msQzjx4cHEhgVjlFG9EOIcNRnmt912Gz/+8Y95/PHH65fNnDmTxx9/nKuvvpqFCxfyyiuvMHXqVPbt28eyZctYtmwZDoeDrKwsVqxYgdFobPnKz/H6LC1N0zSu7hbD1d1ieCrzCqqcHsJDggDIHtWDr/NK2XmknPzSWgpKaygoreXrvNL6mTIhQQb6d4pi3d5jLN5ypP51rcEmBnWJYmBSFEkxoXSOshBiNmI2GggyGjCbDERZgoiSvr0Q4hRNhvmQIUMoKGh427W8vDyGDBkCwHXXXcfkyZOZOnUqOTk5jB07FrPZTFJSEsnJyWzdupVBgwa1fOV+DvNTaZpWH+Qn/v9E0J9K13WKKp0UVTjplWgl2GTE59PZZa/kUEk1Ryud7HFU8XVeCX/+fB9nO4cp2GSgQ2QICREhJMWEMqhLFD1t4WwtKGPjwVL6dorkjsGdsYWHNPg+p8fLF/uO8fmuo+QVV+OoqGNgUhR3X9OVfp0jW3S9CCEunvPqmffs2ZOcnBxuvPFG/vWvf1FYWAiAw+FgwIAB9c9LSEjA4XC0TKXfdwmFeXNpmkZChArgEwwGjSs6RnBFx4gGz3V5fBSW13KkrA6nx4vbq+Py+HB5vZRUu7GX11JYXoejoo7PdxWxcOPJDW6HyBA+3W7n9//eQw+blUhLEAZNo7zWTX5JDZVOD9ZgEynxYXSKsrD020I++KaADpEhdI+3EhZsJL+klmqXhyFdYxjWI46EiBDCQ0zUub2UVLsICzZxWWI4Ph3W7DnK1oIyAEKCjIzum8iVXaIbfJ4qp4fSahedoy1omoau6xwuqyUxIgSTMTBPdxDiUnJeYf7cc8/x3HPP8Ze//IXU1FTMZj/MwQ6JhJL9F/99LxKzyUBybBjJsWFNPlfXdQ4W17C3qIq+nSLoEGlh/9Eq3v8mn++Kqqmoc+P2+ugQGcLALlGkXZ7AdT3iMJtUiFbUuVmy+TCbDpWx/2gV9oo6kqItmE0W/r3T0WBDcSbhwSZMRo1ql5cFa/dzZZcorkqOxmI2kVtYwZo9R3F5fCTHhtKnYwQbD5biqHASHRrEyMtsaiNiNlLr9nGkrJaSahegjlXEWYOJC1e/Y063j25xYQzvFU9MmBld19F1mj17qKzGxfbDFeSX1jCsRxxJMaHN+j4hLnXnFebdu3fntddeA+DAgQOsXr0aUCNxu91e/zyHw0FCQsKFV9mY0BjI/6p1XjvAaJpG17gwusadDP6UeCvTx1zerO+PCAni7qFduXvo6Y95fTp7HJWUVruodHoINRuJDjVTVuNml70Cl9fH8J7x9OkYgaZpVDs9LNxYwD++PMhbXx6i1u0lISKYu67uQpeYUNbtPcrmQ2UM7hrD4ORothWU89muIhadcheoqNAgYsLMGDQNt9fHsUon1d+7zIKmQUyomco6Dy6vD0uQkbBgE9Zg9dVkNGDQwKBpaEC1y0theS1lNQ2vtHlVcjRXdIggJsxMYXktmw6VUVbjwhYeQseoEFLirfSIt3Jtj1g6R4dSXuPm37kOHBV1BBk1LGYT8VY1Y2nzoTK+O1rFzf07cHP/jhgNGkcrnRRXOwGwBBmxhYdgMTc8hlRZ50ZHbRAbOw7i86m9mA6Rshcjzuy8wry4uJjY2Fh8Ph/z5s3jRz/6EQCpqan88pe/JCsrC4fDQV5eHv3792/RgutFJUP1UXBWQbC1dd5DYDRoXN4hotHHhvWMO21ZWLCJe6/tyr3XdgVUEGka9SF137Bup32Prus4PT6qnR6Cg4xYg0//tax1edE0MBk0th+pYPXuIooqnURagjAbDdS4PFQ5PVQ5vVQ7PXh9Or7jo3afrhNpCeKq5Cg6R4fSt2MkiZHBrNzpYPm2QpZuPUJZjZvo0CAGdYlmSNdoHBVO8ktqWbv3GC6PD4CusaEcLqvF7W38YEaQUSMmzMy/dzr4Q85eNOC7o6dfkTMqNIjLEsPpFhfGtsPl7DhSga6rdR15/OC2Osit9kY2HiylvNZNTJiZtMsT6J0YTqQliJAgIz5dxxpsYmBSFBazkY83H+bjzYeJDTNzRYcIqpwecu2VWIONZPbvyICkKLYWlLH/WDVXdIjgyuRoXB4f9vI6SmtcVNV58Oo6YcEmgo0GnF4fGjAoKZrIUHVcyOvTqXJ6qHN78fh0gowa1mBT/fkW+SU1LN16hEhLEP06RdI7MZxgUytMgrhAJdUuVuU66JUQTv9OkS16bkhheS3RoWZCgi7e524yzKdNm8aGDRsoLS1l+PDhPPTQQ9TU1PDOO+8AkJaWxoQJEwDVSx8zZgwZGRkYjUaefPLJ1pnJAhBzPBRK8yCxb+u8h7hgzfkD0TSNkCDjWX/xTx3NDkxSs30uVA9bOL+4oQcAHq8Po0E7bWTs9ensP1rF6t1HWb+/mLQrEri5f0d6J4bj8elUOz0cq3Li9Pi4okMEZqOB5dsLefU/B4i0BHHH4CS6HG/l1Li8FFXWkV9Sy87CCpZtLeSyDhE8PKon1mATZTVuSmtclNW6Katx4aiow+31cVOfBPp2iuSbvFKWbSvk/W/yG/08oWYjNS4vPW1WHBV1fLrdjtlooIfNSm6hOqP5fBk06NspkhqXl4PF1Y1u0JJjQ4mzBrPpUCmn3js9yKhxWWIEHSJDqDm+h9XDZqW7zUpVnYeiyjqKKp0cPf5fUUUdNW4vISYjIUEGQoKMWIKMhAYbCTWbsIUH0zk6FLPJoPZqdEiICCHCYqKkykVJjYuYUDMJkSEcLq3l24Iyquo8RIUGEWEJIspiprzWzSdbj+A8vqFOjAihb6cI4sNDSIgIJiEi5HgYG7AEGekQaSE+PJjDZTXssley217JLnsldW4vnaND66cfa5rGO18d5Mv9JYQEGeqPOV3XI444azC5hRUEmwxc2+P0gdCF0nQ/3LK+oKCAUaNGkZOTQ+fOnc/vRY5shgU3wMR/wBW3tGh9QlyqvD6dilo35bVuXF4fBg2OVbnYeLCU/JIabhnQkaHdY+tbXmaTmtLq8fr44rti9h+ton/nKLrHh7HjSAVb8ssIMxtJjLQQazVjDTZh0DRqXB6cHh/BJgN1bh/r9xfz1f5iIi1BpMRbibOasZiNGDUNt0+nrNpFrr2CgtJaRva28cMhSXh9OlsLytl6uIxtBeUcq3ISFmzC59PZ46ii1q2CPcxsxBYRQrw1mPiIYGzhwYSZTTg9XurcPmrdXurcXmpcXqrqPDgq6zhyfA8p/PheXOUpF8cLMxvr23KaBr1s4cSEqQAvP76h9OkwflAnfjgkie+KqliV6+DAserjbTFXkz8Ho0GjW1wYoWYjh0trG3xPpygLPxqSREmNiy/2HWOPo6rB9ybHhrLm0ZHn9fM/W3YG5hmgANEnRuYH/FuHEBeR0aARHWYmOuzkpIMeNrgmJfa054ad0q4yGQ2M6BXPiF7x9cuuOz5ibI6h3U9//eZIigllbP8Opy33+XQclXVEhAQ1qLO5TtzH98QJdtVODxV17vrWRp3bS1GFk1irudHX13W9fi9sYFIUE646GYwuj49jVU5Kql24vKr9V1hex9FKJ4kRIfRODKeHzdpgT7LG5eFIWS3ltW4GdI5qcGzDUVHHF/uOUVnn4bLEcPp0ap0pwIEb5pYosERDiYS5EIHGYNDoEGk57+///lnSYcGmBqEdEmSkS+yZZyqd7YQ7s8lAxygLHaOaX1+o2UQPW3ijjyVEhHDblefZgTgHgX1oPLqbjMyFEIJAD/OYFBmZCyEEAR/m3aA8HzxNH7AQQoi2LLDDPLob6D4V6EII0Y4FdpifmGsurRYhRDsX4GGeor7KQVAhRDsX2GFuTYCg0DZ9wS0hhGiOwA5zTYPortJmEUK0e4Ed5iBzzYUQgrYQ5jHd1MW2fD5/VyKEEH4T+GEe2x08dVB+yN+VCCGE3wR+mHc8fn/Rw5v8W4cQQvhR4Ie5rQ8YzXBEwlwI0X4FfpibzJDYX0bmQoh2LfDDHKDTlXBkC/i8TT5VCCHaojYS5leBuxqO7fF3JUII4RdtI8w7Xqm+Ht7o3zqEEMJP2kaYx/aA4Ajpmwsh2q22EeYGA3QcKCNzIUS71TbCHFTf3LED3HX+rkQIIS66thPmHa8Enxvs2/xdiRBCXHRtJ8yTrlZfD633bx1CCOEHbSfMwxMhtifk/cfflQghxEXXdsIcoOswNTL3evxdiRBCXFRtL8ydFWDf6u9KhBDiomoyzKdPn87QoUO5+eab65fl5uYyceJExo0bx2233cbWrSo8dV3n2WefJS0tjczMTHbs2NF6lTem6zD1NW/dxX1fIYTwsybD/LbbbuOVV15psOyFF17gwQcfZMmSJTz88MO88MILAKxdu5a8vDxWrlzJM888w6xZs1ql6DMKT4S4XtI3F0K0O02G+ZAhQ4iMjGywTNM0qqurAaisrMRmswGQk5PD+PHj0TSNgQMHUlFRQVFRUSuUfRZdh8FB6ZsLIdoX0/l804wZM5g8eTJz587F5/Px3nvvAeBwOEhMTKx/XmJiIg6Hoz7sL4quw+Cb18D+rTqRSAgh2oHzOgD67rvvMn36dNasWcP06dOZOXNmS9d1/pKP9833/tu/dQghxEV0XmH+8ccfk56eDsCYMWPqD4AmJCRgt9vrn2e320lISGiBMs9BeAL0SIOv/grOyov73kII4SfnFeY2m40NGzYA8OWXX9K1a1cAUlNTWbx4Mbqus2XLFsLDwy9ui+WEG6ZDbSl8Nf/iv7cQQvhBkz3zadOmsWHDBkpLSxk+fDgPPfQQzzzzDLNnz8bj8RAcHMzTTz8NwIgRI1izZg1paWlYLBZmz57d6h+gUZ2vgp43wfo/wdX3Q0iEf+oQQoiLpMkw/93vftfo8kWLFp22TNM0nnrqqQuvqiXc8AT8baQanY941N/VCCFEq2pbZ4CeqtOV0DsD/vtHqC72dzVCCNGq2m6YA4x6ClxVsO5Ff1cihBCtqm2Hue0yGDgJNvwNSvP8XY0QQrSath3mACNngMEEOc80XF52yD/1CCFEK2j7YR7REa59CLYvhD0r1bJNb8JL/WDrB/6tTQghWkjbD3OA4f8LtitgaTbkfQHLj89u2fC3k8/ZsRg2/QN8Pr+UKIQQF6J9hLkpGMbPg6oieONmCImEYY9AwQZ1E+iSA7DoZ/DP/4G/j4aiXf6uWAghzkn7CHOAjgPV3HPNABNegWuzwRgMG1+Hfz+p+uqj58KxvbDgBtj9Lz8XLIQQzdd+whxgxGPw6HfQbTiExsAV41T/PPefMGwaXPNzePArNQvmvTtV0AshRABoX2EOYIk6+e+rfgKeOohMgmv/Ry2z2uDeT6B7Kix9GD76mbrOy6kKvoHCby9WxUII0aTzup55m5F8LVz9AFyWAUGWk8uDrXDne7D2RXXC0YG1cPXPoNdo+PoV2Ph3CAqD+z6FDgPA5wXHdgiOUBsDnwfcderfmqZeU9fVCUzB4afXcWwvhMWBJbrh8koHrH0BKg6Dq1odyO02/Pw/r7sOinZCh4FgaH/bcSHasvYd5poGGc83/pgxCEZOh95jYMUM+OwZ9Z9mgB/8HHI/gXd+CDf/Hlb/Bgq3nP4a3VPhh2+DwQgL74M9K6Df7TD0fyCxr3qOfRv8bRTEdoefrgJzmFpeWwZv3aaCPq4n1JTAe5Ng8kqwXX7mz+SuVXPoY3uo9z2h0g7v/giObIaY7jD4Pug/UW1wADxOMJpPbnxOe906Nb2z2wiISjrbWhVC+IGm67p+sd+0oKCAUaNGkZOTQ+fOnS/225+fskMqjDtdpa774tgBr94ErkqwJqiDq8ZgqC4CQxDUlcG630KXoWAKge9y4LKb4bvPwV0Nwx+DoQ/C31JVG6e2FPpOUAdnXVXw9h2qnTPpQ+g+EsoL1HONwfCjtyAsXv1nDFL15X0B/3ocHDtB96qrRv7wLTCZ4fAmtSGoK1ezePatgvwv1YYp+Tr1fvZtKuTH/wU6Dz75uX0+2P4R5PwayvPVnsE9/zxz6J8Lj0vVdybOKti9HPrcevJzBoJKO3yYBYN+DIMm+bsa0YacLTvb98j8XER1Ua2WExL6wI8XwnefwTW/aNiLPyH+Mlh0P+g+uOVPcOXdKrRXzIS1z6uWTV053LsUDv0XPnsWyg6qYPU44Y7XVZADRHZWrZ+/Z8D8462W0Fj4wRQwh6oZOVFd4PppqqWz7kVY9FOw9VHvZU2EySsgsZ+6imRRLmxbqMIyNFbtbexYDK+mwYC7oMs1gA7/fRmO7YHE/tDjRtVi+u4z6DHq7Ovr6B7Y+p56TUs0pD8LyUPV59+XA1vegf2rYdyfYOBdjb/Gyl+p9zu0Xu0BNdfRPbDv33DoS7VhGvo/DfdSAIq/g81vQe5StSHud3vDx2tKYMciOLpb3eRk5Ay1fkGdfGaNh46DTn/v2jJ4a4Jqux3eqJ6TcEXza29N2xfB3pWQ+ceTG9HywxDe4cxtt6Jd8OljcOtf1Ql4F+rwJvU7lf4sRHa68NdrKc4qtVd8tkGKz3v679HZ6Lr6W47srCZctDIZmbe2vC/UQdZTw0/X1UyZf02H1JnqDFWfT4Vvwdfqao/97mg4Qj7h2D7V0nFWqOmTe1eo5b1Gw20L1Bx6gP/+CVYev51fvztgzPNN/0LVVcCqp9SZsa4qtSyhL1z/S7hiPPjc8KfBEBKl2j3/eQnyv1LTPjsNVu0gUzCseV6FtaaptsyxParvH9MdSvYDOkR0VhuhSoeaQRTRoWEthzeq9lN0srquztjfwpCfNr2+96xQ7STdp0KqslDtfdw6/2R7aNdyeP/4iDksHlw1MOUL9V4AR7bAB3ervTGzVR0DieulPvO+HHj/xxDRCbI3N9yzcJ7Yo/pa7eGsmKmOhfx0lQpNkxmiu6rn6rraUMT3bpm9HPt2WDULRj0JHfqf/vj+1Woj4/NA6q9g+KPq1opv36FmdU14FYzfG9v5vPBqOhz+Bq7/Xxj1fxde5z9uVYOB8I5qr/NEuxHUwMbrUfcf+P6emH077PpEfW9IJHS8Uh3r6jDg7O9XfhhynoaYbmqj3Zi6cvjTEEgZCbd974Y2R7bAhz9Rv0cep/pbSn9W3dGsMc5K9bt78L9qsFTynap18r9PX7/n4WzZKWHuT173hbcP7NtUy6ffxNNHV9++pw7KXpZxbq/p80HpATWK7nRVw7D59n34+H4Is6mWUlxv9Qvr85x8jtGsRvrXPqR68q5qNRo7vBE6D4Gu10PSD9R7zLsWuo+CiW+qDYOnTr3nm+NUu+LBr9Tezb5VkHS1CldXlfrj8vnU68f3VucN6D4VPjHd4M53VeBufR+W/VKNuu5bodbHX34A4YkwaaH6A513nQrAO96AzW+qjVFoLNz+d/Wee1eq4yPdU9VeQmgclB9SewuD71OfufwwvPtD9bO47W9qpL9vlQpQg0mtH80I1z0MV96jRrt7V0LaM3Bd9tl/Hsf2wfL/VTcrHzL59APlug5/H6NqCwpTrbpTf+aOnfDaaDWyju6qAvGu99RxHEOQ+jn2m6hG36eOPL+ar+q0Hr9J+yPbG/6+VhSq1luHAYCmpvjuXKI+qzFIrZuUG075HHvVYGDAnWrjUlcO0d1UyFUUqjpO6DBQ7YlGdFBnai//X/Uena5Sv09HdwG6upBe6v+dPhgA9X2rZqnfF80AP/+i8b2k1b+B1XPUvycthJ5pJx97c5z6Gxs4Cdw1sPENCAqFzN+rtuipdn8KH9wLXqeqteswNRj6al7zfs7NIGEuWo7PB6+Mgupj6he6x43qoKtjpwr1ykI10jsxAm3KF39QLaLgSHCWN3xswqsqFOsqYMV0KD2oRj7B4SqMNSNUOdSxBVeVGrGZguFnnzfchS/KVWEXEgnxl6uQvX/1yVHh5rdhyS/UH7zug57p6ozhsLiTr7F6LqyeDZFd4Gc58N5dKoCyN8ORTWr05qyCO/7eMAy+fvVkm+rQf1VrB8BkgZgUtUF7cMOZDyrbt8M/xqsAc9eosB4zV7XsTtj5T7UnccMM2PMvdZC79xgVNofWq3MpLNHw0xwVsn+6Wq3r4Ai1HnYuVqPXqC4qmGN7qI3Z6t+odtuQn6q9nR++BZdnqmMd61+GNS+Ap1YdxzGHqo1/RCewxKhgrj6qgnbYI2pAsPxRtUf6yE4VeKt/o77H61LHnWJ7qFllNcWw/s9q/V95rzpe0zsDbnn55M+kthT+83v4cp7aUP/oXdXGq1/vr6iNePdRak/kzXFqD+2u99TB/COb1YDCWQ4v9VfHtkoPqMce/FK95sH16ozw9OdOTl0+theW/I8aeGT+Aa66Vy0v/g4WjISYrpD6pLrbmSVabWjfvRP2fw5T/nuyVXeegzgJc9GyPE412jyX/uGZeD2w+OeABpffrIL64Hr1x3/D9Oa1IKqL4YuXYNcyNSrufNXpzyn4Bt64RR18Hvmrhnef0nU1gvPUqXMPGpst5PPBN6+q0Xls95Oj7q7XQ95/1B/pne813R/fu0qF5/XT1Dr88w/Ua/7obfUeeWvhm7+DfasKg2P7VLDc+08Vev96Qr3fxH+o9eVxqT0No1mNPL0udYxky7tQZVcj70GTVFsl8vjf2ua34J/ZasNzxTi1bMs7akNg366O2/g8YA5X7aeITvCH/uoY0E3PqRF90U51QL/f7WrdVhVB/+N7LwaD2vj88yF18Lz7KLhxltqgXnbz6a2MxhRshLduVaP37qlq3ZqCT3/esX1qQ1N2CG75owr9/A3wzkQ10LjzXfV7uu63aoN1y8tqj8OxXbUAY7qpDczPv1Cty7+PUZ8j4wX44B61p/XwVrWxOsFdq1pt+1ap42XJ16mRfcVhuH/NyXbdCRVH1M/ZXaPWa0x3yN7U9DpobLVImAuBOn6xe7kKlgttb+m6Olhc8LXaAKQ/2/g5BE35z+/VhiSxnwqkunIV4l2vV3sbRrMaiZ/Y03FVq42SY7sKkvyv4OAXcNeH0Cv95Ov6vCrUIjs3Puo/sYdzps9WVwZoJw/sfz4H1sxVgRocrkKx95izfzZdVyPkVb9Ws74AfvaZapU0h2OHOmh7/S8bhun31ZSoPaVD608uS+inzgM58RldNfDHQWoDFxqrWl1fLVAb9yvGw8Q31PNynlGTB0Ii1c8i/VnVLvw+jxMW/0JtrNAB7XiL5sbGazz0pTrYHhKljjGduvd2DiTMhWgNFYVq1NXYnkBzed2wMEu1aGJSVFvj8lsgKOTM31NdrHb/j+1Ro+W+E9TIuyUOpJ5J+WF1kDDpanUw+UwHABtTUagOrIM6SN8aPE61Z1Gap0J4yM9O76Pv/pfaK7pxlmrTlebB+r+oFsqJ9geog56rZqnpwA+sOXnuR2Nc1aqNp/vUumllEuZCtDVejxrtfv9gaGtyVqoD0K250RBnJfPMhWhrjKaLG+Rwfm0kcdHIBTqEEKINkDAXQog2QMJcCCHaAAlzIYRoAyTMhRCiDZAwF0KINsAvUxO9Xi8AdrvdH28vhBAB6URmnsjQU/klzI8ePQrApEly4X4hhDhXR48eJTm54TVg/HIGaF1dHdu3byc+Ph6jsQUu1iSEEO2A1+vl6NGj9O3bl5CQhpd88EuYCyGEaFlyAFQIIdqAgArztWvXctNNN5GWlsaCBa109bULUFhYyN13301GRgZjx47ljTfUZTXLysrIysoiPT2drKwsysvLm3ili8vr9TJ+/HgeeOABAPLz87njjjtIS0tj6tSpuFwuP1fYUEVFBdnZ2YwePZoxY8awefPmS3odv/7664wdO5abb76ZadOm4XQ6L6l1PH36dIYOHcrNN99cv+xM61PXdZ599lnS0tLIzMxkx44dl0S9c+fOZfTo0WRmZvLggw9SUVFR/9j8+fNJS0vjpptuYt26dZdEvSe89tpr9O7dm5KSEuAC168eIDwejz5q1Cj90KFDutPp1DMzM/W9e/f6u6wGHA6Hvn37dl3Xdb2yslJPT0/X9+7dq8+dO1efP3++ruu6Pn/+fP3555/3Z5mnee211/Rp06bp999/v67rup6dna1/8sknuq7r+v/93//pb7/9tj/LO81jjz2mf/DBB7qu67rT6dTLy8sv2XVst9v1kSNH6rW1tbquq3X70UcfXVLreMOGDfr27dv1sWPH1i870/pcvXq1PnnyZN3n8+mbN2/Wb7/99kui3nXr1ulut1vXdV1//vnn6+vdu3evnpmZqTudTv3QoUP6qFGjdI/H4/d6dV3Xjxw5ot933336DTfcoBcXF+u6fmHrN2BG5lu3biU5OZmkpCTMZjNjx44lJyfH32U1YLPZ6NOnDwBWq5WUlBQcDgc5OTmMHz8egPHjx7Nq1So/VtmQ3W5n9erV3H67uju9rut8+eWX3HTTTQDceuutl9R6rqys5Ouvv66v12w2ExERcUmvY6/XS11dHR6Ph7q6OuLj4y+pdTxkyBAiIyMbLDvT+jyxXNM0Bg4cSEVFBUVFRd9/yYte77BhwzCZ1OS8gQMH1k/hy8nJYezYsZjNZpKSkkhOTmbr1q1+rxdgzpw5PProo2inXFL4QtZvwIS5w+EgMTGx/v8TEhJwOBx+rOjsCgoKyM3NZcCAARQXF2Oz2QCIj4+nuLjYz9WdNHv2bB599FEMx28GXVpaSkRERP0fRmJi4iW1ngsKCoiJiWH69OmMHz+emTNnUlNTc8mu44SEBO677z5GjhzJsGHDsFqt9OnT55Jex8AZ1+f3/w4vxdo/+ugjhg8fDly6ubFq1SpsNhuXXXZZg+UXsn4DJswDSXV1NdnZ2cyYMQOr1drgMU3TGmyJ/enzzz8nJiaGvn37+ruUZvN4POzcuZM777yTxYsXY7FYTjt+cimt4/LycnJycsjJyWHdunXU1tb6pW97IS6l9dmUefPmYTQaueWWW/xdyhnV1tYyf/58Hn744RZ93YC5OUVCQkKDM0YdDgcJCedw66qLxO12k52dTWZmJunp6p6MsbGxFBUVYbPZKCoqIiYmxs9VKps2beKzzz5j7dq1OJ1OqqqqeO6556ioqMDj8WAymbDb7ZfUek5MTCQxMZEBAwYAMHr0aBYsWHDJruP//ve/dO7cub6e9PR0Nm3adEmvYzjz7+z3/w4vpdoXLVrE6tWref311+s3Ppdibhw6dIiCggLGjVM307bb7dx22218+OGHF7R+A2Zk3q9fP/Ly8sjPz8flcrFs2TJSU1P9XVYDuq4zc+ZMUlJSyMrKql+emprK4sWLAVi8eDGjRo3yU4UN/fKXv2Tt2rV89tln/O53v+Oaa67ht7/9LT/4wQ9YsWIFAB9//PEltZ7j4+NJTExk//79AKxfv57u3btfsuu4Y8eOfPvtt9TW1qLrOuvXr6dHjx6X9DqGM//Onliu6zpbtmwhPDy8vh3jT2vXruWVV15h3rx5WCyW+uWpqaksW7YMl8tFfn4+eXl59O/f34+VQu/evVm/fj2fffYZn332GYmJiSxatIj4+PgLWr8BddLQmjVrmD17Nl6vlwkTJjBlyhR/l9TAN998w6RJk+jVq1d9D3ratGn079+fqVOnUlhYSMeOHXnppZeIioryb7Hf89VXX/Haa68xf/588vPzeeSRRygvL+fyyy/nxRdfxGw2+7vEerm5ucycORO3201SUhJz5szB5/Ndsuv4j3/8I8uXL8dkMnH55Zfz3HPP4XA4Lpl1PG3aNDZs2EBpaSmxsbE89NBD3HjjjY2uT13Xefrpp1m3bh0Wi4XZs2fTr18/v9e7YMECXC5X/c98wIABPP3004BqvXz00UcYjUZmzJjBiBEj/F7vHXfcUf94amoqCxcuJCYm5oLWb0CFuRBCiMYFTJtFCCHEmUmYCyFEGyBhLoQQbYCEuRBCtAES5kII0QZImAshRBsgYS6EEG2AhLkQQrQB/w8+4ap7DC0dtAAAAABJRU5ErkJggg==\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"### Feature Importance\n\nexplainerxgbc = shap.TreeExplainer(optuna_xgb)\nshap_values_XGBoost_test = explainerxgbc.shap_values(X_test)\nvals = np.abs(shap_values_XGBoost_test).mean(0)\nfeature_names = X_test.columns\nfeature_importance_xgb = pd.DataFrame(list(zip(feature_names, vals)),\n                                 columns=['col','feature_importance_vals'])\n\n#select backgroud for shap\nbackground = X_train.sample(1000)\nexplainernn = shap.DeepExplainer(optuna_nn, background.to_numpy())\nshap_values_XGBoost_test = explainernn.shap_values(X_test.to_numpy())\nvals = np.abs(shap_values_XGBoost_test).mean(0)\nfeature_names = X_test.columns\nfeature_importance_nn = pd.DataFrame(list(zip(feature_names, vals)),\n                                 columns=['col','feature_importance_vals'])\nfeature_importance_xgb.sort_values('feature_importance_vals', inplace=True, ascending=False)\n#feature_importance_nn.sort_values('feature_importance_vals', inplace=True, ascending=False)\ndisplay(feature_importance_xgb[:20])\ndisplay(feature_importance_nn[:20])","metadata":{"execution":{"iopub.status.busy":"2022-09-27T16:45:40.041054Z","iopub.execute_input":"2022-09-27T16:45:40.041442Z","iopub.status.idle":"2022-09-27T16:45:40.500348Z","shell.execute_reply.started":"2022-09-27T16:45:40.041387Z","shell.execute_reply":"2022-09-27T16:45:40.496738Z"},"trusted":true},"execution_count":97,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mStagingError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/23795463.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mbackground\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mexplainernn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeepExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptuna_nn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackground\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mshap_values_XGBoost_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainernn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshap_values_XGBoost_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mfeature_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/__init__.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mwere\u001b[0m \u001b[0mchosen\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m\"top\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \"\"\"\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mranked_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_rank_order\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_additivity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_additivity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36mshap_values\u001b[0;34m(self, X, ranked_outputs, output_rank_order, check_additivity)\u001b[0m\n\u001b[1;32m    310\u001b[0m                 \u001b[0;31m# run attribution computation graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m                 \u001b[0mfeature_ind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_output_ranks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0msample_phis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphi_symbolic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_ind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjoint_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m                 \u001b[0;31m# assign the attributions to the right part of the output arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, out, model_inputs, X)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfinal_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_with_overridden_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcustom_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36mexecute_with_overridden_gradients\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# define the computation graph for the attribution values using a custom gradient-like computation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0;31m# reinstate the backpropagatable check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py\u001b[0m in \u001b[0;36manon\u001b[0;34m()\u001b[0m\n\u001b[1;32m    363\u001b[0m                     \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                     \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                 \u001b[0mfinal_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m                     \u001b[0mtf_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_gradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_backprop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_gradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    758\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    759\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 760\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3064\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3065\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3066\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3067\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3306\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3307\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3308\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3309\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3310\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1005\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    666\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    667\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 668\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    669\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    992\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mStagingError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py:252 grad_graph  *\n        x_grad = tape.gradient(out, shap_rAnD)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:1090 gradient  **\n        unconnected_gradients=unconnected_gradients)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py:77 imperative_grad\n        compat.as_str(unconnected_gradients.value))\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:148 _gradient_function\n        grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/registry.py:100 lookup\n        \"%s registry has no entry for: %s\" % (self._name, name))\n\n    LookupError: gradient registry has no entry for: shap_Selu\n"],"ename":"StagingError","evalue":"in user code:\n\n    /opt/conda/lib/python3.7/site-packages/shap/explainers/_deep/deep_tf.py:252 grad_graph  *\n        x_grad = tape.gradient(out, shap_rAnD)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:1090 gradient  **\n        unconnected_gradients=unconnected_gradients)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/imperative_grad.py:77 imperative_grad\n        compat.as_str(unconnected_gradients.value))\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/backprop.py:148 _gradient_function\n        grad_fn = ops._gradient_registry.lookup(op_name)  # pylint: disable=protected-access\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/registry.py:100 lookup\n        \"%s registry has no entry for: %s\" % (self._name, name))\n\n    LookupError: gradient registry has no entry for: shap_Selu\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2022-09-27T16:43:21.059339Z","iopub.execute_input":"2022-09-27T16:43:21.059883Z","iopub.status.idle":"2022-09-27T16:43:21.074355Z","shell.execute_reply.started":"2022-09-27T16:43:21.059851Z","shell.execute_reply":"2022-09-27T16:43:21.073447Z"},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"array([[-0.22899244, -1.26739458,  0.99594085, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.22899244, -1.19330221,  0.99594085, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.22899244, -1.04223701,  0.99594085, ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [-0.67822648, -0.65380383,  2.04005009, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.84250641, -0.72387035,  2.04005009, ...,  0.        ,\n         0.        ,  0.        ],\n       [-0.8777894 , -0.61384683,  2.04005009, ...,  0.        ,\n         0.        ,  0.        ]])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = np.random.choice(X_train.shape[0], 1000, replace=False)\ndisplay(mask[:10])\ndisplay(X_train.iloc[mask])","metadata":{"execution":{"iopub.status.busy":"2022-09-27T16:10:08.941536Z","iopub.execute_input":"2022-09-27T16:10:08.942075Z","iopub.status.idle":"2022-09-27T16:10:09.110184Z","shell.execute_reply.started":"2022-09-27T16:10:08.942038Z","shell.execute_reply":"2022-09-27T16:10:09.109091Z"},"trusted":true},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"array([ 80305, 132682,  37876, 140044, 145230,  72815, 127923, 102834,\n        22572,  29595])"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"        num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n95807     -0.228992    -0.756919 -0.200420  0.114034  0.756480  0.238103   \n156852    -0.228992     1.284559 -0.322588  0.948252  1.590109  0.341196   \n46959      0.028741     1.043630 -0.213631 -0.640662 -1.627888 -1.290158   \n166149    -1.110850    -0.995191 -2.394788 -0.338292  1.974639  2.822136   \n172362    -0.609519    -0.772264  0.243621  0.000432  0.190587 -0.143319   \n...             ...          ...       ...       ...       ...       ...   \n164925    -0.588001     0.122593  0.527780 -0.015136 -1.212386 -0.560920   \n69187     -0.283469    -0.117686  0.278495 -0.732796 -0.986118 -0.452136   \n48747      0.729817    -0.218823  0.308189  0.064935 -0.005579 -0.559774   \n20499     -0.129949     0.458423  1.072899 -0.199401 -1.169933 -0.643927   \n152624    -0.972507    -1.063001  0.324275  0.183589  1.974639 -0.303485   \n\n        num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n95807    -0.565231    -0.867306   1.111877        0.351920       0.455054   \n156852   -0.152824     0.083959   0.180234       -0.383518      -0.425115   \n46959    -0.135237     0.111836  -1.373298       -0.555718      -0.743468   \n166149   -0.029552    -0.998243   0.087111       -1.151687      -1.148836   \n172362   -0.034497    -0.268698   0.087111       -1.103148      -1.122254   \n...            ...          ...        ...             ...            ...   \n164925   -0.045210    -0.050649  -0.554177       -0.721034      -0.675990   \n69187    -0.695649     0.256110   0.661461        0.351878       0.473613   \n48747     0.384429    -0.483994  -0.791337       -1.138832      -1.128485   \n20499    -0.471570     0.706725  -1.371331       -0.776333      -0.802956   \n152624   -0.046859    -1.077381   0.087111       -1.151687      -1.148836   \n\n        num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n95807      -0.148357 -0.039135    0.294719    0.313914     0.163729   \n156852      1.448207  0.196649   -0.142946   -0.127285    -0.300076   \n46959       1.440116 -0.240818   -0.572395   -0.759291    -0.046502   \n166149     -1.843661 -1.068713   -1.155422   -1.244777    -0.276910   \n172362     -0.775736 -0.852199   -1.155422   -0.416780    -0.396230   \n...              ...       ...         ...         ...          ...   \n164925     -0.470283 -0.694829   -0.783572   -0.960880    -0.991977   \n69187       0.050083  0.150459    0.292297    0.249950     0.130405   \n48747       0.463845 -0.869881   -1.155422    0.061007    -0.335021   \n20499       1.236727 -0.762093   -0.819713    0.278328    -0.006165   \n152624     -1.538897 -1.068713   -1.155422   -0.643322    -0.219479   \n\n        num__size  num__lbm  num__lop  num__lgp  num__linv  num__llme  \\\n95807   -0.267890 -0.832382  0.447861  0.705072   1.255524  -0.076198   \n156852   0.126807 -0.120326  0.805255  1.268385   0.228336   0.065931   \n46959    1.532725  0.704911 -0.800893 -1.687934  -0.856356   1.548613   \n166149  -0.635467 -1.895231 -2.509324  0.439276  -0.992112  -1.160041   \n172362  -1.324036  0.813551 -0.268984 -0.024723  -0.334367  -1.358829   \n...           ...       ...       ...       ...        ...        ...   \n164925   0.693716  0.765769  0.416516 -0.972856  -0.628074   0.712898   \n69187   -0.083850  0.757730  0.349248 -0.575234   1.899900  -0.138644   \n48747    0.521404  0.468012 -0.134634 -0.082165  -0.837818   0.606048   \n20499    1.318881  1.051666 -0.527777 -1.372663  -0.495777   1.197828   \n152624  -1.814048  0.907765  0.404861  1.895100  -0.420146  -1.780219   \n\n        num__l1amhd  num__l1MAX  num__l3amhd  num__l3MAX  num__l6amhd  \\\n95807      1.109299   -0.123594     1.129819   -0.071221     1.094246   \n156852     0.163304    0.011411     0.135292    0.269357     0.081083   \n46959     -1.376233   -0.212044    -1.378667   -0.418266    -1.363694   \n166149     0.088152   -1.071380     0.091095   -1.074540     0.093311   \n172362     0.088152    0.866867     0.091095   -0.387505     0.093311   \n...             ...         ...          ...         ...          ...   \n164925    -0.274395   -0.696261    -0.195900   -1.074540     0.017295   \n69187      0.615311    1.007184     0.641079    2.087111     0.621241   \n48747     -0.791226   -0.017160    -0.862383    0.501443    -0.991333   \n20499     -1.357531   -0.614936    -1.289086    0.092912    -1.137052   \n152624     0.088152   -0.660566     0.091095   -1.074540     0.093311   \n\n        num__l6MAX  num__l12amhd  num__l12MAX  num__l12mom122  \\\n95807     1.143549      0.910151    -0.123594       -0.691005   \n156852   -0.719619      0.187095     0.011411        0.757908   \n46959     0.305160     -1.450352    -0.212044        1.105346   \n166149   -1.073193      0.096262    -1.071380       -0.452831   \n172362   -0.244493      0.096262     0.866867        0.033318   \n...            ...           ...          ...             ...   \n164925   -0.276558      0.156689    -0.696261        0.350068   \n69187    -0.334856      0.501835     1.007184       -0.610065   \n48747    -1.073193     -1.025930    -0.017160        0.151496   \n20499    -0.617015     -0.991431    -0.614936       -0.289477   \n152624   -0.960620      0.096262    -0.660566       -0.545408   \n\n        num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n95807            0.435577          0.622072        -0.775770       0.575428   \n156852          -0.446815         -0.414910        -0.408160      -0.452746   \n46959           -0.574079         -0.546443         0.981479      -0.619706   \n166149          -1.156285         -1.153815        -1.546371      -1.244234   \n172362           2.549540          1.418287        -0.512380       0.417254   \n...                   ...               ...              ...            ...   \n164925          -0.410363         -0.313314        -0.293702      -0.706867   \n69187            0.209468          0.390368        -0.138013      -0.031744   \n48747           -0.834497         -0.766891         0.564655      -0.924958   \n20499           -0.616184         -0.582365         0.998853      -0.560763   \n152624          -0.444906         -0.451327        -0.702064      -0.003289   \n\n        num__l12vol12m  num__amhd_miss  cat__ind_1.0  cat__ind_2.0  \\\n95807         0.220169       -0.601648           0.0           0.0   \n156852       -0.522885       -0.601648           0.0           0.0   \n46959        -0.637568       -0.601648           0.0           0.0   \n166149       -0.846934        1.662103           0.0           0.0   \n172362        0.040040        1.662103           0.0           0.0   \n...                ...             ...           ...           ...   \n164925       -0.741452       -0.601648           0.0           0.0   \n69187         0.869540       -0.601648           0.0           0.0   \n48747        -0.856730       -0.601648           0.0           0.0   \n20499        -0.522220       -0.601648           0.0           0.0   \n152624        0.159595        1.662103           0.0           0.0   \n\n        cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  cat__ind_7.0  \\\n95807            0.0           0.0           0.0           0.0           0.0   \n156852           0.0           0.0           0.0           0.0           0.0   \n46959            0.0           0.0           0.0           0.0           0.0   \n166149           0.0           0.0           0.0           0.0           0.0   \n172362           0.0           0.0           0.0           0.0           0.0   \n...              ...           ...           ...           ...           ...   \n164925           0.0           0.0           0.0           0.0           0.0   \n69187            0.0           0.0           0.0           0.0           0.0   \n48747            0.0           0.0           0.0           0.0           0.0   \n20499            0.0           0.0           0.0           0.0           0.0   \n152624           0.0           0.0           0.0           0.0           0.0   \n\n        cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n95807            0.0           0.0            0.0            0.0   \n156852           0.0           0.0            0.0            0.0   \n46959            0.0           0.0            0.0            0.0   \n166149           0.0           0.0            0.0            0.0   \n172362           0.0           0.0            0.0            0.0   \n...              ...           ...            ...            ...   \n164925           0.0           0.0            0.0            0.0   \n69187            0.0           0.0            0.0            0.0   \n48747            0.0           0.0            0.0            0.0   \n20499            0.0           0.0            0.0            0.0   \n152624           0.0           0.0            0.0            0.0   \n\n        cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n95807             0.0            0.0            1.0            0.0   \n156852            0.0            0.0            0.0            0.0   \n46959             0.0            0.0            0.0            0.0   \n166149            0.0            0.0            0.0            0.0   \n172362            0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n164925            0.0            0.0            0.0            0.0   \n69187             0.0            0.0            0.0            0.0   \n48747             0.0            0.0            0.0            0.0   \n20499             0.0            0.0            0.0            0.0   \n152624            0.0            1.0            0.0            0.0   \n\n        cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n95807             0.0            0.0            0.0            0.0   \n156852            0.0            0.0            0.0            0.0   \n46959             0.0            0.0            1.0            0.0   \n166149            0.0            0.0            0.0            0.0   \n172362            0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n164925            0.0            0.0            0.0            0.0   \n69187             0.0            0.0            0.0            0.0   \n48747             0.0            0.0            0.0            0.0   \n20499             0.0            0.0            0.0            1.0   \n152624            0.0            0.0            0.0            0.0   \n\n        cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n95807             0.0            0.0            0.0            0.0   \n156852            0.0            0.0            0.0            0.0   \n46959             0.0            0.0            0.0            0.0   \n166149            0.0            0.0            0.0            0.0   \n172362            0.0            1.0            0.0            0.0   \n...               ...            ...            ...            ...   \n164925            1.0            0.0            0.0            0.0   \n69187             0.0            0.0            0.0            0.0   \n48747             0.0            0.0            0.0            0.0   \n20499             0.0            0.0            0.0            0.0   \n152624            0.0            0.0            0.0            0.0   \n\n        cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n95807             0.0            0.0            0.0            0.0   \n156852            0.0            0.0            0.0            0.0   \n46959             0.0            0.0            0.0            0.0   \n166149            0.0            0.0            0.0            0.0   \n172362            0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n164925            0.0            0.0            0.0            0.0   \n69187             0.0            0.0            0.0            0.0   \n48747             0.0            0.0            0.0            0.0   \n20499             0.0            0.0            0.0            0.0   \n152624            0.0            0.0            0.0            0.0   \n\n        cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n95807             0.0            0.0            0.0            0.0   \n156852            0.0            0.0            0.0            0.0   \n46959             0.0            0.0            0.0            0.0   \n166149            0.0            0.0            0.0            0.0   \n172362            0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n164925            0.0            0.0            0.0            0.0   \n69187             0.0            0.0            0.0            0.0   \n48747             0.0            0.0            0.0            0.0   \n20499             0.0            0.0            0.0            0.0   \n152624            0.0            0.0            0.0            0.0   \n\n        cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n95807             0.0            0.0            0.0            0.0   \n156852            0.0            0.0            0.0            0.0   \n46959             0.0            0.0            0.0            0.0   \n166149            0.0            0.0            0.0            0.0   \n172362            0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n164925            0.0            0.0            0.0            0.0   \n69187             0.0            0.0            0.0            0.0   \n48747             0.0            0.0            0.0            0.0   \n20499             0.0            0.0            0.0            0.0   \n152624            0.0            0.0            0.0            0.0   \n\n        cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n95807             0.0            0.0            0.0            0.0   \n156852            0.0            0.0            0.0            0.0   \n46959             0.0            0.0            0.0            0.0   \n166149            0.0            0.0            0.0            0.0   \n172362            0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n164925            0.0            0.0            0.0            0.0   \n69187             0.0            0.0            1.0            0.0   \n48747             0.0            0.0            0.0            0.0   \n20499             0.0            0.0            0.0            0.0   \n152624            0.0            0.0            0.0            0.0   \n\n        cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n95807             0.0            0.0            0.0            0.0   \n156852            0.0            0.0            0.0            0.0   \n46959             0.0            0.0            0.0            0.0   \n166149            0.0            0.0            0.0            1.0   \n172362            0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n164925            0.0            0.0            0.0            0.0   \n69187             0.0            0.0            0.0            0.0   \n48747             0.0            0.0            1.0            0.0   \n20499             0.0            0.0            0.0            0.0   \n152624            0.0            0.0            0.0            0.0   \n\n        cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  cat__ind_47.0  \\\n95807             0.0            0.0            0.0            0.0   \n156852            0.0            0.0            0.0            0.0   \n46959             0.0            0.0            0.0            0.0   \n166149            0.0            0.0            0.0            0.0   \n172362            0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n164925            0.0            0.0            0.0            0.0   \n69187             0.0            0.0            0.0            0.0   \n48747             0.0            0.0            0.0            0.0   \n20499             0.0            0.0            0.0            0.0   \n152624            0.0            0.0            0.0            0.0   \n\n        cat__ind_48.0  cat__ind_49.0  \n95807             0.0            0.0  \n156852            0.0            1.0  \n46959             0.0            0.0  \n166149            0.0            0.0  \n172362            0.0            0.0  \n...               ...            ...  \n164925            0.0            0.0  \n69187             0.0            0.0  \n48747             0.0            0.0  \n20499             0.0            0.0  \n152624            0.0            0.0  \n\n[1000 rows x 86 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>95807</th>\n      <td>-0.228992</td>\n      <td>-0.756919</td>\n      <td>-0.200420</td>\n      <td>0.114034</td>\n      <td>0.756480</td>\n      <td>0.238103</td>\n      <td>-0.565231</td>\n      <td>-0.867306</td>\n      <td>1.111877</td>\n      <td>0.351920</td>\n      <td>0.455054</td>\n      <td>-0.148357</td>\n      <td>-0.039135</td>\n      <td>0.294719</td>\n      <td>0.313914</td>\n      <td>0.163729</td>\n      <td>-0.267890</td>\n      <td>-0.832382</td>\n      <td>0.447861</td>\n      <td>0.705072</td>\n      <td>1.255524</td>\n      <td>-0.076198</td>\n      <td>1.109299</td>\n      <td>-0.123594</td>\n      <td>1.129819</td>\n      <td>-0.071221</td>\n      <td>1.094246</td>\n      <td>1.143549</td>\n      <td>0.910151</td>\n      <td>-0.123594</td>\n      <td>-0.691005</td>\n      <td>0.435577</td>\n      <td>0.622072</td>\n      <td>-0.775770</td>\n      <td>0.575428</td>\n      <td>0.220169</td>\n      <td>-0.601648</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>156852</th>\n      <td>-0.228992</td>\n      <td>1.284559</td>\n      <td>-0.322588</td>\n      <td>0.948252</td>\n      <td>1.590109</td>\n      <td>0.341196</td>\n      <td>-0.152824</td>\n      <td>0.083959</td>\n      <td>0.180234</td>\n      <td>-0.383518</td>\n      <td>-0.425115</td>\n      <td>1.448207</td>\n      <td>0.196649</td>\n      <td>-0.142946</td>\n      <td>-0.127285</td>\n      <td>-0.300076</td>\n      <td>0.126807</td>\n      <td>-0.120326</td>\n      <td>0.805255</td>\n      <td>1.268385</td>\n      <td>0.228336</td>\n      <td>0.065931</td>\n      <td>0.163304</td>\n      <td>0.011411</td>\n      <td>0.135292</td>\n      <td>0.269357</td>\n      <td>0.081083</td>\n      <td>-0.719619</td>\n      <td>0.187095</td>\n      <td>0.011411</td>\n      <td>0.757908</td>\n      <td>-0.446815</td>\n      <td>-0.414910</td>\n      <td>-0.408160</td>\n      <td>-0.452746</td>\n      <td>-0.522885</td>\n      <td>-0.601648</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>46959</th>\n      <td>0.028741</td>\n      <td>1.043630</td>\n      <td>-0.213631</td>\n      <td>-0.640662</td>\n      <td>-1.627888</td>\n      <td>-1.290158</td>\n      <td>-0.135237</td>\n      <td>0.111836</td>\n      <td>-1.373298</td>\n      <td>-0.555718</td>\n      <td>-0.743468</td>\n      <td>1.440116</td>\n      <td>-0.240818</td>\n      <td>-0.572395</td>\n      <td>-0.759291</td>\n      <td>-0.046502</td>\n      <td>1.532725</td>\n      <td>0.704911</td>\n      <td>-0.800893</td>\n      <td>-1.687934</td>\n      <td>-0.856356</td>\n      <td>1.548613</td>\n      <td>-1.376233</td>\n      <td>-0.212044</td>\n      <td>-1.378667</td>\n      <td>-0.418266</td>\n      <td>-1.363694</td>\n      <td>0.305160</td>\n      <td>-1.450352</td>\n      <td>-0.212044</td>\n      <td>1.105346</td>\n      <td>-0.574079</td>\n      <td>-0.546443</td>\n      <td>0.981479</td>\n      <td>-0.619706</td>\n      <td>-0.637568</td>\n      <td>-0.601648</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>166149</th>\n      <td>-1.110850</td>\n      <td>-0.995191</td>\n      <td>-2.394788</td>\n      <td>-0.338292</td>\n      <td>1.974639</td>\n      <td>2.822136</td>\n      <td>-0.029552</td>\n      <td>-0.998243</td>\n      <td>0.087111</td>\n      <td>-1.151687</td>\n      <td>-1.148836</td>\n      <td>-1.843661</td>\n      <td>-1.068713</td>\n      <td>-1.155422</td>\n      <td>-1.244777</td>\n      <td>-0.276910</td>\n      <td>-0.635467</td>\n      <td>-1.895231</td>\n      <td>-2.509324</td>\n      <td>0.439276</td>\n      <td>-0.992112</td>\n      <td>-1.160041</td>\n      <td>0.088152</td>\n      <td>-1.071380</td>\n      <td>0.091095</td>\n      <td>-1.074540</td>\n      <td>0.093311</td>\n      <td>-1.073193</td>\n      <td>0.096262</td>\n      <td>-1.071380</td>\n      <td>-0.452831</td>\n      <td>-1.156285</td>\n      <td>-1.153815</td>\n      <td>-1.546371</td>\n      <td>-1.244234</td>\n      <td>-0.846934</td>\n      <td>1.662103</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>172362</th>\n      <td>-0.609519</td>\n      <td>-0.772264</td>\n      <td>0.243621</td>\n      <td>0.000432</td>\n      <td>0.190587</td>\n      <td>-0.143319</td>\n      <td>-0.034497</td>\n      <td>-0.268698</td>\n      <td>0.087111</td>\n      <td>-1.103148</td>\n      <td>-1.122254</td>\n      <td>-0.775736</td>\n      <td>-0.852199</td>\n      <td>-1.155422</td>\n      <td>-0.416780</td>\n      <td>-0.396230</td>\n      <td>-1.324036</td>\n      <td>0.813551</td>\n      <td>-0.268984</td>\n      <td>-0.024723</td>\n      <td>-0.334367</td>\n      <td>-1.358829</td>\n      <td>0.088152</td>\n      <td>0.866867</td>\n      <td>0.091095</td>\n      <td>-0.387505</td>\n      <td>0.093311</td>\n      <td>-0.244493</td>\n      <td>0.096262</td>\n      <td>0.866867</td>\n      <td>0.033318</td>\n      <td>2.549540</td>\n      <td>1.418287</td>\n      <td>-0.512380</td>\n      <td>0.417254</td>\n      <td>0.040040</td>\n      <td>1.662103</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>164925</th>\n      <td>-0.588001</td>\n      <td>0.122593</td>\n      <td>0.527780</td>\n      <td>-0.015136</td>\n      <td>-1.212386</td>\n      <td>-0.560920</td>\n      <td>-0.045210</td>\n      <td>-0.050649</td>\n      <td>-0.554177</td>\n      <td>-0.721034</td>\n      <td>-0.675990</td>\n      <td>-0.470283</td>\n      <td>-0.694829</td>\n      <td>-0.783572</td>\n      <td>-0.960880</td>\n      <td>-0.991977</td>\n      <td>0.693716</td>\n      <td>0.765769</td>\n      <td>0.416516</td>\n      <td>-0.972856</td>\n      <td>-0.628074</td>\n      <td>0.712898</td>\n      <td>-0.274395</td>\n      <td>-0.696261</td>\n      <td>-0.195900</td>\n      <td>-1.074540</td>\n      <td>0.017295</td>\n      <td>-0.276558</td>\n      <td>0.156689</td>\n      <td>-0.696261</td>\n      <td>0.350068</td>\n      <td>-0.410363</td>\n      <td>-0.313314</td>\n      <td>-0.293702</td>\n      <td>-0.706867</td>\n      <td>-0.741452</td>\n      <td>-0.601648</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>69187</th>\n      <td>-0.283469</td>\n      <td>-0.117686</td>\n      <td>0.278495</td>\n      <td>-0.732796</td>\n      <td>-0.986118</td>\n      <td>-0.452136</td>\n      <td>-0.695649</td>\n      <td>0.256110</td>\n      <td>0.661461</td>\n      <td>0.351878</td>\n      <td>0.473613</td>\n      <td>0.050083</td>\n      <td>0.150459</td>\n      <td>0.292297</td>\n      <td>0.249950</td>\n      <td>0.130405</td>\n      <td>-0.083850</td>\n      <td>0.757730</td>\n      <td>0.349248</td>\n      <td>-0.575234</td>\n      <td>1.899900</td>\n      <td>-0.138644</td>\n      <td>0.615311</td>\n      <td>1.007184</td>\n      <td>0.641079</td>\n      <td>2.087111</td>\n      <td>0.621241</td>\n      <td>-0.334856</td>\n      <td>0.501835</td>\n      <td>1.007184</td>\n      <td>-0.610065</td>\n      <td>0.209468</td>\n      <td>0.390368</td>\n      <td>-0.138013</td>\n      <td>-0.031744</td>\n      <td>0.869540</td>\n      <td>-0.601648</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>48747</th>\n      <td>0.729817</td>\n      <td>-0.218823</td>\n      <td>0.308189</td>\n      <td>0.064935</td>\n      <td>-0.005579</td>\n      <td>-0.559774</td>\n      <td>0.384429</td>\n      <td>-0.483994</td>\n      <td>-0.791337</td>\n      <td>-1.138832</td>\n      <td>-1.128485</td>\n      <td>0.463845</td>\n      <td>-0.869881</td>\n      <td>-1.155422</td>\n      <td>0.061007</td>\n      <td>-0.335021</td>\n      <td>0.521404</td>\n      <td>0.468012</td>\n      <td>-0.134634</td>\n      <td>-0.082165</td>\n      <td>-0.837818</td>\n      <td>0.606048</td>\n      <td>-0.791226</td>\n      <td>-0.017160</td>\n      <td>-0.862383</td>\n      <td>0.501443</td>\n      <td>-0.991333</td>\n      <td>-1.073193</td>\n      <td>-1.025930</td>\n      <td>-0.017160</td>\n      <td>0.151496</td>\n      <td>-0.834497</td>\n      <td>-0.766891</td>\n      <td>0.564655</td>\n      <td>-0.924958</td>\n      <td>-0.856730</td>\n      <td>-0.601648</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>20499</th>\n      <td>-0.129949</td>\n      <td>0.458423</td>\n      <td>1.072899</td>\n      <td>-0.199401</td>\n      <td>-1.169933</td>\n      <td>-0.643927</td>\n      <td>-0.471570</td>\n      <td>0.706725</td>\n      <td>-1.371331</td>\n      <td>-0.776333</td>\n      <td>-0.802956</td>\n      <td>1.236727</td>\n      <td>-0.762093</td>\n      <td>-0.819713</td>\n      <td>0.278328</td>\n      <td>-0.006165</td>\n      <td>1.318881</td>\n      <td>1.051666</td>\n      <td>-0.527777</td>\n      <td>-1.372663</td>\n      <td>-0.495777</td>\n      <td>1.197828</td>\n      <td>-1.357531</td>\n      <td>-0.614936</td>\n      <td>-1.289086</td>\n      <td>0.092912</td>\n      <td>-1.137052</td>\n      <td>-0.617015</td>\n      <td>-0.991431</td>\n      <td>-0.614936</td>\n      <td>-0.289477</td>\n      <td>-0.616184</td>\n      <td>-0.582365</td>\n      <td>0.998853</td>\n      <td>-0.560763</td>\n      <td>-0.522220</td>\n      <td>-0.601648</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>152624</th>\n      <td>-0.972507</td>\n      <td>-1.063001</td>\n      <td>0.324275</td>\n      <td>0.183589</td>\n      <td>1.974639</td>\n      <td>-0.303485</td>\n      <td>-0.046859</td>\n      <td>-1.077381</td>\n      <td>0.087111</td>\n      <td>-1.151687</td>\n      <td>-1.148836</td>\n      <td>-1.538897</td>\n      <td>-1.068713</td>\n      <td>-1.155422</td>\n      <td>-0.643322</td>\n      <td>-0.219479</td>\n      <td>-1.814048</td>\n      <td>0.907765</td>\n      <td>0.404861</td>\n      <td>1.895100</td>\n      <td>-0.420146</td>\n      <td>-1.780219</td>\n      <td>0.088152</td>\n      <td>-0.660566</td>\n      <td>0.091095</td>\n      <td>-1.074540</td>\n      <td>0.093311</td>\n      <td>-0.960620</td>\n      <td>0.096262</td>\n      <td>-0.660566</td>\n      <td>-0.545408</td>\n      <td>-0.444906</td>\n      <td>-0.451327</td>\n      <td>-0.702064</td>\n      <td>-0.003289</td>\n      <td>0.159595</td>\n      <td>1.662103</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows  86 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"random.sample([1,3,5,6],2)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T16:15:38.954406Z","iopub.execute_input":"2022-09-27T16:15:38.955167Z","iopub.status.idle":"2022-09-27T16:15:38.962729Z","shell.execute_reply.started":"2022-09-27T16:15:38.955131Z","shell.execute_reply":"2022-09-27T16:15:38.961555Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"[3, 6]"},"metadata":{}}]},{"cell_type":"code","source":"X_train[random.sample(list(X_train.index), 10)]","metadata":{"execution":{"iopub.status.busy":"2022-09-27T16:17:44.889794Z","iopub.execute_input":"2022-09-27T16:17:44.890159Z","iopub.status.idle":"2022-09-27T16:17:44.934295Z","shell.execute_reply.started":"2022-09-27T16:17:44.890126Z","shell.execute_reply":"2022-09-27T16:17:44.932793Z"},"trusted":true},"execution_count":76,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/67148567.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1372\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1373\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1374\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: \"None of [Int64Index([32650, 170056, 68280, 65943, 7428, 71796, 96881, 29774, 123174,\\n            164219],\\n           dtype='int64')] are in the [columns]\""],"ename":"KeyError","evalue":"\"None of [Int64Index([32650, 170056, 68280, 65943, 7428, 71796, 96881, 29774, 123174,\\n            164219],\\n           dtype='int64')] are in the [columns]\"","output_type":"error"}]},{"cell_type":"code","source":"random.sample(list(X_train.index), 10)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T16:17:55.507607Z","iopub.execute_input":"2022-09-27T16:17:55.508051Z","iopub.status.idle":"2022-09-27T16:17:55.539546Z","shell.execute_reply.started":"2022-09-27T16:17:55.508012Z","shell.execute_reply":"2022-09-27T16:17:55.538286Z"},"trusted":true},"execution_count":77,"outputs":[{"execution_count":77,"output_type":"execute_result","data":{"text/plain":"[33028, 45842, 90664, 163606, 60404, 128616, 2148, 90999, 161860, 127684]"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Error Analysis","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:40:01.505602Z","iopub.execute_input":"2022-09-27T15:40:01.506022Z","iopub.status.idle":"2022-09-27T15:40:01.514386Z","shell.execute_reply.started":"2022-09-27T15:40:01.505940Z","shell.execute_reply":"2022-09-27T15:40:01.513452Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:50:40.796604Z","iopub.execute_input":"2022-09-27T15:50:40.797009Z","iopub.status.idle":"2022-09-27T15:50:40.821007Z","shell.execute_reply.started":"2022-09-27T15:50:40.796974Z","shell.execute_reply":"2022-09-27T15:50:40.819661Z"},"trusted":true},"execution_count":45,"outputs":[{"execution_count":45,"output_type":"execute_result","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      350   0.064108  0.020667  0.031485    0.055273  0.033387   0.033121   \n\n  xgbo_train  xgbo_val xgbo_test nn4_train   nn4_val  nn4_test nn6_train  \\\n0   0.055539  0.034006  0.033075  0.057466  0.029474  0.022955  0.057071   \n\n    nn6_val nn6_test nn4opt_train nn4opt_val nn4opt_test nn6opt_train  \\\n0  0.027439  0.02969     0.051402   0.027272    0.023739     0.051426   \n\n  nn6opt_val nn6opt_test  \n0   0.027512    0.029561  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n      <th>nn4_train</th>\n      <th>nn4_val</th>\n      <th>nn4_test</th>\n      <th>nn6_train</th>\n      <th>nn6_val</th>\n      <th>nn6_test</th>\n      <th>nn4opt_train</th>\n      <th>nn4opt_val</th>\n      <th>nn4opt_test</th>\n      <th>nn6opt_train</th>\n      <th>nn6opt_val</th>\n      <th>nn6opt_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>350</td>\n      <td>0.064108</td>\n      <td>0.020667</td>\n      <td>0.031485</td>\n      <td>0.055273</td>\n      <td>0.033387</td>\n      <td>0.033121</td>\n      <td>0.055539</td>\n      <td>0.034006</td>\n      <td>0.033075</td>\n      <td>0.057466</td>\n      <td>0.029474</td>\n      <td>0.022955</td>\n      <td>0.057071</td>\n      <td>0.027439</td>\n      <td>0.02969</td>\n      <td>0.051402</td>\n      <td>0.027272</td>\n      <td>0.023739</td>\n      <td>0.051426</td>\n      <td>0.027512</td>\n      <td>0.029561</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general point:\n# compared to NN, xgb is harder to regularize\n# in NN, you can simply shrink coefficient towards constant prediction.\n# in xgb, you can not do that. the only way to regularize is via hyperparameters.\n# in other words, by tweaking hyperpars, in NN you can approach R^2=0.0 prediction from a constant model arbitrarily close\n# in xgb, you can not do that.\n# by setting eta as low as 0.1% you can bring r2 down to 0.1%, but lowering eta further actyally increases abs(r2).\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:40:01.561694Z","iopub.execute_input":"2022-09-27T15:40:01.561982Z","iopub.status.idle":"2022-09-27T15:40:01.567179Z","shell.execute_reply.started":"2022-09-27T15:40:01.561958Z","shell.execute_reply":"2022-09-27T15:40:01.566105Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"print('total time for the script: ', time.time()-time0)\ndisplay(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:40:01.568433Z","iopub.execute_input":"2022-09-27T15:40:01.569039Z","iopub.status.idle":"2022-09-27T15:40:01.594427Z","shell.execute_reply.started":"2022-09-27T15:40:01.568985Z","shell.execute_reply":"2022-09-27T15:40:01.593545Z"},"trusted":true},"execution_count":39,"outputs":[{"name":"stdout","text":"total time for the script:  2493.541243314743\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      350   0.064108  0.020667  0.031485    0.055273  0.033387   0.033121   \n\n  xgbo_train  xgbo_val xgbo_test nn4_train   nn4_val  nn4_test nn6_train  \\\n0   0.055539  0.034006  0.033075  0.057466  0.029474  0.022955  0.057071   \n\n    nn6_val nn6_test nn4opt_train nn4opt_val nn4opt_test nn6opt_train  \\\n0  0.027439  0.02969     0.051402   0.027272    0.023739     0.051426   \n\n  nn6opt_val nn6opt_test  \n0   0.027512    0.029561  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n      <th>nn4_train</th>\n      <th>nn4_val</th>\n      <th>nn4_test</th>\n      <th>nn6_train</th>\n      <th>nn6_val</th>\n      <th>nn6_test</th>\n      <th>nn4opt_train</th>\n      <th>nn4opt_val</th>\n      <th>nn4opt_test</th>\n      <th>nn6opt_train</th>\n      <th>nn6opt_val</th>\n      <th>nn6opt_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>350</td>\n      <td>0.064108</td>\n      <td>0.020667</td>\n      <td>0.031485</td>\n      <td>0.055273</td>\n      <td>0.033387</td>\n      <td>0.033121</td>\n      <td>0.055539</td>\n      <td>0.034006</td>\n      <td>0.033075</td>\n      <td>0.057466</td>\n      <td>0.029474</td>\n      <td>0.022955</td>\n      <td>0.057071</td>\n      <td>0.027439</td>\n      <td>0.02969</td>\n      <td>0.051402</td>\n      <td>0.027272</td>\n      <td>0.023739</td>\n      <td>0.051426</td>\n      <td>0.027512</td>\n      <td>0.029561</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":" \n# def objective_nn(trial):\n    \n#     tf.keras.backend.clear_session()\n    \n#     with strategy.scope():\n#         # Generate our trial model.\n#         model = create_snnn_model(trial)\n\n#         callbacks = [\n#         tf.keras.callbacks.EarlyStopping(patience=40),\n#         TFKerasPruningCallback(trial, \"val_loss\"),\n#     ]\n\n#         # Fit the model on the training data.\n#         # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n#         history = model.fit(X_train, y_train, \n#                                 validation_data=(X_val, y_val),\n#                                 batch_size=2048, \n#                                 epochs=500, \n#                                 verbose=1, \n#                                 callbacks=callbacks)\n\n#         # Evaluate the model accuracy on the validation set.\n#         score = model.evaluate(X_val, y_val, verbose=0)\n#         return score[1]\n\n# trials = 50\n\n# study = optuna.create_study(direction=\"minimize\", \n#                             sampler=optuna.samplers.TPESampler(), \n#                             pruner=optuna.pruners.HyperbandPruner())\n# study.optimize(objective_nn, n_trials=trials)\n# pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n# complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n# temp = study.best_params\n# display(study.best_params, time.time()-time1)\n\n# optimal_hyperpars = list(temp.values())\n# display(optimal_hyperpars)\n# print(time.time()-time1, optimal_hyperpars)\n\n# optuna_nn = create_snnn_model_hyperpars(neurons_base=optimal_hyperpars[0], l2_reg_rate=optimal_hyperpars[1])\n# history = optuna_nn.fit(X_train, y_train, \n#                         validation_data=(X_val, y_val),\n#                         batch_size=2048, \n#                         epochs=1000,\n#                         verbose=1, \n#                         callbacks=[early_stopping50])\n\n# results.loc[results.min_prd==min_prd,'nn4opt_train':'nn4opt_test'] = \\\n# [r2_score(y_train, optuna_nn.predict(X_train)), \n# r2_score(y_val, optuna_nn.predict(X_val)),\n# r2_score(y_test, optuna_nn.predict(X_test))]","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:40:01.595951Z","iopub.execute_input":"2022-09-27T15:40:01.596283Z","iopub.status.idle":"2022-09-27T15:40:01.604188Z","shell.execute_reply.started":"2022-09-27T15:40:01.596251Z","shell.execute_reply":"2022-09-27T15:40:01.603224Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# # try optuna for NN:\n\n# def objective(trial):\n\n#     n_layers = trial.suggest_int('n_layers', 1, 3)\n#     model = tf.keras.Sequential()\n#     for i in range(n_layers):\n#         num_hidden = trial.suggest_int(f'n_units_l{i}', 4, 128, log=True)\n#         model.add(tf.keras.layers.Dense(num_hidden, activation='relu'))\n#     model.add(tf.keras.layers.Dense(1))\n#     display(model.summary())\n#     return accuracy\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=10)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:40:01.605503Z","iopub.execute_input":"2022-09-27T15:40:01.605986Z","iopub.status.idle":"2022-09-27T15:40:01.616509Z","shell.execute_reply.started":"2022-09-27T15:40:01.605953Z","shell.execute_reply":"2022-09-27T15:40:01.615540Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# on nns:\n# - try classic regularizers (l1, l2 etc)\n# - try different architecture (not snnn)\n# classic architecture:\n# He initialization, elu activation, batch norm, l2 reg, adam.\n\n# - try exotic architecture, e.g., wide'n'deep\n# \n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:40:01.618074Z","iopub.execute_input":"2022-09-27T15:40:01.618491Z","iopub.status.idle":"2022-09-27T15:40:01.627931Z","shell.execute_reply.started":"2022-09-27T15:40:01.618458Z","shell.execute_reply":"2022-09-27T15:40:01.626848Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# usually self-norm seems better: it overfits less and runs faster\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T15:40:01.629153Z","iopub.execute_input":"2022-09-27T15:40:01.630155Z","iopub.status.idle":"2022-09-27T15:40:01.637211Z","shell.execute_reply.started":"2022-09-27T15:40:01.630128Z","shell.execute_reply":"2022-09-27T15:40:01.636281Z"},"trusted":true},"execution_count":43,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Load libraries and data.\n2. pEDA. Look at feature distribution, fix them if they do not look right.\n3. Train-test split. Most likely couple years into test set. 2015-2018?. Impute missing values.\n4. Transform numerical features, add ohe for inds.\n5. Fit classic models: ols as a baseline, then xgb.\n6. Fir DL.\n\n","metadata":{}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle, shap\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, KFold, PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\npd.set_option('display.max_columns', 110)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T20:49:59.226285Z","iopub.execute_input":"2022-09-06T20:49:59.226865Z","iopub.status.idle":"2022-09-06T20:49:59.237013Z","shell.execute_reply.started":"2022-09-06T20:49:59.226830Z","shell.execute_reply":"2022-09-06T20:49:59.235575Z"},"trusted":true},"execution_count":186,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-09-06T20:50:51.720778Z","iopub.execute_input":"2022-09-06T20:50:51.721166Z","iopub.status.idle":"2022-09-06T20:50:51.732527Z","shell.execute_reply.started":"2022-09-06T20:50:51.721135Z","shell.execute_reply":"2022-09-06T20:50:51.731242Z"},"trusted":true},"execution_count":190,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T20:50:51.953448Z","iopub.execute_input":"2022-09-06T20:50:51.953789Z","iopub.status.idle":"2022-09-06T20:50:51.960868Z","shell.execute_reply.started":"2022-09-06T20:50:51.953761Z","shell.execute_reply":"2022-09-06T20:50:51.959845Z"},"trusted":true},"execution_count":191,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"#min_prd_list = range(100, 676, 25)\nmin_prd_list = [150, 250, 350, 450, 550, 650]\n#min_prd = min_prd_list[0]\nwindows_width = 3*12\ncv_regularizer=0.2\noptuna_trials = 20\ntime0 = time.time()\n\nresults = pd.DataFrame(columns = ['min_prd', 'xgbf_train', 'xgbf_val', 'xgbf_test', \n                                  'xgbgs_train', 'xgbgs_val', 'xgbgs_test', \n                                  'xgbo_train', 'xgbo_val', 'xgbo_test'])\nresults.min_prd = min_prd_list\n\nfor min_prd in min_prd_list:\n\n    with open('../input/mleap-46-preprocessed/MLEAP_46_v0.pkl', 'rb') as pickled_one:\n        df = pickle.load(pickled_one)\n    df = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+10))]\n    df_cnt = df.count()\n    empty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\n    df.drop(columns=empty_cols, inplace=True)\n    #display(df.shape, df.head(), df.year.describe(), df.count())\n\n    features_miss_dummies = ['amhd', 'BAspr']\n    for col in features_miss_dummies:\n        if col in df.columns:\n            df[col+'_miss'] = df[col].isnull().astype(int)\n\n    temp_cols = ['PERMNO', 'year', 'prd']\n    df.reset_index(inplace=True, drop=True)\n    X = df.copy()\n    y = X.pop('RET')\n\n    train_indx = X.prd<(min_prd+windows_width-1)\n    val_indx = X['prd'].isin(range(min_prd+windows_width-1, min_prd+windows_width+2))\n    val_indx_extra = X['prd'].isin(range(min_prd+windows_width+5, min_prd+windows_width+8))\n    test_indx = X['prd'].isin(range(min_prd+windows_width+2, min_prd+windows_width+5))\n\n    X_train = X[train_indx]\n    X_val = X[val_indx]\n    X_val_extra = X[val_indx_extra]\n    X_test = X[test_indx]\n    y_train = y[train_indx]\n    y_val = y[val_indx]\n    y_val_extra = y[val_indx_extra]\n    y_test = y[test_indx]\n\n    #display(X_train.head(3), X_train.tail(3), y_train.head(3), y_train.tail(3))\n    display(X_train.shape, X_val.shape, X_test.shape, X_train.prd.describe(), X_val.prd.describe(), X_test.prd.describe())\n\n    X_train.drop(columns=temp_cols, inplace=True)\n    X_val.drop(columns=temp_cols, inplace=True)\n    X_val_extra.drop(columns=temp_cols, inplace=True)\n    X_test.drop(columns=temp_cols, inplace=True)\n\n    #display(X_train.tail())\n    col_cat = ['ind']\n    col_num = [x for x in X_train.columns if x not in col_cat]\n    for col in col_num:\n        X_train[col] = X_train[col].fillna(X_train[col].median())\n        X_val[col] = X_val[col].fillna(X_train[col].median())\n        X_val_extra[col] = X_val_extra[col].fillna(X_train[col].median())\n        X_test[col] = X_test[col].fillna(X_train[col].median())\n    for col in col_cat:\n        X_train[col] = X_train[col].fillna(value=-1000)\n        X_val[col] = X_val[col].fillna(value=-1000)\n        X_val_extra[col] = X_val_extra[col].fillna(value=-1000)\n        X_test[col] = X_test[col].fillna(value=-1000)\n\n    #display(X_train.tail())\n    feature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                            (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                            remainder=\"passthrough\")\n\n    print('Number of features before transformation: ', X_train.shape)\n    train_index, val_index, val_index_extra, test_index = X_train.index, X_val.index, X_val_extra.index, X_test.index\n    X_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\n    X_val = pd.DataFrame(feature_transformer.transform(X_val), columns=feature_transformer.get_feature_names_out())\n    X_val_extra = pd.DataFrame(feature_transformer.transform(X_val_extra), columns=feature_transformer.get_feature_names_out())\n    X_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\n    print('time to do feature proprocessing: ')\n    print('Number of features after transformation: ', X_train.shape, X_val.shape, X_val_extra.shape, X_test.shape)\n    X_train.index = train_index\n    X_val.index = val_index\n    X_val_extra.index = val_index_extra\n    X_test.index = test_index\n    #display(X_train.tail())\n\n    X = pd.concat([X_train, X_val])\n    y = pd.concat([y_train, y_val])\n    #display(X,y)\n\n    X_ = pd.concat([X_train, X_val, X_val_extra])\n    y_ = pd.concat([y_train, y_val, y_val_extra])\n    #display(X,y, X_,y_)\n\n    print('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n    print('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\n    xgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=400, max_depth=4, eta=0.02, colsample_bytree=0.4, subsample=0.6)\n    xgb1.fit(X_train, y_train)\n    print('fixed XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\n    print('XGB val:', mean_absolute_error(y_val, xgb1.predict(X_val)), r2_score(y_val, xgb1.predict(X_val)))\n    print('XGB val extra:', mean_absolute_error(y_val_extra, xgb1.predict(X_val_extra)), r2_score(y_val_extra, xgb1.predict(X_val_extra)))\n    print('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbf_train':'xgbf_test'] = \\\n    [r2_score(y_train, xgb1.predict(X_train)), \n    r2_score(y_val, xgb1.predict(X_val)),\n    r2_score(y_test, xgb1.predict(X_test))]\n\n    time1 = time.time()\n\n    # Create a list where train data indices are -1 and validation data indices are 0\n    split_index = [-1 if x in X_train.index else 0 for x in X.index]\n    pds = PredefinedSplit(test_fold = split_index)\n\n    xgb = XGBRegressor(tree_method = 'gpu_hist')\n    param_grid = {'n_estimators':[400, 600, 800], 'max_depth':[2,3,4,5], 'eta':[0.006, 0.012, 0.02], \n                  'subsample':[0.6], 'colsample_bytree':[0.6]}\n    xgbgs = GridSearchCV(estimator = xgb, cv=pds, param_grid=param_grid)\n\n    # Fit with all data\n    xgbgs.fit(X_, y_)\n\n    print('gs XGB', xgbgs.best_params_, xgbgs.best_score_, time.time()-time1)\n    print('XGB train:', mean_absolute_error(y_train, xgbgs.predict(X_train)), r2_score(y_train, xgbgs.predict(X_train)))\n    print('XGB validation:', mean_absolute_error(y_val, xgbgs.predict(X_val)), r2_score(y_val, xgbgs.predict(X_val)))\n    print('XGB validation extra:', mean_absolute_error(y_val_extra, xgbgs.predict(X_val_extra)), r2_score(y_val_extra, xgbgs.predict(X_val_extra)))\n    print('XGB test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_score(y_test, xgbgs.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbgs_train':'xgbgs_test'] = \\\n    [r2_score(y_train, xgbgs.predict(X_train)), \n    r2_score(y_val, xgbgs.predict(X_val)),\n    r2_score(y_test, xgbgs.predict(X_test))]\n\n    time1 = time.time()\n    def objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n        params = {\n        \"tree_method\": 'gpu_hist',\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 1500),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.0005, 0.03),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.05, 0.95),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 0.95),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 50.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 500.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 100.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n        model = XGBRegressor(**params, njobs=-1)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose = False)\n\n        score_train = r2_score(y_train, model.predict(X_train))\n        score_val = r2_score(y_val, model.predict(X_val))\n        score_val_extra = r2_score(y_val_extra, model.predict(X_val_extra)) \n        score_val = (score_val+score_val_extra)/2\n        overfit = np.abs(score_train-score_val)\n\n        return score_val-cv_regularizer*overfit\n\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=optuna_trials)\n    print('Total time for hypermarameter optimization ', time.time()-time1)\n    hp = study.best_params\n    for key, value in hp.items():\n        print(f\"{key:>20s} : {value}\")\n    print(f\"{'best objective value':>20s} : {study.best_value}\")\n    optuna_hyperpars = study.best_params\n    optuna_hyperpars['tree_method']='gpu_hist'\n    optuna_xgb = XGBRegressor(**optuna_hyperpars)\n    optuna_xgb.fit(X, y)\n    print('Optuna XGB train: \\n', \n          mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), '\\nvalidation \\n',\n          mean_absolute_error(y_val, optuna_xgb.predict(X_val)), r2_score(y_val, optuna_xgb.predict(X_val)),\n          mean_absolute_error(y_val_extra, optuna_xgb.predict(X_val_extra)), r2_score(y_val_extra, optuna_xgb.predict(X_val_extra)), '\\ntest \\n',\n          mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbo_train':'xgbo_test'] = \\\n    [r2_score(y_train, optuna_xgb.predict(X_train)), \n    r2_score(y_val, optuna_xgb.predict(X_val)),\n    r2_score(y_test, optuna_xgb.predict(X_test))]\n\n    display(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T20:50:52.118595Z","iopub.execute_input":"2022-09-06T20:50:52.118908Z","iopub.status.idle":"2022-09-06T21:03:10.325356Z","shell.execute_reply.started":"2022-09-06T20:50:52.118880Z","shell.execute_reply":"2022-09-06T21:03:10.324553Z"},"trusted":true},"execution_count":192,"outputs":[{"output_type":"display_data","data":{"text/plain":"(46500, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(4444, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(4403, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    46500.000000\nmean       167.403097\nstd         10.350122\nmin        149.000000\n25%        159.000000\n50%        168.000000\n75%        176.000000\nmax        184.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    4444.000000\nmean      185.997525\nstd         0.816079\nmin       185.000000\n25%       185.000000\n50%       186.000000\n75%       187.000000\nmax       187.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    4403.000000\nmean      188.997956\nstd         0.818208\nmin       188.000000\n25%       188.000000\n50%       189.000000\n75%       190.000000\nmax       190.000000\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (46500, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (46500, 84) (4444, 84) (4381, 84) (4403, 84)\nmae of a constant model 7.989002798117338\nR2 of a constant model 0.0\nfixed XGB train: 7.2936517680386475 0.08209972933431675\nXGB val: 9.333257547436476 0.021046069322619876\nXGB val extra: 7.449474573984752 0.005638548266455157\nXGB test: 10.509277917328513 0.026280919917970325\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.012, 'max_depth': 5, 'n_estimators': 600, 'subsample': 0.6} 0.025436589972033352 51.93192768096924\nXGB train: 7.241671823627634 0.09783003114377553\nXGB validation: 8.837739464119451 0.1338705926983017\nXGB validation extra: 6.983415793741495 0.1367766827196445\nXGB test: 10.455126096822962 0.03312929656455288\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 20:51:48,768]\u001b[0m A new study created in memory with name: no-name-0791c416-b55d-4ec9-8fbe-b5481d6847f8\u001b[0m\n\u001b[32m[I 2022-09-06 20:51:50,588]\u001b[0m Trial 0 finished with value: 0.008695445757376002 and parameters: {'n_estimators': 1321, 'max_depth': 2, 'learning_rate': 0.008543277591917918, 'colsample_bytree': 0.4826539390861307, 'subsample': 0.45123474333775115, 'alpha': 0.15741726997599043, 'lambda': 138.14047179649938, 'gamma': 1.9753679652102876e-07, 'min_child_weight': 4.311908185048308}. Best is trial 0 with value: 0.008695445757376002.\u001b[0m\n\u001b[32m[I 2022-09-06 20:51:53,487]\u001b[0m Trial 1 finished with value: -0.008619362201395614 and parameters: {'n_estimators': 1098, 'max_depth': 5, 'learning_rate': 0.014836051651947764, 'colsample_bytree': 0.36577267476235287, 'subsample': 0.7929507854517949, 'alpha': 11.065871947901467, 'lambda': 35.10346003519957, 'gamma': 0.04324630515088469, 'min_child_weight': 1.2257751491668623}. Best is trial 0 with value: 0.008695445757376002.\u001b[0m\n\u001b[32m[I 2022-09-06 20:51:58,068]\u001b[0m Trial 2 finished with value: 0.008117032744450392 and parameters: {'n_estimators': 1391, 'max_depth': 4, 'learning_rate': 0.0032020044067463858, 'colsample_bytree': 0.17937577333089005, 'subsample': 0.7430571081057071, 'alpha': 0.814745810962842, 'lambda': 6.072035692725846, 'gamma': 8.806293977188148e-06, 'min_child_weight': 8.14485694679093}. Best is trial 0 with value: 0.008695445757376002.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:00,864]\u001b[0m Trial 3 finished with value: 0.009999799972615353 and parameters: {'n_estimators': 824, 'max_depth': 4, 'learning_rate': 0.003403520845827799, 'colsample_bytree': 0.12240849699592635, 'subsample': 0.27150368912496037, 'alpha': 2.72661918847058, 'lambda': 2.622116368635918, 'gamma': 0.30441216595923604, 'min_child_weight': 0.7128149234368911}. Best is trial 3 with value: 0.009999799972615353.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:02,192]\u001b[0m Trial 4 finished with value: 0.00468527291884221 and parameters: {'n_estimators': 1196, 'max_depth': 3, 'learning_rate': 0.02202966288186616, 'colsample_bytree': 0.407090630616736, 'subsample': 0.18865773863680552, 'alpha': 0.5182963583221211, 'lambda': 68.30959991096093, 'gamma': 2.555971442966794e-08, 'min_child_weight': 40.7647479822717}. Best is trial 3 with value: 0.009999799972615353.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:04,316]\u001b[0m Trial 5 finished with value: -0.009661629399318604 and parameters: {'n_estimators': 832, 'max_depth': 6, 'learning_rate': 0.019053441329410642, 'colsample_bytree': 0.7656065671207198, 'subsample': 0.9078314396511618, 'alpha': 0.23448657024101757, 'lambda': 8.460318212599203, 'gamma': 8.863581553075508e-08, 'min_child_weight': 5.971287401814559}. Best is trial 3 with value: 0.009999799972615353.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:11,998]\u001b[0m Trial 6 finished with value: -0.004110052150946845 and parameters: {'n_estimators': 945, 'max_depth': 6, 'learning_rate': 0.0025211225692344692, 'colsample_bytree': 0.86442435634817, 'subsample': 0.7677971677152635, 'alpha': 0.26547689006358255, 'lambda': 10.276281682413295, 'gamma': 2.9519775790115154e-09, 'min_child_weight': 0.2635348096863903}. Best is trial 3 with value: 0.009999799972615353.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:13,651]\u001b[0m Trial 7 finished with value: -0.0014768558906963491 and parameters: {'n_estimators': 1061, 'max_depth': 4, 'learning_rate': 0.02502538028032036, 'colsample_bytree': 0.32823752012532714, 'subsample': 0.46190482281481005, 'alpha': 7.968172749514438, 'lambda': 342.1455574070556, 'gamma': 0.06001265982573797, 'min_child_weight': 0.48122196699009245}. Best is trial 3 with value: 0.009999799972615353.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:14,976]\u001b[0m Trial 8 finished with value: 0.0030629153759605243 and parameters: {'n_estimators': 1238, 'max_depth': 6, 'learning_rate': 0.02396456921507622, 'colsample_bytree': 0.29205495115590274, 'subsample': 0.7870591153626287, 'alpha': 42.76328719609532, 'lambda': 126.2119625473993, 'gamma': 1.5224665647397682e-10, 'min_child_weight': 24.96745515916887}. Best is trial 3 with value: 0.009999799972615353.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:16,485]\u001b[0m Trial 9 finished with value: -0.001495531492106638 and parameters: {'n_estimators': 977, 'max_depth': 5, 'learning_rate': 0.015962605355421593, 'colsample_bytree': 0.6238336577890586, 'subsample': 0.2253072196727207, 'alpha': 6.87621351067787, 'lambda': 0.5513901325539133, 'gamma': 0.13959851078326832, 'min_child_weight': 30.003784488769064}. Best is trial 3 with value: 0.009999799972615353.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:18,314]\u001b[0m Trial 10 finished with value: 0.016040971312241352 and parameters: {'n_estimators': 1485, 'max_depth': 2, 'learning_rate': 0.008546464832335733, 'colsample_bytree': 0.0546830919112066, 'subsample': 0.31375904203463284, 'alpha': 1.6759904606680893, 'lambda': 0.10066796469305248, 'gamma': 12.312742622675936, 'min_child_weight': 0.13824448620284924}. Best is trial 10 with value: 0.016040971312241352.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:20,459]\u001b[0m Trial 11 finished with value: 0.015292896827912395 and parameters: {'n_estimators': 1491, 'max_depth': 2, 'learning_rate': 0.00964059396702065, 'colsample_bytree': 0.0697198416327649, 'subsample': 0.3251706087140183, 'alpha': 2.340216418909299, 'lambda': 0.10178665818389859, 'gamma': 86.02089982850943, 'min_child_weight': 0.11610198091458973}. Best is trial 10 with value: 0.016040971312241352.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:22,277]\u001b[0m Trial 12 finished with value: 0.016034657153401816 and parameters: {'n_estimators': 1462, 'max_depth': 2, 'learning_rate': 0.009829581139209944, 'colsample_bytree': 0.05454201324468802, 'subsample': 0.358806405069908, 'alpha': 1.9239353480027668, 'lambda': 0.11993959103977735, 'gamma': 29.787272483256032, 'min_child_weight': 0.12248844954093865}. Best is trial 10 with value: 0.016040971312241352.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:23,870]\u001b[0m Trial 13 finished with value: 0.011293169958206551 and parameters: {'n_estimators': 1472, 'max_depth': 3, 'learning_rate': 0.010417804011843477, 'colsample_bytree': 0.20193243853278434, 'subsample': 0.11044494198102894, 'alpha': 1.016976162353313, 'lambda': 0.1190761960331351, 'gamma': 80.38844794062534, 'min_child_weight': 0.12777505498205635}. Best is trial 10 with value: 0.016040971312241352.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:25,147]\u001b[0m Trial 14 finished with value: 0.013001972713848314 and parameters: {'n_estimators': 1352, 'max_depth': 2, 'learning_rate': 0.02984586877461074, 'colsample_bytree': 0.08118287088665822, 'subsample': 0.589995490883174, 'alpha': 1.633213440205644, 'lambda': 0.5576067695140342, 'gamma': 0.000995467928872855, 'min_child_weight': 0.22481000121832942}. Best is trial 10 with value: 0.016040971312241352.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:27,609]\u001b[0m Trial 15 finished with value: 0.007713894878740446 and parameters: {'n_estimators': 1407, 'max_depth': 3, 'learning_rate': 0.006810915120403228, 'colsample_bytree': 0.24628635476128263, 'subsample': 0.3704130956029412, 'alpha': 3.9085532286873823, 'lambda': 0.3432197509809215, 'gamma': 3.4519890993230815, 'min_child_weight': 1.5044631633372365}. Best is trial 10 with value: 0.016040971312241352.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:29,015]\u001b[0m Trial 16 finished with value: 0.008780639236260584 and parameters: {'n_estimators': 1255, 'max_depth': 2, 'learning_rate': 0.01251176539103297, 'colsample_bytree': 0.5450610817536984, 'subsample': 0.579854538323939, 'alpha': 20.5615839456232, 'lambda': 1.791950492864071, 'gamma': 0.0001439130408717527, 'min_child_weight': 0.4540524197037206}. Best is trial 10 with value: 0.016040971312241352.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:31,489]\u001b[0m Trial 17 finished with value: 0.006021094631080548 and parameters: {'n_estimators': 1445, 'max_depth': 3, 'learning_rate': 0.005817730206965413, 'colsample_bytree': 0.6639169877975337, 'subsample': 0.40847112580928, 'alpha': 0.5518279908596206, 'lambda': 0.28423998924250016, 'gamma': 6.504639106037521, 'min_child_weight': 0.23552418355982485}. Best is trial 10 with value: 0.016040971312241352.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:32,882]\u001b[0m Trial 18 finished with value: 0.01042438365623824 and parameters: {'n_estimators': 1318, 'max_depth': 2, 'learning_rate': 0.015524622762509446, 'colsample_bytree': 0.17298990099281264, 'subsample': 0.5578184437852509, 'alpha': 4.425075252931769, 'lambda': 1.3661144642875318, 'gamma': 0.0032023904360878367, 'min_child_weight': 0.10827451033875293}. Best is trial 10 with value: 0.016040971312241352.\u001b[0m\n\u001b[32m[I 2022-09-06 20:52:34,440]\u001b[0m Trial 19 finished with value: 0.015070160251243202 and parameters: {'n_estimators': 1266, 'max_depth': 3, 'learning_rate': 0.011701321403929698, 'colsample_bytree': 0.05946441190011278, 'subsample': 0.12652295079699, 'alpha': 1.607384139773389, 'lambda': 0.213390079875952, 'gamma': 2.1383229349623534, 'min_child_weight': 3.210121508282982}. Best is trial 10 with value: 0.016040971312241352.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  45.674782514572144\n        n_estimators : 1485\n           max_depth : 2\n       learning_rate : 0.008546464832335733\n    colsample_bytree : 0.0546830919112066\n           subsample : 0.31375904203463284\n               alpha : 1.6759904606680893\n              lambda : 0.10066796469305248\n               gamma : 12.312742622675936\n    min_child_weight : 0.13824448620284924\nbest objective value : 0.016040971312241352\nOptuna XGB train: \n 7.461428321428935 0.03342392051862897 \nvalidation \n 9.215158870463663 0.04908183313470171 7.33259025879932 0.024970401869853665 \ntest \n 10.525828801484554 0.015595611257659514\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150     0.0821  0.021046  0.026281     0.09783  0.133871   0.033129   \n1      250        NaN       NaN       NaN         NaN       NaN        NaN   \n2      350        NaN       NaN       NaN         NaN       NaN        NaN   \n3      450        NaN       NaN       NaN         NaN       NaN        NaN   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.033424  0.049082  0.015596  \n1        NaN       NaN       NaN  \n2        NaN       NaN       NaN  \n3        NaN       NaN       NaN  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.0821</td>\n      <td>0.021046</td>\n      <td>0.026281</td>\n      <td>0.09783</td>\n      <td>0.133871</td>\n      <td>0.033129</td>\n      <td>0.033424</td>\n      <td>0.049082</td>\n      <td>0.015596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(79162, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6405, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6311, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    79162.000000\nmean       266.337409\nstd         10.396123\nmin        249.000000\n25%        257.000000\n50%        266.000000\n75%        275.000000\nmax        284.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    6405.000000\nmean      285.996565\nstd         0.816362\nmin       285.000000\n25%       285.000000\n50%       286.000000\n75%       287.000000\nmax       287.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    6311.000000\nmean      288.997782\nstd         0.816235\nmin       288.000000\n25%       288.000000\n50%       289.000000\n75%       290.000000\nmax       290.000000\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (79162, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (79162, 85) (6405, 85) (6616, 85) (6311, 85)\nmae of a constant model 8.616058689352922\nR2 of a constant model 0.0\nfixed XGB train: 8.094503280537975 0.08807399835850738\nXGB val: 7.80268307196985 -0.012211276662149384\nXGB val extra: 9.182509786895938 0.006809524790626353\nXGB test: 7.84572645024662 0.015254350212336232\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.006, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6} -0.009211110247068666 58.72147870063782\nXGB train: 8.232025093474888 0.060695330008433745\nXGB validation: 7.630334731974988 0.032497740585895496\nXGB validation extra: 8.985427732621966 0.05388213675932696\nXGB test: 7.787969462570123 0.02523901836111997\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 20:53:43,166]\u001b[0m A new study created in memory with name: no-name-90ef594e-1b7b-4a7d-bd76-37c8b088c7b5\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:44,028]\u001b[0m Trial 0 finished with value: -0.004044818114926419 and parameters: {'n_estimators': 905, 'max_depth': 5, 'learning_rate': 0.025019136230489958, 'colsample_bytree': 0.35916139438456635, 'subsample': 0.7592572890572913, 'alpha': 4.146564363554224, 'lambda': 0.9384009960330469, 'gamma': 2.1122572523780342e-06, 'min_child_weight': 2.339375964301839}. Best is trial 0 with value: -0.004044818114926419.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:44,828]\u001b[0m Trial 1 finished with value: -0.0025380838569263263 and parameters: {'n_estimators': 1020, 'max_depth': 6, 'learning_rate': 0.011358354707639356, 'colsample_bytree': 0.9202013585354419, 'subsample': 0.6318990606656083, 'alpha': 3.4422831871519124, 'lambda': 33.73278951999262, 'gamma': 31.860945804547008, 'min_child_weight': 5.144990748366948}. Best is trial 1 with value: -0.0025380838569263263.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:45,267]\u001b[0m Trial 2 finished with value: -0.0011406897581647591 and parameters: {'n_estimators': 1246, 'max_depth': 4, 'learning_rate': 0.015248497247827956, 'colsample_bytree': 0.45674106740595044, 'subsample': 0.19179407779693047, 'alpha': 39.50458989559971, 'lambda': 20.17509989245294, 'gamma': 0.01815439659136294, 'min_child_weight': 16.38120073546361}. Best is trial 2 with value: -0.0011406897581647591.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:45,796]\u001b[0m Trial 3 finished with value: -0.001193856113828351 and parameters: {'n_estimators': 1206, 'max_depth': 6, 'learning_rate': 0.010504731285794882, 'colsample_bytree': 0.5682802766544826, 'subsample': 0.19641457016749087, 'alpha': 1.112887736253821, 'lambda': 327.25485542460666, 'gamma': 3.1646008957016134, 'min_child_weight': 40.0510646476847}. Best is trial 2 with value: -0.0011406897581647591.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:46,205]\u001b[0m Trial 4 finished with value: -0.0012672017354223963 and parameters: {'n_estimators': 953, 'max_depth': 3, 'learning_rate': 0.026641411530021007, 'colsample_bytree': 0.857662778832054, 'subsample': 0.34729553833136706, 'alpha': 25.929875550990072, 'lambda': 0.5473149099339489, 'gamma': 4.711645777535268e-08, 'min_child_weight': 2.768053883420573}. Best is trial 2 with value: -0.0011406897581647591.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:46,893]\u001b[0m Trial 5 finished with value: -0.0030321595052049287 and parameters: {'n_estimators': 1106, 'max_depth': 6, 'learning_rate': 0.018936457681090556, 'colsample_bytree': 0.35523971678141647, 'subsample': 0.8786135579098788, 'alpha': 0.26953713531834744, 'lambda': 0.19874548473927306, 'gamma': 0.5719217880854872, 'min_child_weight': 36.872759601250145}. Best is trial 2 with value: -0.0011406897581647591.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:47,339]\u001b[0m Trial 6 finished with value: -0.0017070393548904827 and parameters: {'n_estimators': 1199, 'max_depth': 4, 'learning_rate': 0.017230911434652964, 'colsample_bytree': 0.45564274662013515, 'subsample': 0.8025427620129708, 'alpha': 1.8308442301600383, 'lambda': 28.631083896509157, 'gamma': 5.657162949408394e-06, 'min_child_weight': 27.2518827948727}. Best is trial 2 with value: -0.0011406897581647591.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:47,728]\u001b[0m Trial 7 finished with value: -0.001547186025280678 and parameters: {'n_estimators': 1062, 'max_depth': 2, 'learning_rate': 0.02665539906460927, 'colsample_bytree': 0.26079915563865885, 'subsample': 0.7291500463991204, 'alpha': 6.354679644244423, 'lambda': 19.060411230467672, 'gamma': 0.35409462861081875, 'min_child_weight': 45.05460415593754}. Best is trial 2 with value: -0.0011406897581647591.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:48,137]\u001b[0m Trial 8 finished with value: -0.0009446529404480541 and parameters: {'n_estimators': 822, 'max_depth': 3, 'learning_rate': 0.012304235844577256, 'colsample_bytree': 0.9263734477536273, 'subsample': 0.17225892314736901, 'alpha': 0.7032435141344342, 'lambda': 85.75321514198828, 'gamma': 0.000883988570712691, 'min_child_weight': 7.639472778545565}. Best is trial 8 with value: -0.0009446529404480541.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:48,713]\u001b[0m Trial 9 finished with value: -0.002348143315888618 and parameters: {'n_estimators': 1008, 'max_depth': 5, 'learning_rate': 0.01204967208650458, 'colsample_bytree': 0.41623404244225876, 'subsample': 0.731918394935487, 'alpha': 28.34209747401478, 'lambda': 3.717449743686186, 'gamma': 79.79582607355479, 'min_child_weight': 7.889215754896785}. Best is trial 8 with value: -0.0009446529404480541.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:49,127]\u001b[0m Trial 10 finished with value: -0.001108246924652312 and parameters: {'n_estimators': 1462, 'max_depth': 2, 'learning_rate': 0.000537909820259504, 'colsample_bytree': 0.7012707645637287, 'subsample': 0.43891269048815207, 'alpha': 0.11625590097776312, 'lambda': 317.7791980093222, 'gamma': 5.666110569109907e-10, 'min_child_weight': 0.3679552365677239}. Best is trial 8 with value: -0.0009446529404480541.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:49,541]\u001b[0m Trial 11 finished with value: -0.0011027274409679878 and parameters: {'n_estimators': 1445, 'max_depth': 2, 'learning_rate': 0.0008264493263686625, 'colsample_bytree': 0.7281431998625623, 'subsample': 0.41924297388141113, 'alpha': 0.10811159765794107, 'lambda': 430.22352910160515, 'gamma': 1.6859078468960807e-10, 'min_child_weight': 0.2691278859893845}. Best is trial 8 with value: -0.0009446529404480541.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:49,976]\u001b[0m Trial 12 finished with value: -0.001034459572910007 and parameters: {'n_estimators': 821, 'max_depth': 3, 'learning_rate': 0.002648851911171297, 'colsample_bytree': 0.7275633133398112, 'subsample': 0.3409389458334181, 'alpha': 0.5346516106118189, 'lambda': 127.74165262240594, 'gamma': 0.0004026875531357452, 'min_child_weight': 0.11254980133019339}. Best is trial 8 with value: -0.0009446529404480541.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:50,460]\u001b[0m Trial 13 finished with value: -0.0007000329162417662 and parameters: {'n_estimators': 830, 'max_depth': 3, 'learning_rate': 0.0062447349748162365, 'colsample_bytree': 0.08190332495331198, 'subsample': 0.11208694131599871, 'alpha': 0.6032028840910417, 'lambda': 85.07697316153485, 'gamma': 0.0014784995681624508, 'min_child_weight': 0.11043537173040577}. Best is trial 13 with value: -0.0007000329162417662.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:50,974]\u001b[0m Trial 14 finished with value: -0.0005189566141518109 and parameters: {'n_estimators': 825, 'max_depth': 3, 'learning_rate': 0.00559573595612119, 'colsample_bytree': 0.0523163453087722, 'subsample': 0.11170038141210127, 'alpha': 0.6683154027983339, 'lambda': 67.28855624907189, 'gamma': 0.0014705978656061154, 'min_child_weight': 0.6983235193347892}. Best is trial 14 with value: -0.0005189566141518109.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:51,465]\u001b[0m Trial 15 finished with value: -0.0008360671076581728 and parameters: {'n_estimators': 912, 'max_depth': 3, 'learning_rate': 0.006060349801013804, 'colsample_bytree': 0.08289275820289504, 'subsample': 0.1448351336934517, 'alpha': 0.3409607105988387, 'lambda': 9.036159243467932, 'gamma': 0.0062860601365973155, 'min_child_weight': 0.7389427976521742}. Best is trial 14 with value: -0.0005189566141518109.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:52,207]\u001b[0m Trial 16 finished with value: -0.0005694583513770224 and parameters: {'n_estimators': 1340, 'max_depth': 4, 'learning_rate': 0.006601784265983143, 'colsample_bytree': 0.06000259690581011, 'subsample': 0.2586600629025431, 'alpha': 9.944852463673673, 'lambda': 87.23944942806848, 'gamma': 9.078304608024685e-06, 'min_child_weight': 0.8553472349842917}. Best is trial 14 with value: -0.0005189566141518109.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:52,867]\u001b[0m Trial 17 finished with value: -0.0012006990669362503 and parameters: {'n_estimators': 1342, 'max_depth': 5, 'learning_rate': 0.005996458767824283, 'colsample_bytree': 0.22023616717672095, 'subsample': 0.28536767163502225, 'alpha': 9.596084826553929, 'lambda': 3.583024193510177, 'gamma': 1.048137360166833e-05, 'min_child_weight': 1.064791349938829}. Best is trial 14 with value: -0.0005189566141518109.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:53,404]\u001b[0m Trial 18 finished with value: -0.0015649626460394416 and parameters: {'n_estimators': 1323, 'max_depth': 4, 'learning_rate': 0.008186191515375817, 'colsample_bytree': 0.17184446882289597, 'subsample': 0.5563122580785904, 'alpha': 13.645919477347546, 'lambda': 112.20953322531346, 'gamma': 7.986877773599497e-08, 'min_child_weight': 0.9720923092023814}. Best is trial 14 with value: -0.0005189566141518109.\u001b[0m\n\u001b[32m[I 2022-09-06 20:53:53,929]\u001b[0m Trial 19 finished with value: -0.0006524932536163041 and parameters: {'n_estimators': 1367, 'max_depth': 4, 'learning_rate': 0.003819358856710518, 'colsample_bytree': 0.06201886818851651, 'subsample': 0.2664540690439146, 'alpha': 2.014830134343048, 'lambda': 51.38094508782176, 'gamma': 4.597034006831271e-05, 'min_child_weight': 0.4090586901974094}. Best is trial 14 with value: -0.0005189566141518109.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  10.76462459564209\n        n_estimators : 825\n           max_depth : 3\n       learning_rate : 0.00559573595612119\n    colsample_bytree : 0.0523163453087722\n           subsample : 0.11170038141210127\n               alpha : 0.6683154027983339\n              lambda : 67.28855624907189\n               gamma : 0.0014705978656061154\n    min_child_weight : 0.6983235193347892\nbest objective value : -0.0005189566141518109\nOptuna XGB train: \n 8.380199727468208 0.027712231357177042 \nvalidation \n 7.7849493050033365 -0.004911211464112997 9.20099479002355 0.005384356093520282 \ntest \n 7.883021921100659 0.00820921335174929\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150     0.0821  0.021046  0.026281     0.09783  0.133871   0.033129   \n1      250   0.088074 -0.012211  0.015254    0.060695  0.032498   0.025239   \n2      350        NaN       NaN       NaN         NaN       NaN        NaN   \n3      450        NaN       NaN       NaN         NaN       NaN        NaN   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.033424  0.049082  0.015596  \n1   0.027712 -0.004911  0.008209  \n2        NaN       NaN       NaN  \n3        NaN       NaN       NaN  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.0821</td>\n      <td>0.021046</td>\n      <td>0.026281</td>\n      <td>0.09783</td>\n      <td>0.133871</td>\n      <td>0.033129</td>\n      <td>0.033424</td>\n      <td>0.049082</td>\n      <td>0.015596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.088074</td>\n      <td>-0.012211</td>\n      <td>0.015254</td>\n      <td>0.060695</td>\n      <td>0.032498</td>\n      <td>0.025239</td>\n      <td>0.027712</td>\n      <td>-0.004911</td>\n      <td>0.008209</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(86525, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(7274, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(7655, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    86525.000000\nmean       366.510604\nstd         10.388374\nmin        349.000000\n25%        357.000000\n50%        366.000000\n75%        376.000000\nmax        384.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    7274.000000\nmean      386.036294\nstd         0.824686\nmin       385.000000\n25%       385.000000\n50%       386.000000\n75%       387.000000\nmax       387.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    7655.000000\nmean      388.997518\nstd         0.816199\nmin       388.000000\n25%       388.000000\n50%       389.000000\n75%       390.000000\nmax       390.000000\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (86525, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (86525, 86) (7274, 86) (7557, 86) (7655, 86)\nmae of a constant model 9.829729636954092\nR2 of a constant model 0.0\nfixed XGB train: 8.691436727915313 0.10293313911685331\nXGB val: 9.717708335125423 0.03999212025983656\nXGB val extra: 12.284157764400447 0.03759872132638431\nXGB test: 10.465530780239735 -0.0063821723842552025\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 3, 'n_estimators': 800, 'subsample': 0.6} 0.04241707807354467 62.10271215438843\nXGB train: 8.695012267727543 0.09907150614417448\nXGB validation: 9.559585257648125 0.070611187303669\nXGB validation extra: 11.972006867399784 0.08163841109742664\nXGB test: 10.47468523195892 -0.0059672654136349035\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 20:55:05,851]\u001b[0m A new study created in memory with name: no-name-c52b398e-c9f1-4e16-a1b5-79df12a3f545\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:07,488]\u001b[0m Trial 0 finished with value: 0.02319494370475305 and parameters: {'n_estimators': 927, 'max_depth': 3, 'learning_rate': 0.02754978287672675, 'colsample_bytree': 0.7219749452309171, 'subsample': 0.10560158246946975, 'alpha': 18.635417275093108, 'lambda': 386.57017485756205, 'gamma': 1.1070308085286222e-07, 'min_child_weight': 0.9051750123565846}. Best is trial 0 with value: 0.02319494370475305.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:10,997]\u001b[0m Trial 1 finished with value: 0.004370803101923802 and parameters: {'n_estimators': 953, 'max_depth': 6, 'learning_rate': 0.023016092139299775, 'colsample_bytree': 0.7860857969265894, 'subsample': 0.5790148912901593, 'alpha': 2.304332897515641, 'lambda': 3.2734027704827575, 'gamma': 1.1363935243658094e-08, 'min_child_weight': 2.828616052161316}. Best is trial 0 with value: 0.02319494370475305.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:14,866]\u001b[0m Trial 2 finished with value: 0.021661682884910074 and parameters: {'n_estimators': 980, 'max_depth': 5, 'learning_rate': 0.018101334273639986, 'colsample_bytree': 0.12412024811017286, 'subsample': 0.43131777222162493, 'alpha': 0.2994147857280928, 'lambda': 17.232944713724525, 'gamma': 0.0029138378964667024, 'min_child_weight': 0.1709155568903902}. Best is trial 0 with value: 0.02319494370475305.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:16,868]\u001b[0m Trial 3 finished with value: 0.019804133964842523 and parameters: {'n_estimators': 1023, 'max_depth': 4, 'learning_rate': 0.027235634595769005, 'colsample_bytree': 0.25182365216279656, 'subsample': 0.476632295781951, 'alpha': 1.1147184341478742, 'lambda': 0.40030548226927226, 'gamma': 5.03207620549744e-10, 'min_child_weight': 9.357268060260877}. Best is trial 0 with value: 0.02319494370475305.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:19,905]\u001b[0m Trial 4 finished with value: 0.026069081126195853 and parameters: {'n_estimators': 875, 'max_depth': 2, 'learning_rate': 0.011957449005848619, 'colsample_bytree': 0.3372846584676091, 'subsample': 0.7374045903367902, 'alpha': 0.32081186386575633, 'lambda': 0.761257811439041, 'gamma': 1.209324894282375e-05, 'min_child_weight': 1.4931382695907642}. Best is trial 4 with value: 0.026069081126195853.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:30,956]\u001b[0m Trial 5 finished with value: 0.023246362417674093 and parameters: {'n_estimators': 1332, 'max_depth': 6, 'learning_rate': 0.0035199174084247793, 'colsample_bytree': 0.2876608247369554, 'subsample': 0.1871381941932591, 'alpha': 0.26139088317648157, 'lambda': 41.20618889648299, 'gamma': 4.584328799000135e-07, 'min_child_weight': 1.2339229442762252}. Best is trial 4 with value: 0.026069081126195853.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:33,198]\u001b[0m Trial 6 finished with value: 0.027983108326485213 and parameters: {'n_estimators': 1163, 'max_depth': 3, 'learning_rate': 0.022961579030553566, 'colsample_bytree': 0.1782752069745288, 'subsample': 0.3130561290723478, 'alpha': 9.990966782884009, 'lambda': 19.286119272003432, 'gamma': 2.8200575183038326e-06, 'min_child_weight': 1.0286037231012555}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:35,697]\u001b[0m Trial 7 finished with value: 0.02659095164638109 and parameters: {'n_estimators': 1184, 'max_depth': 4, 'learning_rate': 0.017817703358217335, 'colsample_bytree': 0.1799384189820546, 'subsample': 0.5948183621214405, 'alpha': 0.25454864698870344, 'lambda': 11.458864767324854, 'gamma': 5.608560176294606e-07, 'min_child_weight': 1.9080971250344}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:40,101]\u001b[0m Trial 8 finished with value: 0.015111919844288647 and parameters: {'n_estimators': 1432, 'max_depth': 6, 'learning_rate': 0.013431956937232679, 'colsample_bytree': 0.2511937995382276, 'subsample': 0.5602810379946245, 'alpha': 0.325957164919265, 'lambda': 3.6202306925574033, 'gamma': 9.499671983277552e-05, 'min_child_weight': 3.6482955234543906}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:43,242]\u001b[0m Trial 9 finished with value: 0.027229557364325613 and parameters: {'n_estimators': 1028, 'max_depth': 2, 'learning_rate': 0.015314255311232306, 'colsample_bytree': 0.9125940156858406, 'subsample': 0.9046227759962632, 'alpha': 6.040084793740704, 'lambda': 4.027935468180324, 'gamma': 3.8913855855250595e-07, 'min_child_weight': 0.4314570425319639}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:47,355]\u001b[0m Trial 10 finished with value: 0.025943647773416303 and parameters: {'n_estimators': 1189, 'max_depth': 3, 'learning_rate': 0.006837154535860193, 'colsample_bytree': 0.5000808149460059, 'subsample': 0.29461562969278726, 'alpha': 48.195196734493166, 'lambda': 91.0038951520374, 'gamma': 76.03272102221436, 'min_child_weight': 25.39132409770432}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:49,665]\u001b[0m Trial 11 finished with value: 0.02451869866005495 and parameters: {'n_estimators': 1088, 'max_depth': 2, 'learning_rate': 0.021855021135727324, 'colsample_bytree': 0.9421902594525833, 'subsample': 0.8840042317485459, 'alpha': 5.478233123561342, 'lambda': 0.1179282203144466, 'gamma': 0.02131700047267128, 'min_child_weight': 0.29940704290765996}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:54,428]\u001b[0m Trial 12 finished with value: 0.025144821123597085 and parameters: {'n_estimators': 1287, 'max_depth': 3, 'learning_rate': 0.00977806353405021, 'colsample_bytree': 0.5120652957503097, 'subsample': 0.9079024352999259, 'alpha': 7.582271968031191, 'lambda': 1.4117141133899656, 'gamma': 1.6519606849326056e-10, 'min_child_weight': 0.4546679478970784}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:56,601]\u001b[0m Trial 13 finished with value: 0.027778032310800033 and parameters: {'n_estimators': 800, 'max_depth': 2, 'learning_rate': 0.02277570018789542, 'colsample_bytree': 0.6289993482340673, 'subsample': 0.34564326582736904, 'alpha': 8.49083356525894, 'lambda': 71.2910238723102, 'gamma': 0.011061206045546088, 'min_child_weight': 0.36154768963830963}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:55:58,914]\u001b[0m Trial 14 finished with value: 0.024964270000419415 and parameters: {'n_estimators': 820, 'max_depth': 3, 'learning_rate': 0.023454598731224127, 'colsample_bytree': 0.6453769285551997, 'subsample': 0.3323943048843676, 'alpha': 18.001762784002572, 'lambda': 160.6121690660467, 'gamma': 0.34748468839957625, 'min_child_weight': 0.12560914919434252}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:56:00,645]\u001b[0m Trial 15 finished with value: 0.027179805534760466 and parameters: {'n_estimators': 1119, 'max_depth': 2, 'learning_rate': 0.02860220189111761, 'colsample_bytree': 0.43201500478895105, 'subsample': 0.31394719710137997, 'alpha': 1.9199303198206938, 'lambda': 34.86337024581038, 'gamma': 1.6635238092469113, 'min_child_weight': 5.987743822302731}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:56:05,060]\u001b[0m Trial 16 finished with value: 0.026656139939946553 and parameters: {'n_estimators': 1260, 'max_depth': 3, 'learning_rate': 0.019851217059230597, 'colsample_bytree': 0.061874232413758196, 'subsample': 0.38897898643416384, 'alpha': 17.190522781681306, 'lambda': 392.6702934949104, 'gamma': 0.0013050967026210594, 'min_child_weight': 0.587919639729807}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:56:07,388]\u001b[0m Trial 17 finished with value: 0.021417720054774093 and parameters: {'n_estimators': 1420, 'max_depth': 4, 'learning_rate': 0.025461572075541113, 'colsample_bytree': 0.5594500559045534, 'subsample': 0.2312151674670747, 'alpha': 46.4801658702556, 'lambda': 85.72882901905012, 'gamma': 2.7489371389835867e-05, 'min_child_weight': 0.1701437929497284}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:56:09,158]\u001b[0m Trial 18 finished with value: 0.02164649129014693 and parameters: {'n_estimators': 809, 'max_depth': 5, 'learning_rate': 0.029643252033613073, 'colsample_bytree': 0.3834182523462703, 'subsample': 0.11034651459673711, 'alpha': 3.426497391892383, 'lambda': 24.200481367772674, 'gamma': 0.11472838655270379, 'min_child_weight': 0.7806124442657976}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n\u001b[32m[I 2022-09-06 20:56:12,214]\u001b[0m Trial 19 finished with value: 0.026629565098441756 and parameters: {'n_estimators': 1078, 'max_depth': 2, 'learning_rate': 0.0203235106454326, 'colsample_bytree': 0.6362251818681892, 'subsample': 0.6782204991709322, 'alpha': 9.963954746441976, 'lambda': 7.779210028142986, 'gamma': 0.00137143323999129, 'min_child_weight': 0.24502683791719548}. Best is trial 6 with value: 0.027983108326485213.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  66.36472415924072\n        n_estimators : 1163\n           max_depth : 3\n       learning_rate : 0.022961579030553566\n    colsample_bytree : 0.1782752069745288\n           subsample : 0.3130561290723478\n               alpha : 9.990966782884009\n              lambda : 19.286119272003432\n               gamma : 2.8200575183038326e-06\n    min_child_weight : 1.0286037231012555\nbest objective value : 0.027983108326485213\nOptuna XGB train: \n 8.661067165580162 0.10572112796805921 \nvalidation \n 9.548832039552172 0.0735491468683126 12.235867522959138 0.04228809858496896 \ntest \n 10.402803563518528 0.003203422589207916\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150     0.0821  0.021046  0.026281     0.09783  0.133871   0.033129   \n1      250   0.088074 -0.012211  0.015254    0.060695  0.032498   0.025239   \n2      350   0.102933  0.039992 -0.006382    0.099072  0.070611  -0.005967   \n3      450        NaN       NaN       NaN         NaN       NaN        NaN   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.033424  0.049082  0.015596  \n1   0.027712 -0.004911  0.008209  \n2   0.105721  0.073549  0.003203  \n3        NaN       NaN       NaN  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.0821</td>\n      <td>0.021046</td>\n      <td>0.026281</td>\n      <td>0.09783</td>\n      <td>0.133871</td>\n      <td>0.033129</td>\n      <td>0.033424</td>\n      <td>0.049082</td>\n      <td>0.015596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.088074</td>\n      <td>-0.012211</td>\n      <td>0.015254</td>\n      <td>0.060695</td>\n      <td>0.032498</td>\n      <td>0.025239</td>\n      <td>0.027712</td>\n      <td>-0.004911</td>\n      <td>0.008209</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.102933</td>\n      <td>0.039992</td>\n      <td>-0.006382</td>\n      <td>0.099072</td>\n      <td>0.070611</td>\n      <td>-0.005967</td>\n      <td>0.105721</td>\n      <td>0.073549</td>\n      <td>0.003203</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(99700, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(8347, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(8028, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    99700.000000\nmean       466.677944\nstd         10.329035\nmin        449.000000\n25%        458.000000\n50%        467.000000\n75%        476.000000\nmax        484.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    8347.000000\nmean      485.987301\nstd         0.816104\nmin       485.000000\n25%       485.000000\n50%       486.000000\n75%       487.000000\nmax       487.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    8028.000000\nmean      488.993523\nstd         0.818807\nmin       488.000000\n25%       488.000000\n50%       489.000000\n75%       490.000000\nmax       490.000000\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (99700, 44)\ntime to do feature proprocessing: \nNumber of features after transformation:  (99700, 92) (8347, 92) (7745, 92) (8028, 92)\nmae of a constant model 10.691543708506693\nR2 of a constant model 0.0\nfixed XGB train: 9.779149828904508 0.05214422474847957\nXGB val: 13.516048254363001 0.015028489372450715\nXGB val extra: 12.73303930290879 0.005854171915616768\nXGB test: 12.286701608063396 0.015156406670841283\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.012, 'max_depth': 2, 'n_estimators': 800, 'subsample': 0.6} 0.016833899512829764 68.61823558807373\nXGB train: 9.885382908900855 0.028579466724181657\nXGB validation: 13.427178429291631 0.03452564369780553\nXGB validation extra: 12.683936481476703 0.020661230602238678\nXGB test: 12.337062599287876 0.014993687642419373\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 20:57:33,896]\u001b[0m A new study created in memory with name: no-name-34990d8e-baf8-46fc-b51b-263247d0e503\u001b[0m\n\u001b[32m[I 2022-09-06 20:57:38,574]\u001b[0m Trial 0 finished with value: 0.0017489512988990543 and parameters: {'n_estimators': 1229, 'max_depth': 3, 'learning_rate': 0.0016113231364481182, 'colsample_bytree': 0.44680295949902377, 'subsample': 0.3749181794984069, 'alpha': 0.23371736841181442, 'lambda': 151.43682858505903, 'gamma': 23.552519050641852, 'min_child_weight': 1.163427895945382}. Best is trial 0 with value: 0.0017489512988990543.\u001b[0m\n\u001b[32m[I 2022-09-06 20:57:40,546]\u001b[0m Trial 1 finished with value: 0.005474053729399709 and parameters: {'n_estimators': 903, 'max_depth': 2, 'learning_rate': 0.0188427652802813, 'colsample_bytree': 0.2376156268028457, 'subsample': 0.48383010614145827, 'alpha': 0.1723986972471409, 'lambda': 7.696057827560571, 'gamma': 3.938528767492674e-08, 'min_child_weight': 9.444872037068553}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:57:46,297]\u001b[0m Trial 2 finished with value: 0.0020846693839990937 and parameters: {'n_estimators': 1190, 'max_depth': 4, 'learning_rate': 0.0024792970923630947, 'colsample_bytree': 0.1413427644595781, 'subsample': 0.6776937898268599, 'alpha': 2.886800490503464, 'lambda': 48.6265039856628, 'gamma': 1.3443147520272674e-10, 'min_child_weight': 0.533199160672085}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:57:50,595]\u001b[0m Trial 3 finished with value: 0.002725335383814571 and parameters: {'n_estimators': 1454, 'max_depth': 4, 'learning_rate': 0.005797759751947153, 'colsample_bytree': 0.2425578392443195, 'subsample': 0.8205874892720495, 'alpha': 0.12095182953288673, 'lambda': 1.0338993464671287, 'gamma': 1.534408611611655e-06, 'min_child_weight': 20.958653758324107}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:03,480]\u001b[0m Trial 4 finished with value: 0.00012919659288877002 and parameters: {'n_estimators': 1427, 'max_depth': 6, 'learning_rate': 0.000911385516924603, 'colsample_bytree': 0.19306133653089763, 'subsample': 0.7075720221022779, 'alpha': 8.519849232710264, 'lambda': 36.81853197612589, 'gamma': 0.32876574176748785, 'min_child_weight': 0.20857106795969313}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:05,546]\u001b[0m Trial 5 finished with value: -0.004631541942910105 and parameters: {'n_estimators': 976, 'max_depth': 6, 'learning_rate': 0.024559954147110454, 'colsample_bytree': 0.6675047291666936, 'subsample': 0.8272636059277573, 'alpha': 0.2580867480186642, 'lambda': 0.3079196553234949, 'gamma': 0.0004511833530477222, 'min_child_weight': 5.954931180939713}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:10,773]\u001b[0m Trial 6 finished with value: -0.004229992205854615 and parameters: {'n_estimators': 913, 'max_depth': 6, 'learning_rate': 0.007613466552023435, 'colsample_bytree': 0.42927076878021403, 'subsample': 0.6334079096441892, 'alpha': 0.4205111601197495, 'lambda': 12.90243876572344, 'gamma': 6.813006339193887e-08, 'min_child_weight': 0.12762738018793834}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:12,532]\u001b[0m Trial 7 finished with value: 0.0029344977586741984 and parameters: {'n_estimators': 824, 'max_depth': 3, 'learning_rate': 0.0203917398398193, 'colsample_bytree': 0.41191145774478377, 'subsample': 0.5261123685079214, 'alpha': 1.1771937986056666, 'lambda': 1.2231995179062678, 'gamma': 3.501046408362934e-07, 'min_child_weight': 0.19637348946672317}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:18,847]\u001b[0m Trial 8 finished with value: -0.003737260592651637 and parameters: {'n_estimators': 1089, 'max_depth': 5, 'learning_rate': 0.005988202294142082, 'colsample_bytree': 0.08642354184116813, 'subsample': 0.38428614737476663, 'alpha': 0.6690885659039439, 'lambda': 6.050413471611922, 'gamma': 0.00019956756990923606, 'min_child_weight': 13.962961917364765}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:20,167]\u001b[0m Trial 9 finished with value: 0.00441911565739544 and parameters: {'n_estimators': 1459, 'max_depth': 2, 'learning_rate': 0.024327753359428942, 'colsample_bytree': 0.36894717805274585, 'subsample': 0.244494921373593, 'alpha': 7.105710926504621, 'lambda': 0.35318858405397235, 'gamma': 0.5332759174168205, 'min_child_weight': 0.5694848881908562}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:21,440]\u001b[0m Trial 10 finished with value: -0.001929232921236079 and parameters: {'n_estimators': 1051, 'max_depth': 2, 'learning_rate': 0.01491884997360582, 'colsample_bytree': 0.9369658255485729, 'subsample': 0.14401417365833274, 'alpha': 30.548640851544395, 'lambda': 495.84995054275697, 'gamma': 2.4176423336490334e-10, 'min_child_weight': 3.971609922053557}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:22,794]\u001b[0m Trial 11 finished with value: 0.000842422865448173 and parameters: {'n_estimators': 1303, 'max_depth': 2, 'learning_rate': 0.029955207615137754, 'colsample_bytree': 0.30602912793867104, 'subsample': 0.14639108154389405, 'alpha': 5.317292409532021, 'lambda': 0.14211749323256034, 'gamma': 0.024458747895683284, 'min_child_weight': 45.64707839260962}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:24,788]\u001b[0m Trial 12 finished with value: 0.005228948427094748 and parameters: {'n_estimators': 1333, 'max_depth': 2, 'learning_rate': 0.01682634148890364, 'colsample_bytree': 0.5856198626715473, 'subsample': 0.3380954340691979, 'alpha': 20.62749559525551, 'lambda': 2.812146433570295, 'gamma': 98.74848230490163, 'min_child_weight': 1.559999855912436}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:27,448]\u001b[0m Trial 13 finished with value: 0.0030912358022565645 and parameters: {'n_estimators': 1324, 'max_depth': 3, 'learning_rate': 0.013705954780869289, 'colsample_bytree': 0.6156510954280356, 'subsample': 0.4166562850435146, 'alpha': 19.92146052277261, 'lambda': 4.033053867183541, 'gamma': 3.2482608686620406e-08, 'min_child_weight': 2.124190561294632}. Best is trial 1 with value: 0.005474053729399709.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:29,449]\u001b[0m Trial 14 finished with value: 0.005788107265033699 and parameters: {'n_estimators': 808, 'max_depth': 2, 'learning_rate': 0.018684822903725116, 'colsample_bytree': 0.5983914115051032, 'subsample': 0.5199908975473907, 'alpha': 1.581821023352167, 'lambda': 2.0591485907246727, 'gamma': 43.48222566967177, 'min_child_weight': 9.143588925495571}. Best is trial 14 with value: 0.005788107265033699.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:32,416]\u001b[0m Trial 15 finished with value: 0.0039314078582772865 and parameters: {'n_estimators': 800, 'max_depth': 3, 'learning_rate': 0.01112442035634707, 'colsample_bytree': 0.7726612332702323, 'subsample': 0.529822575878812, 'alpha': 1.3534867742691346, 'lambda': 22.010253529033925, 'gamma': 9.098486413716297e-06, 'min_child_weight': 8.759934146117969}. Best is trial 14 with value: 0.005788107265033699.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:34,620]\u001b[0m Trial 16 finished with value: 0.005041623873806589 and parameters: {'n_estimators': 910, 'max_depth': 2, 'learning_rate': 0.019500739445648372, 'colsample_bytree': 0.7495119708122616, 'subsample': 0.5855327611999368, 'alpha': 0.12576001095396067, 'lambda': 1.9227490086953063, 'gamma': 2.419249609573914e-09, 'min_child_weight': 26.22482991444324}. Best is trial 14 with value: 0.005788107265033699.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:36,410]\u001b[0m Trial 17 finished with value: 0.00031331323613610455 and parameters: {'n_estimators': 904, 'max_depth': 5, 'learning_rate': 0.021741514515293738, 'colsample_bytree': 0.5570838487818748, 'subsample': 0.9106142061229903, 'alpha': 2.434971607690746, 'lambda': 10.668974730397146, 'gamma': 0.0008381655091137125, 'min_child_weight': 4.31519614345433}. Best is trial 14 with value: 0.005788107265033699.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:38,498]\u001b[0m Trial 18 finished with value: 0.0019626574223277123 and parameters: {'n_estimators': 1032, 'max_depth': 3, 'learning_rate': 0.027312156919868814, 'colsample_bytree': 0.29838088105803384, 'subsample': 0.4919941164622129, 'alpha': 0.7296256827984743, 'lambda': 0.7118816898275176, 'gamma': 0.009041646254895777, 'min_child_weight': 10.937338606977}. Best is trial 14 with value: 0.005788107265033699.\u001b[0m\n\u001b[32m[I 2022-09-06 20:58:40,373]\u001b[0m Trial 19 finished with value: -0.0008744467961678982 and parameters: {'n_estimators': 850, 'max_depth': 4, 'learning_rate': 0.01769387269117603, 'colsample_bytree': 0.9136705804670482, 'subsample': 0.26370987801384, 'alpha': 0.25243640127824163, 'lambda': 66.96682818310015, 'gamma': 1.107133297974303e-05, 'min_child_weight': 44.55925131999993}. Best is trial 14 with value: 0.005788107265033699.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  66.48019909858704\n        n_estimators : 808\n           max_depth : 2\n       learning_rate : 0.018684822903725116\n    colsample_bytree : 0.5983914115051032\n           subsample : 0.5199908975473907\n               alpha : 1.581821023352167\n              lambda : 2.0591485907246727\n               gamma : 43.48222566967177\n    min_child_weight : 9.143588925495571\nbest objective value : 0.005788107265033699\nOptuna XGB train: \n 9.842545655504647 0.035480232771717746 \nvalidation \n 13.377182455402558 0.038804025057159075 12.702423212818722 0.013373593604877332 \ntest \n 12.28716338943221 0.016992271194231412\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150     0.0821  0.021046  0.026281     0.09783  0.133871   0.033129   \n1      250   0.088074 -0.012211  0.015254    0.060695  0.032498   0.025239   \n2      350   0.102933  0.039992 -0.006382    0.099072  0.070611  -0.005967   \n3      450   0.052144  0.015028  0.015156    0.028579  0.034526   0.014994   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.033424  0.049082  0.015596  \n1   0.027712 -0.004911  0.008209  \n2   0.105721  0.073549  0.003203  \n3    0.03548  0.038804  0.016992  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.0821</td>\n      <td>0.021046</td>\n      <td>0.026281</td>\n      <td>0.09783</td>\n      <td>0.133871</td>\n      <td>0.033129</td>\n      <td>0.033424</td>\n      <td>0.049082</td>\n      <td>0.015596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.088074</td>\n      <td>-0.012211</td>\n      <td>0.015254</td>\n      <td>0.060695</td>\n      <td>0.032498</td>\n      <td>0.025239</td>\n      <td>0.027712</td>\n      <td>-0.004911</td>\n      <td>0.008209</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.102933</td>\n      <td>0.039992</td>\n      <td>-0.006382</td>\n      <td>0.099072</td>\n      <td>0.070611</td>\n      <td>-0.005967</td>\n      <td>0.105721</td>\n      <td>0.073549</td>\n      <td>0.003203</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>0.052144</td>\n      <td>0.015028</td>\n      <td>0.015156</td>\n      <td>0.028579</td>\n      <td>0.034526</td>\n      <td>0.014994</td>\n      <td>0.03548</td>\n      <td>0.038804</td>\n      <td>0.016992</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(78603, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6166, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6178, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    78603.000000\nmean       566.290854\nstd         10.352088\nmin        549.000000\n25%        557.000000\n50%        566.000000\n75%        575.000000\nmax        584.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    6166.00000\nmean      585.99627\nstd         0.81619\nmin       585.00000\n25%       585.00000\n50%       586.00000\n75%       587.00000\nmax       587.00000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    6178.000000\nmean      589.020557\nstd         0.821674\nmin       588.000000\n25%       588.000000\n50%       589.000000\n75%       590.000000\nmax       590.000000\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (78603, 44)\ntime to do feature proprocessing: \nNumber of features after transformation:  (78603, 92) (6166, 92) (6148, 92) (6178, 92)\nmae of a constant model 8.674298431411088\nR2 of a constant model 0.0\nfixed XGB train: 8.272216931589522 0.09792092207106229\nXGB val: 7.209400135191823 0.031526544969337045\nXGB val extra: 8.530902381996878 0.022122275087195442\nXGB test: 7.451467172772173 0.03981848696101298\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 5, 'n_estimators': 600, 'subsample': 0.6} 0.03561110793764344 63.3326358795166\nXGB train: 8.095279869623008 0.1472411923753254\nXGB validation: 6.986981601754949 0.10085027263377855\nXGB validation extra: 8.210557573297903 0.10220867706103698\nXGB test: 7.37366562047179 0.05265705301605805\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 20:59:53,891]\u001b[0m A new study created in memory with name: no-name-8234b4fc-c8f4-462a-a034-6e0496da3a4e\u001b[0m\n\u001b[32m[I 2022-09-06 20:59:56,993]\u001b[0m Trial 0 finished with value: 0.011864362342483682 and parameters: {'n_estimators': 1061, 'max_depth': 2, 'learning_rate': 0.008625293362275526, 'colsample_bytree': 0.5066613063963684, 'subsample': 0.2466456352188935, 'alpha': 41.40117871655134, 'lambda': 29.287989616769682, 'gamma': 3.438789544449251e-09, 'min_child_weight': 0.9603679842789169}. Best is trial 0 with value: 0.011864362342483682.\u001b[0m\n\u001b[32m[I 2022-09-06 20:59:58,947]\u001b[0m Trial 1 finished with value: 0.016059167161322628 and parameters: {'n_estimators': 1355, 'max_depth': 3, 'learning_rate': 0.0089753728600791, 'colsample_bytree': 0.2609874447681335, 'subsample': 0.4135281823804998, 'alpha': 1.1901327743161594, 'lambda': 2.928760343887948, 'gamma': 1.7167605496978679e-06, 'min_child_weight': 0.34771076208924795}. Best is trial 1 with value: 0.016059167161322628.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:07,048]\u001b[0m Trial 2 finished with value: 0.009743216418849433 and parameters: {'n_estimators': 1457, 'max_depth': 6, 'learning_rate': 0.004883190785670136, 'colsample_bytree': 0.506765328557977, 'subsample': 0.4433193149544996, 'alpha': 1.790427722053657, 'lambda': 60.56847101373049, 'gamma': 53.64317155181509, 'min_child_weight': 0.7480503885719456}. Best is trial 1 with value: 0.016059167161322628.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:09,616]\u001b[0m Trial 3 finished with value: 0.00957711589485571 and parameters: {'n_estimators': 901, 'max_depth': 2, 'learning_rate': 0.019649773413621462, 'colsample_bytree': 0.714955219436939, 'subsample': 0.2489621408943921, 'alpha': 0.4300689001523955, 'lambda': 299.75463480280035, 'gamma': 0.0039230222193122515, 'min_child_weight': 30.04763541336213}. Best is trial 1 with value: 0.016059167161322628.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:11,744]\u001b[0m Trial 4 finished with value: 0.012726268386668304 and parameters: {'n_estimators': 1473, 'max_depth': 5, 'learning_rate': 0.014169421908796257, 'colsample_bytree': 0.2645651190249878, 'subsample': 0.16049393510982696, 'alpha': 7.282196457540279, 'lambda': 0.6177384608914307, 'gamma': 6.085918789447861e-05, 'min_child_weight': 1.4800685972508145}. Best is trial 1 with value: 0.016059167161322628.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:13,086]\u001b[0m Trial 5 finished with value: 0.01730759808682522 and parameters: {'n_estimators': 1065, 'max_depth': 4, 'learning_rate': 0.02467620555076708, 'colsample_bytree': 0.24935397974872736, 'subsample': 0.8975350249564168, 'alpha': 0.5367369122825535, 'lambda': 35.450894915639864, 'gamma': 3.29825267624671e-07, 'min_child_weight': 0.10552893598181877}. Best is trial 5 with value: 0.01730759808682522.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:18,182]\u001b[0m Trial 6 finished with value: 0.016490514499646926 and parameters: {'n_estimators': 1016, 'max_depth': 5, 'learning_rate': 0.010115484676275663, 'colsample_bytree': 0.11375153373447609, 'subsample': 0.7952311290451981, 'alpha': 12.971951319623061, 'lambda': 7.950849080291463, 'gamma': 2.44635794930876, 'min_child_weight': 0.22928251705714658}. Best is trial 5 with value: 0.01730759808682522.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:22,377]\u001b[0m Trial 7 finished with value: 0.016484402115972893 and parameters: {'n_estimators': 1026, 'max_depth': 5, 'learning_rate': 0.006545567620592963, 'colsample_bytree': 0.3222449585193875, 'subsample': 0.21829860139939888, 'alpha': 30.192227413068938, 'lambda': 265.9283475341553, 'gamma': 23.722150617831996, 'min_child_weight': 0.2137578646004824}. Best is trial 5 with value: 0.01730759808682522.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:23,621]\u001b[0m Trial 8 finished with value: 0.013875925053370541 and parameters: {'n_estimators': 1360, 'max_depth': 4, 'learning_rate': 0.024114288435155687, 'colsample_bytree': 0.3142705817889969, 'subsample': 0.49869334852058045, 'alpha': 0.1433156283857211, 'lambda': 1.8614984529717822, 'gamma': 1.242453570963174e-10, 'min_child_weight': 0.43589285854537857}. Best is trial 5 with value: 0.01730759808682522.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:27,229]\u001b[0m Trial 9 finished with value: 0.010295093017031487 and parameters: {'n_estimators': 1310, 'max_depth': 2, 'learning_rate': 0.006833569476934807, 'colsample_bytree': 0.8789273517766703, 'subsample': 0.7262478818643043, 'alpha': 0.8955279998826317, 'lambda': 1.1017116277831678, 'gamma': 1.3242888691966497, 'min_child_weight': 1.2265246252309296}. Best is trial 5 with value: 0.01730759808682522.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:29,533]\u001b[0m Trial 10 finished with value: 0.017905245019282052 and parameters: {'n_estimators': 846, 'max_depth': 4, 'learning_rate': 0.02960317967381629, 'colsample_bytree': 0.09060174914160896, 'subsample': 0.9285908684360715, 'alpha': 0.10828194555526617, 'lambda': 23.091970441324097, 'gamma': 2.9287330192697434e-07, 'min_child_weight': 19.811943097834305}. Best is trial 10 with value: 0.017905245019282052.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:32,124]\u001b[0m Trial 11 finished with value: 0.017003426805934974 and parameters: {'n_estimators': 807, 'max_depth': 4, 'learning_rate': 0.028997091706575717, 'colsample_bytree': 0.05889980186490945, 'subsample': 0.9205958707299077, 'alpha': 0.14565828362116534, 'lambda': 0.11394028421517895, 'gamma': 3.2324914192393406e-07, 'min_child_weight': 7.780336594692243}. Best is trial 10 with value: 0.017905245019282052.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:33,457]\u001b[0m Trial 12 finished with value: 0.018089247228049905 and parameters: {'n_estimators': 1188, 'max_depth': 3, 'learning_rate': 0.029799368681763153, 'colsample_bytree': 0.1545285567026467, 'subsample': 0.9181395745409103, 'alpha': 0.43219054541870067, 'lambda': 24.49799764082969, 'gamma': 1.012562705612489e-07, 'min_child_weight': 6.01001784805099}. Best is trial 12 with value: 0.018089247228049905.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:34,834]\u001b[0m Trial 13 finished with value: 0.018773224153184963 and parameters: {'n_estimators': 1209, 'max_depth': 3, 'learning_rate': 0.02939361477229874, 'colsample_bytree': 0.1492037288185989, 'subsample': 0.6649367263076231, 'alpha': 0.27106910725026323, 'lambda': 10.258284607081388, 'gamma': 3.798235773301273e-09, 'min_child_weight': 6.27678326761853}. Best is trial 13 with value: 0.018773224153184963.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:36,923]\u001b[0m Trial 14 finished with value: 0.00992276225717913 and parameters: {'n_estimators': 1210, 'max_depth': 3, 'learning_rate': 0.024410152885396358, 'colsample_bytree': 0.42656623659214143, 'subsample': 0.6493256189999455, 'alpha': 0.3035736310594453, 'lambda': 12.090210358877696, 'gamma': 1.118654732939735e-09, 'min_child_weight': 4.295966949028912}. Best is trial 13 with value: 0.018773224153184963.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:41,384]\u001b[0m Trial 15 finished with value: 0.010118195234256283 and parameters: {'n_estimators': 1200, 'max_depth': 3, 'learning_rate': 0.0013545217080021404, 'colsample_bytree': 0.6678192311432014, 'subsample': 0.6033848857012386, 'alpha': 3.7567177347506844, 'lambda': 71.50057166616891, 'gamma': 1.3969133291622145e-08, 'min_child_weight': 8.44923323913808}. Best is trial 13 with value: 0.018773224153184963.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:43,119]\u001b[0m Trial 16 finished with value: 0.018175856784453874 and parameters: {'n_estimators': 1245, 'max_depth': 3, 'learning_rate': 0.01996931901158829, 'colsample_bytree': 0.15693573427271315, 'subsample': 0.8015595423569674, 'alpha': 0.2975662082925337, 'lambda': 4.457123443237385, 'gamma': 3.719442182583265e-05, 'min_child_weight': 3.175739812787824}. Best is trial 13 with value: 0.018773224153184963.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:45,275]\u001b[0m Trial 17 finished with value: 0.009739235720336109 and parameters: {'n_estimators': 1254, 'max_depth': 3, 'learning_rate': 0.0186610446836929, 'colsample_bytree': 0.42863671474485293, 'subsample': 0.7848550768015821, 'alpha': 0.23291357069592594, 'lambda': 3.6511980461745095, 'gamma': 0.000684556271389262, 'min_child_weight': 2.818347598269193}. Best is trial 13 with value: 0.018773224153184963.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:47,029]\u001b[0m Trial 18 finished with value: 0.0169423938929475 and parameters: {'n_estimators': 1124, 'max_depth': 2, 'learning_rate': 0.014759434630170754, 'colsample_bytree': 0.17056390382884226, 'subsample': 0.6115542989720715, 'alpha': 0.7761236684822299, 'lambda': 0.3139939556683779, 'gamma': 0.024721223891984438, 'min_child_weight': 17.954558667548792}. Best is trial 13 with value: 0.018773224153184963.\u001b[0m\n\u001b[32m[I 2022-09-06 21:00:49,565]\u001b[0m Trial 19 finished with value: 0.00812958271338397 and parameters: {'n_estimators': 1276, 'max_depth': 3, 'learning_rate': 0.020718469054012686, 'colsample_bytree': 0.3914922656405085, 'subsample': 0.7121315703865093, 'alpha': 2.593834532669543, 'lambda': 5.139629228536127, 'gamma': 7.502978118043623e-06, 'min_child_weight': 2.629102003847727}. Best is trial 13 with value: 0.018773224153184963.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  55.67486381530762\n        n_estimators : 1209\n           max_depth : 3\n       learning_rate : 0.02939361477229874\n    colsample_bytree : 0.1492037288185989\n           subsample : 0.6649367263076231\n               alpha : 0.27106910725026323\n              lambda : 10.258284607081388\n               gamma : 3.798235773301273e-09\n    min_child_weight : 6.27678326761853\nbest objective value : 0.018773224153184963\nOptuna XGB train: \n 8.205287936141971 0.11118705305257703 \nvalidation \n 7.074431034975817 0.0690484467800292 8.523097378418504 0.023397012292095676 \ntest \n 7.42142969610539 0.0441291251682141\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150     0.0821  0.021046  0.026281     0.09783  0.133871   0.033129   \n1      250   0.088074 -0.012211  0.015254    0.060695  0.032498   0.025239   \n2      350   0.102933  0.039992 -0.006382    0.099072  0.070611  -0.005967   \n3      450   0.052144  0.015028  0.015156    0.028579  0.034526   0.014994   \n4      550   0.097921  0.031527  0.039818    0.147241   0.10085   0.052657   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.033424  0.049082  0.015596  \n1   0.027712 -0.004911  0.008209  \n2   0.105721  0.073549  0.003203  \n3    0.03548  0.038804  0.016992  \n4   0.111187  0.069048  0.044129  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.0821</td>\n      <td>0.021046</td>\n      <td>0.026281</td>\n      <td>0.09783</td>\n      <td>0.133871</td>\n      <td>0.033129</td>\n      <td>0.033424</td>\n      <td>0.049082</td>\n      <td>0.015596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.088074</td>\n      <td>-0.012211</td>\n      <td>0.015254</td>\n      <td>0.060695</td>\n      <td>0.032498</td>\n      <td>0.025239</td>\n      <td>0.027712</td>\n      <td>-0.004911</td>\n      <td>0.008209</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.102933</td>\n      <td>0.039992</td>\n      <td>-0.006382</td>\n      <td>0.099072</td>\n      <td>0.070611</td>\n      <td>-0.005967</td>\n      <td>0.105721</td>\n      <td>0.073549</td>\n      <td>0.003203</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>0.052144</td>\n      <td>0.015028</td>\n      <td>0.015156</td>\n      <td>0.028579</td>\n      <td>0.034526</td>\n      <td>0.014994</td>\n      <td>0.03548</td>\n      <td>0.038804</td>\n      <td>0.016992</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>0.097921</td>\n      <td>0.031527</td>\n      <td>0.039818</td>\n      <td>0.147241</td>\n      <td>0.10085</td>\n      <td>0.052657</td>\n      <td>0.111187</td>\n      <td>0.069048</td>\n      <td>0.044129</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(61151, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(4761, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(4706, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    61151.000000\nmean       666.117741\nstd         10.398283\nmin        649.000000\n25%        657.000000\n50%        666.000000\n75%        675.000000\nmax        684.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    4761.000000\nmean      685.998110\nstd         0.817223\nmin       685.000000\n25%       685.000000\n50%       686.000000\n75%       687.000000\nmax       687.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    4706.000000\nmean      688.997025\nstd         0.816925\nmin       688.000000\n25%       688.000000\n50%       689.000000\n75%       690.000000\nmax       690.000000\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (61151, 44)\ntime to do feature proprocessing: \nNumber of features after transformation:  (61151, 92) (4761, 92) (4635, 92) (4706, 92)\nmae of a constant model 7.991024255095651\nR2 of a constant model 0.0\nfixed XGB train: 7.37186159053143 0.09370125511856164\nXGB val: 7.769372845620342 0.1202960492766123\nXGB val extra: 8.716766312159114 0.08044960563574766\nXGB test: 8.464771770272549 0.04196543276367293\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 2, 'n_estimators': 600, 'subsample': 0.6} 0.13082812132204125 56.2017879486084\nXGB train: 7.487009718437392 0.05510111043575783\nXGB validation: 7.5185901740389 0.18438306180043718\nXGB validation extra: 8.477010479339 0.12992120563495957\nXGB test: 8.439680663549156 0.040450396213600026\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 21:01:55,441]\u001b[0m A new study created in memory with name: no-name-3c8a59ca-df9f-4079-8474-05ca5eaacee7\u001b[0m\n\u001b[32m[I 2022-09-06 21:01:58,939]\u001b[0m Trial 0 finished with value: 0.08483866775594268 and parameters: {'n_estimators': 1078, 'max_depth': 6, 'learning_rate': 0.015168004750695727, 'colsample_bytree': 0.4393225708497595, 'subsample': 0.2648589961613307, 'alpha': 19.547970679252646, 'lambda': 149.62167954736904, 'gamma': 0.0013357225620263017, 'min_child_weight': 9.898434291610814}. Best is trial 0 with value: 0.08483866775594268.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:00,610]\u001b[0m Trial 1 finished with value: 0.09609649962279755 and parameters: {'n_estimators': 869, 'max_depth': 5, 'learning_rate': 0.024421675450470075, 'colsample_bytree': 0.4949780158287963, 'subsample': 0.5633722364289775, 'alpha': 0.1742445099365509, 'lambda': 0.15515656310900092, 'gamma': 0.008982217973991274, 'min_child_weight': 2.3167427270635157}. Best is trial 1 with value: 0.09609649962279755.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:05,668]\u001b[0m Trial 2 finished with value: 0.08521080878017184 and parameters: {'n_estimators': 1017, 'max_depth': 6, 'learning_rate': 0.018150422295412225, 'colsample_bytree': 0.46548833545272195, 'subsample': 0.12301740278462091, 'alpha': 34.36025174862227, 'lambda': 143.69985251145934, 'gamma': 0.3263631288856906, 'min_child_weight': 1.2279355419638263}. Best is trial 1 with value: 0.09609649962279755.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:12,776]\u001b[0m Trial 3 finished with value: 0.08295181416326786 and parameters: {'n_estimators': 1273, 'max_depth': 6, 'learning_rate': 0.009771058312638928, 'colsample_bytree': 0.25054109740991226, 'subsample': 0.8702319899740832, 'alpha': 0.8377882069706254, 'lambda': 10.308942407829033, 'gamma': 2.664177642683432, 'min_child_weight': 0.9653773377300925}. Best is trial 1 with value: 0.09609649962279755.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:17,436]\u001b[0m Trial 4 finished with value: 0.08853718829305468 and parameters: {'n_estimators': 1096, 'max_depth': 4, 'learning_rate': 0.0038845440835795974, 'colsample_bytree': 0.6802299644171609, 'subsample': 0.6066546083766433, 'alpha': 11.321933922999206, 'lambda': 0.7067432644945322, 'gamma': 1.4283286188940486e-07, 'min_child_weight': 0.4480284652541437}. Best is trial 1 with value: 0.09609649962279755.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:19,688]\u001b[0m Trial 5 finished with value: 0.09669169714639164 and parameters: {'n_estimators': 1117, 'max_depth': 5, 'learning_rate': 0.01707767298881257, 'colsample_bytree': 0.5175510013236203, 'subsample': 0.8986504672516136, 'alpha': 5.727587764153522, 'lambda': 0.774013477690942, 'gamma': 4.9278115605583605e-08, 'min_child_weight': 9.786556853893675}. Best is trial 5 with value: 0.09669169714639164.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:27,680]\u001b[0m Trial 6 finished with value: 0.07434151913543652 and parameters: {'n_estimators': 991, 'max_depth': 6, 'learning_rate': 0.0028018674246630113, 'colsample_bytree': 0.9326455276140714, 'subsample': 0.20276966492661172, 'alpha': 14.34505460166313, 'lambda': 78.16515253353063, 'gamma': 6.659702671085091e-05, 'min_child_weight': 5.904450432579594}. Best is trial 5 with value: 0.09669169714639164.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:31,685]\u001b[0m Trial 7 finished with value: 0.09703501305117151 and parameters: {'n_estimators': 1271, 'max_depth': 5, 'learning_rate': 0.012256463614481503, 'colsample_bytree': 0.4148488731398349, 'subsample': 0.7205717086413961, 'alpha': 28.96946074488171, 'lambda': 89.0672168386702, 'gamma': 5.748474260681307e-10, 'min_child_weight': 3.852745815487951}. Best is trial 7 with value: 0.09703501305117151.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:34,985]\u001b[0m Trial 8 finished with value: 0.09802272726784025 and parameters: {'n_estimators': 955, 'max_depth': 5, 'learning_rate': 0.011401006851647088, 'colsample_bytree': 0.762820540151902, 'subsample': 0.48859846898901027, 'alpha': 19.02808703016862, 'lambda': 14.004850384898583, 'gamma': 7.577120755915676e-05, 'min_child_weight': 13.805862473969178}. Best is trial 8 with value: 0.09802272726784025.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:36,681]\u001b[0m Trial 9 finished with value: 0.09282587488541208 and parameters: {'n_estimators': 1140, 'max_depth': 3, 'learning_rate': 0.016119586435814202, 'colsample_bytree': 0.9151062700264266, 'subsample': 0.3710420525922966, 'alpha': 15.147781536432182, 'lambda': 3.867183863035489, 'gamma': 4.392947090257444e-06, 'min_child_weight': 1.0430836814388893}. Best is trial 8 with value: 0.09802272726784025.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:38,742]\u001b[0m Trial 10 finished with value: 0.0916080215984967 and parameters: {'n_estimators': 814, 'max_depth': 2, 'learning_rate': 0.029957524247095434, 'colsample_bytree': 0.05694918041240821, 'subsample': 0.40931602736098327, 'alpha': 1.7232773426073955, 'lambda': 15.008370662176844, 'gamma': 56.84537329802246, 'min_child_weight': 38.21610476495053}. Best is trial 8 with value: 0.09802272726784025.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:42,230]\u001b[0m Trial 11 finished with value: 0.0951579659322537 and parameters: {'n_estimators': 1499, 'max_depth': 4, 'learning_rate': 0.009255872272941359, 'colsample_bytree': 0.7295820024081883, 'subsample': 0.7232400540240564, 'alpha': 48.548222010581995, 'lambda': 26.276872202209994, 'gamma': 1.1553921382623226e-10, 'min_child_weight': 0.12910897767154803}. Best is trial 8 with value: 0.09802272726784025.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:46,335]\u001b[0m Trial 12 finished with value: 0.09568478733322301 and parameters: {'n_estimators': 1296, 'max_depth': 5, 'learning_rate': 0.010052073537409396, 'colsample_bytree': 0.6603514670088486, 'subsample': 0.7020981041938292, 'alpha': 4.03546903667854, 'lambda': 44.05053487774204, 'gamma': 6.356426512818831e-10, 'min_child_weight': 41.872372460429936}. Best is trial 8 with value: 0.09802272726784025.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:51,458]\u001b[0m Trial 13 finished with value: 0.08816281384415883 and parameters: {'n_estimators': 1267, 'max_depth': 5, 'learning_rate': 0.01198224384664069, 'colsample_bytree': 0.30290791793577465, 'subsample': 0.4468756404946379, 'alpha': 5.285276001839262, 'lambda': 434.4376020987046, 'gamma': 1.6909263588542007e-08, 'min_child_weight': 4.8351801580126255}. Best is trial 8 with value: 0.09802272726784025.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:55,742]\u001b[0m Trial 14 finished with value: 0.09675928759131569 and parameters: {'n_estimators': 1420, 'max_depth': 3, 'learning_rate': 0.00623619671708122, 'colsample_bytree': 0.791772823880383, 'subsample': 0.730360427229261, 'alpha': 0.708950515805843, 'lambda': 3.5470861676981453, 'gamma': 4.176405231884399e-06, 'min_child_weight': 17.21138442328897}. Best is trial 8 with value: 0.09802272726784025.\u001b[0m\n\u001b[32m[I 2022-09-06 21:02:58,047]\u001b[0m Trial 15 finished with value: 0.08844484626885625 and parameters: {'n_estimators': 901, 'max_depth': 4, 'learning_rate': 0.020666509047801557, 'colsample_bytree': 0.29201596106839645, 'subsample': 0.6260373596442517, 'alpha': 36.09387153101393, 'lambda': 450.32713975971086, 'gamma': 0.03568620675861345, 'min_child_weight': 2.851602998848743}. Best is trial 8 with value: 0.09802272726784025.\u001b[0m\n\u001b[32m[I 2022-09-06 21:03:01,356]\u001b[0m Trial 16 finished with value: 0.09817489479603474 and parameters: {'n_estimators': 1223, 'max_depth': 5, 'learning_rate': 0.013105136801826895, 'colsample_bytree': 0.6039600804897133, 'subsample': 0.48983324585405896, 'alpha': 2.215629179557985, 'lambda': 5.439350838163729, 'gamma': 2.2086623386341012e-09, 'min_child_weight': 19.388556551900074}. Best is trial 16 with value: 0.09817489479603474.\u001b[0m\n\u001b[32m[I 2022-09-06 21:03:03,224]\u001b[0m Trial 17 finished with value: 0.09850784510039148 and parameters: {'n_estimators': 1200, 'max_depth': 3, 'learning_rate': 0.02240145304726146, 'colsample_bytree': 0.5926169956173828, 'subsample': 0.4854174352229914, 'alpha': 2.3522083711322637, 'lambda': 1.6967810728701949, 'gamma': 0.00014173274042896675, 'min_child_weight': 20.150061028136818}. Best is trial 17 with value: 0.09850784510039148.\u001b[0m\n\u001b[32m[I 2022-09-06 21:03:04,655]\u001b[0m Trial 18 finished with value: 0.09733820360276108 and parameters: {'n_estimators': 1194, 'max_depth': 3, 'learning_rate': 0.023869323691465192, 'colsample_bytree': 0.6136089917366646, 'subsample': 0.5172023888052807, 'alpha': 1.2686446140599086, 'lambda': 1.280046651827686, 'gamma': 2.5937643925708677e-06, 'min_child_weight': 23.344081164432094}. Best is trial 17 with value: 0.09850784510039148.\u001b[0m\n\u001b[32m[I 2022-09-06 21:03:05,873]\u001b[0m Trial 19 finished with value: 0.09197477094458752 and parameters: {'n_estimators': 1404, 'max_depth': 2, 'learning_rate': 0.02891480227021833, 'colsample_bytree': 0.5934960639735527, 'subsample': 0.3032519735675804, 'alpha': 0.31119704127041753, 'lambda': 0.11278275512650045, 'gamma': 0.0007938510041517227, 'min_child_weight': 26.88359415147266}. Best is trial 17 with value: 0.09850784510039148.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  70.43555879592896\n        n_estimators : 1200\n           max_depth : 3\n       learning_rate : 0.02240145304726146\n    colsample_bytree : 0.5926169956173828\n           subsample : 0.4854174352229914\n               alpha : 2.3522083711322637\n              lambda : 1.6967810728701949\n               gamma : 0.00014173274042896675\n    min_child_weight : 20.150061028136818\nbest objective value : 0.09850784510039148\nOptuna XGB train: \n 7.3186498911668485 0.10939244155060657 \nvalidation \n 7.334971701984184 0.23820870218226287 8.587103916958567 0.0992828398909098 \ntest \n 8.455738819021107 0.03597001318805482\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150     0.0821  0.021046  0.026281     0.09783  0.133871   0.033129   \n1      250   0.088074 -0.012211  0.015254    0.060695  0.032498   0.025239   \n2      350   0.102933  0.039992 -0.006382    0.099072  0.070611  -0.005967   \n3      450   0.052144  0.015028  0.015156    0.028579  0.034526   0.014994   \n4      550   0.097921  0.031527  0.039818    0.147241   0.10085   0.052657   \n5      650   0.093701  0.120296  0.041965    0.055101  0.184383    0.04045   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.033424  0.049082  0.015596  \n1   0.027712 -0.004911  0.008209  \n2   0.105721  0.073549  0.003203  \n3    0.03548  0.038804  0.016992  \n4   0.111187  0.069048  0.044129  \n5   0.109392  0.238209   0.03597  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.0821</td>\n      <td>0.021046</td>\n      <td>0.026281</td>\n      <td>0.09783</td>\n      <td>0.133871</td>\n      <td>0.033129</td>\n      <td>0.033424</td>\n      <td>0.049082</td>\n      <td>0.015596</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.088074</td>\n      <td>-0.012211</td>\n      <td>0.015254</td>\n      <td>0.060695</td>\n      <td>0.032498</td>\n      <td>0.025239</td>\n      <td>0.027712</td>\n      <td>-0.004911</td>\n      <td>0.008209</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.102933</td>\n      <td>0.039992</td>\n      <td>-0.006382</td>\n      <td>0.099072</td>\n      <td>0.070611</td>\n      <td>-0.005967</td>\n      <td>0.105721</td>\n      <td>0.073549</td>\n      <td>0.003203</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>0.052144</td>\n      <td>0.015028</td>\n      <td>0.015156</td>\n      <td>0.028579</td>\n      <td>0.034526</td>\n      <td>0.014994</td>\n      <td>0.03548</td>\n      <td>0.038804</td>\n      <td>0.016992</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>0.097921</td>\n      <td>0.031527</td>\n      <td>0.039818</td>\n      <td>0.147241</td>\n      <td>0.10085</td>\n      <td>0.052657</td>\n      <td>0.111187</td>\n      <td>0.069048</td>\n      <td>0.044129</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>0.093701</td>\n      <td>0.120296</td>\n      <td>0.041965</td>\n      <td>0.055101</td>\n      <td>0.184383</td>\n      <td>0.04045</td>\n      <td>0.109392</td>\n      <td>0.238209</td>\n      <td>0.03597</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"display(results.iloc[:,1:].mean())\n# cv_regularizer = 0.5\n# optuna_trials = 80\nprint(time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T21:03:50.858311Z","iopub.execute_input":"2022-09-06T21:03:50.858892Z","iopub.status.idle":"2022-09-06T21:03:50.873034Z","shell.execute_reply.started":"2022-09-06T21:03:50.858856Z","shell.execute_reply":"2022-09-06T21:03:50.871903Z"},"trusted":true},"execution_count":193,"outputs":[{"output_type":"display_data","data":{"text/plain":"xgbf_train     0.086146\nxgbf_val       0.035946\nxgbf_test      0.022016\nxgbgs_train    0.081420\nxgbgs_val      0.092790\nxgbgs_test     0.026750\nxgbo_train     0.070486\nxgbo_val       0.077297\nxgbo_test      0.020683\ndtype: float64"},"metadata":{}},{"name":"stdout","text":"778.7103786468506\n","output_type":"stream"}]},{"cell_type":"code","source":"# general point:\n# compared to NN, xgb is harder to regularize\n# in NN, you can simply shrink coefficient towards constant prediction.\n# in xgb, you can not do that. the only way to regularize is via hyperparameters.\n# in other words, by tweaking hyperpars, in NN you can approach R^2=0.0 prediction from a constant model arbitrarily close\n# in xgb, you can not do that.\n# by setting eta as low as 0.1% you can bring r2 down to 0.1%, but lowering eta further actyally increases abs(r2).\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:39:46.588332Z","iopub.execute_input":"2022-09-06T19:39:46.589277Z","iopub.status.idle":"2022-09-06T19:39:46.597724Z","shell.execute_reply.started":"2022-09-06T19:39:46.589242Z","shell.execute_reply":"2022-09-06T19:39:46.596791Z"},"trusted":true},"execution_count":88,"outputs":[]},{"cell_type":"code","source":"optuna_xgb","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:39:46.599286Z","iopub.execute_input":"2022-09-06T19:39:46.599627Z","iopub.status.idle":"2022-09-06T19:39:46.616001Z","shell.execute_reply.started":"2022-09-06T19:39:46.599594Z","shell.execute_reply":"2022-09-06T19:39:46.614929Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"XGBRegressor(alpha=6.445217963477007, base_score=0.5, booster='gbtree',\n             callbacks=None, colsample_bylevel=1, colsample_bynode=1,\n             colsample_bytree=0.43897829894896606, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None,\n             gamma=5.149447116926591e-08, gpu_id=0, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             lambda=32.55095201366502, learning_rate=0.018838898431855655,\n             max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=5,\n             max_leaves=0, min_child_weight=5.5475398343610465, missing=nan,\n             monotone_constraints='()', n_estimators=855, n_jobs=0,\n             num_parallel_tree=1, predictor='auto', random_state=0, ...)"},"metadata":{}}]},{"cell_type":"code","source":"explainerxgbc = shap.TreeExplainer(optuna_xgb)\nshap_values_XGBoost_test = explainerxgbc.shap_values(X_test)\n\nvals = np.abs(shap_values_XGBoost_test).mean(0)\nfeature_names = X_test.columns\nfeature_importance = pd.DataFrame(list(zip(feature_names, vals)),\n                                 columns=['col_name','feature_importance_vals'])\nfeature_importance.sort_values(by=['feature_importance_vals'],\n                              ascending=False, inplace=True)\n\nshap.summary_plot(shap_values_XGBoost_test, X_test, \n                  plot_type=\"bar\", plot_size=(6,6), max_display=20)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:40:51.981860Z","iopub.execute_input":"2022-09-06T19:40:51.982485Z","iopub.status.idle":"2022-09-06T19:40:53.305810Z","shell.execute_reply.started":"2022-09-06T19:40:51.982448Z","shell.execute_reply":"2022-09-06T19:40:53.304889Z"},"trusted":true},"execution_count":93,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x432 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAekAAAGoCAYAAABiyh1eAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAB6bUlEQVR4nO3deVhV1f748Teg3oMCBooMauolx66ZaQ4pOKAyGDKolP4U0ZTS0IQoFYcwceDmFDdJDQnzpiHXgUk0zaG0VOqKaYOafkUEFAxFDpMM+/eHj+d6AhL1IAf6vJ7Hx3P22nvtzz4c+Jy19jprGSiKoiCEEEIIvWNY1wEIIYQQomqSpIUQQgg9JUlaCCGE0FOSpIUQQgg9JUlaCCGE0FOSpOuxhISEug5BCCFELZIkLYQQQugpSdJCCCGEnpIkLYQQQugpSdJCCCGEnpIkLYQQQugpSdJCCCGEnpIkLYQQQugpSdJCCCGEnpIkLYQQQugpSdJCCCGEnpIkLYQQQugpA0VRlLoOQjwag5VldR2CEEIAoAQ1qusQGiRpSQshhBB6SpK0EEIIoacaZJKOiooiICDgserw8PCQpSCFEELUKb26ieDn50efPn2YOnXqY9UzZcoUHUX0YOvWrWPfvn3k5eXRpEkTevbsSWBgINbW1pp9EhMT+eSTT7hx4wbPPPMMc+fOpWvXrtXWmZ6ezvLly/nxxx8xMzNj/PjxTJgw4UlcjhBCCD3SIFvST9LIkSPZunUrR44cISEhAWtra4KDgzXlqamprFixgnnz5nHo0CGGDh3KW2+9hVqtrrK+8vJyAgICaN++PQcOHGD16tVs3ryZL7/88kldkhBCCD1RK0m6sLCQtWvX4u7ujoODA2PHjuXUqVPs27ePcePGMWjQIJycnFi6dClFRUUAhIWFkZqayqZNm7C3t8fLy+uRz79hwwZmzJihee7m5kZUVBTTp0/H3t4eb29vTp8+rSkvKytj9erVDB8+HCcnJ6Kjo2t8rvbt22NiYgKAoigYGhqSlpamKd+1axdDhgyhX79+NGnSBB8fHxo3bszhw4errO/UqVNkZWXh7++PSqWiS5cueHl5sWPHjod7EYQQQtR7tdLdvWTJEnJycoiIiMDW1parV68Cd5N3aGgoHTp0ICMjg8DAQDZt2oS/vz9z5szh4sWLOunurkp8fDyrVq2iffv2rF27lpCQEHbt2gVAdHQ0R48eJSoqCktLS9asWUNWVlaN6967dy/Lly+noKAAIyMjrfvhFy5c4OWXX9Y8NzAwoHPnzpw/f77Kus6fP0+7du1o2rSpZluXLl2IjY192EsWQghRz+k8Sefm5rJ//35iYmJo3bo1AG3bttX6/97jMWPGkJSUpOsQquTl5YWdnR1wd1DYtm3bUKvVmJiYkJSUxKRJkzTxzZ49m7i4uBrX7ezsjLOzMzdu3CAuLo5nnnlGU1ZQUKBpad9jampKQUFBlXUVFhY+1P5CCCEaLp0n6czMTADatWtXqez48eNERkZy+fJlSktLKS8vx8LCQtchVKlly5aax8bGxsD/Emh2dja2trZa5Y8SV8uWLfH09MTd3Z3ExESaN29Os2bNKt1/zs/Pp02bNlXW0bRp0yr3b9as2UPHI4QQon7T+T3pe8nuypUrWttLS0sJCgpixIgRJCYmcuTIEWbOnMn9E54ZGtbNODZLS0vNhwuAoqIibt68+Uh1lZeXU1RURE5ODgAdO3bk119/1ZQrisL58+fp1KlTlcd36tSJtLQ0zb16gHPnztGxY8dHikcIIUT9pfOsaGFhgaOjIytWrCAzMxNFUUhPTyctLY3S0lLMzMxQqVRcunSJ7du3ax3bokULzf3rJ8nV1ZUtW7Zw9epViouLCQ8Pp6Ki4oHHVVRUEBMTQ25uLgDXr18nLCwMW1tb2rdvD4CnpyeHDh3i5MmTlJaW8u9//5s7d+4wePDgKuvs2bMnNjY2rFu3juLiYs6dO8fOnTsfayCdEEKI+qlWmq6LFi2ic+fO+Pn54eDgwNtvv41arWbu3LmEh4djb29PWFgYzs7OWseNHz+en3/+mcGDB+Pt7V0boVVp8uTJ9O/fH19fX9zd3bGyssLGxqZGxx47doxXXnmFgQMH4uvri0qlIiIigkaN7t5JeP7555kzZw5Lly5l8ODB7N+/nw8//FBz3/natWvY29tz6tQpAIyMjFizZg0XL17E0dGRt956i4kTJ+Lk5FQ7Fy+EEEJvyQIb9VhCQgJubm51HYYQQohaIpOZCCGEEHpKr6YF/SNvb+8qv69sbm5e7cCu4OBgXFxcdBbDsmXLSE5OrrIsNjZWa/pPIYQQQpeku7sek+5uIYRo2KS7WwghhNBT0pKuxwxWltV1CKKeUYL0+g6XEOIPpCUthBBC6ClJ0kIIIYSekiQthBBC6Cm5QfUERUREcPToUS5dusQLL7xARESEVvnJkyf59NNPOX/+PHl5eSQlJWFlZVVH0QohhKhr0pJ+gtq0acMbb7yBp6dnleXGxsaMHDmSxYsXP+HIhBBC6KPHbkm7ubnh6elJSkoKZ8+excbGhvnz59OjRw9CQkIwMjJi4cKFWvtPnz4dV1dXEhIS2LRpE2PHjuXzzz9HrVbj5eWFr68vS5cu5eTJk7Rs2ZKFCxfy/PPPP3KMISEhlJeX06hRIw4dOoSxsTFvvfUWHTp0YOnSpVy+fJmuXbsSGhqKpaUlALdu3WL16tWcOHECgH79+hEYGEjz5s011+Hu7k5KSgo///wztra2hIaGcvHiRdavX8/NmzcZNmwY8+bN08zjPWrUKAB++eUX0tLSKsXZvXt3unfvrrUilxBCiL8unbSk4+PjCQoK4vDhw/Tt25eQkJAaH5uVlYVarSYuLo7IyEhiYmKYNWsWPj4+HDx4kKFDh+qkZXnw4EEcHR05ePAgr732GkuXLmX9+vV88MEHfPnllxgYGLBhwwbN/gsXLiQ/P5/Y2FhiY2O5desWixYt0qozKSmJuXPncujQITp16kRQUBA//PAD27ZtIyYmhq+//pr9+/c/duxCCCH+mnSSpL28vLCzs8PIyAgPDw/S09NRq9U1OlalUjFt2jQaN25Mp06d6NixI88++yzdu3fHyMgIFxeXh6qvOr1792bgwIEYGhry8ssvU1RUxMiRI7GyskKlUuHo6MjPP/8MQE5ODt999x0BAQGYmZlhZmZGQEAAx44d48aNG5o6PT096dChA40aNcLJyYmMjAxmzJiBsbEx1tbW9OrVS1OnEEII8bB0kqRbtmypeWxsbAxAQUFBjY41NzfH0PB/YahUKlq0aKH1/GHqq0mM9+r847bCwkLg7rrQALa2tpryNm3aAHeXlqyuTiMjI8zNzausUwghhHhYtTpwrGnTphQVFWmel5WVkZubW5un1Il7I6rvX9wjIyMDQBbUEEII8cTUapLu2rUrKSkpZGRkcOfOHSIiIigr0/+pLC0tLenXrx9r1qwhPz+f27dvs3btWl566SWt1vPDKisro6SkhPLycioqKigpKeHOnTua8j9uKy0tpaSkhIqKise+JiGEEPVPrX5P2sXFhdTUVCZMmICxsTG+vr60atWqNk+pM0uWLGH16tWMHj0agL59+/L2228/Vp2hoaEkJiZqng8YMAAbGxsSEhIA+O9//8sbb7yhKffw8ABg/fr19O7d+7HOLYQQov6RBTbqMVmqUgghGjaZzEQIIYTQU/VqWlBvb2+twVz3mJubc/PmzSqPCQ4OxsXFpbZDE0IIIXSuXiXp7du313UIQgghxBMj3d1CCCGEnpKBY/WYwUr9/zqbqHtKUL3qMBNC3Eda0kIIIYSekiQthBBC6KkGmaSjoqIICAh4rDo8PDw0k4wIIYQQdUGvblb5+fnRp08fpk6d+lj1TJkyRUcRPdi+ffuIjY3lwoULFBcXa9afvufUqVOsXLmSrKwsysvLadOmDa+99hpDhw6tts7c3FyWL1/OiRMnaNKkCaNGjcLf319rIRIhhBANn14l6frIzMyMMWPGUFJSwrJlyyqVt2vXjpUrV2oW5jh16hQzZ86kQ4cOdOjQoco6FyxYQLNmzdizZw+3bt1i1qxZmJmZ4evrW5uXIoQQQs/USpIuLCxk48aNHDp0iJs3b2JlZUVwcDDZ2dlER0eTmZmJSqXCwcGBwMBAjI2NCQsLIzU1lTNnzrB582YsLS3ZuXPnI51/w4YNnD59moiICADc3Nzw9PQkJSWFs2fPYmNjw/z58+nRowdwd+GL8PBwkpOTMTQ0ZNy4cTU+V//+/QH4/vvvqyy3sLDQPK6oqMDQ0BBFUUhPT68ySWdkZHDy5El2796NiYkJJiYm+Pj4EBUVJUlaCCH+YmolSS9ZsoScnBwiIiKwtbXl6tWrwN3kHRoaSocOHcjIyCAwMJBNmzbh7+/PnDlzuHjxok66u6sSHx/PqlWraN++PWvXriUkJIRdu3YBEB0dzdGjR4mKisLS0pI1a9ZUObPZ4xg8eDBFRUWUl5fzwgsv0K9fvyr3u3DhAiYmJpr1qwG6dOlCZmYmarUaExMTncYlhBBCf+k8Sefm5rJ//35iYmJo3bo1AG3bttX6/97jMWPGkJSUpOsQquTl5YWdnR1wd1DYtm3bNEkvKSmJSZMmaeKbPXs2cXFxOj3/4cOHuXPnDt9++y2XL1/GyMioyv0KCgoqJWJTU9Nqy4QQQjRcOk/SmZmZwN17sX90/PhxIiMjuXz5MqWlpZSXl2t1B9em+9eBNjY2Bv6X9LKzs7G1tdUqr424mjRpwuDBg5k1axampqaaZTDv16xZM9Rqtda2/Px8TZkQQoi/Dp0PF76X7K5cuaK1vbS0lKCgIEaMGEFiYiJHjhxh5syZ3D/hWV2NXra0tNR8uAAoKiqqdsEOXSgvLyc9Pb3Kso4dO6JWqzW3CADOnTuHra2ttKKFEOIvRudZ0cLCAkdHR1asWEFmZqZmkFRaWhqlpaWYmZmhUqm4dOlSpQUzWrRooZWcnhRXV1e2bNnC1atXKS4uJjw8nIqKihodW15eTklJCWVld6foLCkpoaSkRPPh46uvvuK3336jrKyMkpISdu3axffff1/tPenWrVvTp08fwsPDUavVZGRksHnzZry8vHRzsUIIIeqNWmm6Llq0iM6dO+Pn54eDgwNvv/02arWauXPnEh4ejr29PWFhYTg7O2sdN378eH7++WcGDx6Mt7d3bYRWpcmTJ9O/f398fX1xd3fHysoKGxubGh27Z88eBgwYgL+/P+Xl5QwYMIABAwZoBp7duHGDd955hyFDhuDi4kJ8fDxLly7VStL29vYkJydrnoeGhqIoCq6urvj4+DBo0CB8fHx0e9FCCCH0niywUY8lJCTg5uZW12EIIYSoJTKFlRBCCKGn9HrGMW9v7yq/r2xubl7twK7g4GBcXFx0FsOyZcu0uqLvFxsbq5lJTAghhNA16e6ux6S7WwghGjbp7hZCCCH0lLSk6zGDlWV1HYLQISVIr+8+CSHqgLSkhRBCCD0lSVoIIYTQU5KkhRBCCD3VIJN0VFQUAQEBj1WHh4cHCQkJOopICCGEeHh6laT9/PyIjIx87HqmTJnCmjVrdBDRg61bt45Ro0YxaNAghg8fzrvvvsu1a9eq3Dc8PJzevXuzZ8+eP60zPT2dGTNmMHDgQFxdXfn3v/9dG6ELIYTQc3qVpOujkSNHsnXrVo4cOUJCQgLW1tYEBwdX2u/s2bN8++23WktmVqW8vJyAgADat2/PgQMHWL16NZs3b+bLL7+srUsQQgihp2rlOx+FhYVs3LiRQ4cOcfPmTaysrAgODiY7O5vo6GgyMzNRqVQ4ODgQGBiIsbExYWFhpKamcubMGTZv3oylpSU7d+58pPNv2LCB06dPExERAYCbmxuenp6kpKRw9uxZbGxsmD9/Pj169ACgrKyM8PBwkpOTMTQ0ZNy4cTU+V/v27TWPFUXB0NCQtLQ0rX3u3LnDkiVLmD9/PvPnz//T+k6dOkVWVhb+/v6oVCq6dOmCl5cXO3bsYMSIETWOSwghRP1XK0l6yZIl5OTkEBERga2trWb5ycLCQkJDQ+nQoQMZGRkEBgayadMm/P39mTNnDhcvXqRPnz5MnTpV5zHFx8ezatUq2rdvz9q1awkJCWHXrl0AREdHc/ToUaKiorC0tGTNmjVVTkdanb1797J8+XIKCgowMjKqdD9848aNvPjiizz33HMPrOv8+fO0a9eOpk2barZ16dKF2NjYGscjhBCiYdB5ks7NzWX//v3ExMTQunVrANq2bav1/73HY8aMISkpSdchVMnLyws7Ozvg7qCwbdu2oVarMTExISkpiUmTJmnimz17NnFxcTWu29nZGWdnZ27cuEFcXBzPPPOMpuznn3/mwIEDbN26tUZ1FRYWYmJiorXN1NSUgoKCGscjhBCiYdB5ks7MzASgXbt2lcqOHz9OZGQkly9fprS0lPLyciwsLHQdQpXuvxdsbGwMQEFBASYmJmRnZ2Nra6tV/ihxtWzZEk9PT9zd3UlMTKRp06YsXryYOXPmaLWM/0zTpk1Rq9Va2/Lz82nWrNlDxyOEEKJ+0/nAsXvJ7sqVK1rbS0tLCQoKYsSIESQmJnLkyBFmzpzJ/bOSGhrWzTg2S0tLzYcLgKKiompX2XqQ8vJyioqKyMnJIScnh0uXLrFgwQIcHR1xdHTk+vXrrFixggULFlR5fKdOnUhLS6OoqEiz7dy5c3Ts2PGR4hFCCFF/6TwrWlhY4OjoyIoVK8jMzERRFNLT00lLS6O0tBQzMzNUKhWXLl1i+/btWse2aNFCc//6SXJ1dWXLli1cvXqV4uJiwsPDqaioeOBxFRUVxMTEkJubC8D169cJCwvD1taW9u3bY2VlRWJiIlu3btX8s7S0ZMaMGQQFBVVZZ8+ePbGxsWHdunUUFxdz7tw5du7ciZeXl06vWQghhP6rlYFjixYtYv369fj5+ZGXl4eNjQ3BwcHMnTuX8PBwli5dSrdu3XB2diY+Pl5z3Pjx41m8eDGDBw+mVatWlZJ4bZk8eTK3b9/G19cXIyMjxo0bh42NTY2OPXbsGJGRkRQVFWFqakqvXr2IiIigUaO7L62VlZXW/oaGhpiZmfHUU08BcO3aNcaOHUt4eDg9e/bEyMiINWvWsGzZMhwdHTE1NWXixIk4OTlVOnd852RZqlIIIRowWQWrHpP1pIUQomGTyUyEEEIIPaXXC9h6e3tX+X1lc3Pzagd2BQcH4+LiorMYli1bRnJycpVlsbGxWFtb6+xcQgghxP2ku7sek+5uIYRo2KS7WwghhNBT0pKuxwxWltV1COIPlCC9voMkhKhnpCUthBBC6ClJ0kIIIYSeapBJOioqqtJKVA/Lw8ODhIQEHUUkhBBCPDy9uoHm5+enk6Uqp0yZoqOIaq6iooKpU6fy448/kpSUpJlp7MCBA2zcuJGcnBwA/v73vzNjxgx69epVbV3p6eksX76cH3/8ETMzM8aPH8+ECROeyHUIIYTQH3qVpOuzrVu3olKpKm3/xz/+QUREBC1btqSiooKvvvqKt956i+TkZExNTSvtX15eTkBAAH369GH16tVcvnyZmTNn0qpVK0aMGPEkLkUIIYSeqJUkXVhYyMaNGzl06BA3b97EysqK4OBgsrOziY6OJjMzE5VKhYODA4GBgRgbGxMWFkZqaipnzpxh8+bNWFpasnPnzkc6/4YNGzh9+jQREREAuLm54enpSUpKCmfPnsXGxob58+fTo0cPAMrKyggPDyc5ORlDQ0PGjRv3UOdLS0sjNjaWf/7zn/y///f/tMrun+xEURQMDQ0pLi7m+vXrVSbpU6dOkZWVhb+/PyqVii5duuDl5cWOHTskSQshxF9MrSTpJUuWkJOTQ0REBLa2tpqVrQoLCwkNDaVDhw5kZGQQGBjIpk2b8Pf3Z86cOVy8eFEn3d1ViY+PZ9WqVbRv3561a9cSEhLCrl27AIiOjubo0aNERUVhaWnJmjVrqpzprCoVFRW8//77zJ49u8qkC3cX0Xj11VcpLCykoqKCESNG8Mwzz1S57/nz52nXrp3W+tNdunQhNjb2Ia9YCCFEfafzJJ2bm8v+/fuJiYmhdevWALRt21br/3uPx4wZQ1JSkq5DqJKXlxd2dnbA3UFh27ZtQ61WY2JiQlJSEpMmTdLEN3v2bOLi4mpU77Zt22jRogVDhgzRWpP6ftbW1hw+fJiioiIOHDjAnTt3qq2vsLAQExMTrW2mpqYUFBTUKB4hhBANh86T9L1E1a5du0plx48fJzIyksuXL1NaWkp5eTkWFha6DqFKLVu21Dw2NjYGoKCgABMTE7Kzs7G1tdUqr0lc6enpfP7553z22Wc1isHY2Bg3NzfGjh2Lra0t/fv3r7RP06ZNUavVWtvy8/Np1qxZjc4hhBCi4dD5V7DuJbsrV65obS8tLSUoKIgRI0aQmJjIkSNHmDlzJvdPeGZoWDffCLO0tNRqBRcVFVW7gMf9UlNTuXnzJq+88gqOjo6aEdjjxo370+7p8vLySq/PPZ06dSItLY2ioiLNtnPnztGxY8eaXo4QQogGQudZ0cLCAkdHR1asWEFmZiaKopCenk5aWhqlpaWYmZmhUqm4dOkS27dv1zq2RYsWmvvXT5Krqytbtmzh6tWrFBcXEx4eTkVFxQOPGz58OLt372br1q1s3bqVDz/8EICPPvqIkSNHApCYmEh6ejoVFRUUFBTwySefcO3aNV588cUq6+zZsyc2NjasW7eO4uJizp07x86dO/Hy8tLdBQshhKgXamXg2KJFi1i/fj1+fn7k5eVhY2NDcHAwc+fOJTw8nKVLl9KtWzecnZ2Jj4/XHDd+/HgWL17M4MGDadWqVaUkXlsmT57M7du38fX1xcjIiHHjxmFjY/PA41QqldbXrsrLy4G7HzbuDfy6cuUK69ev59atW6hUKjp27MjatWv5+9//DtwdVDZ27FjCw8Pp2bMnRkZGrFmzhmXLluHo6IipqSkTJ07Eycmp0vnjOyfLKlhCCNGAyQIb9ZgsVSmEEA1bg5wWVAghhGgI9HrGMW9v7yq/r2xubl7twK7g4GBcXFx0FsOyZctITk6usiw2NlZrshIhhBBCl6S7ux6T7m4hhGjYpLtbCCGE0FPSkq7HDFaW1XUI4j5KkF7fPRJC1EPSkhZCCCH0lCRpIYQQQk81yCQdFRVFQEDAY9Xh4eFBQkKCjiISQgghHp5e3UTz8/PTyVKVU6ZM0VFED7Zu3Tr27dtHXl4eTZo0oWfPngQGBmq+mlXVV7iKioqYPXu2Zq7vP8rNzWX58uWcOHGCJk2aMGrUKPz9/etsbnMhhBB1Q/7qP6aRI0eydetWjhw5QkJCAtbW1gQHB2vKg4OD+eabbzT/PvjgA4yMjKqc5vOeBQsWALBnzx6io6M5fPhwjVfaEkII0XDUSku6sLCQjRs3cujQIW7evImVlRXBwcFkZ2cTHR1NZmYmKpUKBwcHAgMDMTY2JiwsjNTUVM6cOcPmzZuxtLRk586dj3T+DRs2cPr0aSIiIgBwc3PD09OTlJQUzp49i42NDfPnz6dHjx4AlJWVER4eTnJyMoaGhowbN67G52rfvr3msaIoGBoakpaWVu3+O3fuxMHBAUtLyyrLMzIyOHnyJLt378bExAQTExN8fHyIiorC19e3xnEJIYSo/2olSS9ZsoScnBwiIiKwtbXVrGxVWFhIaGgoHTp0ICMjg8DAQDZt2oS/vz9z5szh4sWLOunurkp8fDyrVq2iffv2rF27lpCQEHbt2gVAdHQ0R48eJSoqCktLS9asWVPlTGfV2bt3L8uXL6egoAAjI6Nq74ffuHGDI0eOaFbLqsqFCxcwMTGhTZs2mm1dunQhMzMTtVqNiYlJjeMSQghRv+k8Sefm5rJ//35iYmJo3bo1AG3bttX6/97jMWPGkJSUpOsQquTl5YWdnR1wd1DYtm3bNEkvKSmJSZMmaeKbPXs2cXFxNa7b2dkZZ2dnbty4QVxcHM8880yV+8XFxWFtbU3fvn2rraugoKBSIjY1Na22TAghRMOl8ySdmZkJQLt27SqVHT9+nMjISC5fvkxpaSnl5eVYWFjoOoQqtWzZUvPY2NgY+F/Sy87OxtbWVqv8UeJq2bIlnp6euLu7k5iYSPPmzTVlFRUV7N69m9GjR2NgYFBtHc2aNUOtVmtty8/P15QJIYT469D5wLF7ye7KlSta20tLSwkKCmLEiBEkJiZy5MgRZs6cyf0TntXV6GVLS0vNhwu4O/q6ugU8HqS8vJyioiJycnK0tn/77bfcuHEDd3f3Pz2+Y8eOqNVqzS0CgHPnzmFrayutaCGE+IvReVa0sLDA0dGRFStWkJmZiaIopKenk5aWRmlpKWZmZqhUKi5dusT27du1jm3RooVWcnpSXF1d2bJlC1evXqW4uJjw8HAqKioeeFxFRQUxMTHk5uYCcP36dcLCwrC1tdUaUAZ3B4wNGTIEc3PzP62zdevW9OnTh/DwcNRqNRkZGWzevBkvL69Hvj4hhBD1U60MHFu0aBHr16/Hz8+PvLw8bGxsCA4OZu7cuYSHh7N06VK6deuGs7Mz8fHxmuPGjx/P4sWLGTx4MK1ataqUxGvL5MmTuX37Nr6+vhgZGTFu3DhsbGxqdOyxY8eIjIykqKgIU1NTevXqRUREBI0a/e+lzc7O5tixY5rR5n9kb2+vtcRmaGgoy5cvx9XVlcaNGzNq1Ch8fHwqHRffOVlWwRJCiAZMFtiox2SpSiGEaNhkMhMhhBBCT+nVtKB/5O3tXeX3lc3Nzasd2HV/t7EuVDWt5z2xsbGa6T+FEEIIXZPu7npMuruFEKJhk+5uIYQQQk9JS7oeM1hZVtchNChKkF7f/RFC/AVJS1oIIYTQU5KkhRBCCD0lSVoIIYTQU5KkhRBCCD3VYJK0m5sbUVFRTJ8+HXt7e7y9vTl9+jQAISEhLFmypNL+e/bsAe5+lcnDw4PPP/8cV1dXHBwcWLt2Lbdu3eKdd95h0KBBjB49mtTU1MeKsaysjFWrVjF8+HCcnJzYvHkzHh4eJCQkaMURHR2Nk5MTw4cPZ82aNZSVyQAxIYT4K2owSRogPj6eoKAgDh8+TN++fQkJCanxsVlZWajVauLi4oiMjCQmJoZZs2bh4+PDwYMHGTp0KIsXL36s+D799FO+/fZbPv30U+Li4sjOzq40WUtWVhbXr18nLi6OTz/9lG+++YbPPvvssc4rhBCifmpQSdrLyws7OzuMjIzw8PAgPT290trM1VGpVEybNo3GjRvTqVMnOnbsyLPPPkv37t0xMjLCxcXloeqrSlJSEj4+PrRp0waVSsXMmTMrLc9paGjIW2+9hUqlok2bNvj4+JCYmPjI5xRCCFF/Nagk3bJlS81jY2NjAAoKCmp0rLm5uVbCVKlUtGjRQuv5w9RXlZycHK3VtVQqVaWlKy0sLDTnArCxseH69euPfE4hhBD1V4NK0tVp2rQpRUVFmudlZWWaNaCfJEtLS63u7eLi4kpzkOfm5lJcXKx5npWVhZWV1ROLUQghhP74SyTprl27kpKSQkZGBnfu3CEiIqJOBmO5urqyZcsWMjIyKCkpYd26dVRUVGjtU1FRQXh4OMXFxVy9epUtW7YwcuTIJx6rEEKIuveXmAfRxcWF1NRUJkyYgLGxMb6+vrRq1eqJxzF58mTy8vKYNGkSRkZGjBs3DktLSxo3bqzZx8bGhlatWuHu7k55eTkuLi5MmjTpiccqhBCi7snc3XWosLCQIUOGsHHjRnr06EFCQgKbNm1i9+7dNTpeVsESQoiG7S/R3a0v8vLy+PbbbykrK0OtVvPBBx9ga2vLs88+W9ehCSGE0EN/ie5uXfP29q70/Wa4O0L8jwPB7gkODqZ///58/PHHzJ07l0aNGtG1a1dWr15No0byYxBCCFGZdHfXY9LdLYQQDZt0dwshhBB6SpK0EEIIoaeku7seM1gpC2/oihIk4wKEEPpHWtJCCCGEnpIkLYQQQugpSdI65OfnR2RkZF2HIYQQooGQG3G1KCIigqNHj3Lp0iVeeOEFIiIi6jokIYQQ9Yi0pGtRmzZteOONN/D09KzrUIQQQtRDTyxJu7m5ERUVxfTp07G3t8fb25vTp08DEBISwpIlSyrtv2fPHuDupB0eHh58/vnnuLq64uDgwNq1a7l16xbvvPMOgwYNYvTo0aSmpj5yfBcvXqRfv35aM4YpioK7uzuJiYkA3Lp1i0WLFuHk5ISTkxPvvfceeXl51dY5atQoHBwceOqpp6os9/PzY/Xq1QQFBeHg4IC7uzsnT57kxIkTeHt7M2jQIIKCgh5rDWshhBD11xNtScfHxxMUFMThw4fp27cvISEhNT42KysLtVpNXFwckZGRxMTEMGvWLHx8fDh48CBDhw5l8eLFjxybnZ0dnTp1Ijk5WbPthx9+4NatWwwbNgyAhQsXkp+fT2xsLLGxsZqk/Tj27NmDr68vhw4dYsSIESxatIhdu3bxySefEB8fT1paGl988cVjnUMIIUT99ESTtJeXF3Z2dhgZGeHh4UF6ejpqtbpGx6pUKqZNm0bjxo3p1KkTHTt25Nlnn6V79+4YGRnh4uLyUPVVZdSoUSQkJGiex8fHM3z4cFQqFTk5OXz33XcEBARgZmaGmZkZAQEBHDt2jBs3bjzyOYcPH84//vEPzTXcuHGDiRMn0rx5c5o3b87AgQP55ZdfHrl+IYQQ9dcTTdItW7bUPDY2NgaocVeuubk5hob/C1elUtGiRQut5w9TX1WcnJy4cuUKv/76KwUFBRw8eJBRo0YBcP36dQBsbW01+7dp0waAa9euPfI5q7qG+18nlUpFYWHhI9cvhBCi/tKLgWNNmzalqKhI87ysrIzc3NwnHoepqSmDBg0iISGB/fv3Y21tzXPPPQeAlZUVgNbqVxkZGQBYW1s/8ViFEEI0fHqRpLt27UpKSgoZGRncuXOHiIgIysrqZsrLUaNGsXfvXnbt2qW1wpSlpSX9+vVjzZo15Ofnc/v2bdauXctLL72k1fK9X1lZGSUlJZSXl1NRUUFJSQl37tx5UpcihBCintOL70m7uLiQmprKhAkTMDY2xtfXl1atWtVJLH369EGlUvHrr7+yatUqrbIlS5awevVqRo8eDUDfvn15++23q60rNDRUMzIcYMCAAdjY2Gjd934c8Z2TZalKIYRowGSBjXpM1pMWQoiGTS+6u4UQQghRmV50d+uat7e31gCve8zNzbUmK7lfcHAwLi4utR2aEEIIUWMNMklv3769rkMQQgghHpt0dwshhBB6SgaO1WMGK+vma2r1gRLUIDuJhBB/MdKSFkIIIfSUJGkhhBBCT0mSFkIIIfSU3LjToYiICI4ePcqlS5d44YUXiIiI0Cr/97//TXJyMlevXuVvf/sbL7zwArNnz9bM/X306FH+/e9/c+HCBSoqKrCzs+PNN9+kZ8+edXE5Qggh6pi0pHWoTZs2vPHGG3h6elZZXlpayjvvvMOXX37Jrl27MDY2Zvbs2Zry/Px8XnnlFXbv3s3+/ftxdnZm1qxZj7XKlhBCiPrrgS1pNzc3PD09SUlJ4ezZs9jY2DB//nx69OhBSEgIRkZGLFy4UGv/6dOn4+rqSkJCAps2bWLs2LF8/vnnqNVqvLy88PX1ZenSpZw8eZKWLVuycOFCnn/++Ue+iJCQEMrLy2nUqBGHDh3C2NiYt956iw4dOrB06VIuX75M165dCQ0NxdLSEoBbt26xevVqTpw4AUC/fv0IDAykefPmmutwd3cnJSWFn3/+GVtbW0JDQ7l48SLr16/n5s2bDBs2jHnz5tGo0d2X8d6ylr/88gtpaWmV4pw8ebLm8d/+9jcmTZrEmDFjyMvLo3nz5pUmUxkzZgyffPIJP//8s6y0JYQQf0E1aknHx8cTFBTE4cOH6du3LyEhITU+QVZWFmq1mri4OCIjI4mJiWHWrFn4+Phw8OBBhg4dyuLFix81fo2DBw/i6OjIwYMHee2111i6dCnr16/ngw8+4Msvv8TAwIANGzZo9l+4cCH5+fnExsYSGxvLrVu3WLRokVadSUlJzJ07l0OHDtGpUyeCgoL44Ycf2LZtGzExMXz99dfs37//kWM+efIkVlZWmg8Gf/Tbb79x69YtnnnmmUc+hxBCiPqrRknay8sLOzs7jIyM8PDwID09HbVaXaMTqFQqpk2bRuPGjenUqRMdO3bk2WefpXv37hgZGeHi4vJQ9VWnd+/eDBw4EENDQ15++WWKiooYOXIkVlZWqFQqHB0d+fnnnwHIycnhu+++IyAgADMzM8zMzAgICODYsWPcuHFDU6enpycdOnSgUaNGODk5kZGRwYwZMzA2Nsba2ppevXpp6nxYp0+f5qOPPmLevHlVlufm5vLuu+8yYcIEnn766Uc6hxBCiPqtRkn6/vWSjY2NASgoKKjRCczNzTE0/N9pVCoVLVq00Hr+MPXVJMZ7df5xW2FhIQDXr18HwNbWVlPepk0bAK37v3883sjICHNz8yrrfBinTp0iICCA4OBgBg4cWKk8JyeHN954g759++Lv7//Q9QshhGgYHmvgWNOmTSkqKtI8LysrIzc397GDqm1WVlYAWotwZGRkANT6vd97LfgFCxbg7OxcqTwzM5OpU6fy0ksvMWfOHAwMDGo1HiGEEPrrsZJ0165dSUlJISMjgzt37hAREUFZmf5PVWlpaUm/fv1Ys2YN+fn53L59m7Vr1/LSSy9ptZ4fVllZGSUlJZSXl1NRUUFJSQl37tzRlH/11VfMnTuX0NBQhg4dWun4y5cvM3XqVJycnLRGfQshhPhreqzvSbu4uJCamsqECRMwNjbG19eXVq1a6Sq2WrVkyRJWr17N6NGjAejbty9vv/32Y9UZGhpKYmKi5vmAAQOwsbEhISEBgA8//JDi4uJK96FjY2OxtrZm8+bNZGdns23bNrZt26Ypl2U0hRDir0kW2KjHEhIScHNzq+swhBBC1BKZzEQIIYTQU3o1Lai3t7fWYK57zM3NuXnzZpXHSFewEEKIhkqvkvT27dvrOgQhhBBCb0h3txBCCKGnZOBYPWawUv+/7lZXlCC96iQSQohHIi1pIYQQQk9Jkq5FGzZsYMaMGXUdhhBCiHpKkrQQQgihpyRJCyGEEHqq3iZpNzc3oqKimD59Ovb29nh7e3P69GkAQkJCWLJkSaX99+zZA9ydqcvDw4PPP/8cV1dXHBwcWLt2Lbdu3eKdd95h0KBBjB49mtTUVJ3GfOPGDQICAhg0aBBeXl7s3r2b3r17k5mZqYl7wYIFLFy4kEGDBuHu7q6ZUlQIIcRfT71N0gDx8fEEBQVx+PBh+vbtS0hISI2PzcrKQq1WExcXR2RkJDExMcyaNQsfHx8OHjzI0KFDWbx4sU7jXbhwIY0aNSIpKYnIyEjNh4b77d+/n/79+/PVV18RHBzMihUrNB8+hBBC/LXU6yTt5eWFnZ0dRkZGeHh4kJ6ejlqtrtGxKpWKadOm0bhxYzp16kTHjh159tln6d69O0ZGRri4uDxUfQ9y/fp1UlJSeOuttzAxMcHCwoKpU6dW2q979+64urrSqFEj+vbty9ChQ7UW7RBCCPHXUa+T9P3LShobGwNQUFBQo2PNzc0xNPzf5atUKlq0aKH1/GHqe5CcnBxAe71qGxubSvv9cZuNjQ3Xr1/XSQxCCCHql3qdpKvTtGlTioqKNM/LysrIzc2tw4jurmENcO3aNc22+x/f88e5y7OysrCysqrd4IQQQuilBpmku3btSkpKChkZGdy5c4eIiAjKyup2di4rKyt69erFRx99REFBATdv3mTTpk2V9jtz5gx79+6lvLyclJQUDh48yMiRI+sgYiGEEHWtQSZpFxcXHBwcmDBhAh4eHlhbW9OqVau6DoulS5dSXFyMq6srr732GsOGDQOgSZMmmn2GDx/OsWPHGDp0KEuWLOHdd9/l+eefr6OIhRBC1CWZu7sOfffdd7z99tscO3YMAwMDQkJCMDIyYuHChTU6Xuburp7M3S2EaAjkL9kTdO7cOQwNDXnmmWfIyMjg448/Zvjw4RgYGDxSffGdk3Fzc9NxlEIIIfSFJOka8Pb2rjSgC+6OEL9582aVxwQHB+Pi4qK1LT8/n9DQUG7cuIGJiQkvvfQSAQEBtRKzEEKI+k+6u+uxhIQEaUkLIUQD1iAHjgkhhBANgSRpIYQQQk9Jd3c9JqO7K5NR3UKIhkRa0kIIIYSekiQthBBC6KkGmaSjoqIe+6tNHh4espazEEKIOqVXN/D8/Pzo06dPlUs4PowpU6boKKIHW7duHfv27SMvL48mTZrQs2dPAgMDNatdJSQk8P7772tW1QKwt7dn2bJl1daZnp7O8uXL+fHHHzEzM2P8+PFMmDCh1q9FCCGEftGrJF0fjRw5kkmTJmFiYkJxcTEREREEBwcTFRWl2ad169bs3r27RvWVl5cTEBBAnz59WL16NZcvX2bmzJm0atWKESNG1NJVCCGE0Ee1kqQLCwvZuHEjhw4d4ubNm1hZWREcHEx2djbR0dFkZmaiUqlwcHAgMDAQY2NjwsLCSE1N5cyZM2zevBlLS0t27tz5SOffsGEDp0+fJiIiAgA3Nzc8PT1JSUnh7Nmz2NjYMH/+fHr06AHcXcoyPDyc5ORkDA0NGTduXI3P1b59e81jRVEwNDQkLS3tkeIGOHXqFFlZWfj7+6NSqejSpQteXl7s2LFDkrQQQvzF1EqSXrJkCTk5OURERGBra8vVq1eBu8k7NDSUDh06kJGRQWBgIJs2bcLf3585c+Zw8eJFnXR3VyU+Pp5Vq1bRvn171q5dS0hICLt27QIgOjqao0ePEhUVhaWlJWvWrKlyGtDq7N27l+XLl1NQUICRkVGl++HXr1/HycmJRo0a8dxzz+Hv70/r1q2rrOv8+fO0a9eOpk2barZ16dKF2NjYR7hqIYQQ9ZnOB47l5uayf/9+5s2bR+vWrTEwMKBt27a0bduWAQMGYGdnh6GhIW3btmXMmDGcPHlS1yFUycvLCzs7O4yMjPDw8CA9PR21Wg1AUlISPj4+tG3bFpVKxezZsx9q0QtnZ2eOHDnC3r178fPz45lnntGU9ezZky+++ILk5GQ2b97M3/72N958802KioqqrKuwsBATExOtbaamphQUFDzCVQshhKjPdN6SzszMBKBdu3aVyo4fP05kZCSXL1+mtLSU8vJyLCwsdB1ClVq2bKl5bGxsDEBBQQEmJiZkZ2dja2urVf4ocbVs2RJPT0/c3d1JTEykefPmtGnTRqt8wYIFDBo0iDNnztCnT59KdTRt2lTz4eGe/Px8mjVr9tDxCCGEqN903pK+l+yuXLmitb20tJSgoCBGjBhBYmIiR44cYebMmdw/4ZmhYd18I8zS0lLz4QKgqKio2tWtHqS8vJyioiJycnKq3cfAwIDqJnrr1KkTaWlpWi3tc+fO0bFjx0eKRwghRP2l86xoYWGBo6MjK1asIDMzE0VRSE9PJy0tjdLSUszMzFCpVFy6dInt27drHduiRQvN/esnydXVlS1btnD16lWKi4sJDw+noqLigcdVVFQQExNDbm4ucPfec1hYGLa2tpoBZUePHuX69esoikJeXh5hYWE89dRTdO/evco6e/bsiY2NDevWraO4uJhz586xc+dOvLy8dHa9Qggh6odaabouWrSIzp074+fnh4ODA2+//TZqtZq5c+cSHh6Ovb09YWFhODs7ax03fvx4fv75ZwYPHoy3t3dthFalyZMn079/f3x9fXF3d8fKygobG5saHXvs2DFeeeUVBg4ciK+vLyqVioiICBo1unsn4YcffmDSpEnY29vj7e1NXl4e69at0wwMu3btGvb29pw6dQoAIyMj1qxZw8WLF3F0dOStt95i4sSJODk51c7FCyGE0FuywEY9JutJCyFEw9YgpwUVQgghGgK9nnHM29u7yu8rm5ubVzuwKzg4GBcXF53FsGzZMpKTk6ssi42N1Uz/KYQQQuiadHfXY9LdLYQQDZt0dwshhBB6SlrS9ZjByrK6DuGJUoL0+u6MEELonLSkhRBCCD0lSbqWbdiwgRkzZtR1GEIIIeohSdJCCCGEnpIkLYQQQuipep2k3dzciIqKYvr06ZppN0+fPg1ASEgIS5YsqbT/nj17gLtfX/Lw8ODzzz/H1dUVBwcH1q5dy61bt3jnnXcYNGgQo0ePJjU1Vacx37p1i0WLFuHk5ISTkxPvvfceeXl5WjF+8sknvPbaa9jb2zNx4kR++uknncYghBCifqjXSRogPj6eoKAgDh8+TN++fQkJCanxsVlZWajVauLi4oiMjCQmJoZZs2bh4+PDwYMHGTp0KIsXL9ZpvAsXLiQ/P5/Y2FhiY2M1Sft+O3bsICgoiIMHD2rm7/7j8pVCCCEavnqfpL28vLCzs8PIyAgPDw/S09NrnNBUKhXTpk2jcePGdOrUiY4dO/Lss8/SvXt3jIyMcHFxeaj6HiQnJ4fvvvuOgIAAzMzMMDMzIyAggGPHjnHjxg3Nfu7u7nTt2pXGjRszadIk/va3v3H06FGdxCCEEKL+qPdJumXLlprHxsbGABQUFNToWHNzc601rFUqFS1atNB6/jD1Pcj169eB/625DdCmTRvg7mpY99y/ApeBgQHW1taaY4UQQvx11PskXZ2mTZtSVFSkeV5WVqZZ97muWFlZAWjNR56RkQGgNQf4/eWKonDt2jXNsUIIIf46GmyS7tq1KykpKWRkZHDnzh0iIiIoK6vbGbosLS3p168fa9asIT8/n9u3b7N27VpeeuklrR6B+Ph4fv31V8rKyvjss88oLi5m4MCBdRi5EEKIutBg51l0cXEhNTWVCRMmYGxsjK+vL61atarrsFiyZAmrV69m9OjRAPTt25e3335bax9PT08++OADzp8/T7t27fjwww8xMTGpi3CFEELUIZm7W8+4ubkxffp0XF1dH7ivzN0thBANm/zVq8fiOyfLUpVCCNGASZKuIW9vb60BXfeYm5tz8+bNKo8JDg7GxcWltkMTQgjRQEl3dz2WkJAgLWkhhGjAGuzobiGEEKK+kyQthBBC6Cnp7q7HZHS3EEI0bNKSFkIIIfSUJGkhhBBCT0mSFkIIIfSU3OSrBTdu3MDb2xszMzN2796ttX3VqlWkpKRQXl5O586dCQwMpFOnTgAkJiayc+dO/u///g9DQ0OeffZZZs2axTPPPFNHVyKEEKIuSUu6FixdupQuXbpU2h4WFkZeXh47d+7kyy+/pGvXrgQEBHBv7F5hYSF+fn7s2bOH5ORkOnfuzJtvvklxcfGTvgQhhBB64IFJ2s3NjaioKKZPn469vT3e3t6cPn0agJCQEJYsWVJp/z179gB3J9vw8PDg888/x9XVFQcHB9auXcutW7d45513GDRoEKNHjyY1NfWxLiIkJISFCxeyePFiBg8ejIuLC3v37uXcuXP4+Pjg4ODA66+/Tk5OjuaYW7dusWjRIpycnHBycuK9994jLy9P6zoiIyN5/fXXsbe355VXXuHChQvs3bsXDw8PBg0axJIlSyqtrJWUlER5eXmVM42lp6czbNgwzMzMaNy4Me7u7ly/fl1zXm9vb/r164exsTFNmjRh6tSp/P7771y+fPmxXh8hhBD1U41a0vHx8QQFBXH48GH69u1LSEhIjU+QlZWFWq0mLi6OyMhIYmJimDVrFj4+Phw8eJChQ4eyePHiR41f4+DBgzg6OnLw4EFee+01li5dyvr16/nggw/48ssvMTAwYMOGDZr9Fy5cSH5+PrGxscTGxmqS9v2SkpKYO3cuhw4dolOnTgQFBfHDDz+wbds2YmJi+Prrr9m/f79m/xs3bvDxxx8THBxcZYwTJ07k4MGD3Lx5k5KSEnbt2sXzzz/PU089VeX+KSkpqFQq2rZt+9ivjxBCiPqnRknay8sLOzs7jIyM8PDwID09HbVaXaMTqFQqpk2bRuPGjenUqRMdO3bk2WefpXv37hgZGeHi4vJQ9VWnd+/eDBw4EENDQ15++WWKiooYOXIkVlZWqFQqHB0d+fnnnwHIycnhu+++IyAgADMzM8zMzAgICODYsWPcuHFDU6enpycdOnSgUaNGODk5kZGRwYwZMzA2Nsba2ppevXpp6gRYvnw5EydOxNrausoYe/ToQUVFBcOHD8fBwYFDhw6xYMGCKvdNS0tj8eLFzJ49m2bNmj3WayOEEKJ+qlGSbtmypeaxsbExAAUFBTU6gbm5OYaG/zuNSqWiRYsWWs8fpr6axHivzj9uKywsBOD69esA2NraasrbtGkDwLVr16qt08jICHNz8yrr3Lt3Lzdv3mTs2LFVxldRUcGbb77J008/zeHDhzl69ChTpkzRdGnf79KlS7zxxhtMmDCBMWPGPMSrIIQQoiF5rIFjTZs2paioSPO8rKyM3Nzcxw6qtllZWQForWqVkZEBUG0r+EGOHz/OhQsXGD58OI6OjnzwwQdkZmbi6OjI+fPnuX37NhkZGbzyyiuYmJjQuHFjPDw8UBSFM2fOaOr59ddfef3115k0aRKTJk16jKsUQghR3z1Wku7atSspKSlkZGRw584dIiIiKg2k0keWlpb069ePNWvWkJ+fz+3bt1m7di0vvfSSVuv5YQQGBvKf//yHrVu3snXrVl5//XWsra3ZunUrf//733nqqad4+umniY2NpaioiLKyMuLi4igoKNB8xSo1NZXp06czY8YMXn31VV1eshBCiHrosb4n7eLiQmpqKhMmTMDY2BhfX19atWqlq9hq1ZIlS1i9ejWjR48GoG/fvrz99tuPXN+9e9v3Pzc0NNS02gFWrVrFhx9+yMsvv0xZWRlt27ZlxYoVmq72jz/+GLVazerVq1m9erXmuPDwcHr27PnIsQkhhKifZIGNekzWkxZCiIZNJjMRQggh9JReTQvq7e2tNZjrHnNzc27evFnlMcHBwVVOHCKEEELUd3qVpLdv317XIQghhBB6Q7q7hRBCCD0lA8fqMYOV+v91N11RgvSq00cIIZ4IaUkLIYQQekqStBBCCKGnGmSSjoqKIiAg4LHq8PDwICEhQUcRCSGEEA9Pr270+fn50adPH6ZOnfpY9UyZMkVHET3YunXr2LdvH3l5eTRp0oSePXsSGBiomQP8wIEDbNy4UbOW9d///ndmzJhBr169qq0zPT2d5cuX8+OPP2JmZsb48eOZMGHCE7keIYQQ+qNBtqSfpJEjR7J161aOHDlCQkIC1tbWWutJ/+Mf/yAiIoJDhw7x1Vdf8eqrr/LWW2+Rn59fZX3l5eUEBATQvn17Dhw4wOrVq9m8eTNffvnlk7okIYQQeqJWWtKFhYVs3LiRQ4cOcfPmTaysrAgODiY7O5vo6GgyMzNRqVQ4ODgQGBiIsbExYWFhpKamcubMGTZv3oylpSU7d+58pPNv2LCB06dPExERAYCbmxuenp6kpKRw9uxZbGxsmD9/Pj169ADurt4VHh5OcnIyhoaGjBs3rsbnat++veaxoigYGhqSlpam2Xb/qlr3youLi7l+/TqmpqaV6jt16hRZWVn4+/ujUqno0qULXl5e7NixgxEjRjzsSyGEEKIeq5UkvWTJEnJycoiIiMDW1parV68Cd5N3aGgoHTp0ICMjg8DAQDZt2oS/vz9z5szh4sWLOunurkp8fDyrVq2iffv2rF27lpCQEHbt2gVAdHQ0R48eJSoqCktLS9asWVPlzGfV2bt3L8uXL6egoAAjI6NK98OvXbvGq6++SmFhIRUVFYwYMUKz8tUfnT9/nnbt2tG0aVPNti5duhAbG/sIVy2EEKI+03mSzs3NZf/+/cTExNC6dWsA2rZtq/X/vcdjxowhKSlJ1yFUycvLCzs7O+DuoLBt27ahVqsxMTEhKSmJSZMmaeKbPXs2cXFxNa7b2dkZZ2dnbty4QVxcXKUEbG1tzeHDhykqKuLAgQPcuXOn2roKCwsxMTHR2mZqakpBQUGN4xFCCNEw6DxJZ2ZmAtCuXbtKZcePHycyMpLLly9TWlpKeXk5FhYWug6hSvevE21sbAxAQUEBJiYmZGdnY2trq1X+KHG1bNkST09P3N3dSUxMpHnz5lrlxsbGuLm5MXbsWGxtbenfv3+lOpo2bYpardbalp+fT7NmzR46HiGEEPWbzgeO3Ut2V65c0dpeWlpKUFAQI0aMIDExkSNHjjBz5kzun/DM0LBuxrFZWlpqPlwAFBUVVbugx4OUl5dTVFSkGc1d3T5/fH3u6dSpE2lpaRQVFWm2nTt3jo4dOz5SPEIIIeovnWdFCwsLHB0dWbFiBZmZmSiKQnp6OmlpaZSWlmJmZoZKpeLSpUuVFtRo0aKF5v71k+Tq6sqWLVu4evUqxcXFhIeHU1FR8cDjKioqiImJITc3F4Dr168TFhaGra2tZkBZYmIi6enpVFRUUFBQwCeffMK1a9d48cUXq6yzZ8+e2NjYsG7dOoqLizl37hw7d+7Ey8tLZ9crhBCifqiVpuuiRYvo3Lkzfn5+ODg48Pbbb6NWq5k7dy7h4eHY29sTFhaGs7Oz1nHjx4/n559/ZvDgwXh7e9dGaFWaPHky/fv3x9fXF3d3d6ysrLCxsanRsceOHeOVV15h4MCB+Pr6olKpiIiIoFGju3cSrly5wvTp03FwcMDd3Z3//ve/rF27lr///e/A3UFl9vb2nDp1CgAjIyPWrFnDxYsXcXR05K233mLixIk4OTnVzsULIYTQW7LARj2WkJCAm5tbXYchhBCilshkJkIIIYSe0qtpQf/I29u7yu8rm5ubVzuwKzg4GBcXF53FsGzZMpKTk6ssi42N1ZqsRAghhNAl6e6ux6S7WwghGjbp7hZCCCH0lLSk6zGDlWV1HcITowTp9Z0ZIYSoFdKSFkIIIfSUJGkhhBBCT0mSFkIIIfRUg0zSUVFRlZaLfFgeHh4kJCToKCIhhBDi4enVaBw/Pz+drCc9ZcoUHUX0YPv27SM2NpYLFy5QXFzMiRMnKu3zn//8h61bt5KTk0Pbtm0JDAykd+/e1daZm5vL8uXLOXHiBE2aNGHUqFH4+/vX2QIkQggh6ob81X9MZmZmjBkzhsDAwCrLDxw4wPr161m+fDmHDx/Gy8uL2bNnc+3atWrrXLBgAQB79uwhOjqaw4cP89lnn9VK/EIIIfRXrbSkCwsL2bhxI4cOHeLmzZtYWVkRHBxMdnY20dHRZGZmolKpcHBwIDAwEGNjY8LCwkhNTeXMmTNs3rwZS0tLdu7c+Ujn37BhA6dPnyYiIgIANzc3PD09SUlJ4ezZs9jY2DB//nx69OgBQFlZGeHh4SQnJ2NoaMi4ceNqfK57a0J///33VZYfOHAAFxcXOnfuDMCYMWP47LPPSEhIYNq0aZX2z8jI4OTJk+zevRsTExNMTEzw8fEhKioKX1/fh3kZhBBC1HO1kqSXLFlCTk4OERER2NraapafLCwsJDQ0lA4dOpCRkUFgYCCbNm3C39+fOXPmcPHiRZ10d1clPj6eVatW0b59e9auXUtISAi7du0CIDo6mqNHjxIVFYWlpSVr1qypcjrSR1HV19AVReH8+fNV7n/hwgVMTExo06aNZluXLl3IzMxErVZjYmKik7iEEELoP513d+fm5rJ//37mzZtH69atMTAwoG3btrRt25YBAwZgZ2eHoaEhbdu2ZcyYMZw8eVLXIVTJy8sLOzs7jIyM8PDwID09HbVaDUBSUhI+Pj60bdsWlUrF7NmzMTAw0Ml57e3t2bNnDz///DNlZWXExMRw7do1CgoKqty/oKCgUiI2NTXVlAkhhPjr0HlLOjMzE4B27dpVKjt+/DiRkZFcvnyZ0tJSysvLsbCw0HUIVWrZsqXmsbGxMfC/hJidnY2tra1Wua7iGjlyJDdu3GDBggXk5eUxaNAg+vTpg5mZWZX7N2vWTPPh4Z78/HxNmRBCiL8Onbek7yW7K1euaG0vLS0lKCiIESNGkJiYyJEjR5g5c6ZWd3BdjV62tLTUfLgAKCoqqnaVrYdlYGCAr68vO3fu5KuvvmLevHlcunSJXr16Vbl/x44dUavVmlsEAOfOncPW1la6uoUQ4i9G51nRwsICR0dHVqxYQWZmJoqikJ6eTlpaGqWlpZiZmaFSqbh06RLbt2/XOrZFixZayelJcXV1ZcuWLVy9epXi4mLCw8OpqKio0bHl5eWUlJRQVnZ3Hu2SkhJKSko0Hz7UajX/93//h6Io3Lx5k+XLl2NiYsLLL79cZX2tW7emT58+hIeHo1arycjIYPPmzXh5eenmYoUQQtQbtTJwbNGiRaxfvx4/Pz/y8vKwsbEhODiYuXPnEh4eztKlS+nWrRvOzs7Ex8drjhs/fjyLFy9m8ODBtGrVqlISry2TJ0/m9u3b+Pr6YmRkxLhx47CxsanRsXv27GHx4sWa5wMGDADuDlSztbVFrVYzZ84csrKyaNy4MQMGDGD9+vWoVCrNMfb29lrrYIeGhrJ8+XJcXV1p3Lgxo0aNwsfHp9K54zsny1KVQgjRgMkqWPWYrCcthBANm0xmIoQQQugpvZoW9I+8vb2r/L6yubl5tQO77u821oVly5aRnJxcZVlsbCzW1tY6O5cQQghxP+nurseku1sIIRo26e4WQggh9JS0pOsxg5VldR3CE6ME6fWdGSGEqBXSkhZCCCH0lCRpIYQQQk9JktYhPz8/IiMj6zoMIYQQDYQk6Vr2xRdf4OXlxcCBAxk5ciRxcXF1HZIQQoh6Qkbj1KLIyEj27NlDaGgoXbp04fbt29y6dauuwxJCCFFPPLGWtJubG1FRUUyfPh17e3u8vb05ffo0ACEhISxZsqTS/nv27AHufh/Yw8ODzz//HFdXVxwcHFi7di23bt3inXfeYdCgQYwePZrU1NRHju/ixYv069dPa5IURVFwd3cnMTERgFu3brFo0SKcnJxwcnLivffeIy8vr8r68vPz+fTTTwkKCqJbt24YGhry1FNP0b59e61rjIyM5PXXX8fe3p5XXnmFCxcusHfvXjw8PBg0aBBLlizRLN4hhBDir+WJdnfHx8cTFBTE4cOH6du3LyEhITU+NisrC7VaTVxcHJGRkcTExDBr1ix8fHw4ePAgQ4cO1Vro4mHZ2dnRqVMnrdnFfvjhB27dusWwYcMAWLhwIfn5+cTGxhIbG6tJ2lU5c+YMJSUlnD9/nlGjRuHk5ERwcDC///671n5JSUnMnTuXQ4cO0alTJ4KCgvjhhx/Ytm0bMTExfP311+zfv/+Rr0sIIUT99USTtJeXF3Z2dhgZGeHh4UF6ejpqtbpGx6pUKqZNm0bjxo3p1KkTHTt25Nlnn6V79+4YGRnh4uLyUPVVZdSoUSQkJGiex8fHM3z4cFQqFTk5OXz33XcEBARgZmaGmZkZAQEBHDt2jBs3blSq61639vHjx4mOjuY///kPJSUlLFy4UGs/T09POnToQKNGjXByciIjI4MZM2ZgbGyMtbU1vXr14ueff37kaxJCCFF/PdEk3bJlS81jY2NjAAoKCmp0rLm5OYaG/wtXpVLRokULrecPU19VnJycuHLlCr/++isFBQUcPHiQUaNGAXD9+nUAbG1tNfu3adMGgGvXrlWqq2nTpsDdZTAtLCwwNTXFz8+PlJQUioqKNPvd/5qoVCqMjIwwNzfX2lZYWPjI1ySEEKL+0ovR3U2bNtVKXGVlZeTm5j7xOExNTRk0aBAJCQns378fa2trnnvuOQCsrKwAtBb8yMjIAKhykY3OnTsDYGBgUNthCyGEaKD0Ikl37dqVlJQUMjIyuHPnDhEREXU2WGrUqFHs3buXXbt2aS1eYWlpSb9+/VizZg35+fncvn2btWvX8tJLL2m1hu+xsbFhwIABREdHk5eXR0FBAZGRkfTv31/TiyCEEEL8Gb34CpaLiwupqalMmDABY2NjfH19adWqVZ3E0qdPH1QqFb/++iurVq3SKluyZAmrV69m9OjRAPTt25e333672rref/99/vnPfzJq1Cj+9re/0bdvX4KDg3UWa3znZFkFSwghGjBZYKMek6UqhRCiYdOL7m4hhBBCVKYX3d265u3trTXA6x5zc3OtyUruFxwcjIuLS22HJoQQQtRYg0zS27dvr+sQhBBCiMcm3d1CCCGEnpKBY/WYwcq/xpzeSlCD7PARQogHkpa0EEIIoackSQshhBB6qkEm6aioKAICAh6rDg8PD63FNoQQQognTa9u9vn5+dGnTx+mTp36WPVMmTJFRxE9WHh4OEePHuX69esYGxszcOBAZs6cSfPmzQE4f/48H330EefOneP3338nMjKS559//k/rzM3NZfny5Zw4cYImTZowatQo/P39tRYYEUII0fDJX/3HZGRkxPvvv89XX33Ftm3byM7O1lonu3HjxgwZMoQ1a9bUuM4FCxYAsGfPHqKjozl8+DCfffaZrkMXQgih52qlJV1YWMjGjRs5dOgQN2/exMrKiuDgYLKzs4mOjiYzMxOVSoWDgwOBgYEYGxsTFhZGamoqZ86cYfPmzVhaWrJz585HOv+GDRs4ffo0ERERALi5ueHp6UlKSgpnz57FxsaG+fPn06NHD+Duqlvh4eEkJydjaGjIuHHjanyuN998U/PY3NycV199lXnz5mm2dejQgQ4dOtS4voyMDE6ePMnu3bsxMTHBxMQEHx8foqKi8PX1rXE9Qggh6r9aSdJLliwhJyeHiIgIbG1tuXr1KnA3eYeGhtKhQwcyMjIIDAxk06ZN+Pv7M2fOHC5evKiT7u6qxMfHs2rVKtq3b8/atWsJCQlh165dAERHR3P06FGioqKwtLRkzZo1Vc5YVhMpKSl07NjxkeO8cOECJiYmmrWqAbp06UJmZiZqtRoTE5NHrlsIIUT9ovPu7tzcXPbv38+8efNo3bo1BgYGtG3blrZt2zJgwADs7OwwNDSkbdu2jBkzhpMnT+o6hCp5eXlhZ2eHkZERHh4epKeno1arAUhKSsLHx4e2bduiUqmYPXv2I60D/dVXX7Fjxw6CgoIeOc6CgoJKidjU1FRTJoQQ4q9D5y3pzMxMANq1a1ep7Pjx40RGRnL58mVKS0spLy/HwsJC1yFU6f41n++t53wvIWZnZ2Nra6tV/rBxHThwgGXLlrF69Wq6dOnyyHE2a9ZM8+Hhnvz8fE2ZEEKIvw6dt6TvJbsrV65obS8tLSUoKIgRI0aQmJjIkSNHmDlzJvdPeFZXo5ctLS01Hy4AioqKql2Ioyrx8fGaBN27d+/HiqVjx46o1WrNLQKAc+fOYWtrK13dQgjxF6PzrGhhYYGjoyMrVqwgMzMTRVFIT08nLS2N0tJSzMzMUKlUXLp0qdJCGC1atNBKTk+Kq6srW7Zs4erVqxQXFxMeHk5FRUWNjv3iiy/48MMP+de//lXlV6sURaGkpISSkhLg7oeVkpISysvLq6yvdevW9OnTh/DwcNRqNRkZGWzevBkvL69Hvj4hhBD1U60MHFu0aBHr16/Hz8+PvLw8bGxsCA4OZu7cuYSHh7N06VK6deuGs7Mz8fHxmuPGjx/P4sWLGTx4MK1atXpiq1lNnjyZ27dv4+vri5GREePGjcPGxqZGx65cuRIjIyPeeOMNre3ffPMNAFlZWYwaNUqzffr06QC89957uLm5AWBvb6+1VGZoaCjLly/H1dWVxo0bM2rUKHx8fCqdO75zsqYOIYQQDY8ssFGPJSQkSJIWQogGTCYzEUIIIfSUXk0L+kfe3t5Vfl/Z3Ny82oFd93cb68KyZctITk6usiw2NhZra2udnUsIIYS4n3R312PS3S2EEA2bdHcLIYQQekpa0vWYwcqyug6hVilBen03Rgghap20pIUQQgg9JUlaCCGE0FMNMklHRUUREBDwWHV4eHiQkJCgo4iEEEKIh6dXN/38/Px0slTllClTdBTRg4WHh3P06FGuX7+OsbExAwcOZObMmTRv3hyA8vJy1q1bx759+8jPz8fGxoZp06YxbNiwautMT09n+fLl/Pjjj5iZmTF+/HgmTJjwpC5JCCGEnmiQLeknycjIiPfff5+vvvqKbdu2kZ2dTUhIiKY8NjaWPXv2sG7dOo4cOcL06dNZsGABly9frrK+8vJyAgICaN++PQcOHGD16tVs3ryZL7/88slckBBCCL1RKy3pwsJCNm7cyKFDh7h58yZWVlYEBweTnZ1NdHQ0mZmZqFQqHBwcCAwMxNjYmLCwMFJTUzlz5gybN2/G0tKSnTt3PtL5N2zYwOnTp4mIiADAzc0NT09PUlJSOHv2LDY2NsyfP58ePXoAUFZWRnh4OMnJyRgaGjJu3Lgan+vNN9/UPDY3N+fVV19l3rx5mm3p6en06tWL9u3bAzB48GCaN2/Ob7/9ptl2v1OnTpGVlYW/vz8qlYouXbrg5eXFjh07GDFixCO8GkIIIeqrWmlJL1myhLNnzxIREcGRI0dYvXo1LVu2xMTEhNDQUA4dOkRkZCSpqals2rQJgDlz5vD888/z2muv8c033zxygq5OfHw8QUFBHD58mL59+2q1dqOjozl69ChRUVHExcWRlZVV5UxnNZGSkkLHjh01zz09Pbl48SKXLl2ivLycAwcOUF5ezgsvvFDl8efPn6ddu3Y0bdpUs61Lly5cuHDhkeIRQghRf+m8JZ2bm8v+/fuJiYmhdevWALRt21br/3uPx4wZQ1JSkq5DqJKXlxd2dnbA3UFh27ZtQ61WY2JiQlJSEpMmTdLEN3v2bOLi4h76HF999RU7duxg48aNmm2tW7emZ8+evPLKKxgaGtK4cWPef/99LCwsqqyjsLCw0rrRpqamFBQUPHQ8Qggh6jedJ+nMzEwA2rVrV6ns+PHjREZGcvnyZUpLSykvL682Welay5YtNY+NjY0BKCgowMTEhOzsbGxtbbXKHzauAwcOsGzZMlavXk2XLl0021esWEF6ejrx8fFYWVlx5swZgoKCaNq0Kf369atUT9OmTVGr1Vrb8vPzadas2UPFI4QQov7TeXf3vWR35coVre2lpaUEBQUxYsQIEhMTOXLkCDNnzuT+Cc8MDetmHJulpaXmwwVAUVFRtQt4VCU+Pl6ToHv37q1V9ssvv+Dq6oqNjQ2Ghob06NGD559/nmPHjlVZV6dOnUhLS6OoqEiz7dy5c1pd6EIIIf4adJ4VLSwscHR0ZMWKFWRmZqIoCunp6aSlpVFaWoqZmRkqlYpLly6xfft2rWNbtGjB1atXdR3SA7m6urJlyxauXr1KcXEx4eHhVFRU1OjYL774gg8//JB//etfPP/885XKe/ToQXJyMtnZ2QCcPXuW//73v1qt7fv17NkTGxsb1q1bR3FxMefOnWPnzp14eXk98vUJIYSon2pldPeiRYtYv349fn5+5OXlYWNjQ3BwMHPnziU8PJylS5fSrVs3nJ2diY+P1xw3fvx4Fi9ezODBg2nVqlWlJF5bJk+ezO3bt/H19cXIyIhx48ZhY2NTo2NXrlyJkZERb7zxhtb2b775BoC33nqL8PBwJk2aREFBARYWFvy///f/GDlyJADXrl1j7NixhIeH07NnT4yMjFizZg3Lli3D0dERU1NTJk6ciJOTU6Vzx3dOllWwhBCiAZMFNuoxWapSCCEaNpnMRAghhNBTejUt6B95e3tX+X1lc3Pzagd2BQcH4+LiorMYli1bRnJycpVlsbGxWFtb6+xcQgghxP2ku7sek+5uIYRo2KS7WwghhNBT0pKuxwxWltV1CLVGCdLrOzFCCPFESEtaCCGE0FOSpIUQQgg91SCTdFRUFAEBAY9Vh4eHBwkJCTqKSAghhHh4enXjz8/Pjz59+jB16tTHqmfKlCk6iujB1q1bx759+8jLy6NJkyb07NmTwMBAzVezzp8/z0cffcS5c+f4/fffiYyMrHL60Pvl5uayfPlyTpw4QZMmTRg1ahT+/v51Nre5EEKIuiF/9R/TyJEj2bp1K0eOHCEhIQFra2uCg4M15Y0bN2bIkCGsWbOmxnUuWLAAgD179hAdHc3hw4f57LPPdB67EEII/VYrLenCwkI2btzIoUOHuHnzJlZWVgQHB5OdnU10dDSZmZmoVCocHBwIDAzE2NiYsLAwUlNTOXPmDJs3b8bS0pKdO3c+0vk3bNjA6dOniYiIAMDNzQ1PT09SUlI4e/YsNjY2zJ8/nx49egBQVlZGeHg4ycnJGBoaMm7cuBqfq3379prHiqJgaGhIWlqaZluHDh3o0KFDjevLyMjg5MmT7N69GxMTE0xMTPDx8SEqKgpfX98a1yOEEKL+q5UkvWTJEnJycoiIiMDW1lazslVhYSGhoaF06NCBjIwMAgMD2bRpE/7+/syZM4eLFy/qpLu7KvHx8axatYr27duzdu1aQkJC2LVrFwDR0dEcPXqUqKgoLC0tWbNmTZUznVVn7969LF++nIKCAoyMjB7rfviFCxcwMTGhTZs2mm1dunQhMzMTtVqNiYnJI9cthBCiftF5ks7NzWX//v3ExMTQunVrANq2bav1/73HY8aMISkpSdchVMnLyws7Ozvg7qCwbdu2aZJeUlISkyZN0sQ3e/Zs4uLialy3s7Mzzs7O3Lhxg7i4OJ555plHjrOgoKBSIjY1Na22TAghRMOl8ySdmZkJQLt27SqVHT9+nMjISC5fvkxpaSnl5eVYWFjoOoQqtWzZUvPY2NgY+F/Sy87OxtbWVqv8UeJq2bIlnp6euLu7k5iYSPPmzR+6jmbNmqFWq7W25efna8qEEEL8deh84Ni9ZHflyhWt7aWlpQQFBTFixAgSExM5cuQIM2fO5P4Jz+pq9LKlpaXmwwVAUVFRtQt4PEh5eTlFRUXk5OQ80vEdO3ZErVZrbhEAnDt3DltbW2lFCyHEX4zOs6KFhQWOjo6sWLGCzMxMFEUhPT2dtLQ0SktLMTMzQ6VScenSJbZv3651bIsWLbSS05Pi6urKli1buHr1KsXFxYSHh1NRUfHA4yoqKoiJiSE3NxeA69evExYWhq2trWZAmaIolJSUUFJSAtz9sFJSUkJ5eXmVdbZu3Zo+ffoQHh6OWq0mIyODzZs34+XlpZuLFUIIUW/UysCxRYsWsX79evz8/MjLy8PGxobg4GDmzp1LeHg4S5cupVu3bjg7OxMfH685bvz48SxevJjBgwfTqlWrSkm8tkyePJnbt2/j6+uLkZER48aNw8bGpkbHHjt2jMjISIqKijA1NaVXr15ERETQqNHdlzYrK4tRo0Zp9p8+fToA7733nmYFK3t7e60lNkNDQ1m+fDmurq40btyYUaNG4ePjU+nc8Z2TZRUsIYRowGSBjXpMlqoUQoiGTSYzEUIIIfSUXk0L+kfe3t5Vfl/Z3Ny82oFd93cb68KyZctITk6usiw2NlYz/acQQgiha9LdXY9Jd7cQQjRs0t0thBBC6ClpSddjBivL6joEnVOC9PoOjBBCPFHSkhZCCCH0lCRpIYQQQk81yCQdFRX1WCtRwd1FOBISEnQUkRBCCPHw9OoGoJ+fn06WqpwyZYqOInqwffv2ERsby4ULFyguLubEiROV9tmyZQv/+c9/uHnzJi1atGD8+PGMHTu22jpzc3NZvnw5J06coEmTJowaNQp/f/86m9tcCCFE3dCrJF0fmZmZMWbMGEpKSli2bFml8iNHjrBhwwY+/vhjunfvzo8//siMGTNo27Yt/fr1q7LOBQsW0KxZM/bs2cOtW7eYNWsWZmZm+Pr61vLVCCGE0Ce1kqQLCwvZuHEjhw4d4ubNm1hZWREcHEx2djbR0dFkZmaiUqlwcHAgMDAQY2NjwsLCSE1N5cyZM2zevBlLS0t27tz5SOffsGEDp0+fJiIiAgA3Nzc8PT1JSUnh7Nmz2NjYMH/+fHr06AFAWVkZ4eHhJCcnY2hoyLhx42p8rv79+wPw/fffV1menp5Op06d6N69OwDPPfccHTt25MKFC1Um6YyMDE6ePMnu3bsxMTHBxMQEHx8foqKiJEkLIcRfTK30ny5ZsoSzZ88SERHBkSNHWL16NS1btsTExITQ0FAOHTpEZGQkqampbNq0CYA5c+bw/PPP89prr/HNN988coKuTnx8PEFBQRw+fJi+ffsSEhKiKYuOjubo0aNERUURFxdHVlZWlTOdPQonJyfUajWpqalUVFRw6tQprly5oknuf3ThwgVMTExo06aNZluXLl3IzMystM60EEKIhk3nLenc3Fz2799PTEwMrVu3BqBt27Za/997PGbMGJKSknQdQpW8vLyws7MD7g4K27ZtG2q1GhMTE5KSkpg0aZImvtmzZxMXF6eT85qbm+Po6Mgbb7yhWTs7MDCQZ555psr9CwoKKq0bbWpqWm2ZEEKIhkvnSTozMxOAdu3aVSo7fvw4kZGRXL58mdLSUsrLy7GwsNB1CFVq2bKl5rGxsTHwv6SXnZ2Nra2tVrmu4tq0aRP79u1j69atdOjQgUuXLhEYGMjf/vY3PDw8Ku3frFmzSi3m/Px8TZkQQoi/Dp13d99LdleuXNHaXlpaSlBQECNGjCAxMZEjR44wc+ZM7p/wrK5GL1taWmo+XAAUFRVVu4DHw/rll18YPHgwf//73zEwMMDOzo7BgwfzzTffVLl/x44dUavVXL16VbPt3Llz2NraSitaCCH+YnSeFS0sLHB0dGTFihVkZmaiKArp6emkpaVRWlqKmZkZKpWKS5cusX37dq1jW7RooZWcnhRXV1e2bNnC1atXKS4uJjw8nIqKihodW15eTklJCWVld6foLCkpoaSkRPPho0ePHhw+fFjzoeX//u//OHz4MF27dq2yvtatW9OnTx/Cw8NRq9VkZGSwefNmvLy8dHClQggh6pNaabouWrSIzp074+fnh4ODA2+//TZqtZq5c+cSHh6Ovb09YWFhODs7ax03fvx4fv75ZwYPHoy3t3dthFalyZMn079/f3x9fXF3d8fKygobG5saHbtnzx4GDBiAv78/5eXlDBgwgAEDBmgGnk2cOJEhQ4bw5ptvYm9vj7+/P4MHD9YaqW1vb6+1HGZoaCiKouDq6oqPjw+DBg3Cx8dHp9cshBBC/8kCG/WYLFUphBANm0xhJYQQQugpvZ5xzNvbu8rvK5ubm1c7sCs4OBgXFxedxbBs2TKtruj7xcbGYm1trbNzCSGEEPeT7u56TLq7hRCiYZPubiGEEEJPSZIWQggh9JQkaSGEEEJPSZIWQggh9JQkaSGEEEJPSZIWQggh9JRef09aVK+srIzc3Nw6metcCCF0zdramkaNJCX9kXxPup66evUqjo6OdR2GEELoxFdffUWbNm3qOgy9I0m6niorK+PatWt1HYYQQuiEtKSrJklaCCGE0FMycEwIIYTQU5KkhRBCCD0lSVrPpaWlMXnyZLy8vJg8eTJXrlyptE95eTlhYWG4u7vj4eHB7t27n3ygtaAm1378+HEmTpxI//79Wbt27ZMPspbU5NojIyPx9vbm1VdfZcKECXz33Xd1EKlu1eS64+PjefXVVxk/fjyvvPIKX3zxRR1Eqns1ufZ7Ll++zIABAxrUe15UQxF67fXXX1eSkpIURVGUpKQk5fXXX6+0T0JCgvLmm28q5eXlSm5uruLi4qJkZGQ86VB1ribXfuXKFeXXX39V1q1bp6xZs+YJR1h7anLt3377rVJUVKQoiqKcO3dOGTRokOZ5fVWT687Pz1cqKioURVEUtVqtjBw5Ujl//vwTjbM21OTaFUVRysrKlGnTpinBwcEN6j0vqiYtaT2Wm5vLr7/+ipOTEwBOTk78+uuvldbS3r9/Px4eHhgaGmJubs6gQYM4cOBAXYSsMzW99rZt29K5c2eMjIzqIsxaUdNr79+/PyqVCoCOHTuiKAp5eXlPPF5dqel1m5iYYGBgAEBxcTFlZWWa5/VVTa8dIDo6Gnt7e55++uknHaaoA5Kk9dj169dp1aqVJgEZGRlhaWnJ9evXtfa7du0aNjY2mufW1taV9qlvanrtDdGjXHtSUhJt2rTBysrqSYWpcw9z3UeOHMHb2xs3NzcmTpzIM88886TD1amaXvv58+c5fvw448ePr4swRR2QJC1EPffDDz/w8ccfs3Tp0roO5YkZNGgQ27dvZ+fOnezZs4fLly/XdUi1rqysjKVLlzJv3rwG1XMk/px8c1yPWVlZkZ2dTXl5OUZGRpSXl5OTk1OptWRtbU1WVhbPPvssULllXR/V9Nobooe59h9//JFFixaxatUq2rdv/+SD1aFH+ZlbW1vz7LPPcvTo0Xp9/TW59hs3bnD16lXeeustAPLz81EUhYKCAubPn19XoYtaJi1pPWZhYUGnTp3Yt28fAPv27aNz586Ym5tr7Tds2DB2795NRUUFN2/e5MiRI/V+ytCaXntDVNNr/+mnn5g3bx5hYWF06dKlLkLVqZpe9//93/9pHt+6dYvvv/++3nd31+Tara2t+eqrr0hISCAhIYFx48bh6ekpCbqBkxnH9Nzly5d57733yM/Px9TUlMWLF9O+fXtmzZrFG2+8Qbdu3SgvL+ef//wnx48fB2DSpEl4eXnVceSPrybXnpqaSnBwMAUFBSiKgomJCQsXLqR///51Hf5jqcm1+/j4kJmZSatWrTTHvf/++/U6YdXkuletWsWJEydo1KgRiqLg7u7Oq6++WtehP7aaXPv9NmzYQFFREbNnz66bgMUTIUlaCCGE0FPS3S2EEELoKUnSQgghhJ6SJC2EEELoKUnSQgghhJ6SJC2EEELoKUnSteSbb77RmrrvxIkTDB06tA4jenLmzp2r0+9uXr16lc6dO2ue5+bmMmTIEHJzcx947LZt23jnnXd0Fkt98P3339O7d++6DuMvKS4u7qF+z3X9uyL+XG39bjzsz33lypU1XsFMknQtUBSF5cuXM3PmzD/db+vWrbz88su88MILvPjii3h5ebFnzx5N+dChQ4mLi6t0XFXbFUXBycmJF154gYKCAq2yEydO0LlzZ3r27EnPnj0ZOHAg8+bN49atW49+kXXIwsKCl19+mXXr1v3pfoWFhYSHhz/w59DQ9O7dm++//76uw6jWv/71L3x9fes6jL+E2nqtJ06cSEREhM7rrW1//N2oq/fitGnT2Lp1a43WIpAkXQuOHj1KaWkp/fr1q3afxMRE1q1bx9KlS/nhhx/45ptvCA4OxszM7JHOefz4cdLT0zE0NCQpKalSuZGREadOneLUqVNs27aNU6dOsWzZskc6lz4YPXo0O3fuRK1WV7tPfHw8nTp1qrPVgsrLy6moqKiTcwsh9Ffz5s2xt7ev0Vro9T5JDx06lIiICCZOnEjPnj1xc3Pj119/JTExkeHDh9OrVy/mz59PWVmZ5pjMzExmzZrFgAEDGDhwIAsXLtT6Y7969WocHR3p2bMnw4YNIzo6WlN2r+t19+7duLq60rNnT6ZMmUJ2drZmnwMHDtC/f/8/XT7v1KlT9O7dmx49emBgYIBKpaJ3794MHDjwkV6HmJgY7O3tcXd3f+APvm3btgwZMoRffvmlUllZWRkDBw6stNTl3LlzmTdvHgDfffcdY8eO5cUXX6Rfv34EBATw+++/V3u+zp07a316PXHihNbsSWVlZaxfvx4nJyd69+7Nq6++ypkzZ/70Gtq3b4+5uTnffvtttfscOHCAAQMGaG3bvHkzzs7O9OzZk8GDB7Nq1SrKy8sBCAsLY8aMGVr7nzhxgp49e1JYWAjcXYXotddeo1+/fprjS0tLgf+9N2JjY3F1daVHjx78/vvvJCUlMWrUKF544QUGDhzIokWLNPUB5OTk8MYbb9CrVy+cnJyIjY2lc+fOXL16VbPP9u3befnll+nVqxceHh4cPXq02uv+4+s7d+5c3nnnHebNm0fv3r2xt7cnMTGRX375hdGjR9OzZ08mTpyo9al+6NChfPTRR4wbN46ePXvi5eXFjz/+qCl/0HugtLRU8zO993u0d+9e9uzZw4YNGzh58qSmZyc9Pb3K6zh58iRjx46lV69eODs7a72v713jnj17GDZsGL169eKtt9760w9tj/K34tdff8XHx4cXX3wRR0dHIiIiNO8XuDt3upeXFz179mTcuHGVrqWoqIiwsDCGDh1Knz59eO2110hLS6s2xj+6efMm7777LgMGDGDAgAHMmTNHqwfsj71q996D165dq/a13rlzJ8OHD2fjxo0MHDiQ/v37s2LFikrv42vXrmnqvXcM3J3V7vvvvyciIoKePXtqltf8o3/9619MmjSJDz74gH79+tG3b18+/fRTMjIy8PHx0byvLl68qDnmcX9X7r3XFyxYoHmvV/W+AR74+tzvj7cldPFzHzBgQM2WFK6rhax1ZciQIcrw4cOV3377Tblz547y9ttvK46OjsqCBQuUgoICJSMjQ+nXr58SFxenKIqiFBcXK8OGDVPWrl2rFBUVKbdu3VKmTp2qzJ07V1Pn7t27lWvXrikVFRXKt99+q3Tv3l35+uuvFUVRlPT0dKVTp06Kn5+f8vvvvyv5+fnKK6+8osyfP19z/JgxY5TNmzdrxXn8+HFlyJAhmud79uxR/vGPfyirV69Wvv32WyUvL6/Ka9u9e/cDt//+++/Ks88+q+zbt0/56aeflE6dOilnzpzROnfXrl01zy9fvqyMGDFC65rvFxYWpkyfPl3zXK1WK88//7ySkpKiKIqipKSkKKdPn1ZKS0uV7OxsZfz48UpAQIBm/zlz5ijBwcGa5506ddIcW1U8q1evVsaMGaNcuXJFKSsrU7Zv36706dNHuXXrlqIo/3vN/+j1119XVq9eXeU1KIqi9O/fXzlw4IDWtr179ypXrlxRKioqlJ9++knp37+/sm3bNkVRFOXChQvKs88+q/z++++a/d99911l3rx5iqIoyo0bN5Q+ffoo27ZtU0pKSpRr164pnp6eyr/+9S+tOH18fJTs7GylpKREKSsrUw4fPqycP39eKS8vVy5fvqy4uLgoK1eu1JzDx8dH8ff3V/Lz85UbN24oEyZMUDp16qSkp6criqIoMTExyrBhw5RffvlFKS8vVw4fPqw8//zzyuXLl6u87j++vnPmzFG6d++uHDp0SCkvL1e2bt2qPP/888rrr7+uZGVlKYWFhcrEiRO13sNDhgxRBgwYoJw5c0YpKSlRNmzYoPTt21fJz89XFOXB74F//vOfiouLi/LLL78oFRUVSlZWlvLLL78oiqIo4eHhyqRJk6r9uSmKoly5ckXp3r27smPHDqW0tFQ5deqU8uKLLyp79uzRXGOnTp2UefPmKWq1WsnJyVGGDx+uREREVFvnw/6tuH37ttK/f3/lo48+UkpKSpTffvtNGTp0qPLJJ59oyvv06aNs2LBBKSkpUU6fPq289NJLWr/ngYGBip+fn5KTk6OUlJQoH374oeLk5KTcuXNH87O5/3flj6ZMmaK8/vrryq1bt5Rbt24p06ZNU6ZNm6Z1Tff/Lbj3HszKyqr2td6xY4fSrVs3JSQkRCkqKlLS0tKUESNGKB9//HGVddw7ZtiwYZrnEyZMUNatW1dt3PfO3a1bN2X79u2a34MuXbookyZN0voZ+Pr6ao553N+Ve+/1AwcOKOXl5cq+ffuUbt26KVevXlUUpfLvRnWvz/3Xeq/eez8nXfzcFUVRzpw5o3Tu3FkpKSn509ex3rekAby9vbGzs6Nx48a4ubmRnp5OQEAATZs2xdbWlj59+nD27FkADh06hKIovPXWW6hUKpo3b85bb71FQkKC5hOyu7s7VlZWGBgY0L9/fwYPHsx3332ndc4333wTCwsLTExMcHNz09QPcPv2bUxMTP40ZhcXF8LDw7l48SJvv/02ffv2ZeLEiZw/f15rv/fee4/evXtr/cvMzNTaZ8eOHZiamjJkyBC6detGt27d2L59u9Y+5eXl9O7dmxdffJHJkyfTt29fTcv4j0aPHs3XX3+taRklJyfTqlUrzYCL3r1789xzz9GoUSMsLS2ZOnVqpdenphRF4bPPPuPdd9+lbdu2GBkZMXbsWFq1asXhw4f/9NhmzZqRl5dXbXlVPwcnJyfatm2LgYEB3bp1w93dXRP7M888Q9euXYmPjwdArVazb98+Ro8eDcDu3bvp3Lkzr776Kk2aNMHKyorXX3+90vgAf39/LC0tadKkCUZGRgwaNIiOHTtiaGhIu3btGD9+vOac165d4/jx47z77ruYmJjQokWLSq35zz77jDfffJMuXbpgaGjIoEGD6Nu3b5W3Napzr+VvaGiIh4cHhYWFuLu7Y21tjbGxMU5OTlrvYYAxY8bwj3/8gyZNmjBt2jRUKhWHDh0C/vw9oCgKW7du5d1336VLly4YGBhgbW39UIuAJCUl0a1bN7y8vGjUqBHPP/88r7zyCv/5z3+09gsKCqJZs2a0bNkSR0fHStfwRw/zt+Lw4cM0btyYGTNm0KRJE+zs7Jg2bRqxsbHA3b8lxsbGTJs2jSZNmvDcc88xZswYzblyc3NJTEzkvffeo2XLljRp0gR/f39ycnI4ffr0A1+D69evc/ToUebOnUvz5s1p3rw5c+fO5ciRI1o9d4/CwMCAd999F5VKxdNPP83UqVPZtWvXY9VZlfbt2zN27FjN78FTTz3FwIEDtX4G9//MHvd3Be6+1x0dHTE0NGTEiBGYmppW2Wv4qHT1czcxMUFRFPLz8//0fA1iqUpLS0vNY5VKhZGRERYWFpptxsbGmsFUV69eJSsrq9IIPwMDA27cuIGVlRWfffYZsbGxXLt2DUVRKC4uxs3NTWv/+xc1uL9+ADMzsz/tdrtnyJAhDBkyBICLFy+yePFi3njjDb766itNV/nixYtxd3fXOu7+UYSKohAbG8uoUaNo3LgxcPeP68qVKzVvZrh7T7qmg4ns7Ozo1q0b8fHxTJ48mZ07d2ot2HH27FnWrFnDr7/+SlFREYqiaHVJPYybN29SWFjIG2+8oXV7oKys7IGDKgoKCmjTpk215VX9HBITE/n000+5evUqZWVllJaW0qNHD025l5cX27Ztw9fXl+TkZKysrOjVqxdw973z3//+V+u9oyhKpfvOrVu31np+7Ngx1q1bx6VLl7hz5w4VFRWa9+e9a7x/aVFbW1ut469evcrixYsJDQ3VbCsvL3+oZTvv/x0xNjauctsfBxzefx0GBgbY2NhoukD/7D2Qm5tLYWHhYy0dmZWVVeln+/TTT/PVV19pnv/x97xp06aVruGPHuZvRVZWFra2tlrvy6efflrzGly7dq1S+f0x3+uCHTVqlFYMZWVlWl3J1bm3z/113htfce3aNa2/QQ+rRYsWmvcB3P1Z1ySmh3X/6w13X98//gzu/5k97u9KVeesyfviYejq565WqzEwMMDU1PRPz9cgkvTDsLW1pX379tW2Qn744QdWrlxJdHQ0PXr0wMjIiFmzZqE8xDokXbt25bfffnuouOzs7PD19WX69Onk5eXx1FNP1ei448ePk5aWxo4dO0hMTATuvhkKCwtJTEx85NWBvLy8+Pzzzxk6dCinT59mzZo1mrLAwECcnJz48MMPMTEx4dChQ7zxxhvV1tW0aVOKioo0z+9vBZibm9O0aVM+/fRTnnvuuYeK8fz583h6elZb3rVrVy5evKhZtjMrK4t33nmHf/3rXzg4ONCkSRPCwsK0PsmPHDmS5cuX89NPP7Fr1y5NKxruvndeeuklNm7c+KdxGRr+r4Pqzp07vPnmm7zzzjuMHj0alUrFv//9b6KiogA0iTYrK4u2bdsCVOopsbW1ZebMmbi4uNTkZdGZjIwMzWNFUcjKysLa2hr48/eAhYUFxsbGpKWlVZmo/2ysxj02NjYcOXJEa1t6evoTXSfdxsaGzMxMFEXRxJyenq55DaysrCqV3z+O4N6HnC+//FLrg0BN3TtPRkYG7dq105z//rJmzZpV+7sF1b/Wv//+O0VFRZpEnZGRoVUnoPXBu6b1Pg5d/K48rKqu44+vKdy9/nvvPV393C9cuEDHjh1p0qTJn8bYILq7H8aQIUM0g1rUajWKonD9+nX2798P3P10c+/TtYGBAYcPH+brr79+qHMMGzZMs2xkdf7zn/+QnJys+a7vtWvX+OKLL3jmmWdqnKABvvjiC1588UWSk5PZvXs3u3fvJjExES8vr0pd3g9j5MiRXLlyhdDQUF566SWtVptarcbU1JRmzZqRmZn5wKT1j3/8g927d3Pnzh2uXr3Kp59+qikzMDDAx8eHf/7zn1y+fBm420L+5ptv/rQlnZaWxs2bN3nppZeq3WfYsGFaA8sKCws1n8wbN25Mampqpa5qMzMzhg8fztq1azl9+jQeHh6aMg8PD86ePct//vMfSkpKqKioID09/U/fH6Wlpdy5cwczMzNUKhW//fYb//73vzXl1tbW9OnTh5UrV6JWq8nNzeXjjz/WqsPX15ePPvqIX375RdOz8/3332sNuKkNO3bs4KeffqK0tJTIyEiKiooYPHgw8OfvAQMDA8aNG8cHH3zA+fPnURSFa9eu8euvvwJ3WzpZWVncuXOn2nOPHDmSn376id27d1NWVsaPP/5ITEyM1oem2jZ48GDu3LnD+vXruXPnDpcuXeKTTz7RdG0OGTKEwsJCIiMjKS0t5aeffmLHjh2a41u0aMHLL79MSEiI5r18+/Zt9u/fX6OWnZWVFQMHDmTFihXcvn2bvLw8wsLCcHBw0LSin332WZKSkigoKCA3N7fS16Kqe60VRWHlypUUFxeTnp7Opk2bNO91c3NzWrduzY4dOygvL+fcuXOV/pZYWlpy5cqVh3tBH0AXvysPq6rXp2vXrvz+++8cOnSIiooK9u/fT0pKiqZcVz/3Y8eOaRoQf+Yvl6SNjY3ZvHkzv/32Gy4uLvTq1YtJkyZp7lncGyE9duxY+vXrx759+xg2bNhDncPe3h4jIyNOnDhR7T7Nmzdn27ZtuLq68vzzzzN27FhMTU1Zv359jc/z+++/89VXXzFlyhQsLS21/k2bNo2ff/75gaOkq2NqasqwYcP4+uuvK/1hfP/994mNjeWFF17A398fZ2fnP61r4cKFpKWl0bdvX2bPnl1preuZM2fi6OjIjBkzeOGFF3BycuKLL774096LHTt24Onp+addRe7u7vz666+a1oednR0zZ85kxowZ9O7dm40bNzJy5MhKx3l5efH1118zcOBArS5FS0tLPvvsMw4cOMDQoUN58cUXefPNN6sdnQx3P5WHhITwwQcf0LNnTxYvXszLL7+stc+qVasoLi5m0KBBjBs3TvN63vuE7e3tzdSpU5k3bx4vvvgigwcP5uOPP9YahVwbXnnlFUJDQ+nTpw/Jycls3LhR83o/6D0QEBCAs7Mzb775Ji+88AITJ07U/FF3dnbG2tqagQMH0rt37ypfv7Zt27Jx40b+/e9/07dvX9555x1mzZqFq6trrV7z/UxNTYmKiuLbb79lwIABTJ06FQ8PDyZPngzc/UC3YcMGkpOT6dOnD6GhoZV6rkJDQ+nQoYPWiPK9e/fWuCX6wQcf0KxZM5ydnXFxccHU1JSwsDBN+ezZszE0NGTgwIFMnDix0vu5utfa1tYWKysrHB0dGTt2LPb29kydOlVz3IoVKzh8+DC9e/dmxYoVWvdc4e6a9WfPnqV3795V/g49Cl38rjysql6fp59+mvnz57Nw4UL69OnDN998w4gRIzTH6OLnfvv2bb7++mvGjRv34CD/dFiZeGRHjhxRxo8fr3n+x9Hdoub+OLr7999/VwYPHqw1Crs6W7duVYKCgmozPJ37+uuvlX/84x9KRUVFncVQ3TcLRP1X1ejl+kofflcexcqVK//0myn3+8vdk35SHBwccHBwqOswGiQLCwvNKOMHGTduXM0+rdahX375BQMDA833PdeuXYurq2ut3PcToj5rKL8rb7/9do33lST9hLRu3RofH5+6DqNeMjMzw9/fv67DqDV5eXksXLiQnJwcTExMcHBwYO7cuXUdlhB656/4u2KgKA8xbFkIIYQQT8xfbuCYEEIIUV9IkhZCCCH0lCRpIYQQQk9JkhZCCCH0lCRpIYQQQk/9f1pCMkAm4fDEAAAAAElFTkSuQmCC\n"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate performance of XGB models:\nr2_xgb1 = r2_score(y_test, xgb1.predict(X_test))\nr2_xgbgs = r2_score(y_test, xgbgs.predict(X_test))\nr2_xgbo = r2_score(y_test, optuna_xgb.predict(X_test))\n\nprint('Min_prd: ', min_prd)\nprint('Constant guess: ', mean_absolute_error(y_test, np.ones(len(y_test))*y_test.mean()), \n      r2_score(y_test, np.ones(len(y_test))*y_test.mean()))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_xgb1)\nprint('XGB GS test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_xgbgs)\nprint('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_xgbo)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:41:06.869568Z","iopub.execute_input":"2022-09-06T19:41:06.870028Z","iopub.status.idle":"2022-09-06T19:41:07.010243Z","shell.execute_reply.started":"2022-09-06T19:41:06.869967Z","shell.execute_reply":"2022-09-06T19:41:07.009120Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"Min_prd:  650\nConstant guess:  7.208431720537165 0.0\nXGB test: 6.663347837413472 0.12923565664287784\nXGB GS test: 6.526015408868233 0.17990277498412266\nOptuna XGB test: 6.610174256287323 0.1524427584789505\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total time for a script: ', time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.131169Z","iopub.status.idle":"2022-09-06T00:36:53.131976Z","shell.execute_reply.started":"2022-09-06T00:36:53.131718Z","shell.execute_reply":"2022-09-06T00:36:53.131742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.iloc[:,1:].mean()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.133368Z","iopub.status.idle":"2022-09-06T00:36:53.134132Z","shell.execute_reply.started":"2022-09-06T00:36:53.133849Z","shell.execute_reply":"2022-09-06T00:36:53.133875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3yr window, trials=20, cv_reg=0.03: 0.88%. runs 1 hr.\n# 3yr, t=40, cv_reg=0.04: 0.96%.\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.135461Z","iopub.status.idle":"2022-09-06T00:36:53.136216Z","shell.execute_reply.started":"2022-09-06T00:36:53.135933Z","shell.execute_reply":"2022-09-06T00:36:53.135958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(X_train, X_val, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:45:09.378763Z","iopub.execute_input":"2022-09-06T00:45:09.379158Z","iopub.status.idle":"2022-09-06T00:45:09.707560Z","shell.execute_reply.started":"2022-09-06T00:45:09.379127Z","shell.execute_reply":"2022-09-06T00:45:09.706650Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n0         2.440252     0.030032 -1.497142  1.080837  0.724789  0.010338   \n1         1.943299    -0.222745 -1.497142  1.080837  0.724789  0.010338   \n2         2.107274    -0.246470 -1.497142  1.080837  0.724789  0.010338   \n3         1.685620    -0.374008 -1.497142  1.080837  0.724789  0.010338   \n4         0.690976    -0.960773 -1.116726  0.846425  0.659979 -0.425881   \n...            ...          ...       ...       ...       ...       ...   \n76883    -0.566013    -0.667712  0.154976 -0.449011  0.225954 -0.367814   \n76884    -0.611810    -0.723772  0.154976 -0.449011  0.225954 -0.367814   \n76885    -0.605239    -0.659530  0.154976 -0.449011  0.225954 -0.367814   \n76886    -0.636869     0.194059  0.154976 -0.449011  0.225954 -0.367814   \n76887    -0.612396     1.516430  0.154976 -0.449011  0.225954 -0.367814   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n0       -0.001164    -0.769063   0.061164       -0.521796      -0.613313   \n1       -0.101503    -0.679058   0.111973       -0.476189      -0.779485   \n2       -0.589553    -0.665144   0.154523       -0.408752      -0.524974   \n3       -0.618918    -0.822612   0.211327       -0.498429      -0.506168   \n4       -1.749070    -0.921894   0.244523        0.492161       0.500216   \n...           ...          ...        ...             ...            ...   \n76883    0.612983     0.285701   1.814529        0.666135       0.386534   \n76884   -0.496413    -0.259853   1.815901       -0.143222      -0.002847   \n76885   -0.174600    -0.146213   1.836054       -0.038214       0.183720   \n76886   -0.157636    -0.246570   1.827610       -0.591596      -0.602846   \n76887   -0.217149    -0.008820   1.811085       -0.820075      -0.854234   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n0          0.588274 -0.300375   -0.346009   -0.753732    -0.858156   \n1          0.796765  0.336568    0.151111   -0.611420    -0.768969   \n2          0.808655 -0.458075   -0.365669   -0.517451    -0.719170   \n3          0.804064 -0.440318   -0.506501   -0.577175    -0.709441   \n4          0.802548 -0.766582    0.249959   -0.365264    -0.586161   \n...             ...       ...         ...         ...          ...   \n76883     -1.700196  0.520511    0.372631   -0.104971    -0.283154   \n76884     -1.605932 -0.449028   -0.208101   -0.059078    -0.341420   \n76885     -1.478466 -0.567504   -0.317410   -0.098314    -0.326505   \n76886     -1.377626 -0.812586   -0.835152   -0.279218    -0.371848   \n76887     -1.399491 -1.068270   -1.062484   -0.517437    -0.444693   \n\n       num__BAspr  num__size  num__lbm  num__lop  num__lgp  num__linv  \\\n0       -0.246214  -0.287970 -2.285884  0.420012  0.545504  -1.453573   \n1       -0.067952  -0.289231 -2.285884  0.420012  0.545504  -1.453573   \n2       -0.391055  -0.327156 -2.285884  0.420012  0.545504  -1.453573   \n3        0.015571  -0.361888 -2.285884  0.420012  0.545504  -1.453573   \n4       -0.355706  -0.554908 -1.350873  1.033453  0.734261  -0.196466   \n...           ...        ...       ...       ...       ...        ...   \n76883    0.578188  -1.096146 -0.204261 -0.003571  0.471093  -0.542937   \n76884    1.810455  -1.123628 -0.204261 -0.003571  0.471093  -0.542937   \n76885    4.784910  -1.130472 -0.204261 -0.003571  0.471093  -0.542937   \n76886    0.264961  -1.136254 -0.204261 -0.003571  0.471093  -0.542937   \n76887   -0.264628  -1.145775 -0.204261 -0.003571  0.471093  -0.542937   \n\n       num__llme  num__l1amhd  num__l1MAX  num__l1BAspr  num__l3amhd  \\\n0      -0.079910     0.006677   -0.437054     -0.263244    -0.017419   \n1      -0.103939     0.066074   -0.314577     -0.248255    -0.011362   \n2      -0.115354     0.117049    0.323205     -0.070163     0.018450   \n3      -0.092495     0.159737   -0.472485     -0.392958     0.078257   \n4      -0.107086     0.216727   -0.454704      0.013279     0.129583   \n...          ...          ...         ...           ...          ...   \n76883  -1.281176     1.768226   -0.382531      4.403380     1.779250   \n76884  -1.115603     1.825157    0.507389      0.575359     1.787660   \n76885  -1.172635     1.826533   -0.463426      1.806448     1.792151   \n76886  -1.154434     1.846752   -0.582058      4.778059     1.849475   \n76887  -1.218224     1.838280   -0.827462      0.262431     1.850861   \n\n       num__l3MAX  num__l3BAspr  num__l6amhd  num__l6MAX  num__l6BAspr  \\\n0       -0.263946     -0.307586    -0.080405    0.615517     -0.196028   \n1       -1.055385     -0.403408    -0.066179   -0.733851     -0.412131   \n2       -0.444897     -0.264606    -0.061915   -0.784494     -0.392271   \n3       -0.321323     -0.249523     0.002885   -0.268268     -0.303054   \n4        0.322169     -0.070310     0.009010   -1.065697     -0.400682   \n...           ...           ...          ...         ...           ...   \n76883    0.919108      0.244168     1.925952   -0.366884      2.330492   \n76884    1.129089     -0.080817     1.849914   -0.779174      0.290555   \n76885   -0.389885      4.431401     1.829081   -0.505598      0.306861   \n76886    0.508003      0.579277     1.819658    0.923738      0.259099   \n76887   -0.471504      1.818117     1.828163    1.135308     -0.072012   \n\n       num__l12amhd  num__l12MAX  num__l12BAspr  num__l12mom122  \\\n0         -0.055520    -0.437054      -0.300325        1.723461   \n1         -0.085070    -0.314577      -0.258891        0.909845   \n2         -0.064350     0.323205      -0.377451        0.554145   \n3         -0.075501    -0.472485      -0.344694        0.702872   \n4         -0.079914    -0.454704      -0.286153        0.816731   \n...             ...          ...            ...             ...   \n76883      1.981934    -0.382531       2.973730       -1.310315   \n76884      1.990553     0.507389       0.907904       -1.211626   \n76885      1.990080    -0.463426       1.463785       -0.708503   \n76886      1.995408    -0.582058       1.706591       -0.915727   \n76887      2.011370    -0.827462       0.550428        0.462225   \n\n       num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n0              -0.612851         -0.587156        -0.248782      -0.897198   \n1              -0.555438         -0.378270        -0.329453      -0.907155   \n2              -0.862790         -0.815012        -0.193126      -0.992504   \n3              -0.546726         -0.572587         0.058540      -0.900193   \n4              -0.796444         -0.772408         0.374460      -0.883001   \n...                  ...               ...              ...            ...   \n76883           0.503558          0.788956        -1.184905       0.752818   \n76884           0.495542          0.588720        -1.014403       0.106444   \n76885          -0.311360         -0.419508        -0.842612       0.006417   \n76886           0.107819          0.304902        -0.620406      -0.061611   \n76887           0.356451          0.487998        -0.286047      -0.059314   \n\n       num__l12vol12m  num__amhd_miss  num__BAspr_miss  cat__ind_1.0  \\\n0           -0.999512       -0.178017        -0.078007           0.0   \n1           -0.987610       -0.178017        -0.078007           0.0   \n2           -0.974490       -0.178017        -0.078007           0.0   \n3           -0.950343       -0.178017        -0.078007           0.0   \n4           -0.930604       -0.178017        -0.078007           0.0   \n...               ...             ...              ...           ...   \n76883        1.244635       -0.178017        -0.078007           0.0   \n76884        1.229502       -0.178017        -0.078007           0.0   \n76885        1.229408       -0.178017        -0.078007           0.0   \n76886        1.250421       -0.178017        -0.078007           0.0   \n76887        1.007059       -0.178017        -0.078007           0.0   \n\n       cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  \\\n0               0.0           0.0           0.0           0.0           0.0   \n1               0.0           0.0           0.0           0.0           0.0   \n2               0.0           0.0           0.0           0.0           0.0   \n3               0.0           0.0           0.0           0.0           0.0   \n4               0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n76883           0.0           0.0           0.0           0.0           0.0   \n76884           0.0           0.0           0.0           0.0           0.0   \n76885           0.0           0.0           0.0           0.0           0.0   \n76886           0.0           0.0           0.0           0.0           0.0   \n76887           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n0               0.0           0.0           0.0            0.0            0.0   \n1               0.0           0.0           0.0            0.0            0.0   \n2               0.0           0.0           0.0            0.0            0.0   \n3               0.0           0.0           0.0            0.0            0.0   \n4               0.0           0.0           0.0            0.0            0.0   \n...             ...           ...           ...            ...            ...   \n76883           0.0           0.0           0.0            0.0            0.0   \n76884           0.0           0.0           0.0            0.0            0.0   \n76885           0.0           0.0           0.0            0.0            0.0   \n76886           0.0           0.0           0.0            0.0            0.0   \n76887           0.0           0.0           0.0            0.0            0.0   \n\n       cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n0                0.0            0.0            0.0            1.0   \n1                0.0            0.0            0.0            1.0   \n2                0.0            0.0            0.0            1.0   \n3                0.0            0.0            0.0            1.0   \n4                0.0            0.0            0.0            1.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            1.0            0.0   \n76884            0.0            0.0            1.0            0.0   \n76885            0.0            0.0            1.0            0.0   \n76886            0.0            0.0            1.0            0.0   \n76887            0.0            0.0            1.0            0.0   \n\n       cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  cat__ind_47.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_48.0  cat__ind_49.0  \n0                0.0            0.0  \n1                0.0            0.0  \n2                0.0            0.0  \n3                0.0            0.0  \n4                0.0            0.0  \n...              ...            ...  \n76883            0.0            0.0  \n76884            0.0            0.0  \n76885            0.0            0.0  \n76886            0.0            0.0  \n76887            0.0            0.0  \n\n[69441 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.440252</td>\n      <td>0.030032</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.001164</td>\n      <td>-0.769063</td>\n      <td>0.061164</td>\n      <td>-0.521796</td>\n      <td>-0.613313</td>\n      <td>0.588274</td>\n      <td>-0.300375</td>\n      <td>-0.346009</td>\n      <td>-0.753732</td>\n      <td>-0.858156</td>\n      <td>-0.246214</td>\n      <td>-0.287970</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.079910</td>\n      <td>0.006677</td>\n      <td>-0.437054</td>\n      <td>-0.263244</td>\n      <td>-0.017419</td>\n      <td>-0.263946</td>\n      <td>-0.307586</td>\n      <td>-0.080405</td>\n      <td>0.615517</td>\n      <td>-0.196028</td>\n      <td>-0.055520</td>\n      <td>-0.437054</td>\n      <td>-0.300325</td>\n      <td>1.723461</td>\n      <td>-0.612851</td>\n      <td>-0.587156</td>\n      <td>-0.248782</td>\n      <td>-0.897198</td>\n      <td>-0.999512</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.943299</td>\n      <td>-0.222745</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.101503</td>\n      <td>-0.679058</td>\n      <td>0.111973</td>\n      <td>-0.476189</td>\n      <td>-0.779485</td>\n      <td>0.796765</td>\n      <td>0.336568</td>\n      <td>0.151111</td>\n      <td>-0.611420</td>\n      <td>-0.768969</td>\n      <td>-0.067952</td>\n      <td>-0.289231</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.103939</td>\n      <td>0.066074</td>\n      <td>-0.314577</td>\n      <td>-0.248255</td>\n      <td>-0.011362</td>\n      <td>-1.055385</td>\n      <td>-0.403408</td>\n      <td>-0.066179</td>\n      <td>-0.733851</td>\n      <td>-0.412131</td>\n      <td>-0.085070</td>\n      <td>-0.314577</td>\n      <td>-0.258891</td>\n      <td>0.909845</td>\n      <td>-0.555438</td>\n      <td>-0.378270</td>\n      <td>-0.329453</td>\n      <td>-0.907155</td>\n      <td>-0.987610</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.107274</td>\n      <td>-0.246470</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.589553</td>\n      <td>-0.665144</td>\n      <td>0.154523</td>\n      <td>-0.408752</td>\n      <td>-0.524974</td>\n      <td>0.808655</td>\n      <td>-0.458075</td>\n      <td>-0.365669</td>\n      <td>-0.517451</td>\n      <td>-0.719170</td>\n      <td>-0.391055</td>\n      <td>-0.327156</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.115354</td>\n      <td>0.117049</td>\n      <td>0.323205</td>\n      <td>-0.070163</td>\n      <td>0.018450</td>\n      <td>-0.444897</td>\n      <td>-0.264606</td>\n      <td>-0.061915</td>\n      <td>-0.784494</td>\n      <td>-0.392271</td>\n      <td>-0.064350</td>\n      <td>0.323205</td>\n      <td>-0.377451</td>\n      <td>0.554145</td>\n      <td>-0.862790</td>\n      <td>-0.815012</td>\n      <td>-0.193126</td>\n      <td>-0.992504</td>\n      <td>-0.974490</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.685620</td>\n      <td>-0.374008</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.618918</td>\n      <td>-0.822612</td>\n      <td>0.211327</td>\n      <td>-0.498429</td>\n      <td>-0.506168</td>\n      <td>0.804064</td>\n      <td>-0.440318</td>\n      <td>-0.506501</td>\n      <td>-0.577175</td>\n      <td>-0.709441</td>\n      <td>0.015571</td>\n      <td>-0.361888</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.092495</td>\n      <td>0.159737</td>\n      <td>-0.472485</td>\n      <td>-0.392958</td>\n      <td>0.078257</td>\n      <td>-0.321323</td>\n      <td>-0.249523</td>\n      <td>0.002885</td>\n      <td>-0.268268</td>\n      <td>-0.303054</td>\n      <td>-0.075501</td>\n      <td>-0.472485</td>\n      <td>-0.344694</td>\n      <td>0.702872</td>\n      <td>-0.546726</td>\n      <td>-0.572587</td>\n      <td>0.058540</td>\n      <td>-0.900193</td>\n      <td>-0.950343</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.690976</td>\n      <td>-0.960773</td>\n      <td>-1.116726</td>\n      <td>0.846425</td>\n      <td>0.659979</td>\n      <td>-0.425881</td>\n      <td>-1.749070</td>\n      <td>-0.921894</td>\n      <td>0.244523</td>\n      <td>0.492161</td>\n      <td>0.500216</td>\n      <td>0.802548</td>\n      <td>-0.766582</td>\n      <td>0.249959</td>\n      <td>-0.365264</td>\n      <td>-0.586161</td>\n      <td>-0.355706</td>\n      <td>-0.554908</td>\n      <td>-1.350873</td>\n      <td>1.033453</td>\n      <td>0.734261</td>\n      <td>-0.196466</td>\n      <td>-0.107086</td>\n      <td>0.216727</td>\n      <td>-0.454704</td>\n      <td>0.013279</td>\n      <td>0.129583</td>\n      <td>0.322169</td>\n      <td>-0.070310</td>\n      <td>0.009010</td>\n      <td>-1.065697</td>\n      <td>-0.400682</td>\n      <td>-0.079914</td>\n      <td>-0.454704</td>\n      <td>-0.286153</td>\n      <td>0.816731</td>\n      <td>-0.796444</td>\n      <td>-0.772408</td>\n      <td>0.374460</td>\n      <td>-0.883001</td>\n      <td>-0.930604</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76883</th>\n      <td>-0.566013</td>\n      <td>-0.667712</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>0.612983</td>\n      <td>0.285701</td>\n      <td>1.814529</td>\n      <td>0.666135</td>\n      <td>0.386534</td>\n      <td>-1.700196</td>\n      <td>0.520511</td>\n      <td>0.372631</td>\n      <td>-0.104971</td>\n      <td>-0.283154</td>\n      <td>0.578188</td>\n      <td>-1.096146</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.281176</td>\n      <td>1.768226</td>\n      <td>-0.382531</td>\n      <td>4.403380</td>\n      <td>1.779250</td>\n      <td>0.919108</td>\n      <td>0.244168</td>\n      <td>1.925952</td>\n      <td>-0.366884</td>\n      <td>2.330492</td>\n      <td>1.981934</td>\n      <td>-0.382531</td>\n      <td>2.973730</td>\n      <td>-1.310315</td>\n      <td>0.503558</td>\n      <td>0.788956</td>\n      <td>-1.184905</td>\n      <td>0.752818</td>\n      <td>1.244635</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76884</th>\n      <td>-0.611810</td>\n      <td>-0.723772</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.496413</td>\n      <td>-0.259853</td>\n      <td>1.815901</td>\n      <td>-0.143222</td>\n      <td>-0.002847</td>\n      <td>-1.605932</td>\n      <td>-0.449028</td>\n      <td>-0.208101</td>\n      <td>-0.059078</td>\n      <td>-0.341420</td>\n      <td>1.810455</td>\n      <td>-1.123628</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.115603</td>\n      <td>1.825157</td>\n      <td>0.507389</td>\n      <td>0.575359</td>\n      <td>1.787660</td>\n      <td>1.129089</td>\n      <td>-0.080817</td>\n      <td>1.849914</td>\n      <td>-0.779174</td>\n      <td>0.290555</td>\n      <td>1.990553</td>\n      <td>0.507389</td>\n      <td>0.907904</td>\n      <td>-1.211626</td>\n      <td>0.495542</td>\n      <td>0.588720</td>\n      <td>-1.014403</td>\n      <td>0.106444</td>\n      <td>1.229502</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76885</th>\n      <td>-0.605239</td>\n      <td>-0.659530</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.174600</td>\n      <td>-0.146213</td>\n      <td>1.836054</td>\n      <td>-0.038214</td>\n      <td>0.183720</td>\n      <td>-1.478466</td>\n      <td>-0.567504</td>\n      <td>-0.317410</td>\n      <td>-0.098314</td>\n      <td>-0.326505</td>\n      <td>4.784910</td>\n      <td>-1.130472</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.172635</td>\n      <td>1.826533</td>\n      <td>-0.463426</td>\n      <td>1.806448</td>\n      <td>1.792151</td>\n      <td>-0.389885</td>\n      <td>4.431401</td>\n      <td>1.829081</td>\n      <td>-0.505598</td>\n      <td>0.306861</td>\n      <td>1.990080</td>\n      <td>-0.463426</td>\n      <td>1.463785</td>\n      <td>-0.708503</td>\n      <td>-0.311360</td>\n      <td>-0.419508</td>\n      <td>-0.842612</td>\n      <td>0.006417</td>\n      <td>1.229408</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76886</th>\n      <td>-0.636869</td>\n      <td>0.194059</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.157636</td>\n      <td>-0.246570</td>\n      <td>1.827610</td>\n      <td>-0.591596</td>\n      <td>-0.602846</td>\n      <td>-1.377626</td>\n      <td>-0.812586</td>\n      <td>-0.835152</td>\n      <td>-0.279218</td>\n      <td>-0.371848</td>\n      <td>0.264961</td>\n      <td>-1.136254</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.154434</td>\n      <td>1.846752</td>\n      <td>-0.582058</td>\n      <td>4.778059</td>\n      <td>1.849475</td>\n      <td>0.508003</td>\n      <td>0.579277</td>\n      <td>1.819658</td>\n      <td>0.923738</td>\n      <td>0.259099</td>\n      <td>1.995408</td>\n      <td>-0.582058</td>\n      <td>1.706591</td>\n      <td>-0.915727</td>\n      <td>0.107819</td>\n      <td>0.304902</td>\n      <td>-0.620406</td>\n      <td>-0.061611</td>\n      <td>1.250421</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76887</th>\n      <td>-0.612396</td>\n      <td>1.516430</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.217149</td>\n      <td>-0.008820</td>\n      <td>1.811085</td>\n      <td>-0.820075</td>\n      <td>-0.854234</td>\n      <td>-1.399491</td>\n      <td>-1.068270</td>\n      <td>-1.062484</td>\n      <td>-0.517437</td>\n      <td>-0.444693</td>\n      <td>-0.264628</td>\n      <td>-1.145775</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.218224</td>\n      <td>1.838280</td>\n      <td>-0.827462</td>\n      <td>0.262431</td>\n      <td>1.850861</td>\n      <td>-0.471504</td>\n      <td>1.818117</td>\n      <td>1.828163</td>\n      <td>1.135308</td>\n      <td>-0.072012</td>\n      <td>2.011370</td>\n      <td>-0.827462</td>\n      <td>0.550428</td>\n      <td>0.462225</td>\n      <td>0.356451</td>\n      <td>0.487998</td>\n      <td>-0.286047</td>\n      <td>-0.059314</td>\n      <td>1.007059</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>69441 rows  92 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n36       -0.447636     1.503621 -0.616017  0.807145  0.566597 -0.676510   \n76        1.082342     2.932444  0.409891 -0.407999  0.054079 -0.291606   \n116      -0.038815     1.490517 -0.764228  1.661493  1.180223 -0.148263   \n183       0.725718     2.932444  1.004495 -1.707367 -0.945570 -0.468686   \n223       1.156386     1.442577 -1.101432  1.080693  0.088267 -0.321219   \n...            ...          ...       ...       ...       ...       ...   \n76808    -0.208849     1.877030 -0.097743  0.250530 -0.844100  1.878685   \n76820    -0.208849     0.180506 -0.479475  0.136375 -0.134496 -0.308538   \n76832    -0.208849    -0.030012 -0.391261  0.852059 -0.361299  1.687970   \n76848    -0.208849     2.667662 -1.806515 -1.257885  1.031139 -0.502160   \n76888    -0.681134     0.851401  0.154976 -0.449011  0.225954 -0.367814   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n36       0.041370    -0.444748   0.286301       -0.897920      -0.885205   \n76      -0.494304     2.201976   1.496388       -0.425977      -0.353926   \n116      0.073340     0.476343   0.792124       -1.047382      -0.969301   \n183      1.596658     2.201976   1.897724        1.685851       0.595604   \n223      0.139841     0.631494  -2.086450       -1.158357      -1.062442   \n...           ...          ...        ...             ...            ...   \n76808   -0.188176     1.104220  -0.374849       -0.970493      -0.937521   \n76820   -1.749070    -0.208232  -0.673038        2.482681       2.569855   \n76832    0.340067    -0.707754  -0.263601       -0.838144      -0.726073   \n76848    0.796485     2.201976  -0.277872       -0.478127      -0.643053   \n76888   -0.197249    -0.371359   1.771866       -0.237781      -0.182437   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n36         0.669968 -0.996079   -0.899538   -0.823745    -0.714925   \n76        -0.845733 -0.672869   -0.675338   -0.443324    -0.122958   \n116       -1.318185 -1.001774   -1.186375   -1.471881    -1.437878   \n183       -0.920109  2.095057    1.318192    2.145037     2.084527   \n223       -0.191328 -0.798725   -0.987976   -1.260964    -1.413439   \n...             ...       ...         ...         ...          ...   \n76808      0.150749 -0.914616   -1.141697   -0.557570    -0.777044   \n76820     -0.799717 -0.861063    1.967085   -0.098038    -0.578841   \n76832      0.518787 -1.026050   -0.958595   -0.389652    -0.598841   \n76848      0.846803 -0.448241   -0.343692   -0.448423    -0.564333   \n76888     -1.507184 -0.333492   -0.526458   -0.562515    -0.479081   \n\n       num__BAspr  num__size  num__lbm  num__lop  num__lgp  num__linv  \\\n36      -0.357029  -0.373732 -0.394250 -0.569229 -0.440817   0.242727   \n76       0.004582  -1.042218  0.421057  0.116165  0.326541  -1.125382   \n116     -0.154287  -0.845688 -0.283119  1.622306  1.928796  -1.026000   \n183     -0.322355  -1.567221  1.564946 -2.080395 -1.770265  -0.728135   \n223     -0.466201   2.825176 -0.664197  0.890849 -0.008299   0.969864   \n...           ...        ...       ...       ...       ...        ...   \n76808   -0.434216   0.249006  0.140510  0.324503 -0.716776   1.592674   \n76820   -0.449947   0.357319 -0.000264 -0.144985 -0.379309   1.387099   \n76832   -0.409310  -0.068315  0.117335  1.076196 -0.028161   1.083701   \n76848   -0.464551   0.750689 -0.923249 -1.272632  0.706670  -0.675277   \n76888    0.067450  -1.151990 -0.204261 -0.003571  0.471093  -0.542937   \n\n       num__llme  num__l1amhd  num__l1MAX  num__l1BAspr  num__l3amhd  \\\n36     -0.275573     0.267270   -0.269506     -0.318411     0.205912   \n76     -1.573113     1.527156   -0.594924      0.712950     1.761770   \n116    -1.010973     0.805248   -0.393897     -0.194811     0.822153   \n183    -2.319771     1.908623    0.307918      0.395609     2.123751   \n223     2.717759    -2.082807   -1.103496     -0.468662    -2.064685   \n...          ...          ...         ...           ...          ...   \n76808   0.001962    -0.378625   -0.626435     -0.437696    -0.401772   \n76820   0.508948    -0.659527   -0.753338     -0.460048    -0.628558   \n76832   0.048485    -0.272724   -0.908995     -0.445693    -0.278341   \n76848   0.285313    -0.255770   -0.190112     -0.466064    -0.178502   \n76888  -1.137309     1.821702   -1.083483     -0.266651     1.871219   \n\n       num__l3MAX  num__l3BAspr  num__l6amhd  num__l6MAX  num__l6BAspr  \\\n36      -0.104132     -0.331265     0.072720   -0.946680     -0.412905   \n76      -0.679498      0.038660     1.822095    0.159410      0.331043   \n116     -1.143382     -0.218875     0.889428   -0.854871     -0.437584   \n183      2.460112      0.569178     2.168014    2.476404      4.909389   \n223     -1.156894     -0.468742    -2.049128   -1.029393     -0.463778   \n...           ...           ...          ...         ...           ...   \n76808   -0.636862     -0.263074    -0.374597   -0.687537     -0.456005   \n76820    0.006270     -0.462151    -0.576820   -0.657888     -0.354463   \n76832   -0.623313     -0.452715    -0.304512   -0.314697     -0.448004   \n76848    0.670976     -0.467300    -0.090091    0.268836     -0.460818   \n76888   -0.591198      4.808439     1.832704   -0.395160      4.525245   \n\n       num__l12amhd  num__l12MAX  num__l12BAspr  num__l12mom122  \\\n36         0.484859    -0.269506      -0.423203        2.672025   \n76         2.221642    -0.594924       0.995690        1.607994   \n116        1.100826    -0.393897      -0.315926        1.487194   \n183       -0.140420     0.307918       4.010015        2.157728   \n223       -1.998789    -1.103496      -0.456639        1.337993   \n...             ...          ...            ...             ...   \n76808     -0.054501    -0.626435      -0.255555        1.189582   \n76820     -0.463467    -0.753338      -0.432884        1.033342   \n76832     -0.192267    -0.908995      -0.424018        2.672025   \n76848      0.050297    -0.190112      -0.402299        1.856043   \n76888      2.050753    -1.083483       2.233048        2.518535   \n\n       num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n36             -0.595621         -0.602738         0.655205      -0.705329   \n76             -0.186524         -0.124067        -2.448014       0.095269   \n116            -1.219011         -1.187009        -0.936426      -1.256309   \n183             1.577745          1.776789         3.967493       2.218228   \n223            -1.117885         -1.013970        -0.382679      -1.246534   \n...                  ...               ...              ...            ...   \n76808          -0.540612         -0.551645         0.953966      -0.311244   \n76820          -0.835113         -0.701370        -0.737806      -0.829634   \n76832          -0.765681         -0.780979         0.172734      -0.440386   \n76848           0.576474          0.139332         0.663355      -0.276019   \n76888           0.155802          0.049098        -0.276814      -0.146158   \n\n       num__l12vol12m  num__amhd_miss  num__BAspr_miss  cat__ind_1.0  \\\n36          -0.016263       -0.178017        -0.078007           0.0   \n76           0.861902       -0.178017        -0.078007           0.0   \n116         -0.357296       -0.178017        -0.078007           0.0   \n183          2.230892       -0.178017        -0.078007           0.0   \n223         -1.062468       -0.178017        -0.078007           0.0   \n...               ...             ...              ...           ...   \n76808        0.013890       -0.178017        -0.078007           0.0   \n76820       -0.770382       -0.178017        -0.078007           0.0   \n76832       -0.020364       -0.178017        -0.078007           0.0   \n76848        0.117571       -0.178017        -0.078007           0.0   \n76888        0.813927       -0.178017        -0.078007           0.0   \n\n       cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  \\\n36              0.0           0.0           0.0           0.0           0.0   \n76              0.0           0.0           0.0           0.0           0.0   \n116             1.0           0.0           0.0           0.0           0.0   \n183             0.0           0.0           0.0           0.0           0.0   \n223             0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n76808           0.0           0.0           0.0           0.0           0.0   \n76820           0.0           0.0           0.0           0.0           0.0   \n76832           0.0           0.0           0.0           0.0           0.0   \n76848           0.0           0.0           0.0           0.0           0.0   \n76888           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n36              0.0           0.0           0.0            0.0            0.0   \n76              0.0           0.0           0.0            0.0            0.0   \n116             0.0           0.0           0.0            0.0            0.0   \n183             0.0           0.0           0.0            0.0            0.0   \n223             0.0           0.0           0.0            0.0            0.0   \n...             ...           ...           ...            ...            ...   \n76808           0.0           0.0           0.0            0.0            0.0   \n76820           0.0           0.0           0.0            0.0            0.0   \n76832           0.0           0.0           0.0            0.0            0.0   \n76848           0.0           0.0           0.0            0.0            0.0   \n76888           0.0           0.0           0.0            0.0            0.0   \n\n       cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n36               0.0            0.0            0.0            1.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            1.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            1.0            0.0   \n\n       cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            1.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            1.0            0.0   \n76820            0.0            0.0            1.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              1.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            1.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            1.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  cat__ind_47.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_48.0  cat__ind_49.0  \n36               0.0            0.0  \n76               0.0            0.0  \n116              0.0            0.0  \n183              0.0            0.0  \n223              0.0            0.0  \n...              ...            ...  \n76808            0.0            0.0  \n76820            0.0            0.0  \n76832            0.0            0.0  \n76848            0.0            0.0  \n76888            0.0            0.0  \n\n[1881 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36</th>\n      <td>-0.447636</td>\n      <td>1.503621</td>\n      <td>-0.616017</td>\n      <td>0.807145</td>\n      <td>0.566597</td>\n      <td>-0.676510</td>\n      <td>0.041370</td>\n      <td>-0.444748</td>\n      <td>0.286301</td>\n      <td>-0.897920</td>\n      <td>-0.885205</td>\n      <td>0.669968</td>\n      <td>-0.996079</td>\n      <td>-0.899538</td>\n      <td>-0.823745</td>\n      <td>-0.714925</td>\n      <td>-0.357029</td>\n      <td>-0.373732</td>\n      <td>-0.394250</td>\n      <td>-0.569229</td>\n      <td>-0.440817</td>\n      <td>0.242727</td>\n      <td>-0.275573</td>\n      <td>0.267270</td>\n      <td>-0.269506</td>\n      <td>-0.318411</td>\n      <td>0.205912</td>\n      <td>-0.104132</td>\n      <td>-0.331265</td>\n      <td>0.072720</td>\n      <td>-0.946680</td>\n      <td>-0.412905</td>\n      <td>0.484859</td>\n      <td>-0.269506</td>\n      <td>-0.423203</td>\n      <td>2.672025</td>\n      <td>-0.595621</td>\n      <td>-0.602738</td>\n      <td>0.655205</td>\n      <td>-0.705329</td>\n      <td>-0.016263</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>1.082342</td>\n      <td>2.932444</td>\n      <td>0.409891</td>\n      <td>-0.407999</td>\n      <td>0.054079</td>\n      <td>-0.291606</td>\n      <td>-0.494304</td>\n      <td>2.201976</td>\n      <td>1.496388</td>\n      <td>-0.425977</td>\n      <td>-0.353926</td>\n      <td>-0.845733</td>\n      <td>-0.672869</td>\n      <td>-0.675338</td>\n      <td>-0.443324</td>\n      <td>-0.122958</td>\n      <td>0.004582</td>\n      <td>-1.042218</td>\n      <td>0.421057</td>\n      <td>0.116165</td>\n      <td>0.326541</td>\n      <td>-1.125382</td>\n      <td>-1.573113</td>\n      <td>1.527156</td>\n      <td>-0.594924</td>\n      <td>0.712950</td>\n      <td>1.761770</td>\n      <td>-0.679498</td>\n      <td>0.038660</td>\n      <td>1.822095</td>\n      <td>0.159410</td>\n      <td>0.331043</td>\n      <td>2.221642</td>\n      <td>-0.594924</td>\n      <td>0.995690</td>\n      <td>1.607994</td>\n      <td>-0.186524</td>\n      <td>-0.124067</td>\n      <td>-2.448014</td>\n      <td>0.095269</td>\n      <td>0.861902</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>-0.038815</td>\n      <td>1.490517</td>\n      <td>-0.764228</td>\n      <td>1.661493</td>\n      <td>1.180223</td>\n      <td>-0.148263</td>\n      <td>0.073340</td>\n      <td>0.476343</td>\n      <td>0.792124</td>\n      <td>-1.047382</td>\n      <td>-0.969301</td>\n      <td>-1.318185</td>\n      <td>-1.001774</td>\n      <td>-1.186375</td>\n      <td>-1.471881</td>\n      <td>-1.437878</td>\n      <td>-0.154287</td>\n      <td>-0.845688</td>\n      <td>-0.283119</td>\n      <td>1.622306</td>\n      <td>1.928796</td>\n      <td>-1.026000</td>\n      <td>-1.010973</td>\n      <td>0.805248</td>\n      <td>-0.393897</td>\n      <td>-0.194811</td>\n      <td>0.822153</td>\n      <td>-1.143382</td>\n      <td>-0.218875</td>\n      <td>0.889428</td>\n      <td>-0.854871</td>\n      <td>-0.437584</td>\n      <td>1.100826</td>\n      <td>-0.393897</td>\n      <td>-0.315926</td>\n      <td>1.487194</td>\n      <td>-1.219011</td>\n      <td>-1.187009</td>\n      <td>-0.936426</td>\n      <td>-1.256309</td>\n      <td>-0.357296</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>0.725718</td>\n      <td>2.932444</td>\n      <td>1.004495</td>\n      <td>-1.707367</td>\n      <td>-0.945570</td>\n      <td>-0.468686</td>\n      <td>1.596658</td>\n      <td>2.201976</td>\n      <td>1.897724</td>\n      <td>1.685851</td>\n      <td>0.595604</td>\n      <td>-0.920109</td>\n      <td>2.095057</td>\n      <td>1.318192</td>\n      <td>2.145037</td>\n      <td>2.084527</td>\n      <td>-0.322355</td>\n      <td>-1.567221</td>\n      <td>1.564946</td>\n      <td>-2.080395</td>\n      <td>-1.770265</td>\n      <td>-0.728135</td>\n      <td>-2.319771</td>\n      <td>1.908623</td>\n      <td>0.307918</td>\n      <td>0.395609</td>\n      <td>2.123751</td>\n      <td>2.460112</td>\n      <td>0.569178</td>\n      <td>2.168014</td>\n      <td>2.476404</td>\n      <td>4.909389</td>\n      <td>-0.140420</td>\n      <td>0.307918</td>\n      <td>4.010015</td>\n      <td>2.157728</td>\n      <td>1.577745</td>\n      <td>1.776789</td>\n      <td>3.967493</td>\n      <td>2.218228</td>\n      <td>2.230892</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>1.156386</td>\n      <td>1.442577</td>\n      <td>-1.101432</td>\n      <td>1.080693</td>\n      <td>0.088267</td>\n      <td>-0.321219</td>\n      <td>0.139841</td>\n      <td>0.631494</td>\n      <td>-2.086450</td>\n      <td>-1.158357</td>\n      <td>-1.062442</td>\n      <td>-0.191328</td>\n      <td>-0.798725</td>\n      <td>-0.987976</td>\n      <td>-1.260964</td>\n      <td>-1.413439</td>\n      <td>-0.466201</td>\n      <td>2.825176</td>\n      <td>-0.664197</td>\n      <td>0.890849</td>\n      <td>-0.008299</td>\n      <td>0.969864</td>\n      <td>2.717759</td>\n      <td>-2.082807</td>\n      <td>-1.103496</td>\n      <td>-0.468662</td>\n      <td>-2.064685</td>\n      <td>-1.156894</td>\n      <td>-0.468742</td>\n      <td>-2.049128</td>\n      <td>-1.029393</td>\n      <td>-0.463778</td>\n      <td>-1.998789</td>\n      <td>-1.103496</td>\n      <td>-0.456639</td>\n      <td>1.337993</td>\n      <td>-1.117885</td>\n      <td>-1.013970</td>\n      <td>-0.382679</td>\n      <td>-1.246534</td>\n      <td>-1.062468</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76808</th>\n      <td>-0.208849</td>\n      <td>1.877030</td>\n      <td>-0.097743</td>\n      <td>0.250530</td>\n      <td>-0.844100</td>\n      <td>1.878685</td>\n      <td>-0.188176</td>\n      <td>1.104220</td>\n      <td>-0.374849</td>\n      <td>-0.970493</td>\n      <td>-0.937521</td>\n      <td>0.150749</td>\n      <td>-0.914616</td>\n      <td>-1.141697</td>\n      <td>-0.557570</td>\n      <td>-0.777044</td>\n      <td>-0.434216</td>\n      <td>0.249006</td>\n      <td>0.140510</td>\n      <td>0.324503</td>\n      <td>-0.716776</td>\n      <td>1.592674</td>\n      <td>0.001962</td>\n      <td>-0.378625</td>\n      <td>-0.626435</td>\n      <td>-0.437696</td>\n      <td>-0.401772</td>\n      <td>-0.636862</td>\n      <td>-0.263074</td>\n      <td>-0.374597</td>\n      <td>-0.687537</td>\n      <td>-0.456005</td>\n      <td>-0.054501</td>\n      <td>-0.626435</td>\n      <td>-0.255555</td>\n      <td>1.189582</td>\n      <td>-0.540612</td>\n      <td>-0.551645</td>\n      <td>0.953966</td>\n      <td>-0.311244</td>\n      <td>0.013890</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76820</th>\n      <td>-0.208849</td>\n      <td>0.180506</td>\n      <td>-0.479475</td>\n      <td>0.136375</td>\n      <td>-0.134496</td>\n      <td>-0.308538</td>\n      <td>-1.749070</td>\n      <td>-0.208232</td>\n      <td>-0.673038</td>\n      <td>2.482681</td>\n      <td>2.569855</td>\n      <td>-0.799717</td>\n      <td>-0.861063</td>\n      <td>1.967085</td>\n      <td>-0.098038</td>\n      <td>-0.578841</td>\n      <td>-0.449947</td>\n      <td>0.357319</td>\n      <td>-0.000264</td>\n      <td>-0.144985</td>\n      <td>-0.379309</td>\n      <td>1.387099</td>\n      <td>0.508948</td>\n      <td>-0.659527</td>\n      <td>-0.753338</td>\n      <td>-0.460048</td>\n      <td>-0.628558</td>\n      <td>0.006270</td>\n      <td>-0.462151</td>\n      <td>-0.576820</td>\n      <td>-0.657888</td>\n      <td>-0.354463</td>\n      <td>-0.463467</td>\n      <td>-0.753338</td>\n      <td>-0.432884</td>\n      <td>1.033342</td>\n      <td>-0.835113</td>\n      <td>-0.701370</td>\n      <td>-0.737806</td>\n      <td>-0.829634</td>\n      <td>-0.770382</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76832</th>\n      <td>-0.208849</td>\n      <td>-0.030012</td>\n      <td>-0.391261</td>\n      <td>0.852059</td>\n      <td>-0.361299</td>\n      <td>1.687970</td>\n      <td>0.340067</td>\n      <td>-0.707754</td>\n      <td>-0.263601</td>\n      <td>-0.838144</td>\n      <td>-0.726073</td>\n      <td>0.518787</td>\n      <td>-1.026050</td>\n      <td>-0.958595</td>\n      <td>-0.389652</td>\n      <td>-0.598841</td>\n      <td>-0.409310</td>\n      <td>-0.068315</td>\n      <td>0.117335</td>\n      <td>1.076196</td>\n      <td>-0.028161</td>\n      <td>1.083701</td>\n      <td>0.048485</td>\n      <td>-0.272724</td>\n      <td>-0.908995</td>\n      <td>-0.445693</td>\n      <td>-0.278341</td>\n      <td>-0.623313</td>\n      <td>-0.452715</td>\n      <td>-0.304512</td>\n      <td>-0.314697</td>\n      <td>-0.448004</td>\n      <td>-0.192267</td>\n      <td>-0.908995</td>\n      <td>-0.424018</td>\n      <td>2.672025</td>\n      <td>-0.765681</td>\n      <td>-0.780979</td>\n      <td>0.172734</td>\n      <td>-0.440386</td>\n      <td>-0.020364</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76848</th>\n      <td>-0.208849</td>\n      <td>2.667662</td>\n      <td>-1.806515</td>\n      <td>-1.257885</td>\n      <td>1.031139</td>\n      <td>-0.502160</td>\n      <td>0.796485</td>\n      <td>2.201976</td>\n      <td>-0.277872</td>\n      <td>-0.478127</td>\n      <td>-0.643053</td>\n      <td>0.846803</td>\n      <td>-0.448241</td>\n      <td>-0.343692</td>\n      <td>-0.448423</td>\n      <td>-0.564333</td>\n      <td>-0.464551</td>\n      <td>0.750689</td>\n      <td>-0.923249</td>\n      <td>-1.272632</td>\n      <td>0.706670</td>\n      <td>-0.675277</td>\n      <td>0.285313</td>\n      <td>-0.255770</td>\n      <td>-0.190112</td>\n      <td>-0.466064</td>\n      <td>-0.178502</td>\n      <td>0.670976</td>\n      <td>-0.467300</td>\n      <td>-0.090091</td>\n      <td>0.268836</td>\n      <td>-0.460818</td>\n      <td>0.050297</td>\n      <td>-0.190112</td>\n      <td>-0.402299</td>\n      <td>1.856043</td>\n      <td>0.576474</td>\n      <td>0.139332</td>\n      <td>0.663355</td>\n      <td>-0.276019</td>\n      <td>0.117571</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76888</th>\n      <td>-0.681134</td>\n      <td>0.851401</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.197249</td>\n      <td>-0.371359</td>\n      <td>1.771866</td>\n      <td>-0.237781</td>\n      <td>-0.182437</td>\n      <td>-1.507184</td>\n      <td>-0.333492</td>\n      <td>-0.526458</td>\n      <td>-0.562515</td>\n      <td>-0.479081</td>\n      <td>0.067450</td>\n      <td>-1.151990</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.137309</td>\n      <td>1.821702</td>\n      <td>-1.083483</td>\n      <td>-0.266651</td>\n      <td>1.871219</td>\n      <td>-0.591198</td>\n      <td>4.808439</td>\n      <td>1.832704</td>\n      <td>-0.395160</td>\n      <td>4.525245</td>\n      <td>2.050753</td>\n      <td>-1.083483</td>\n      <td>2.233048</td>\n      <td>2.518535</td>\n      <td>0.155802</td>\n      <td>0.049098</td>\n      <td>-0.276814</td>\n      <td>-0.146158</td>\n      <td>0.813927</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1881 rows  92 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n37       -0.370711     1.045747 -0.616017  0.807145  0.566597 -0.676510   \n77        1.562862     2.932444  0.409891 -0.407999  0.054079 -0.291606   \n117      -0.017971     1.548951 -0.764228  1.661493  1.180223 -0.148263   \n184       0.995702     2.932444  1.004495 -1.707367 -0.945570 -0.468686   \n224       1.147943     1.282408 -1.101432  1.080693  0.088267 -0.321219   \n...            ...          ...       ...       ...       ...       ...   \n76809    -0.208849     1.361380 -0.097743  0.250530 -0.844100  1.878685   \n76821    -0.208849    -0.112372 -0.479475  0.136375 -0.134496 -0.308538   \n76833    -0.208849    -0.300522 -0.391261  0.852059 -0.361299  1.687970   \n76849    -0.208849     1.938339 -1.806515 -1.257885  1.031139 -0.502160   \n76889    -0.690620     0.644627  0.154976 -0.449011  0.225954 -0.367814   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n37       0.375087     0.151844   0.324364       -0.926429      -0.835087   \n77       1.619434     1.765912   1.497371       -0.321803      -0.310475   \n117      0.001117     0.335290   0.784002       -1.173535      -1.074915   \n184     -0.118384     2.201976   1.897724        1.946351       2.146835   \n224      0.057190     0.589992  -2.076221       -1.190013      -1.156021   \n...           ...          ...        ...             ...            ...   \n76809   -1.457988     0.954821  -0.377764       -0.554896      -0.484308   \n76821    0.503895    -0.692448  -0.679762       -0.529885      -0.387674   \n76833   -1.046258    -0.672714  -0.253326       -0.697715      -0.685183   \n76849   -0.298017     2.201976  -0.300283       -0.712674      -0.693444   \n76889   -0.064676    -0.604371   1.771012       -0.826214      -0.853646   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n37         0.819344 -0.767050   -0.415347   -0.837416    -0.742606   \n77        -0.819265 -0.310528   -0.604683   -0.665054    -0.198205   \n117       -1.418253 -1.138869   -1.358698   -1.498212    -1.529875   \n184       -0.986489  1.522755    1.681061    2.134220     2.084527   \n224       -0.079273 -0.976936   -0.943820   -1.358742    -1.372436   \n...             ...       ...         ...         ...          ...   \n76809      0.210192 -0.860239   -0.538398   -0.528166    -0.760821   \n76821     -0.777245 -0.624373   -0.747129   -0.121647    -0.547000   \n76833      0.464065 -1.147242   -0.860877   -0.522436    -0.591345   \n76849      0.865974 -0.488813   -0.815114   -0.553735    -0.551772   \n76889     -1.525174 -0.806854   -1.030321   -0.850105    -0.520813   \n\n       num__BAspr  num__size  num__lbm  num__lop  num__lgp  num__linv  \\\n37      -0.273727  -0.347390 -0.394250 -0.569229 -0.440817   0.242727   \n77       0.019889  -0.948273  0.421057  0.116165  0.326541  -1.125382   \n117     -0.196580  -0.841639 -0.283119  1.622306  1.928796  -1.026000   \n184      2.409377  -1.570553  1.564946 -2.080395 -1.770265  -0.728135   \n224     -0.454743   2.832604 -0.664197  0.890849 -0.008299   0.969864   \n...           ...        ...       ...       ...       ...        ...   \n76809   -0.397030   0.215362  0.140510  0.324503 -0.716776   1.592674   \n76821   -0.452278   0.391971 -0.000264 -0.144985 -0.379309   1.387099   \n76833   -0.440970  -0.131511  0.117335  1.076196 -0.028161   1.083701   \n76849   -0.451074   0.746059 -0.923249 -1.272632  0.706670  -0.675277   \n76889    1.154986  -1.151990 -0.204261 -0.003571  0.471093  -0.542937   \n\n       num__llme  num__l1amhd  num__l1MAX  num__l1BAspr  num__l3amhd  \\\n37     -0.416237     0.291945   -1.011197     -0.358964     0.243817   \n77     -1.415715     1.505979   -0.687561      0.002301     1.688294   \n117    -0.975310     0.799418   -1.016899     -0.156416     0.818801   \n184    -2.259244     1.908623    2.084007     -0.324324     1.933517   \n224     2.738320    -2.088544   -0.813583     -0.468032    -2.077690   \n...          ...          ...         ...           ...          ...   \n76809   0.017774    -0.371362   -0.929627     -0.436078    -0.384307   \n76821   0.496130    -0.670522   -0.876003     -0.451794    -0.642843   \n76833   0.063224    -0.259750   -1.041207     -0.411195    -0.267628   \n76849   0.369738    -0.274068   -0.462638     -0.466384    -0.209352   \n76889  -1.076504     1.782355   -0.347738      0.065109     1.862689   \n\n       num__l3MAX  num__l3BAspr  num__l6amhd  num__l6MAX  num__l6BAspr  \\\n37      -0.839387     -0.324342     0.095959   -0.731315     -0.357240   \n77      -0.429360      0.854002     1.819425    0.480177      0.086252   \n117     -0.878843     -0.312308     0.904621   -1.013453     -0.255808   \n184      1.854294      0.172582     2.168014    2.476404      4.909389   \n224     -0.794177     -0.457275    -2.050656    0.040278     -0.467010   \n...           ...           ...          ...         ...           ...   \n76809    0.216762     -0.442030    -0.394194   -0.249067     -0.408983   \n76821   -0.738654     -0.463502    -0.585459   -0.954593     -0.462559   \n76833   -0.419104     -0.448052    -0.286616    0.417637     -0.447920   \n76849   -0.783334     -0.467451    -0.108760    0.803386     -0.464383   \n76889   -0.838800      0.264378     1.890670    0.509522      0.600523   \n\n       num__l12amhd  num__l12MAX  num__l12BAspr  num__l12mom122  \\\n37         0.260356    -1.011197      -0.440167        2.672025   \n77         2.144114    -0.687561       2.133388        2.306243   \n117        1.068031    -1.016899      -0.067369        1.237983   \n184       -0.140420     2.084007       3.674272        0.206004   \n224       -2.036770    -0.813583      -0.457157        1.051903   \n...             ...          ...            ...             ...   \n76809     -0.169345    -0.929627      -0.440086        1.073954   \n76821     -0.505047    -0.876003      -0.409439        1.389443   \n76833     -0.268217    -1.041207      -0.449478        0.965275   \n76849      0.019749    -0.462638      -0.443034        0.389688   \n76889      2.019325    -0.347738       2.482332        1.956218   \n\n       num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n37             -0.081076         -0.058999         0.166738      -0.602469   \n77              1.035512          1.048353        -2.348474       0.271762   \n117            -0.498111         -0.379121        -1.343626      -1.164610   \n184             2.463395          2.423646        -0.487464       2.218228   \n224            -1.203442         -1.138263        -0.489305      -1.280598   \n...                  ...               ...              ...            ...   \n76809          -0.449619         -0.438896         1.020797      -0.401676   \n76821          -1.141501         -1.187009        -0.780925      -0.876849   \n76833          -0.818476         -0.690079         0.571615      -0.709454   \n76849          -0.734271         -0.657172         0.494761      -0.346837   \n76889          -0.317879         -0.206623        -1.289419      -0.263619   \n\n       num__l12vol12m  num__amhd_miss  num__BAspr_miss  cat__ind_1.0  \\\n37          -0.391493       -0.178017        -0.078007           0.0   \n77           0.884246       -0.178017        -0.078007           0.0   \n117         -0.710352       -0.178017        -0.078007           0.0   \n184          2.230892       -0.178017        -0.078007           0.0   \n224         -1.270395       -0.178017        -0.078007           0.0   \n...               ...             ...              ...           ...   \n76809       -0.174816       -0.178017        -0.078007           0.0   \n76821       -0.868594       -0.178017        -0.078007           0.0   \n76833       -0.189070       -0.178017        -0.078007           0.0   \n76849       -0.123037       -0.178017        -0.078007           0.0   \n76889        0.330423       -0.178017        -0.078007           0.0   \n\n       cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  \\\n37              0.0           0.0           0.0           0.0           0.0   \n77              0.0           0.0           0.0           0.0           0.0   \n117             1.0           0.0           0.0           0.0           0.0   \n184             0.0           0.0           0.0           0.0           0.0   \n224             0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n76809           0.0           0.0           0.0           0.0           0.0   \n76821           0.0           0.0           0.0           0.0           0.0   \n76833           0.0           0.0           0.0           0.0           0.0   \n76849           0.0           0.0           0.0           0.0           0.0   \n76889           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n37              0.0           0.0           0.0            0.0            0.0   \n77              0.0           0.0           0.0            0.0            0.0   \n117             0.0           0.0           0.0            0.0            0.0   \n184             0.0           0.0           0.0            0.0            0.0   \n224             0.0           0.0           0.0            0.0            0.0   \n...             ...           ...           ...            ...            ...   \n76809           0.0           0.0           0.0            0.0            0.0   \n76821           0.0           0.0           0.0            0.0            0.0   \n76833           0.0           0.0           0.0            0.0            0.0   \n76849           0.0           0.0           0.0            0.0            0.0   \n76889           0.0           0.0           0.0            0.0            0.0   \n\n       cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n37               0.0            0.0            0.0            1.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            1.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            1.0            0.0   \n\n       cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            1.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            1.0            0.0   \n76821            0.0            0.0            1.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              1.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            1.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            1.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  cat__ind_47.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_48.0  cat__ind_49.0  \n37               0.0            0.0  \n77               0.0            0.0  \n117              0.0            0.0  \n184              0.0            0.0  \n224              0.0            0.0  \n...              ...            ...  \n76809            0.0            0.0  \n76821            0.0            0.0  \n76833            0.0            0.0  \n76849            0.0            0.0  \n76889            0.0            0.0  \n\n[1873 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37</th>\n      <td>-0.370711</td>\n      <td>1.045747</td>\n      <td>-0.616017</td>\n      <td>0.807145</td>\n      <td>0.566597</td>\n      <td>-0.676510</td>\n      <td>0.375087</td>\n      <td>0.151844</td>\n      <td>0.324364</td>\n      <td>-0.926429</td>\n      <td>-0.835087</td>\n      <td>0.819344</td>\n      <td>-0.767050</td>\n      <td>-0.415347</td>\n      <td>-0.837416</td>\n      <td>-0.742606</td>\n      <td>-0.273727</td>\n      <td>-0.347390</td>\n      <td>-0.394250</td>\n      <td>-0.569229</td>\n      <td>-0.440817</td>\n      <td>0.242727</td>\n      <td>-0.416237</td>\n      <td>0.291945</td>\n      <td>-1.011197</td>\n      <td>-0.358964</td>\n      <td>0.243817</td>\n      <td>-0.839387</td>\n      <td>-0.324342</td>\n      <td>0.095959</td>\n      <td>-0.731315</td>\n      <td>-0.357240</td>\n      <td>0.260356</td>\n      <td>-1.011197</td>\n      <td>-0.440167</td>\n      <td>2.672025</td>\n      <td>-0.081076</td>\n      <td>-0.058999</td>\n      <td>0.166738</td>\n      <td>-0.602469</td>\n      <td>-0.391493</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>1.562862</td>\n      <td>2.932444</td>\n      <td>0.409891</td>\n      <td>-0.407999</td>\n      <td>0.054079</td>\n      <td>-0.291606</td>\n      <td>1.619434</td>\n      <td>1.765912</td>\n      <td>1.497371</td>\n      <td>-0.321803</td>\n      <td>-0.310475</td>\n      <td>-0.819265</td>\n      <td>-0.310528</td>\n      <td>-0.604683</td>\n      <td>-0.665054</td>\n      <td>-0.198205</td>\n      <td>0.019889</td>\n      <td>-0.948273</td>\n      <td>0.421057</td>\n      <td>0.116165</td>\n      <td>0.326541</td>\n      <td>-1.125382</td>\n      <td>-1.415715</td>\n      <td>1.505979</td>\n      <td>-0.687561</td>\n      <td>0.002301</td>\n      <td>1.688294</td>\n      <td>-0.429360</td>\n      <td>0.854002</td>\n      <td>1.819425</td>\n      <td>0.480177</td>\n      <td>0.086252</td>\n      <td>2.144114</td>\n      <td>-0.687561</td>\n      <td>2.133388</td>\n      <td>2.306243</td>\n      <td>1.035512</td>\n      <td>1.048353</td>\n      <td>-2.348474</td>\n      <td>0.271762</td>\n      <td>0.884246</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>-0.017971</td>\n      <td>1.548951</td>\n      <td>-0.764228</td>\n      <td>1.661493</td>\n      <td>1.180223</td>\n      <td>-0.148263</td>\n      <td>0.001117</td>\n      <td>0.335290</td>\n      <td>0.784002</td>\n      <td>-1.173535</td>\n      <td>-1.074915</td>\n      <td>-1.418253</td>\n      <td>-1.138869</td>\n      <td>-1.358698</td>\n      <td>-1.498212</td>\n      <td>-1.529875</td>\n      <td>-0.196580</td>\n      <td>-0.841639</td>\n      <td>-0.283119</td>\n      <td>1.622306</td>\n      <td>1.928796</td>\n      <td>-1.026000</td>\n      <td>-0.975310</td>\n      <td>0.799418</td>\n      <td>-1.016899</td>\n      <td>-0.156416</td>\n      <td>0.818801</td>\n      <td>-0.878843</td>\n      <td>-0.312308</td>\n      <td>0.904621</td>\n      <td>-1.013453</td>\n      <td>-0.255808</td>\n      <td>1.068031</td>\n      <td>-1.016899</td>\n      <td>-0.067369</td>\n      <td>1.237983</td>\n      <td>-0.498111</td>\n      <td>-0.379121</td>\n      <td>-1.343626</td>\n      <td>-1.164610</td>\n      <td>-0.710352</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>0.995702</td>\n      <td>2.932444</td>\n      <td>1.004495</td>\n      <td>-1.707367</td>\n      <td>-0.945570</td>\n      <td>-0.468686</td>\n      <td>-0.118384</td>\n      <td>2.201976</td>\n      <td>1.897724</td>\n      <td>1.946351</td>\n      <td>2.146835</td>\n      <td>-0.986489</td>\n      <td>1.522755</td>\n      <td>1.681061</td>\n      <td>2.134220</td>\n      <td>2.084527</td>\n      <td>2.409377</td>\n      <td>-1.570553</td>\n      <td>1.564946</td>\n      <td>-2.080395</td>\n      <td>-1.770265</td>\n      <td>-0.728135</td>\n      <td>-2.259244</td>\n      <td>1.908623</td>\n      <td>2.084007</td>\n      <td>-0.324324</td>\n      <td>1.933517</td>\n      <td>1.854294</td>\n      <td>0.172582</td>\n      <td>2.168014</td>\n      <td>2.476404</td>\n      <td>4.909389</td>\n      <td>-0.140420</td>\n      <td>2.084007</td>\n      <td>3.674272</td>\n      <td>0.206004</td>\n      <td>2.463395</td>\n      <td>2.423646</td>\n      <td>-0.487464</td>\n      <td>2.218228</td>\n      <td>2.230892</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>224</th>\n      <td>1.147943</td>\n      <td>1.282408</td>\n      <td>-1.101432</td>\n      <td>1.080693</td>\n      <td>0.088267</td>\n      <td>-0.321219</td>\n      <td>0.057190</td>\n      <td>0.589992</td>\n      <td>-2.076221</td>\n      <td>-1.190013</td>\n      <td>-1.156021</td>\n      <td>-0.079273</td>\n      <td>-0.976936</td>\n      <td>-0.943820</td>\n      <td>-1.358742</td>\n      <td>-1.372436</td>\n      <td>-0.454743</td>\n      <td>2.832604</td>\n      <td>-0.664197</td>\n      <td>0.890849</td>\n      <td>-0.008299</td>\n      <td>0.969864</td>\n      <td>2.738320</td>\n      <td>-2.088544</td>\n      <td>-0.813583</td>\n      <td>-0.468032</td>\n      <td>-2.077690</td>\n      <td>-0.794177</td>\n      <td>-0.457275</td>\n      <td>-2.050656</td>\n      <td>0.040278</td>\n      <td>-0.467010</td>\n      <td>-2.036770</td>\n      <td>-0.813583</td>\n      <td>-0.457157</td>\n      <td>1.051903</td>\n      <td>-1.203442</td>\n      <td>-1.138263</td>\n      <td>-0.489305</td>\n      <td>-1.280598</td>\n      <td>-1.270395</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76809</th>\n      <td>-0.208849</td>\n      <td>1.361380</td>\n      <td>-0.097743</td>\n      <td>0.250530</td>\n      <td>-0.844100</td>\n      <td>1.878685</td>\n      <td>-1.457988</td>\n      <td>0.954821</td>\n      <td>-0.377764</td>\n      <td>-0.554896</td>\n      <td>-0.484308</td>\n      <td>0.210192</td>\n      <td>-0.860239</td>\n      <td>-0.538398</td>\n      <td>-0.528166</td>\n      <td>-0.760821</td>\n      <td>-0.397030</td>\n      <td>0.215362</td>\n      <td>0.140510</td>\n      <td>0.324503</td>\n      <td>-0.716776</td>\n      <td>1.592674</td>\n      <td>0.017774</td>\n      <td>-0.371362</td>\n      <td>-0.929627</td>\n      <td>-0.436078</td>\n      <td>-0.384307</td>\n      <td>0.216762</td>\n      <td>-0.442030</td>\n      <td>-0.394194</td>\n      <td>-0.249067</td>\n      <td>-0.408983</td>\n      <td>-0.169345</td>\n      <td>-0.929627</td>\n      <td>-0.440086</td>\n      <td>1.073954</td>\n      <td>-0.449619</td>\n      <td>-0.438896</td>\n      <td>1.020797</td>\n      <td>-0.401676</td>\n      <td>-0.174816</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76821</th>\n      <td>-0.208849</td>\n      <td>-0.112372</td>\n      <td>-0.479475</td>\n      <td>0.136375</td>\n      <td>-0.134496</td>\n      <td>-0.308538</td>\n      <td>0.503895</td>\n      <td>-0.692448</td>\n      <td>-0.679762</td>\n      <td>-0.529885</td>\n      <td>-0.387674</td>\n      <td>-0.777245</td>\n      <td>-0.624373</td>\n      <td>-0.747129</td>\n      <td>-0.121647</td>\n      <td>-0.547000</td>\n      <td>-0.452278</td>\n      <td>0.391971</td>\n      <td>-0.000264</td>\n      <td>-0.144985</td>\n      <td>-0.379309</td>\n      <td>1.387099</td>\n      <td>0.496130</td>\n      <td>-0.670522</td>\n      <td>-0.876003</td>\n      <td>-0.451794</td>\n      <td>-0.642843</td>\n      <td>-0.738654</td>\n      <td>-0.463502</td>\n      <td>-0.585459</td>\n      <td>-0.954593</td>\n      <td>-0.462559</td>\n      <td>-0.505047</td>\n      <td>-0.876003</td>\n      <td>-0.409439</td>\n      <td>1.389443</td>\n      <td>-1.141501</td>\n      <td>-1.187009</td>\n      <td>-0.780925</td>\n      <td>-0.876849</td>\n      <td>-0.868594</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76833</th>\n      <td>-0.208849</td>\n      <td>-0.300522</td>\n      <td>-0.391261</td>\n      <td>0.852059</td>\n      <td>-0.361299</td>\n      <td>1.687970</td>\n      <td>-1.046258</td>\n      <td>-0.672714</td>\n      <td>-0.253326</td>\n      <td>-0.697715</td>\n      <td>-0.685183</td>\n      <td>0.464065</td>\n      <td>-1.147242</td>\n      <td>-0.860877</td>\n      <td>-0.522436</td>\n      <td>-0.591345</td>\n      <td>-0.440970</td>\n      <td>-0.131511</td>\n      <td>0.117335</td>\n      <td>1.076196</td>\n      <td>-0.028161</td>\n      <td>1.083701</td>\n      <td>0.063224</td>\n      <td>-0.259750</td>\n      <td>-1.041207</td>\n      <td>-0.411195</td>\n      <td>-0.267628</td>\n      <td>-0.419104</td>\n      <td>-0.448052</td>\n      <td>-0.286616</td>\n      <td>0.417637</td>\n      <td>-0.447920</td>\n      <td>-0.268217</td>\n      <td>-1.041207</td>\n      <td>-0.449478</td>\n      <td>0.965275</td>\n      <td>-0.818476</td>\n      <td>-0.690079</td>\n      <td>0.571615</td>\n      <td>-0.709454</td>\n      <td>-0.189070</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76849</th>\n      <td>-0.208849</td>\n      <td>1.938339</td>\n      <td>-1.806515</td>\n      <td>-1.257885</td>\n      <td>1.031139</td>\n      <td>-0.502160</td>\n      <td>-0.298017</td>\n      <td>2.201976</td>\n      <td>-0.300283</td>\n      <td>-0.712674</td>\n      <td>-0.693444</td>\n      <td>0.865974</td>\n      <td>-0.488813</td>\n      <td>-0.815114</td>\n      <td>-0.553735</td>\n      <td>-0.551772</td>\n      <td>-0.451074</td>\n      <td>0.746059</td>\n      <td>-0.923249</td>\n      <td>-1.272632</td>\n      <td>0.706670</td>\n      <td>-0.675277</td>\n      <td>0.369738</td>\n      <td>-0.274068</td>\n      <td>-0.462638</td>\n      <td>-0.466384</td>\n      <td>-0.209352</td>\n      <td>-0.783334</td>\n      <td>-0.467451</td>\n      <td>-0.108760</td>\n      <td>0.803386</td>\n      <td>-0.464383</td>\n      <td>0.019749</td>\n      <td>-0.462638</td>\n      <td>-0.443034</td>\n      <td>0.389688</td>\n      <td>-0.734271</td>\n      <td>-0.657172</td>\n      <td>0.494761</td>\n      <td>-0.346837</td>\n      <td>-0.123037</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76889</th>\n      <td>-0.690620</td>\n      <td>0.644627</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.064676</td>\n      <td>-0.604371</td>\n      <td>1.771012</td>\n      <td>-0.826214</td>\n      <td>-0.853646</td>\n      <td>-1.525174</td>\n      <td>-0.806854</td>\n      <td>-1.030321</td>\n      <td>-0.850105</td>\n      <td>-0.520813</td>\n      <td>1.154986</td>\n      <td>-1.151990</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.076504</td>\n      <td>1.782355</td>\n      <td>-0.347738</td>\n      <td>0.065109</td>\n      <td>1.862689</td>\n      <td>-0.838800</td>\n      <td>0.264378</td>\n      <td>1.890670</td>\n      <td>0.509522</td>\n      <td>0.600523</td>\n      <td>2.019325</td>\n      <td>-0.347738</td>\n      <td>2.482332</td>\n      <td>1.956218</td>\n      <td>-0.317879</td>\n      <td>-0.206623</td>\n      <td>-1.289419</td>\n      <td>-0.263619</td>\n      <td>0.330423</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1873 rows  92 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neurons_base = 16\ndropout_rate = 0.05\n# n_b=8 was ok with small overfit.\n# n_b=32 starts clearly overfitting. \n# 128 fits clearly slower than 64 and becomes somewhat unstable. regularization could make it work, but i see no reason to go wider.\n# 64 seems to have nice balance of flexibility and runtime, but its variance may be too large. dropout makes variance vene worse.\n# 6 hidden layers is probably most this architecture can hold\n\n# in this framework the optimal model seems to have width of 16 or 32, somehow regularized. try l1/l2?\n# w32 can take at most 0.03 dropout.\n# w16 looks good w/o dropout.\n\n# more general point:\n# main drawback of dropout is in incresing variance\n# for textbook problems with high s/n ratio (e.g., mnist) this may be ok.\n# for application like this with very low s/n ratio dropout may be a bad idea.\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*32, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train.shape[1:]),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*16, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    Dense(1)])\n\nprint(model_snn.count_params())","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:45:30.311529Z","iopub.execute_input":"2022-09-06T00:45:30.312230Z","iopub.status.idle":"2022-09-06T00:45:30.385460Z","shell.execute_reply.started":"2022-09-06T00:45:30.312193Z","shell.execute_reply":"2022-09-06T00:45:30.384376Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"222721\n","output_type":"stream"}]},{"cell_type":"code","source":"neurons_base = 4\ndropout_rate = 0.01\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train.shape[1:]),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    Dense(1)])\n\nprint(model_snn.count_params())\n\n# similar problem as before: model seems ok in terms of flexibility and variance, but adding dropout breaks it before i can fix overfitting.\n# the solution is to either use smaller models or to use laternative regularizers (which do not increase variance.)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T19:49:53.032114Z","iopub.execute_input":"2022-09-06T19:49:53.032709Z","iopub.status.idle":"2022-09-06T19:49:53.094099Z","shell.execute_reply.started":"2022-09-06T19:49:53.032673Z","shell.execute_reply":"2022-09-06T19:49:53.093029Z"},"trusted":true},"execution_count":104,"outputs":[{"name":"stdout","text":"3681\n","output_type":"stream"}]},{"cell_type":"code","source":"neurons_base = 32\nl2_reg_rate = 0.1\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn.count_params())","metadata":{"execution":{"iopub.status.busy":"2022-09-06T20:37:08.249743Z","iopub.execute_input":"2022-09-06T20:37:08.250122Z","iopub.status.idle":"2022-09-06T20:37:08.296943Z","shell.execute_reply.started":"2022-09-06T20:37:08.250090Z","shell.execute_reply":"2022-09-06T20:37:08.295926Z"},"trusted":true},"execution_count":180,"outputs":[{"name":"stdout","text":"67073\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping50 = EarlyStopping(patience=50, restore_best_weights=True)\ntime1 = time.time()\noptimizer_adam = tf.keras.optimizers.Adam()\nmodel_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_snn.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=2, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nprint([r2_score(y_train, model_snn.predict(X_train)), \n       r2_score(y_val, model_snn.predict(X_val)),\n       r2_score(y_test, model_snn.predict(X_test))])\nprint(time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T20:37:08.440947Z","iopub.execute_input":"2022-09-06T20:37:08.441836Z","iopub.status.idle":"2022-09-06T20:37:26.731966Z","shell.execute_reply.started":"2022-09-06T20:37:08.441794Z","shell.execute_reply":"2022-09-06T20:37:26.731051Z"},"trusted":true},"execution_count":181,"outputs":[{"name":"stdout","text":"Epoch 1/1000\n30/30 - 1s - loss: 166.2244 - mean_squared_error: 123.1386 - val_loss: 172.7545 - val_mean_squared_error: 134.9378\nEpoch 2/1000\n30/30 - 0s - loss: 154.6469 - mean_squared_error: 120.8503 - val_loss: 160.1106 - val_mean_squared_error: 130.1437\nEpoch 3/1000\n30/30 - 0s - loss: 146.7836 - mean_squared_error: 119.6482 - val_loss: 156.0668 - val_mean_squared_error: 131.6753\nEpoch 4/1000\n30/30 - 0s - loss: 141.4286 - mean_squared_error: 119.0860 - val_loss: 152.1978 - val_mean_squared_error: 131.8606\nEpoch 5/1000\n30/30 - 0s - loss: 137.7179 - mean_squared_error: 118.9093 - val_loss: 146.2824 - val_mean_squared_error: 128.9557\nEpoch 6/1000\n30/30 - 0s - loss: 135.0744 - mean_squared_error: 118.9021 - val_loss: 143.2238 - val_mean_squared_error: 128.1778\nEpoch 7/1000\n30/30 - 0s - loss: 132.3937 - mean_squared_error: 118.2304 - val_loss: 142.2088 - val_mean_squared_error: 128.9362\nEpoch 8/1000\n30/30 - 0s - loss: 130.7082 - mean_squared_error: 118.1584 - val_loss: 139.0246 - val_mean_squared_error: 127.1990\nEpoch 9/1000\n30/30 - 0s - loss: 129.5851 - mean_squared_error: 118.3260 - val_loss: 139.3333 - val_mean_squared_error: 128.6131\nEpoch 10/1000\n30/30 - 0s - loss: 128.1951 - mean_squared_error: 117.9527 - val_loss: 139.2288 - val_mean_squared_error: 129.4922\nEpoch 11/1000\n30/30 - 0s - loss: 127.3558 - mean_squared_error: 117.9941 - val_loss: 137.3209 - val_mean_squared_error: 128.3421\nEpoch 12/1000\n30/30 - 0s - loss: 126.3858 - mean_squared_error: 117.7538 - val_loss: 140.6836 - val_mean_squared_error: 132.3965\nEpoch 13/1000\n30/30 - 0s - loss: 125.7892 - mean_squared_error: 117.7944 - val_loss: 143.5469 - val_mean_squared_error: 135.8457\nEpoch 14/1000\n30/30 - 0s - loss: 125.3360 - mean_squared_error: 117.8731 - val_loss: 135.3569 - val_mean_squared_error: 128.1133\nEpoch 15/1000\n30/30 - 0s - loss: 124.3744 - mean_squared_error: 117.3514 - val_loss: 137.9427 - val_mean_squared_error: 131.1591\nEpoch 16/1000\n30/30 - 0s - loss: 123.8558 - mean_squared_error: 117.2556 - val_loss: 135.2585 - val_mean_squared_error: 128.8439\nEpoch 17/1000\n30/30 - 0s - loss: 123.5630 - mean_squared_error: 117.3049 - val_loss: 136.5573 - val_mean_squared_error: 130.4766\nEpoch 18/1000\n30/30 - 0s - loss: 123.6021 - mean_squared_error: 117.6656 - val_loss: 140.0476 - val_mean_squared_error: 134.2591\nEpoch 19/1000\n30/30 - 0s - loss: 123.1266 - mean_squared_error: 117.4447 - val_loss: 134.0182 - val_mean_squared_error: 128.4670\nEpoch 20/1000\n30/30 - 0s - loss: 122.7385 - mean_squared_error: 117.3068 - val_loss: 134.3055 - val_mean_squared_error: 128.9965\nEpoch 21/1000\n30/30 - 0s - loss: 122.9458 - mean_squared_error: 117.7340 - val_loss: 132.9016 - val_mean_squared_error: 127.7888\nEpoch 22/1000\n30/30 - 0s - loss: 122.3879 - mean_squared_error: 117.3572 - val_loss: 135.8598 - val_mean_squared_error: 130.9268\nEpoch 23/1000\n30/30 - 0s - loss: 121.7266 - mean_squared_error: 116.8680 - val_loss: 133.8875 - val_mean_squared_error: 129.1260\nEpoch 24/1000\n30/30 - 0s - loss: 121.6902 - mean_squared_error: 117.0180 - val_loss: 136.5695 - val_mean_squared_error: 131.9946\nEpoch 25/1000\n30/30 - 0s - loss: 121.8013 - mean_squared_error: 117.2856 - val_loss: 140.5348 - val_mean_squared_error: 136.0887\nEpoch 26/1000\n30/30 - 0s - loss: 121.2210 - mean_squared_error: 116.8163 - val_loss: 133.3471 - val_mean_squared_error: 129.0056\nEpoch 27/1000\n30/30 - 0s - loss: 120.8802 - mean_squared_error: 116.6135 - val_loss: 130.9997 - val_mean_squared_error: 126.7947\nEpoch 28/1000\n30/30 - 0s - loss: 120.5102 - mean_squared_error: 116.3445 - val_loss: 136.9414 - val_mean_squared_error: 132.8383\nEpoch 29/1000\n30/30 - 0s - loss: 120.7096 - mean_squared_error: 116.6550 - val_loss: 140.5391 - val_mean_squared_error: 136.5454\nEpoch 30/1000\n30/30 - 0s - loss: 120.6917 - mean_squared_error: 116.7303 - val_loss: 132.7732 - val_mean_squared_error: 128.8637\nEpoch 31/1000\n30/30 - 0s - loss: 120.5922 - mean_squared_error: 116.7282 - val_loss: 130.6431 - val_mean_squared_error: 126.8077\nEpoch 32/1000\n30/30 - 0s - loss: 120.6284 - mean_squared_error: 116.8355 - val_loss: 133.0226 - val_mean_squared_error: 129.2768\nEpoch 33/1000\n30/30 - 0s - loss: 120.3240 - mean_squared_error: 116.5978 - val_loss: 133.1191 - val_mean_squared_error: 129.4292\nEpoch 34/1000\n30/30 - 0s - loss: 120.2142 - mean_squared_error: 116.5594 - val_loss: 135.6090 - val_mean_squared_error: 131.9758\nEpoch 35/1000\n30/30 - 0s - loss: 120.4607 - mean_squared_error: 116.8418 - val_loss: 134.3644 - val_mean_squared_error: 130.7958\nEpoch 36/1000\n30/30 - 0s - loss: 120.2359 - mean_squared_error: 116.6990 - val_loss: 131.1594 - val_mean_squared_error: 127.6362\nEpoch 37/1000\n30/30 - 0s - loss: 119.6033 - mean_squared_error: 116.1084 - val_loss: 134.2946 - val_mean_squared_error: 130.8350\nEpoch 38/1000\n30/30 - 0s - loss: 119.6905 - mean_squared_error: 116.2547 - val_loss: 131.2778 - val_mean_squared_error: 127.8687\nEpoch 39/1000\n30/30 - 0s - loss: 119.7331 - mean_squared_error: 116.3433 - val_loss: 131.3425 - val_mean_squared_error: 127.9903\nEpoch 40/1000\n30/30 - 0s - loss: 119.6502 - mean_squared_error: 116.3131 - val_loss: 131.3050 - val_mean_squared_error: 127.9887\nEpoch 41/1000\n30/30 - 0s - loss: 119.6303 - mean_squared_error: 116.3357 - val_loss: 130.4228 - val_mean_squared_error: 127.1341\nEpoch 42/1000\n30/30 - 0s - loss: 119.1629 - mean_squared_error: 115.8878 - val_loss: 134.1681 - val_mean_squared_error: 130.9164\nEpoch 43/1000\n30/30 - 0s - loss: 119.5824 - mean_squared_error: 116.3487 - val_loss: 134.5378 - val_mean_squared_error: 131.3283\nEpoch 44/1000\n30/30 - 0s - loss: 119.2241 - mean_squared_error: 116.0257 - val_loss: 132.7228 - val_mean_squared_error: 129.5284\nEpoch 45/1000\n30/30 - 0s - loss: 118.5445 - mean_squared_error: 115.3708 - val_loss: 134.1010 - val_mean_squared_error: 130.9522\nEpoch 46/1000\n30/30 - 0s - loss: 118.7267 - mean_squared_error: 115.5942 - val_loss: 130.9152 - val_mean_squared_error: 127.7914\nEpoch 47/1000\n30/30 - 0s - loss: 119.0664 - mean_squared_error: 115.9581 - val_loss: 130.4132 - val_mean_squared_error: 127.3367\nEpoch 48/1000\n30/30 - 0s - loss: 118.5967 - mean_squared_error: 115.5089 - val_loss: 132.5721 - val_mean_squared_error: 129.4907\nEpoch 49/1000\n30/30 - 0s - loss: 118.4754 - mean_squared_error: 115.4124 - val_loss: 133.9466 - val_mean_squared_error: 130.8966\nEpoch 50/1000\n30/30 - 0s - loss: 118.8938 - mean_squared_error: 115.8670 - val_loss: 132.7570 - val_mean_squared_error: 129.7296\nEpoch 51/1000\n30/30 - 0s - loss: 118.8666 - mean_squared_error: 115.8397 - val_loss: 133.2933 - val_mean_squared_error: 130.2792\nEpoch 52/1000\n30/30 - 0s - loss: 118.3864 - mean_squared_error: 115.3747 - val_loss: 133.1740 - val_mean_squared_error: 130.1673\nEpoch 53/1000\n30/30 - 0s - loss: 118.0551 - mean_squared_error: 115.0647 - val_loss: 134.8406 - val_mean_squared_error: 131.8570\nEpoch 54/1000\n30/30 - 0s - loss: 118.3201 - mean_squared_error: 115.3441 - val_loss: 136.6063 - val_mean_squared_error: 133.6261\nEpoch 55/1000\n30/30 - 0s - loss: 118.2083 - mean_squared_error: 115.2426 - val_loss: 129.3979 - val_mean_squared_error: 126.4370\nEpoch 56/1000\n30/30 - 0s - loss: 117.9671 - mean_squared_error: 115.0100 - val_loss: 131.5668 - val_mean_squared_error: 128.6131\nEpoch 57/1000\n30/30 - 0s - loss: 118.3479 - mean_squared_error: 115.4167 - val_loss: 132.1465 - val_mean_squared_error: 129.2191\nEpoch 58/1000\n30/30 - 0s - loss: 118.1964 - mean_squared_error: 115.2717 - val_loss: 132.5522 - val_mean_squared_error: 129.6314\nEpoch 59/1000\n30/30 - 0s - loss: 118.1303 - mean_squared_error: 115.2180 - val_loss: 132.1080 - val_mean_squared_error: 129.1821\nEpoch 60/1000\n30/30 - 0s - loss: 117.9735 - mean_squared_error: 115.0506 - val_loss: 135.3322 - val_mean_squared_error: 132.4249\nEpoch 61/1000\n30/30 - 0s - loss: 117.5410 - mean_squared_error: 114.6325 - val_loss: 136.7475 - val_mean_squared_error: 133.8427\nEpoch 62/1000\n30/30 - 0s - loss: 117.6071 - mean_squared_error: 114.6973 - val_loss: 134.6751 - val_mean_squared_error: 131.7677\nEpoch 63/1000\n30/30 - 0s - loss: 117.7768 - mean_squared_error: 114.8631 - val_loss: 132.3927 - val_mean_squared_error: 129.4959\nEpoch 64/1000\n30/30 - 0s - loss: 117.9736 - mean_squared_error: 115.0775 - val_loss: 134.8552 - val_mean_squared_error: 131.9679\nEpoch 65/1000\n30/30 - 0s - loss: 117.5954 - mean_squared_error: 114.7054 - val_loss: 134.8492 - val_mean_squared_error: 131.9606\nEpoch 66/1000\n30/30 - 0s - loss: 117.5205 - mean_squared_error: 114.6107 - val_loss: 134.7242 - val_mean_squared_error: 131.8053\nEpoch 67/1000\n30/30 - 0s - loss: 117.4779 - mean_squared_error: 114.5650 - val_loss: 130.6418 - val_mean_squared_error: 127.7264\nEpoch 68/1000\n30/30 - 0s - loss: 117.3613 - mean_squared_error: 114.4430 - val_loss: 136.4679 - val_mean_squared_error: 133.5778\nEpoch 69/1000\n30/30 - 0s - loss: 118.0612 - mean_squared_error: 115.1744 - val_loss: 140.0005 - val_mean_squared_error: 137.1393\nEpoch 70/1000\n30/30 - 0s - loss: 117.4664 - mean_squared_error: 114.5903 - val_loss: 130.3718 - val_mean_squared_error: 127.4851\nEpoch 71/1000\n30/30 - 0s - loss: 117.4265 - mean_squared_error: 114.5452 - val_loss: 132.5691 - val_mean_squared_error: 129.6912\nEpoch 72/1000\n30/30 - 0s - loss: 117.2619 - mean_squared_error: 114.3873 - val_loss: 139.8367 - val_mean_squared_error: 136.9597\nEpoch 73/1000\n30/30 - 0s - loss: 117.7063 - mean_squared_error: 114.8158 - val_loss: 130.0357 - val_mean_squared_error: 127.1332\nEpoch 74/1000\n30/30 - 0s - loss: 117.4962 - mean_squared_error: 114.5978 - val_loss: 134.7018 - val_mean_squared_error: 131.8080\nEpoch 75/1000\n30/30 - 0s - loss: 117.1478 - mean_squared_error: 114.2531 - val_loss: 131.1367 - val_mean_squared_error: 128.2629\nEpoch 76/1000\n30/30 - 0s - loss: 117.3164 - mean_squared_error: 114.4349 - val_loss: 137.2722 - val_mean_squared_error: 134.3924\nEpoch 77/1000\n30/30 - 0s - loss: 117.0142 - mean_squared_error: 114.1325 - val_loss: 132.7592 - val_mean_squared_error: 129.8838\nEpoch 78/1000\n30/30 - 0s - loss: 116.9591 - mean_squared_error: 114.0789 - val_loss: 134.6820 - val_mean_squared_error: 131.8127\nEpoch 79/1000\n30/30 - 0s - loss: 117.2218 - mean_squared_error: 114.3552 - val_loss: 130.1118 - val_mean_squared_error: 127.2668\nEpoch 80/1000\n30/30 - 0s - loss: 116.7799 - mean_squared_error: 113.9208 - val_loss: 130.5665 - val_mean_squared_error: 127.7030\nEpoch 81/1000\n30/30 - 0s - loss: 116.6845 - mean_squared_error: 113.8125 - val_loss: 135.0560 - val_mean_squared_error: 132.1777\nEpoch 82/1000\n30/30 - 0s - loss: 116.6233 - mean_squared_error: 113.7502 - val_loss: 129.0400 - val_mean_squared_error: 126.1734\nEpoch 83/1000\n30/30 - 0s - loss: 116.8504 - mean_squared_error: 113.9794 - val_loss: 131.8682 - val_mean_squared_error: 128.9997\nEpoch 84/1000\n30/30 - 0s - loss: 116.2634 - mean_squared_error: 113.3852 - val_loss: 133.2243 - val_mean_squared_error: 130.3405\nEpoch 85/1000\n30/30 - 0s - loss: 116.6571 - mean_squared_error: 113.7752 - val_loss: 135.2907 - val_mean_squared_error: 132.4231\nEpoch 86/1000\n30/30 - 0s - loss: 116.0702 - mean_squared_error: 113.1740 - val_loss: 130.5357 - val_mean_squared_error: 127.6243\nEpoch 87/1000\n30/30 - 0s - loss: 116.8620 - mean_squared_error: 113.9746 - val_loss: 136.2567 - val_mean_squared_error: 133.3684\nEpoch 88/1000\n30/30 - 0s - loss: 116.6164 - mean_squared_error: 113.7156 - val_loss: 131.3129 - val_mean_squared_error: 128.4245\nEpoch 89/1000\n30/30 - 0s - loss: 116.0258 - mean_squared_error: 113.1287 - val_loss: 134.8427 - val_mean_squared_error: 131.9524\nEpoch 90/1000\n30/30 - 0s - loss: 115.9884 - mean_squared_error: 113.0758 - val_loss: 135.7714 - val_mean_squared_error: 132.8486\nEpoch 91/1000\n30/30 - 0s - loss: 115.6993 - mean_squared_error: 112.7596 - val_loss: 133.7947 - val_mean_squared_error: 130.8548\nEpoch 92/1000\n30/30 - 0s - loss: 116.0385 - mean_squared_error: 113.1028 - val_loss: 133.1410 - val_mean_squared_error: 130.2059\nEpoch 93/1000\n30/30 - 0s - loss: 116.6249 - mean_squared_error: 113.7030 - val_loss: 136.8950 - val_mean_squared_error: 133.9698\nEpoch 94/1000\n30/30 - 0s - loss: 116.0703 - mean_squared_error: 113.1156 - val_loss: 133.4918 - val_mean_squared_error: 130.5497\nEpoch 95/1000\n30/30 - 0s - loss: 115.8196 - mean_squared_error: 112.8765 - val_loss: 133.9499 - val_mean_squared_error: 131.0079\nEpoch 96/1000\n30/30 - 0s - loss: 115.8105 - mean_squared_error: 112.8667 - val_loss: 133.3794 - val_mean_squared_error: 130.4382\nEpoch 97/1000\n30/30 - 0s - loss: 115.6041 - mean_squared_error: 112.6521 - val_loss: 134.0698 - val_mean_squared_error: 131.1097\nEpoch 98/1000\n30/30 - 0s - loss: 116.1976 - mean_squared_error: 113.2362 - val_loss: 133.1839 - val_mean_squared_error: 130.2334\nEpoch 99/1000\n30/30 - 0s - loss: 116.5717 - mean_squared_error: 113.6229 - val_loss: 131.1653 - val_mean_squared_error: 128.2043\nEpoch 100/1000\n30/30 - 0s - loss: 116.0424 - mean_squared_error: 113.0724 - val_loss: 134.9317 - val_mean_squared_error: 131.9756\nEpoch 101/1000\n30/30 - 0s - loss: 115.8083 - mean_squared_error: 112.8521 - val_loss: 139.1764 - val_mean_squared_error: 136.2362\nEpoch 102/1000\n30/30 - 0s - loss: 115.9686 - mean_squared_error: 113.0240 - val_loss: 135.4797 - val_mean_squared_error: 132.5223\nEpoch 103/1000\n30/30 - 0s - loss: 115.5907 - mean_squared_error: 112.6226 - val_loss: 132.4556 - val_mean_squared_error: 129.4870\nEpoch 104/1000\n30/30 - 0s - loss: 115.6327 - mean_squared_error: 112.6583 - val_loss: 133.6206 - val_mean_squared_error: 130.6325\nEpoch 105/1000\n30/30 - 0s - loss: 115.6427 - mean_squared_error: 112.6458 - val_loss: 131.3275 - val_mean_squared_error: 128.3279\nEpoch 106/1000\n30/30 - 0s - loss: 115.9065 - mean_squared_error: 112.9078 - val_loss: 132.5925 - val_mean_squared_error: 129.5902\nEpoch 107/1000\n30/30 - 0s - loss: 115.4508 - mean_squared_error: 112.4550 - val_loss: 137.3324 - val_mean_squared_error: 134.3409\nEpoch 108/1000\n30/30 - 0s - loss: 115.9112 - mean_squared_error: 112.9207 - val_loss: 132.6994 - val_mean_squared_error: 129.7184\nEpoch 109/1000\n30/30 - 0s - loss: 115.3458 - mean_squared_error: 112.3366 - val_loss: 133.0289 - val_mean_squared_error: 130.0166\nEpoch 110/1000\n30/30 - 0s - loss: 115.1077 - mean_squared_error: 112.0932 - val_loss: 135.7531 - val_mean_squared_error: 132.7396\nEpoch 111/1000\n30/30 - 0s - loss: 115.1867 - mean_squared_error: 112.1732 - val_loss: 133.1764 - val_mean_squared_error: 130.1581\nEpoch 112/1000\n30/30 - 0s - loss: 115.3198 - mean_squared_error: 112.2996 - val_loss: 135.4929 - val_mean_squared_error: 132.4752\nEpoch 113/1000\n30/30 - 0s - loss: 115.2983 - mean_squared_error: 112.2769 - val_loss: 131.8351 - val_mean_squared_error: 128.8115\nEpoch 114/1000\n30/30 - 0s - loss: 114.9725 - mean_squared_error: 111.9431 - val_loss: 131.5454 - val_mean_squared_error: 128.5226\nEpoch 115/1000\n30/30 - 0s - loss: 115.6479 - mean_squared_error: 112.6231 - val_loss: 134.9689 - val_mean_squared_error: 131.9533\nEpoch 116/1000\n30/30 - 0s - loss: 115.1852 - mean_squared_error: 112.1585 - val_loss: 132.6909 - val_mean_squared_error: 129.6512\nEpoch 117/1000\n30/30 - 0s - loss: 115.5446 - mean_squared_error: 112.5098 - val_loss: 140.2048 - val_mean_squared_error: 137.1833\nEpoch 118/1000\n30/30 - 0s - loss: 114.6361 - mean_squared_error: 111.5953 - val_loss: 135.1901 - val_mean_squared_error: 132.1393\nEpoch 119/1000\n30/30 - 0s - loss: 114.6689 - mean_squared_error: 111.6062 - val_loss: 134.2323 - val_mean_squared_error: 131.1695\nEpoch 120/1000\n30/30 - 0s - loss: 115.4035 - mean_squared_error: 112.3390 - val_loss: 132.3616 - val_mean_squared_error: 129.3028\nEpoch 121/1000\n30/30 - 0s - loss: 114.5709 - mean_squared_error: 111.5044 - val_loss: 132.5025 - val_mean_squared_error: 129.4250\nEpoch 122/1000\n30/30 - 0s - loss: 116.1135 - mean_squared_error: 113.0387 - val_loss: 142.5371 - val_mean_squared_error: 139.4827\nEpoch 123/1000\n30/30 - 0s - loss: 116.9480 - mean_squared_error: 113.8801 - val_loss: 138.2028 - val_mean_squared_error: 135.1124\nEpoch 124/1000\n30/30 - 0s - loss: 115.2332 - mean_squared_error: 112.1145 - val_loss: 137.1764 - val_mean_squared_error: 134.0697\nEpoch 125/1000\n30/30 - 0s - loss: 115.2919 - mean_squared_error: 112.2015 - val_loss: 134.7237 - val_mean_squared_error: 131.6454\nEpoch 126/1000\n30/30 - 0s - loss: 115.0373 - mean_squared_error: 111.9631 - val_loss: 139.5605 - val_mean_squared_error: 136.4865\nEpoch 127/1000\n30/30 - 0s - loss: 115.7412 - mean_squared_error: 112.6661 - val_loss: 133.1363 - val_mean_squared_error: 130.0489\nEpoch 128/1000\n30/30 - 0s - loss: 114.6661 - mean_squared_error: 111.5712 - val_loss: 136.5401 - val_mean_squared_error: 133.4469\nEpoch 129/1000\n30/30 - 0s - loss: 115.3232 - mean_squared_error: 112.2381 - val_loss: 132.6221 - val_mean_squared_error: 129.5235\nEpoch 130/1000\n30/30 - 0s - loss: 115.1321 - mean_squared_error: 112.0289 - val_loss: 134.9235 - val_mean_squared_error: 131.8170\nEpoch 131/1000\n30/30 - 0s - loss: 114.8586 - mean_squared_error: 111.7470 - val_loss: 137.0663 - val_mean_squared_error: 133.9537\nEpoch 132/1000\n30/30 - 0s - loss: 114.2850 - mean_squared_error: 111.1488 - val_loss: 137.1393 - val_mean_squared_error: 134.0069\nMinimum Validation Loss: 129.0400\n[0.09330294178167986, 0.14672701808106892, 0.12498357939701565]\n18.135513305664062\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"X_train.skew()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T01:58:14.615470Z","iopub.execute_input":"2022-09-06T01:58:14.616043Z","iopub.status.idle":"2022-09-06T01:58:14.787794Z","shell.execute_reply.started":"2022-09-06T01:58:14.615998Z","shell.execute_reply":"2022-09-06T01:58:14.786906Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"num__mom482       2.014820\nnum__mom242       1.343044\nnum__bm           0.122894\nnum__op          -0.848916\nnum__gp           0.607772\n                   ...    \ncat__ind_45.0    19.090080\ncat__ind_46.0    79.436081\ncat__ind_47.0    12.898892\ncat__ind_48.0    20.318332\ncat__ind_49.0    13.162803\nLength: 92, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"X_val","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.150674Z","iopub.status.idle":"2022-09-06T00:36:53.151429Z","shell.execute_reply.started":"2022-09-06T00:36:53.151166Z","shell.execute_reply":"2022-09-06T00:36:53.151191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# on nns:\n# - try classic regularizers (l1, l2 etc)\n# - try different architecture (not snnn)\n# classic architecture:\n# He initialization, elu activation, batch norm, l2 reg, adam.\n\n# - try exotic architecture, e.g., wide'n'deep\n# \n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# classic architecture:\n\nneurons_base = 32\nl2_reg_rate = 0.5\nhe_init = tf.keras.initializers.HeNormal()\n\nmodel_nn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"elu\", kernel_initializer=he_init, \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.BatchNormalization(),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"elu\", kernel_initializer=he_init,\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.BatchNormalization(),    \n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"elu\", kernel_initializer=he_init,\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.BatchNormalization(),    \n    tf.keras.layers.Dense(units=neurons_base, activation=\"elu\", kernel_initializer=he_init,\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(1)])\n\nprint(model_nn.count_params())\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T20:35:05.415443Z","iopub.execute_input":"2022-09-06T20:35:05.415892Z","iopub.status.idle":"2022-09-06T20:35:05.530840Z","shell.execute_reply.started":"2022-09-06T20:35:05.415854Z","shell.execute_reply":"2022-09-06T20:35:05.529848Z"},"trusted":true},"execution_count":176,"outputs":[{"name":"stdout","text":"68865\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping50 = EarlyStopping(patience=50, restore_best_weights=True)\ntime1 = time.time()\noptimizer_adam = tf.keras.optimizers.Adam()\nmodel_nn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_nn.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=2, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nprint([r2_score(y_train, model_nn.predict(X_train)), \n       r2_score(y_val, model_nn.predict(X_val)),\n       r2_score(y_test, model_nn.predict(X_test))])\nprint(time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T20:35:05.606202Z","iopub.execute_input":"2022-09-06T20:35:05.606530Z","iopub.status.idle":"2022-09-06T20:35:50.233161Z","shell.execute_reply.started":"2022-09-06T20:35:05.606503Z","shell.execute_reply":"2022-09-06T20:35:50.232273Z"},"trusted":true},"execution_count":177,"outputs":[{"name":"stdout","text":"Epoch 1/1000\n30/30 - 1s - loss: 535.3593 - mean_squared_error: 123.5355 - val_loss: 480.1155 - val_mean_squared_error: 138.7751\nEpoch 2/1000\n30/30 - 0s - loss: 409.8263 - mean_squared_error: 121.1279 - val_loss: 379.2305 - val_mean_squared_error: 140.5430\nEpoch 3/1000\n30/30 - 0s - loss: 323.0998 - mean_squared_error: 119.8930 - val_loss: 312.2409 - val_mean_squared_error: 142.4989\nEpoch 4/1000\n30/30 - 0s - loss: 265.4126 - mean_squared_error: 119.5592 - val_loss: 264.3673 - val_mean_squared_error: 141.3278\nEpoch 5/1000\n30/30 - 0s - loss: 225.5078 - mean_squared_error: 119.2006 - val_loss: 231.8146 - val_mean_squared_error: 141.6265\nEpoch 6/1000\n30/30 - 0s - loss: 196.7393 - mean_squared_error: 118.5110 - val_loss: 210.5220 - val_mean_squared_error: 143.8578\nEpoch 7/1000\n30/30 - 0s - loss: 176.6699 - mean_squared_error: 118.5655 - val_loss: 189.9470 - val_mean_squared_error: 140.0985\nEpoch 8/1000\n30/30 - 0s - loss: 162.2228 - mean_squared_error: 118.5190 - val_loss: 178.0574 - val_mean_squared_error: 140.3195\nEpoch 9/1000\n30/30 - 0s - loss: 151.4521 - mean_squared_error: 118.1927 - val_loss: 168.2508 - val_mean_squared_error: 139.3369\nEpoch 10/1000\n30/30 - 0s - loss: 144.2276 - mean_squared_error: 118.6220 - val_loss: 163.0820 - val_mean_squared_error: 140.6693\nEpoch 11/1000\n30/30 - 0s - loss: 138.3161 - mean_squared_error: 118.3008 - val_loss: 159.7920 - val_mean_squared_error: 142.1383\nEpoch 12/1000\n30/30 - 0s - loss: 133.9299 - mean_squared_error: 118.1041 - val_loss: 156.0112 - val_mean_squared_error: 141.9567\nEpoch 13/1000\n30/30 - 0s - loss: 131.0710 - mean_squared_error: 118.2979 - val_loss: 150.6700 - val_mean_squared_error: 139.1845\nEpoch 14/1000\n30/30 - 0s - loss: 128.5625 - mean_squared_error: 118.1168 - val_loss: 148.0869 - val_mean_squared_error: 138.6272\nEpoch 15/1000\n30/30 - 0s - loss: 127.0562 - mean_squared_error: 118.3454 - val_loss: 147.7262 - val_mean_squared_error: 139.7548\nEpoch 16/1000\n30/30 - 0s - loss: 125.4572 - mean_squared_error: 118.0773 - val_loss: 145.9542 - val_mean_squared_error: 139.1619\nEpoch 17/1000\n30/30 - 0s - loss: 124.6164 - mean_squared_error: 118.2726 - val_loss: 141.0480 - val_mean_squared_error: 135.1647\nEpoch 18/1000\n30/30 - 0s - loss: 124.0707 - mean_squared_error: 118.4891 - val_loss: 138.8317 - val_mean_squared_error: 133.5670\nEpoch 19/1000\n30/30 - 0s - loss: 123.3436 - mean_squared_error: 118.3287 - val_loss: 140.0426 - val_mean_squared_error: 135.3150\nEpoch 20/1000\n30/30 - 0s - loss: 122.1883 - mean_squared_error: 117.7663 - val_loss: 141.7940 - val_mean_squared_error: 137.6849\nEpoch 21/1000\n30/30 - 0s - loss: 122.0469 - mean_squared_error: 118.1256 - val_loss: 140.9446 - val_mean_squared_error: 137.1881\nEpoch 22/1000\n30/30 - 0s - loss: 121.5261 - mean_squared_error: 117.9314 - val_loss: 137.8754 - val_mean_squared_error: 134.4489\nEpoch 23/1000\n30/30 - 0s - loss: 121.3343 - mean_squared_error: 118.0025 - val_loss: 137.5328 - val_mean_squared_error: 134.2917\nEpoch 24/1000\n30/30 - 0s - loss: 121.0361 - mean_squared_error: 117.9174 - val_loss: 137.0122 - val_mean_squared_error: 134.0187\nEpoch 25/1000\n30/30 - 0s - loss: 121.1009 - mean_squared_error: 118.1406 - val_loss: 136.6183 - val_mean_squared_error: 133.7115\nEpoch 26/1000\n30/30 - 0s - loss: 120.4836 - mean_squared_error: 117.6841 - val_loss: 132.8885 - val_mean_squared_error: 130.2176\nEpoch 27/1000\n30/30 - 0s - loss: 120.1880 - mean_squared_error: 117.6150 - val_loss: 135.6804 - val_mean_squared_error: 133.2317\nEpoch 28/1000\n30/30 - 0s - loss: 119.8043 - mean_squared_error: 117.4349 - val_loss: 136.6524 - val_mean_squared_error: 134.3332\nEpoch 29/1000\n30/30 - 0s - loss: 120.2850 - mean_squared_error: 117.9767 - val_loss: 135.1787 - val_mean_squared_error: 132.8629\nEpoch 30/1000\n30/30 - 0s - loss: 119.8896 - mean_squared_error: 117.5705 - val_loss: 135.9104 - val_mean_squared_error: 133.6380\nEpoch 31/1000\n30/30 - 0s - loss: 119.7517 - mean_squared_error: 117.5357 - val_loss: 138.4444 - val_mean_squared_error: 136.3141\nEpoch 32/1000\n30/30 - 0s - loss: 119.5088 - mean_squared_error: 117.4432 - val_loss: 133.7859 - val_mean_squared_error: 131.7840\nEpoch 33/1000\n30/30 - 0s - loss: 119.6759 - mean_squared_error: 117.6489 - val_loss: 135.6380 - val_mean_squared_error: 133.5896\nEpoch 34/1000\n30/30 - 0s - loss: 119.4415 - mean_squared_error: 117.4643 - val_loss: 128.2000 - val_mean_squared_error: 126.2595\nEpoch 35/1000\n30/30 - 0s - loss: 119.4241 - mean_squared_error: 117.4567 - val_loss: 136.7879 - val_mean_squared_error: 134.8413\nEpoch 36/1000\n30/30 - 0s - loss: 119.2316 - mean_squared_error: 117.3450 - val_loss: 134.6256 - val_mean_squared_error: 132.8058\nEpoch 37/1000\n30/30 - 0s - loss: 118.8906 - mean_squared_error: 117.0923 - val_loss: 132.5926 - val_mean_squared_error: 130.8051\nEpoch 38/1000\n30/30 - 0s - loss: 119.4689 - mean_squared_error: 117.6460 - val_loss: 130.8971 - val_mean_squared_error: 129.0472\nEpoch 39/1000\n30/30 - 0s - loss: 118.9267 - mean_squared_error: 117.1034 - val_loss: 134.5078 - val_mean_squared_error: 132.7182\nEpoch 40/1000\n30/30 - 0s - loss: 119.0926 - mean_squared_error: 117.3057 - val_loss: 133.2222 - val_mean_squared_error: 131.4347\nEpoch 41/1000\n30/30 - 0s - loss: 119.0069 - mean_squared_error: 117.2491 - val_loss: 135.2498 - val_mean_squared_error: 133.5248\nEpoch 42/1000\n30/30 - 0s - loss: 119.1515 - mean_squared_error: 117.4351 - val_loss: 132.6265 - val_mean_squared_error: 130.8616\nEpoch 43/1000\n30/30 - 0s - loss: 119.0223 - mean_squared_error: 117.2253 - val_loss: 131.5919 - val_mean_squared_error: 129.8168\nEpoch 44/1000\n30/30 - 0s - loss: 118.9645 - mean_squared_error: 117.2163 - val_loss: 133.2459 - val_mean_squared_error: 131.5013\nEpoch 45/1000\n30/30 - 0s - loss: 118.8160 - mean_squared_error: 117.0711 - val_loss: 135.1117 - val_mean_squared_error: 133.3588\nEpoch 46/1000\n30/30 - 0s - loss: 118.7918 - mean_squared_error: 117.0195 - val_loss: 138.7934 - val_mean_squared_error: 137.0798\nEpoch 47/1000\n30/30 - 0s - loss: 118.8371 - mean_squared_error: 117.1586 - val_loss: 127.6448 - val_mean_squared_error: 125.9696\nEpoch 48/1000\n30/30 - 0s - loss: 118.7605 - mean_squared_error: 117.0183 - val_loss: 133.8349 - val_mean_squared_error: 132.0747\nEpoch 49/1000\n30/30 - 0s - loss: 119.0657 - mean_squared_error: 117.3416 - val_loss: 131.2599 - val_mean_squared_error: 129.5441\nEpoch 50/1000\n30/30 - 0s - loss: 118.7141 - mean_squared_error: 116.9972 - val_loss: 130.5917 - val_mean_squared_error: 128.8818\nEpoch 51/1000\n30/30 - 0s - loss: 118.4611 - mean_squared_error: 116.7860 - val_loss: 133.0357 - val_mean_squared_error: 131.3890\nEpoch 52/1000\n30/30 - 0s - loss: 118.7269 - mean_squared_error: 117.0722 - val_loss: 132.1194 - val_mean_squared_error: 130.4516\nEpoch 53/1000\n30/30 - 0s - loss: 118.5870 - mean_squared_error: 116.8854 - val_loss: 133.8910 - val_mean_squared_error: 132.1881\nEpoch 54/1000\n30/30 - 0s - loss: 118.6509 - mean_squared_error: 116.9667 - val_loss: 135.1724 - val_mean_squared_error: 133.5087\nEpoch 55/1000\n30/30 - 0s - loss: 118.4112 - mean_squared_error: 116.7726 - val_loss: 139.5017 - val_mean_squared_error: 137.8931\nEpoch 56/1000\n30/30 - 0s - loss: 118.5968 - mean_squared_error: 116.9828 - val_loss: 129.3333 - val_mean_squared_error: 127.7272\nEpoch 57/1000\n30/30 - 0s - loss: 118.5663 - mean_squared_error: 116.9409 - val_loss: 132.2799 - val_mean_squared_error: 130.6429\nEpoch 58/1000\n30/30 - 0s - loss: 118.5113 - mean_squared_error: 116.8695 - val_loss: 130.4472 - val_mean_squared_error: 128.8197\nEpoch 59/1000\n30/30 - 0s - loss: 118.6677 - mean_squared_error: 117.0474 - val_loss: 131.6803 - val_mean_squared_error: 130.0694\nEpoch 60/1000\n30/30 - 0s - loss: 118.2780 - mean_squared_error: 116.6641 - val_loss: 129.5022 - val_mean_squared_error: 127.9056\nEpoch 61/1000\n30/30 - 0s - loss: 118.3154 - mean_squared_error: 116.7126 - val_loss: 136.6896 - val_mean_squared_error: 135.0827\nEpoch 62/1000\n30/30 - 0s - loss: 118.1732 - mean_squared_error: 116.5294 - val_loss: 133.8606 - val_mean_squared_error: 132.1613\nEpoch 63/1000\n30/30 - 0s - loss: 118.1806 - mean_squared_error: 116.5414 - val_loss: 132.5531 - val_mean_squared_error: 130.9893\nEpoch 64/1000\n30/30 - 0s - loss: 118.3760 - mean_squared_error: 116.7879 - val_loss: 133.2494 - val_mean_squared_error: 131.6329\nEpoch 65/1000\n30/30 - 0s - loss: 118.5260 - mean_squared_error: 116.9066 - val_loss: 132.6417 - val_mean_squared_error: 131.0313\nEpoch 66/1000\n30/30 - 0s - loss: 118.2295 - mean_squared_error: 116.6238 - val_loss: 133.2527 - val_mean_squared_error: 131.6536\nEpoch 67/1000\n30/30 - 0s - loss: 118.1711 - mean_squared_error: 116.5686 - val_loss: 131.1089 - val_mean_squared_error: 129.4965\nEpoch 68/1000\n30/30 - 0s - loss: 118.1388 - mean_squared_error: 116.5374 - val_loss: 129.8322 - val_mean_squared_error: 128.2349\nEpoch 69/1000\n30/30 - 0s - loss: 118.0399 - mean_squared_error: 116.4546 - val_loss: 131.6279 - val_mean_squared_error: 130.0603\nEpoch 70/1000\n30/30 - 0s - loss: 117.9065 - mean_squared_error: 116.3166 - val_loss: 136.2862 - val_mean_squared_error: 134.6541\nEpoch 71/1000\n30/30 - 0s - loss: 118.1590 - mean_squared_error: 116.4561 - val_loss: 137.5851 - val_mean_squared_error: 135.8599\nEpoch 72/1000\n30/30 - 0s - loss: 118.2440 - mean_squared_error: 116.5569 - val_loss: 131.6544 - val_mean_squared_error: 129.9942\nEpoch 73/1000\n30/30 - 0s - loss: 118.1488 - mean_squared_error: 116.5078 - val_loss: 137.5712 - val_mean_squared_error: 135.9328\nEpoch 74/1000\n30/30 - 0s - loss: 118.2488 - mean_squared_error: 116.5690 - val_loss: 133.9228 - val_mean_squared_error: 132.2470\nEpoch 75/1000\n30/30 - 0s - loss: 118.2059 - mean_squared_error: 116.5544 - val_loss: 133.5157 - val_mean_squared_error: 131.8486\nEpoch 76/1000\n30/30 - 0s - loss: 118.1598 - mean_squared_error: 116.4086 - val_loss: 129.5119 - val_mean_squared_error: 127.7368\nEpoch 77/1000\n30/30 - 0s - loss: 118.0016 - mean_squared_error: 116.2817 - val_loss: 133.4431 - val_mean_squared_error: 131.7875\nEpoch 78/1000\n30/30 - 0s - loss: 117.7753 - mean_squared_error: 116.1158 - val_loss: 127.7539 - val_mean_squared_error: 126.0963\nEpoch 79/1000\n30/30 - 0s - loss: 118.0780 - mean_squared_error: 116.4298 - val_loss: 133.7467 - val_mean_squared_error: 132.0779\nEpoch 80/1000\n30/30 - 0s - loss: 118.0124 - mean_squared_error: 116.3213 - val_loss: 136.1515 - val_mean_squared_error: 134.4683\nEpoch 81/1000\n30/30 - 0s - loss: 117.8243 - mean_squared_error: 116.1447 - val_loss: 130.6570 - val_mean_squared_error: 129.0025\nEpoch 82/1000\n30/30 - 0s - loss: 117.9310 - mean_squared_error: 116.2691 - val_loss: 127.5125 - val_mean_squared_error: 125.8332\nEpoch 83/1000\n30/30 - 0s - loss: 117.7754 - mean_squared_error: 116.0702 - val_loss: 132.5657 - val_mean_squared_error: 130.8538\nEpoch 84/1000\n30/30 - 0s - loss: 117.8753 - mean_squared_error: 116.1785 - val_loss: 131.6119 - val_mean_squared_error: 129.9395\nEpoch 85/1000\n30/30 - 0s - loss: 118.3223 - mean_squared_error: 116.6435 - val_loss: 131.4847 - val_mean_squared_error: 129.7316\nEpoch 86/1000\n30/30 - 0s - loss: 117.9497 - mean_squared_error: 116.1874 - val_loss: 130.3732 - val_mean_squared_error: 128.6414\nEpoch 87/1000\n30/30 - 0s - loss: 117.8523 - mean_squared_error: 116.1401 - val_loss: 135.0059 - val_mean_squared_error: 133.3169\nEpoch 88/1000\n30/30 - 0s - loss: 118.1452 - mean_squared_error: 116.4201 - val_loss: 128.6828 - val_mean_squared_error: 126.9409\nEpoch 89/1000\n30/30 - 0s - loss: 118.3469 - mean_squared_error: 116.6377 - val_loss: 128.9850 - val_mean_squared_error: 127.2722\nEpoch 90/1000\n30/30 - 0s - loss: 117.9391 - mean_squared_error: 116.1889 - val_loss: 130.0042 - val_mean_squared_error: 128.2459\nEpoch 91/1000\n30/30 - 0s - loss: 117.6178 - mean_squared_error: 115.8969 - val_loss: 128.5188 - val_mean_squared_error: 126.8532\nEpoch 92/1000\n30/30 - 0s - loss: 117.9486 - mean_squared_error: 116.2660 - val_loss: 131.0737 - val_mean_squared_error: 129.3220\nEpoch 93/1000\n30/30 - 0s - loss: 117.9298 - mean_squared_error: 116.1515 - val_loss: 134.6504 - val_mean_squared_error: 132.9116\nEpoch 94/1000\n30/30 - 0s - loss: 117.9399 - mean_squared_error: 116.2307 - val_loss: 129.9795 - val_mean_squared_error: 128.3086\nEpoch 95/1000\n30/30 - 0s - loss: 118.2021 - mean_squared_error: 116.4943 - val_loss: 129.3738 - val_mean_squared_error: 127.5921\nEpoch 96/1000\n30/30 - 0s - loss: 118.0287 - mean_squared_error: 116.2637 - val_loss: 133.2436 - val_mean_squared_error: 131.5286\nEpoch 97/1000\n30/30 - 0s - loss: 117.7861 - mean_squared_error: 116.1059 - val_loss: 128.9559 - val_mean_squared_error: 127.2524\nEpoch 98/1000\n30/30 - 0s - loss: 117.9329 - mean_squared_error: 116.2204 - val_loss: 131.6531 - val_mean_squared_error: 129.9065\nEpoch 99/1000\n30/30 - 0s - loss: 118.1088 - mean_squared_error: 116.3400 - val_loss: 127.5847 - val_mean_squared_error: 125.8441\nEpoch 100/1000\n30/30 - 0s - loss: 117.9751 - mean_squared_error: 116.2066 - val_loss: 127.6967 - val_mean_squared_error: 125.9401\nEpoch 101/1000\n30/30 - 0s - loss: 117.5664 - mean_squared_error: 115.8136 - val_loss: 130.4908 - val_mean_squared_error: 128.7158\nEpoch 102/1000\n30/30 - 0s - loss: 117.6239 - mean_squared_error: 115.9044 - val_loss: 125.8136 - val_mean_squared_error: 124.1537\nEpoch 103/1000\n30/30 - 0s - loss: 117.5868 - mean_squared_error: 115.9084 - val_loss: 129.2929 - val_mean_squared_error: 127.6108\nEpoch 104/1000\n30/30 - 0s - loss: 117.4603 - mean_squared_error: 115.8034 - val_loss: 130.4963 - val_mean_squared_error: 128.8238\nEpoch 105/1000\n30/30 - 0s - loss: 117.7528 - mean_squared_error: 116.0444 - val_loss: 131.9663 - val_mean_squared_error: 130.2613\nEpoch 106/1000\n30/30 - 0s - loss: 117.3751 - mean_squared_error: 115.6753 - val_loss: 131.3710 - val_mean_squared_error: 129.6842\nEpoch 107/1000\n30/30 - 0s - loss: 117.6638 - mean_squared_error: 115.9285 - val_loss: 127.0669 - val_mean_squared_error: 125.3280\nEpoch 108/1000\n30/30 - 0s - loss: 117.5838 - mean_squared_error: 115.8457 - val_loss: 140.1741 - val_mean_squared_error: 138.4136\nEpoch 109/1000\n30/30 - 0s - loss: 117.9755 - mean_squared_error: 116.1996 - val_loss: 131.4174 - val_mean_squared_error: 129.6059\nEpoch 110/1000\n30/30 - 0s - loss: 117.6169 - mean_squared_error: 115.8163 - val_loss: 130.4001 - val_mean_squared_error: 128.6159\nEpoch 111/1000\n30/30 - 0s - loss: 117.5439 - mean_squared_error: 115.7771 - val_loss: 131.5115 - val_mean_squared_error: 129.7106\nEpoch 112/1000\n30/30 - 0s - loss: 117.5155 - mean_squared_error: 115.7087 - val_loss: 127.9841 - val_mean_squared_error: 126.2307\nEpoch 113/1000\n30/30 - 0s - loss: 117.4985 - mean_squared_error: 115.7686 - val_loss: 140.8951 - val_mean_squared_error: 139.1538\nEpoch 114/1000\n30/30 - 0s - loss: 117.4877 - mean_squared_error: 115.7092 - val_loss: 131.1047 - val_mean_squared_error: 129.3051\nEpoch 115/1000\n30/30 - 0s - loss: 117.5024 - mean_squared_error: 115.7749 - val_loss: 133.6502 - val_mean_squared_error: 131.9848\nEpoch 116/1000\n30/30 - 0s - loss: 117.5557 - mean_squared_error: 115.8412 - val_loss: 128.3479 - val_mean_squared_error: 126.5476\nEpoch 117/1000\n30/30 - 0s - loss: 117.3917 - mean_squared_error: 115.6061 - val_loss: 128.7527 - val_mean_squared_error: 127.0098\nEpoch 118/1000\n30/30 - 0s - loss: 117.4950 - mean_squared_error: 115.7655 - val_loss: 139.2708 - val_mean_squared_error: 137.5034\nEpoch 119/1000\n30/30 - 0s - loss: 117.4714 - mean_squared_error: 115.6733 - val_loss: 130.9297 - val_mean_squared_error: 129.1599\nEpoch 120/1000\n30/30 - 0s - loss: 117.6518 - mean_squared_error: 115.8874 - val_loss: 130.5471 - val_mean_squared_error: 128.7721\nEpoch 121/1000\n30/30 - 0s - loss: 117.3412 - mean_squared_error: 115.5562 - val_loss: 129.8110 - val_mean_squared_error: 128.0498\nEpoch 122/1000\n30/30 - 0s - loss: 117.3118 - mean_squared_error: 115.5721 - val_loss: 131.6577 - val_mean_squared_error: 129.9497\nEpoch 123/1000\n30/30 - 0s - loss: 117.2190 - mean_squared_error: 115.4957 - val_loss: 130.7321 - val_mean_squared_error: 129.0049\nEpoch 124/1000\n30/30 - 0s - loss: 117.3518 - mean_squared_error: 115.6250 - val_loss: 127.9711 - val_mean_squared_error: 126.2706\nEpoch 125/1000\n30/30 - 0s - loss: 117.3202 - mean_squared_error: 115.6216 - val_loss: 129.4586 - val_mean_squared_error: 127.7151\nEpoch 126/1000\n30/30 - 0s - loss: 117.1942 - mean_squared_error: 115.4262 - val_loss: 128.1836 - val_mean_squared_error: 126.4348\nEpoch 127/1000\n30/30 - 0s - loss: 117.0224 - mean_squared_error: 115.2894 - val_loss: 127.3979 - val_mean_squared_error: 125.6709\nEpoch 128/1000\n30/30 - 0s - loss: 117.2734 - mean_squared_error: 115.5572 - val_loss: 132.7552 - val_mean_squared_error: 130.9760\nEpoch 129/1000\n30/30 - 0s - loss: 117.4397 - mean_squared_error: 115.6399 - val_loss: 133.8684 - val_mean_squared_error: 132.1197\nEpoch 130/1000\n30/30 - 0s - loss: 116.9274 - mean_squared_error: 115.2041 - val_loss: 127.3095 - val_mean_squared_error: 125.5994\nEpoch 131/1000\n30/30 - 0s - loss: 117.1250 - mean_squared_error: 115.3538 - val_loss: 134.5387 - val_mean_squared_error: 132.7571\nEpoch 132/1000\n30/30 - 0s - loss: 117.0953 - mean_squared_error: 115.3091 - val_loss: 137.1081 - val_mean_squared_error: 135.3416\nEpoch 133/1000\n30/30 - 0s - loss: 117.1937 - mean_squared_error: 115.4243 - val_loss: 137.8611 - val_mean_squared_error: 136.1050\nEpoch 134/1000\n30/30 - 0s - loss: 117.3281 - mean_squared_error: 115.5211 - val_loss: 129.8593 - val_mean_squared_error: 127.9449\nEpoch 135/1000\n30/30 - 0s - loss: 117.4182 - mean_squared_error: 115.5117 - val_loss: 127.1317 - val_mean_squared_error: 125.2903\nEpoch 136/1000\n30/30 - 0s - loss: 117.1908 - mean_squared_error: 115.3967 - val_loss: 129.1764 - val_mean_squared_error: 127.3658\nEpoch 137/1000\n30/30 - 0s - loss: 117.0183 - mean_squared_error: 115.1718 - val_loss: 128.9089 - val_mean_squared_error: 127.1044\nEpoch 138/1000\n30/30 - 0s - loss: 117.0396 - mean_squared_error: 115.2676 - val_loss: 134.3368 - val_mean_squared_error: 132.5857\nEpoch 139/1000\n30/30 - 0s - loss: 117.1699 - mean_squared_error: 115.3665 - val_loss: 129.3336 - val_mean_squared_error: 127.5331\nEpoch 140/1000\n30/30 - 0s - loss: 117.0344 - mean_squared_error: 115.2023 - val_loss: 134.2530 - val_mean_squared_error: 132.4033\nEpoch 141/1000\n30/30 - 0s - loss: 117.4573 - mean_squared_error: 115.6254 - val_loss: 137.5419 - val_mean_squared_error: 135.6933\nEpoch 142/1000\n30/30 - 0s - loss: 117.0881 - mean_squared_error: 115.2644 - val_loss: 128.8531 - val_mean_squared_error: 127.0763\nEpoch 143/1000\n30/30 - 0s - loss: 116.9071 - mean_squared_error: 115.0827 - val_loss: 126.5470 - val_mean_squared_error: 124.7187\nEpoch 144/1000\n30/30 - 0s - loss: 117.1215 - mean_squared_error: 115.3054 - val_loss: 130.3817 - val_mean_squared_error: 128.5727\nEpoch 145/1000\n30/30 - 0s - loss: 117.0718 - mean_squared_error: 115.2691 - val_loss: 130.5223 - val_mean_squared_error: 128.6524\nEpoch 146/1000\n30/30 - 0s - loss: 117.0486 - mean_squared_error: 115.1378 - val_loss: 141.1749 - val_mean_squared_error: 139.3040\nEpoch 147/1000\n30/30 - 0s - loss: 117.0556 - mean_squared_error: 115.2384 - val_loss: 139.3700 - val_mean_squared_error: 137.5895\nEpoch 148/1000\n30/30 - 0s - loss: 116.7274 - mean_squared_error: 114.9190 - val_loss: 128.7616 - val_mean_squared_error: 126.9283\nEpoch 149/1000\n30/30 - 0s - loss: 116.9338 - mean_squared_error: 115.1024 - val_loss: 130.8640 - val_mean_squared_error: 129.0053\nEpoch 150/1000\n30/30 - 0s - loss: 117.0332 - mean_squared_error: 115.2078 - val_loss: 130.1416 - val_mean_squared_error: 128.3484\nEpoch 151/1000\n30/30 - 0s - loss: 116.9355 - mean_squared_error: 115.1310 - val_loss: 131.0320 - val_mean_squared_error: 129.1889\nEpoch 152/1000\n30/30 - 0s - loss: 116.8661 - mean_squared_error: 115.0203 - val_loss: 136.4604 - val_mean_squared_error: 134.6102\nMinimum Validation Loss: 125.8136\n[0.07564237280667085, 0.16038548856122226, 0.10392479598356585]\n44.52666783332825\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Load libraries and data.\n2. pEDA. Look at feature distribution, fix them if they do not look right.\n3. Train-test split. Most likely couple years into test set. 2015-2018?. Impute missing values.\n4. Transform numerical features, add ohe for inds.\n5. Fit classic models: ols as a baseline, then xgb.\n6. Fir DL.\n\n\nNotes:\nideally, I want to use time-based cross-validation.\nsince I have panel data, it is not a trivial task.\nneed to find some solution online.\ne.g., https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8.\n","metadata":{}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, time, math, re, warnings, random, gc, dill, optuna\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\npd.set_option('display.max_columns', 110)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:11:51.123283Z","iopub.execute_input":"2022-08-24T23:11:51.124288Z","iopub.status.idle":"2022-08-24T23:11:51.134303Z","shell.execute_reply.started":"2022-08-24T23:11:51.124245Z","shell.execute_reply":"2022-08-24T23:11:51.133214Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:11:51.136491Z","iopub.execute_input":"2022-08-24T23:11:51.137038Z","iopub.status.idle":"2022-08-24T23:11:51.149078Z","shell.execute_reply.started":"2022-08-24T23:11:51.136999Z","shell.execute_reply":"2022-08-24T23:11:51.147946Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:11:51.151274Z","iopub.execute_input":"2022-08-24T23:11:51.151994Z","iopub.status.idle":"2022-08-24T23:11:51.163318Z","shell.execute_reply.started":"2022-08-24T23:11:51.151940Z","shell.execute_reply":"2022-08-24T23:11:51.162242Z"},"trusted":true},"execution_count":84,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"# 1. Import data #\n\ntime0 = time.time()\ndf = pd.read_csv('../input/cpcrsp-46/IMLEAP_v4.csv')\ndf.dropna(axis=0, subset=['bm', 'lbm', 'llme', 'lop', 'op', 'linv', 'mom122', 'beta_bw', 'ind'], inplace=True)\ndf.reset_index(inplace=True, drop=True)\ndf = df.sample(500000)\ndisplay(df.shape, df.head(), df.count())","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:11:51.167919Z","iopub.execute_input":"2022-08-24T23:11:51.168354Z","iopub.status.idle":"2022-08-24T23:12:05.283987Z","shell.execute_reply.started":"2022-08-24T23:11:51.168196Z","shell.execute_reply":"2022-08-24T23:12:05.282697Z"},"trusted":true},"execution_count":85,"outputs":[{"output_type":"display_data","data":{"text/plain":"(500000, 46)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         PERMNO  prd     mom482     mom242  year      RET   ind        bm  \\\n975260    77550  448        NaN -25.172631  1995  20.3633  37.0 -0.921099   \n558659    45495  358 -61.508327   0.853492  1988  19.5400  30.0  0.988902   \n56548     11357  477 -80.628208 -71.003968  1998  -3.8783  35.0 -1.121656   \n816941    65365  489  83.282637  71.404567  1999 -12.3326  15.0 -0.548171   \n1207188   88664  384        NaN -55.289083  1990  11.8100  36.0 -0.040085   \n\n               op        gp       inv      mom11     mom122      amhd  \\\n975260   0.051304  0.238825 -0.058591  -5.211900   0.229153  1.447775   \n558659  -0.035440  0.000000 -0.031670  18.757600  -4.975264  0.793624   \n56548   -0.214332  0.266466 -0.230583 -11.704500  45.957415  6.206419   \n816941   0.227291  0.537594  0.081865  26.592465  27.236824  1.767805   \n1207188  0.099706  0.470150  0.107593 -11.751100 -38.371240  7.026280   \n\n         ivol_capm  ivol_ff5   beta_bw      MAX     vol1m     vol6m    vol12m  \\\n975260    1.068911  0.968758  0.579414   1.6864  1.195066  1.661067  1.879295   \n558659    4.504974  3.831267  1.279885  11.3486  5.195766  5.413255  4.641449   \n56548     5.063521  3.754773  1.235358  20.3862  3.044110  7.983689  7.756113   \n816941    2.334386  2.080898  0.703605   7.7608  2.696661  2.930362  2.614708   \n1207188   2.375807  2.075048 -0.123267   3.0960  2.146361  5.348871  4.975181   \n\n            BAspr      size       lbm       lop       lgp      linv      llme  \\\n975260   2.400000  6.051830 -1.382857  0.048376  0.204294  0.800300  5.926726   \n558659        NaN  6.108052  0.465380  0.054390  0.083257  0.001939  5.652421   \n56548    2.272727  2.511560 -1.966005 -0.110497  0.121356  0.528983  2.058122   \n816941   1.875000  6.264728 -0.544275  0.222820  0.543868  0.069823  5.742089   \n1207188  6.060606  1.949618 -0.141066  0.042753  0.379966  0.800300  2.506179   \n\n           l1amhd    l1MAX   l1BAspr    l3amhd      l3MAX   l3BAspr    l6amhd  \\\n975260   1.494200   2.4180  3.174603  1.559902   2.498000  2.500000  1.542140   \n558659   0.835524  13.4955       NaN  1.059063  13.306300       NaN  1.445068   \n56548    6.209650   7.9790  5.084746  6.233199  21.135115  4.411765  5.162468   \n816941   1.762728   2.5491  2.732240  1.782924   9.781900  2.185792  1.926236   \n1207188  6.987584  12.4700  5.882353  6.911758  11.081100  6.666667  6.903764   \n\n           l6MAX    l6BAspr   l12amhd   l12MAX  l12BAspr  l12mom122  \\\n975260    4.9305   1.818182  1.400283   2.4180  1.666667  -7.775972   \n558659   10.3686        NaN  1.805513  13.4955       NaN -44.494912   \n56548    13.1349  11.764706  4.285314   7.9790  8.333333 -61.937195   \n816941    3.1725   1.578947  1.972619   2.5491  2.909091   8.350156   \n1207188   3.8132   7.692308  6.533060  12.4700  6.896552 -12.350618   \n\n         l12ivol_capm  l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  \n975260       1.560456     1.506983    0.982477  2.175515   2.480386  \n558659       4.727320     3.287101    1.102594  3.600322   3.525096  \n56548        7.874195     6.946893    1.788206  7.484225   7.154533  \n816941       1.571150     1.233580    0.583033  1.862874   1.595597  \n1207188      4.120387     3.502844    0.335250  4.462936   5.153330  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>975260</th>\n      <td>77550</td>\n      <td>448</td>\n      <td>NaN</td>\n      <td>-25.172631</td>\n      <td>1995</td>\n      <td>20.3633</td>\n      <td>37.0</td>\n      <td>-0.921099</td>\n      <td>0.051304</td>\n      <td>0.238825</td>\n      <td>-0.058591</td>\n      <td>-5.211900</td>\n      <td>0.229153</td>\n      <td>1.447775</td>\n      <td>1.068911</td>\n      <td>0.968758</td>\n      <td>0.579414</td>\n      <td>1.6864</td>\n      <td>1.195066</td>\n      <td>1.661067</td>\n      <td>1.879295</td>\n      <td>2.400000</td>\n      <td>6.051830</td>\n      <td>-1.382857</td>\n      <td>0.048376</td>\n      <td>0.204294</td>\n      <td>0.800300</td>\n      <td>5.926726</td>\n      <td>1.494200</td>\n      <td>2.4180</td>\n      <td>3.174603</td>\n      <td>1.559902</td>\n      <td>2.498000</td>\n      <td>2.500000</td>\n      <td>1.542140</td>\n      <td>4.9305</td>\n      <td>1.818182</td>\n      <td>1.400283</td>\n      <td>2.4180</td>\n      <td>1.666667</td>\n      <td>-7.775972</td>\n      <td>1.560456</td>\n      <td>1.506983</td>\n      <td>0.982477</td>\n      <td>2.175515</td>\n      <td>2.480386</td>\n    </tr>\n    <tr>\n      <th>558659</th>\n      <td>45495</td>\n      <td>358</td>\n      <td>-61.508327</td>\n      <td>0.853492</td>\n      <td>1988</td>\n      <td>19.5400</td>\n      <td>30.0</td>\n      <td>0.988902</td>\n      <td>-0.035440</td>\n      <td>0.000000</td>\n      <td>-0.031670</td>\n      <td>18.757600</td>\n      <td>-4.975264</td>\n      <td>0.793624</td>\n      <td>4.504974</td>\n      <td>3.831267</td>\n      <td>1.279885</td>\n      <td>11.3486</td>\n      <td>5.195766</td>\n      <td>5.413255</td>\n      <td>4.641449</td>\n      <td>NaN</td>\n      <td>6.108052</td>\n      <td>0.465380</td>\n      <td>0.054390</td>\n      <td>0.083257</td>\n      <td>0.001939</td>\n      <td>5.652421</td>\n      <td>0.835524</td>\n      <td>13.4955</td>\n      <td>NaN</td>\n      <td>1.059063</td>\n      <td>13.306300</td>\n      <td>NaN</td>\n      <td>1.445068</td>\n      <td>10.3686</td>\n      <td>NaN</td>\n      <td>1.805513</td>\n      <td>13.4955</td>\n      <td>NaN</td>\n      <td>-44.494912</td>\n      <td>4.727320</td>\n      <td>3.287101</td>\n      <td>1.102594</td>\n      <td>3.600322</td>\n      <td>3.525096</td>\n    </tr>\n    <tr>\n      <th>56548</th>\n      <td>11357</td>\n      <td>477</td>\n      <td>-80.628208</td>\n      <td>-71.003968</td>\n      <td>1998</td>\n      <td>-3.8783</td>\n      <td>35.0</td>\n      <td>-1.121656</td>\n      <td>-0.214332</td>\n      <td>0.266466</td>\n      <td>-0.230583</td>\n      <td>-11.704500</td>\n      <td>45.957415</td>\n      <td>6.206419</td>\n      <td>5.063521</td>\n      <td>3.754773</td>\n      <td>1.235358</td>\n      <td>20.3862</td>\n      <td>3.044110</td>\n      <td>7.983689</td>\n      <td>7.756113</td>\n      <td>2.272727</td>\n      <td>2.511560</td>\n      <td>-1.966005</td>\n      <td>-0.110497</td>\n      <td>0.121356</td>\n      <td>0.528983</td>\n      <td>2.058122</td>\n      <td>6.209650</td>\n      <td>7.9790</td>\n      <td>5.084746</td>\n      <td>6.233199</td>\n      <td>21.135115</td>\n      <td>4.411765</td>\n      <td>5.162468</td>\n      <td>13.1349</td>\n      <td>11.764706</td>\n      <td>4.285314</td>\n      <td>7.9790</td>\n      <td>8.333333</td>\n      <td>-61.937195</td>\n      <td>7.874195</td>\n      <td>6.946893</td>\n      <td>1.788206</td>\n      <td>7.484225</td>\n      <td>7.154533</td>\n    </tr>\n    <tr>\n      <th>816941</th>\n      <td>65365</td>\n      <td>489</td>\n      <td>83.282637</td>\n      <td>71.404567</td>\n      <td>1999</td>\n      <td>-12.3326</td>\n      <td>15.0</td>\n      <td>-0.548171</td>\n      <td>0.227291</td>\n      <td>0.537594</td>\n      <td>0.081865</td>\n      <td>26.592465</td>\n      <td>27.236824</td>\n      <td>1.767805</td>\n      <td>2.334386</td>\n      <td>2.080898</td>\n      <td>0.703605</td>\n      <td>7.7608</td>\n      <td>2.696661</td>\n      <td>2.930362</td>\n      <td>2.614708</td>\n      <td>1.875000</td>\n      <td>6.264728</td>\n      <td>-0.544275</td>\n      <td>0.222820</td>\n      <td>0.543868</td>\n      <td>0.069823</td>\n      <td>5.742089</td>\n      <td>1.762728</td>\n      <td>2.5491</td>\n      <td>2.732240</td>\n      <td>1.782924</td>\n      <td>9.781900</td>\n      <td>2.185792</td>\n      <td>1.926236</td>\n      <td>3.1725</td>\n      <td>1.578947</td>\n      <td>1.972619</td>\n      <td>2.5491</td>\n      <td>2.909091</td>\n      <td>8.350156</td>\n      <td>1.571150</td>\n      <td>1.233580</td>\n      <td>0.583033</td>\n      <td>1.862874</td>\n      <td>1.595597</td>\n    </tr>\n    <tr>\n      <th>1207188</th>\n      <td>88664</td>\n      <td>384</td>\n      <td>NaN</td>\n      <td>-55.289083</td>\n      <td>1990</td>\n      <td>11.8100</td>\n      <td>36.0</td>\n      <td>-0.040085</td>\n      <td>0.099706</td>\n      <td>0.470150</td>\n      <td>0.107593</td>\n      <td>-11.751100</td>\n      <td>-38.371240</td>\n      <td>7.026280</td>\n      <td>2.375807</td>\n      <td>2.075048</td>\n      <td>-0.123267</td>\n      <td>3.0960</td>\n      <td>2.146361</td>\n      <td>5.348871</td>\n      <td>4.975181</td>\n      <td>6.060606</td>\n      <td>1.949618</td>\n      <td>-0.141066</td>\n      <td>0.042753</td>\n      <td>0.379966</td>\n      <td>0.800300</td>\n      <td>2.506179</td>\n      <td>6.987584</td>\n      <td>12.4700</td>\n      <td>5.882353</td>\n      <td>6.911758</td>\n      <td>11.081100</td>\n      <td>6.666667</td>\n      <td>6.903764</td>\n      <td>3.8132</td>\n      <td>7.692308</td>\n      <td>6.533060</td>\n      <td>12.4700</td>\n      <td>6.896552</td>\n      <td>-12.350618</td>\n      <td>4.120387</td>\n      <td>3.502844</td>\n      <td>0.335250</td>\n      <td>4.462936</td>\n      <td>5.153330</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"PERMNO          500000\nprd             500000\nmom482          425640\nmom242          490560\nyear            500000\nRET             500000\nind             500000\nbm              500000\nop              500000\ngp              500000\ninv             499583\nmom11           500000\nmom122          500000\namhd            411621\nivol_capm       499984\nivol_ff5        499984\nbeta_bw         500000\nMAX             500000\nvol1m           499927\nvol6m           499516\nvol12m          498831\nBAspr           290939\nsize            500000\nlbm             500000\nlop             500000\nlgp             500000\nlinv            500000\nllme            500000\nl1amhd          411360\nl1MAX           499982\nl1BAspr         290183\nl3amhd          410762\nl3MAX           499891\nl3BAspr         288561\nl6amhd          409900\nl6MAX           499769\nl6BAspr         285987\nl12amhd         407598\nl12MAX          499982\nl12BAspr        281245\nl12mom122       496568\nl12ivol_capm    499529\nl12ivol_ff5     499529\nl12beta_bw      499691\nl12vol6m        498595\nl12vol12m       493780\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# 2. pEDA #\n\ndf.RET.hist()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:12:05.286301Z","iopub.execute_input":"2022-08-24T23:12:05.286792Z","iopub.status.idle":"2022-08-24T23:12:05.516852Z","shell.execute_reply.started":"2022-08-24T23:12:05.286752Z","shell.execute_reply":"2022-08-24T23:12:05.515958Z"},"trusted":true},"execution_count":86,"outputs":[{"execution_count":86,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# explore feature distibution, adjust if seems unreasonable","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:12:05.518104Z","iopub.execute_input":"2022-08-24T23:12:05.518501Z","iopub.status.idle":"2022-08-24T23:12:05.524609Z","shell.execute_reply.started":"2022-08-24T23:12:05.518464Z","shell.execute_reply":"2022-08-24T23:12:05.523347Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"# add dummies for some missing features\n\nfeatures_miss_dummies = ['amhd', 'BAspr']\n\nfor col in features_miss_dummies:\n    df[col+'_miss'] = df[col].isnull().astype(int)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:12:05.527874Z","iopub.execute_input":"2022-08-24T23:12:05.528609Z","iopub.status.idle":"2022-08-24T23:12:05.574941Z","shell.execute_reply.started":"2022-08-24T23:12:05.528562Z","shell.execute_reply":"2022-08-24T23:12:05.574032Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"         PERMNO  prd     mom482     mom242  year      RET   ind        bm  \\\n975260    77550  448        NaN -25.172631  1995  20.3633  37.0 -0.921099   \n558659    45495  358 -61.508327   0.853492  1988  19.5400  30.0  0.988902   \n56548     11357  477 -80.628208 -71.003968  1998  -3.8783  35.0 -1.121656   \n816941    65365  489  83.282637  71.404567  1999 -12.3326  15.0 -0.548171   \n1207188   88664  384        NaN -55.289083  1990  11.8100  36.0 -0.040085   \n\n               op        gp       inv      mom11     mom122      amhd  \\\n975260   0.051304  0.238825 -0.058591  -5.211900   0.229153  1.447775   \n558659  -0.035440  0.000000 -0.031670  18.757600  -4.975264  0.793624   \n56548   -0.214332  0.266466 -0.230583 -11.704500  45.957415  6.206419   \n816941   0.227291  0.537594  0.081865  26.592465  27.236824  1.767805   \n1207188  0.099706  0.470150  0.107593 -11.751100 -38.371240  7.026280   \n\n         ivol_capm  ivol_ff5   beta_bw      MAX     vol1m     vol6m    vol12m  \\\n975260    1.068911  0.968758  0.579414   1.6864  1.195066  1.661067  1.879295   \n558659    4.504974  3.831267  1.279885  11.3486  5.195766  5.413255  4.641449   \n56548     5.063521  3.754773  1.235358  20.3862  3.044110  7.983689  7.756113   \n816941    2.334386  2.080898  0.703605   7.7608  2.696661  2.930362  2.614708   \n1207188   2.375807  2.075048 -0.123267   3.0960  2.146361  5.348871  4.975181   \n\n            BAspr      size       lbm       lop       lgp      linv      llme  \\\n975260   2.400000  6.051830 -1.382857  0.048376  0.204294  0.800300  5.926726   \n558659        NaN  6.108052  0.465380  0.054390  0.083257  0.001939  5.652421   \n56548    2.272727  2.511560 -1.966005 -0.110497  0.121356  0.528983  2.058122   \n816941   1.875000  6.264728 -0.544275  0.222820  0.543868  0.069823  5.742089   \n1207188  6.060606  1.949618 -0.141066  0.042753  0.379966  0.800300  2.506179   \n\n           l1amhd    l1MAX   l1BAspr    l3amhd      l3MAX   l3BAspr    l6amhd  \\\n975260   1.494200   2.4180  3.174603  1.559902   2.498000  2.500000  1.542140   \n558659   0.835524  13.4955       NaN  1.059063  13.306300       NaN  1.445068   \n56548    6.209650   7.9790  5.084746  6.233199  21.135115  4.411765  5.162468   \n816941   1.762728   2.5491  2.732240  1.782924   9.781900  2.185792  1.926236   \n1207188  6.987584  12.4700  5.882353  6.911758  11.081100  6.666667  6.903764   \n\n           l6MAX    l6BAspr   l12amhd   l12MAX  l12BAspr  l12mom122  \\\n975260    4.9305   1.818182  1.400283   2.4180  1.666667  -7.775972   \n558659   10.3686        NaN  1.805513  13.4955       NaN -44.494912   \n56548    13.1349  11.764706  4.285314   7.9790  8.333333 -61.937195   \n816941    3.1725   1.578947  1.972619   2.5491  2.909091   8.350156   \n1207188   3.8132   7.692308  6.533060  12.4700  6.896552 -12.350618   \n\n         l12ivol_capm  l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  \\\n975260       1.560456     1.506983    0.982477  2.175515   2.480386   \n558659       4.727320     3.287101    1.102594  3.600322   3.525096   \n56548        7.874195     6.946893    1.788206  7.484225   7.154533   \n816941       1.571150     1.233580    0.583033  1.862874   1.595597   \n1207188      4.120387     3.502844    0.335250  4.462936   5.153330   \n\n         amhd_miss  BAspr_miss  \n975260           0           0  \n558659           0           1  \n56548            0           0  \n816941           0           0  \n1207188          0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>975260</th>\n      <td>77550</td>\n      <td>448</td>\n      <td>NaN</td>\n      <td>-25.172631</td>\n      <td>1995</td>\n      <td>20.3633</td>\n      <td>37.0</td>\n      <td>-0.921099</td>\n      <td>0.051304</td>\n      <td>0.238825</td>\n      <td>-0.058591</td>\n      <td>-5.211900</td>\n      <td>0.229153</td>\n      <td>1.447775</td>\n      <td>1.068911</td>\n      <td>0.968758</td>\n      <td>0.579414</td>\n      <td>1.6864</td>\n      <td>1.195066</td>\n      <td>1.661067</td>\n      <td>1.879295</td>\n      <td>2.400000</td>\n      <td>6.051830</td>\n      <td>-1.382857</td>\n      <td>0.048376</td>\n      <td>0.204294</td>\n      <td>0.800300</td>\n      <td>5.926726</td>\n      <td>1.494200</td>\n      <td>2.4180</td>\n      <td>3.174603</td>\n      <td>1.559902</td>\n      <td>2.498000</td>\n      <td>2.500000</td>\n      <td>1.542140</td>\n      <td>4.9305</td>\n      <td>1.818182</td>\n      <td>1.400283</td>\n      <td>2.4180</td>\n      <td>1.666667</td>\n      <td>-7.775972</td>\n      <td>1.560456</td>\n      <td>1.506983</td>\n      <td>0.982477</td>\n      <td>2.175515</td>\n      <td>2.480386</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>558659</th>\n      <td>45495</td>\n      <td>358</td>\n      <td>-61.508327</td>\n      <td>0.853492</td>\n      <td>1988</td>\n      <td>19.5400</td>\n      <td>30.0</td>\n      <td>0.988902</td>\n      <td>-0.035440</td>\n      <td>0.000000</td>\n      <td>-0.031670</td>\n      <td>18.757600</td>\n      <td>-4.975264</td>\n      <td>0.793624</td>\n      <td>4.504974</td>\n      <td>3.831267</td>\n      <td>1.279885</td>\n      <td>11.3486</td>\n      <td>5.195766</td>\n      <td>5.413255</td>\n      <td>4.641449</td>\n      <td>NaN</td>\n      <td>6.108052</td>\n      <td>0.465380</td>\n      <td>0.054390</td>\n      <td>0.083257</td>\n      <td>0.001939</td>\n      <td>5.652421</td>\n      <td>0.835524</td>\n      <td>13.4955</td>\n      <td>NaN</td>\n      <td>1.059063</td>\n      <td>13.306300</td>\n      <td>NaN</td>\n      <td>1.445068</td>\n      <td>10.3686</td>\n      <td>NaN</td>\n      <td>1.805513</td>\n      <td>13.4955</td>\n      <td>NaN</td>\n      <td>-44.494912</td>\n      <td>4.727320</td>\n      <td>3.287101</td>\n      <td>1.102594</td>\n      <td>3.600322</td>\n      <td>3.525096</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>56548</th>\n      <td>11357</td>\n      <td>477</td>\n      <td>-80.628208</td>\n      <td>-71.003968</td>\n      <td>1998</td>\n      <td>-3.8783</td>\n      <td>35.0</td>\n      <td>-1.121656</td>\n      <td>-0.214332</td>\n      <td>0.266466</td>\n      <td>-0.230583</td>\n      <td>-11.704500</td>\n      <td>45.957415</td>\n      <td>6.206419</td>\n      <td>5.063521</td>\n      <td>3.754773</td>\n      <td>1.235358</td>\n      <td>20.3862</td>\n      <td>3.044110</td>\n      <td>7.983689</td>\n      <td>7.756113</td>\n      <td>2.272727</td>\n      <td>2.511560</td>\n      <td>-1.966005</td>\n      <td>-0.110497</td>\n      <td>0.121356</td>\n      <td>0.528983</td>\n      <td>2.058122</td>\n      <td>6.209650</td>\n      <td>7.9790</td>\n      <td>5.084746</td>\n      <td>6.233199</td>\n      <td>21.135115</td>\n      <td>4.411765</td>\n      <td>5.162468</td>\n      <td>13.1349</td>\n      <td>11.764706</td>\n      <td>4.285314</td>\n      <td>7.9790</td>\n      <td>8.333333</td>\n      <td>-61.937195</td>\n      <td>7.874195</td>\n      <td>6.946893</td>\n      <td>1.788206</td>\n      <td>7.484225</td>\n      <td>7.154533</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>816941</th>\n      <td>65365</td>\n      <td>489</td>\n      <td>83.282637</td>\n      <td>71.404567</td>\n      <td>1999</td>\n      <td>-12.3326</td>\n      <td>15.0</td>\n      <td>-0.548171</td>\n      <td>0.227291</td>\n      <td>0.537594</td>\n      <td>0.081865</td>\n      <td>26.592465</td>\n      <td>27.236824</td>\n      <td>1.767805</td>\n      <td>2.334386</td>\n      <td>2.080898</td>\n      <td>0.703605</td>\n      <td>7.7608</td>\n      <td>2.696661</td>\n      <td>2.930362</td>\n      <td>2.614708</td>\n      <td>1.875000</td>\n      <td>6.264728</td>\n      <td>-0.544275</td>\n      <td>0.222820</td>\n      <td>0.543868</td>\n      <td>0.069823</td>\n      <td>5.742089</td>\n      <td>1.762728</td>\n      <td>2.5491</td>\n      <td>2.732240</td>\n      <td>1.782924</td>\n      <td>9.781900</td>\n      <td>2.185792</td>\n      <td>1.926236</td>\n      <td>3.1725</td>\n      <td>1.578947</td>\n      <td>1.972619</td>\n      <td>2.5491</td>\n      <td>2.909091</td>\n      <td>8.350156</td>\n      <td>1.571150</td>\n      <td>1.233580</td>\n      <td>0.583033</td>\n      <td>1.862874</td>\n      <td>1.595597</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1207188</th>\n      <td>88664</td>\n      <td>384</td>\n      <td>NaN</td>\n      <td>-55.289083</td>\n      <td>1990</td>\n      <td>11.8100</td>\n      <td>36.0</td>\n      <td>-0.040085</td>\n      <td>0.099706</td>\n      <td>0.470150</td>\n      <td>0.107593</td>\n      <td>-11.751100</td>\n      <td>-38.371240</td>\n      <td>7.026280</td>\n      <td>2.375807</td>\n      <td>2.075048</td>\n      <td>-0.123267</td>\n      <td>3.0960</td>\n      <td>2.146361</td>\n      <td>5.348871</td>\n      <td>4.975181</td>\n      <td>6.060606</td>\n      <td>1.949618</td>\n      <td>-0.141066</td>\n      <td>0.042753</td>\n      <td>0.379966</td>\n      <td>0.800300</td>\n      <td>2.506179</td>\n      <td>6.987584</td>\n      <td>12.4700</td>\n      <td>5.882353</td>\n      <td>6.911758</td>\n      <td>11.081100</td>\n      <td>6.666667</td>\n      <td>6.903764</td>\n      <td>3.8132</td>\n      <td>7.692308</td>\n      <td>6.533060</td>\n      <td>12.4700</td>\n      <td>6.896552</td>\n      <td>-12.350618</td>\n      <td>4.120387</td>\n      <td>3.502844</td>\n      <td>0.335250</td>\n      <td>4.462936</td>\n      <td>5.153330</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 3. Train-test split #\n\ntemp_cols = ['PERMNO', 'prd', 'year']\ntest_size = 0.1\ndf.reset_index(inplace=True, drop=True)\n#random.seed(2)\ntest_index = random.sample(list(df.index), int(test_size*df.shape[0]))\ntrain = df.iloc[list(set(df.index)-set(test_index))]\ntest = df.iloc[test_index]\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\ntrain.drop(columns=temp_cols, inplace=True)\ntest.drop(columns=temp_cols, inplace=True)\ndisplay(train.shape, test.shape, train.head(3), test.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:12:05.576393Z","iopub.execute_input":"2022-08-24T23:12:05.577455Z","iopub.status.idle":"2022-08-24T23:12:06.031243Z","shell.execute_reply.started":"2022-08-24T23:12:05.577410Z","shell.execute_reply":"2022-08-24T23:12:06.030038Z"},"trusted":true},"execution_count":89,"outputs":[{"output_type":"display_data","data":{"text/plain":"(450000, 45)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(50000, 45)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"      mom482     mom242      RET   ind        bm        op        gp  \\\n0        NaN -25.172631  20.3633  37.0 -0.921099  0.051304  0.238825   \n1 -61.508327   0.853492  19.5400  30.0  0.988902 -0.035440  0.000000   \n2 -80.628208 -71.003968  -3.8783  35.0 -1.121656 -0.214332  0.266466   \n\n        inv    mom11     mom122      amhd  ivol_capm  ivol_ff5   beta_bw  \\\n0 -0.058591  -5.2119   0.229153  1.447775   1.068911  0.968758  0.579414   \n1 -0.031670  18.7576  -4.975264  0.793624   4.504974  3.831267  1.279885   \n2 -0.230583 -11.7045  45.957415  6.206419   5.063521  3.754773  1.235358   \n\n       MAX     vol1m     vol6m    vol12m     BAspr      size       lbm  \\\n0   1.6864  1.195066  1.661067  1.879295  2.400000  6.051830 -1.382857   \n1  11.3486  5.195766  5.413255  4.641449       NaN  6.108052  0.465380   \n2  20.3862  3.044110  7.983689  7.756113  2.272727  2.511560 -1.966005   \n\n        lop       lgp      linv      llme    l1amhd    l1MAX   l1BAspr  \\\n0  0.048376  0.204294  0.800300  5.926726  1.494200   2.4180  3.174603   \n1  0.054390  0.083257  0.001939  5.652421  0.835524  13.4955       NaN   \n2 -0.110497  0.121356  0.528983  2.058122  6.209650   7.9790  5.084746   \n\n     l3amhd      l3MAX   l3BAspr    l6amhd    l6MAX    l6BAspr   l12amhd  \\\n0  1.559902   2.498000  2.500000  1.542140   4.9305   1.818182  1.400283   \n1  1.059063  13.306300       NaN  1.445068  10.3686        NaN  1.805513   \n2  6.233199  21.135115  4.411765  5.162468  13.1349  11.764706  4.285314   \n\n    l12MAX  l12BAspr  l12mom122  l12ivol_capm  l12ivol_ff5  l12beta_bw  \\\n0   2.4180  1.666667  -7.775972      1.560456     1.506983    0.982477   \n1  13.4955       NaN -44.494912      4.727320     3.287101    1.102594   \n2   7.9790  8.333333 -61.937195      7.874195     6.946893    1.788206   \n\n   l12vol6m  l12vol12m  amhd_miss  BAspr_miss  \n0  2.175515   2.480386          0           0  \n1  3.600322   3.525096          0           1  \n2  7.484225   7.154533          0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>NaN</td>\n      <td>-25.172631</td>\n      <td>20.3633</td>\n      <td>37.0</td>\n      <td>-0.921099</td>\n      <td>0.051304</td>\n      <td>0.238825</td>\n      <td>-0.058591</td>\n      <td>-5.2119</td>\n      <td>0.229153</td>\n      <td>1.447775</td>\n      <td>1.068911</td>\n      <td>0.968758</td>\n      <td>0.579414</td>\n      <td>1.6864</td>\n      <td>1.195066</td>\n      <td>1.661067</td>\n      <td>1.879295</td>\n      <td>2.400000</td>\n      <td>6.051830</td>\n      <td>-1.382857</td>\n      <td>0.048376</td>\n      <td>0.204294</td>\n      <td>0.800300</td>\n      <td>5.926726</td>\n      <td>1.494200</td>\n      <td>2.4180</td>\n      <td>3.174603</td>\n      <td>1.559902</td>\n      <td>2.498000</td>\n      <td>2.500000</td>\n      <td>1.542140</td>\n      <td>4.9305</td>\n      <td>1.818182</td>\n      <td>1.400283</td>\n      <td>2.4180</td>\n      <td>1.666667</td>\n      <td>-7.775972</td>\n      <td>1.560456</td>\n      <td>1.506983</td>\n      <td>0.982477</td>\n      <td>2.175515</td>\n      <td>2.480386</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-61.508327</td>\n      <td>0.853492</td>\n      <td>19.5400</td>\n      <td>30.0</td>\n      <td>0.988902</td>\n      <td>-0.035440</td>\n      <td>0.000000</td>\n      <td>-0.031670</td>\n      <td>18.7576</td>\n      <td>-4.975264</td>\n      <td>0.793624</td>\n      <td>4.504974</td>\n      <td>3.831267</td>\n      <td>1.279885</td>\n      <td>11.3486</td>\n      <td>5.195766</td>\n      <td>5.413255</td>\n      <td>4.641449</td>\n      <td>NaN</td>\n      <td>6.108052</td>\n      <td>0.465380</td>\n      <td>0.054390</td>\n      <td>0.083257</td>\n      <td>0.001939</td>\n      <td>5.652421</td>\n      <td>0.835524</td>\n      <td>13.4955</td>\n      <td>NaN</td>\n      <td>1.059063</td>\n      <td>13.306300</td>\n      <td>NaN</td>\n      <td>1.445068</td>\n      <td>10.3686</td>\n      <td>NaN</td>\n      <td>1.805513</td>\n      <td>13.4955</td>\n      <td>NaN</td>\n      <td>-44.494912</td>\n      <td>4.727320</td>\n      <td>3.287101</td>\n      <td>1.102594</td>\n      <td>3.600322</td>\n      <td>3.525096</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-80.628208</td>\n      <td>-71.003968</td>\n      <td>-3.8783</td>\n      <td>35.0</td>\n      <td>-1.121656</td>\n      <td>-0.214332</td>\n      <td>0.266466</td>\n      <td>-0.230583</td>\n      <td>-11.7045</td>\n      <td>45.957415</td>\n      <td>6.206419</td>\n      <td>5.063521</td>\n      <td>3.754773</td>\n      <td>1.235358</td>\n      <td>20.3862</td>\n      <td>3.044110</td>\n      <td>7.983689</td>\n      <td>7.756113</td>\n      <td>2.272727</td>\n      <td>2.511560</td>\n      <td>-1.966005</td>\n      <td>-0.110497</td>\n      <td>0.121356</td>\n      <td>0.528983</td>\n      <td>2.058122</td>\n      <td>6.209650</td>\n      <td>7.9790</td>\n      <td>5.084746</td>\n      <td>6.233199</td>\n      <td>21.135115</td>\n      <td>4.411765</td>\n      <td>5.162468</td>\n      <td>13.1349</td>\n      <td>11.764706</td>\n      <td>4.285314</td>\n      <td>7.9790</td>\n      <td>8.333333</td>\n      <td>-61.937195</td>\n      <td>7.874195</td>\n      <td>6.946893</td>\n      <td>1.788206</td>\n      <td>7.484225</td>\n      <td>7.154533</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"      mom482     mom242     RET   ind        bm        op        gp       inv  \\\n0 -20.828373  70.462651  2.6859  35.0  0.347390  0.115061  0.451738  0.039947   \n1  61.671787 -32.352049  4.6343  43.0 -1.213509  0.172609  0.760632  0.308129   \n2  25.852995  11.489748  5.5624  37.0 -0.530173  0.089027  0.666595  0.064416   \n\n     mom11     mom122      amhd  ivol_capm  ivol_ff5   beta_bw     MAX  \\\n0   0.7036  14.522862  0.655957   0.862797  0.790158  1.617982  2.6426   \n1  -6.6447 -33.857840  0.638863   2.236571  2.117016  0.742146  4.6086   \n2  13.0835 -29.816563  2.882095   3.386518  2.970708  0.798584  7.1239   \n\n      vol1m     vol6m    vol12m     BAspr      size       lbm       lop  \\\n0  1.518158  1.901592  2.286481       NaN  6.707809  0.760985  0.141000   \n1  2.463900  2.176849  2.204248  1.904762  5.518971 -1.200893  0.247001   \n2  3.590315  5.020900  4.294800  4.761905  4.909726 -0.447240  0.087903   \n\n        lgp      linv      llme    l1amhd    l1MAX   l1BAspr    l3amhd  \\\n0  0.477248  0.150771  6.515295  0.787637   2.5899       NaN  0.815655   \n1  0.827619  0.127441  5.941799  0.515963   5.2810  0.925926  0.498100   \n2  0.690142 -0.023398  4.960394  2.795867  13.1339  5.405405  2.781578   \n\n    l3MAX   l3BAspr    l6amhd   l6MAX   l6BAspr   l12amhd   l12MAX  l12BAspr  \\\n0  4.9810       NaN  0.792492  5.0667       NaN  1.376020   2.5899       NaN   \n1  3.7017  0.588235  0.580842  3.2487  0.606061  0.757378   5.2810  1.219512   \n2  9.5028  4.651163  2.577284  5.2402  8.571429  3.388609  13.1339  8.000000   \n\n   l12mom122  l12ivol_capm  l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  \\\n0   9.756944      1.850699     1.680564    1.479741  2.756999   3.498191   \n1  22.609636      1.329074     1.050572    1.054568  1.759477   1.793722   \n2  19.535028      2.905208     2.753667    0.537195  2.480957   3.135451   \n\n   amhd_miss  BAspr_miss  \n0          0           1  \n1          0           0  \n2          0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-20.828373</td>\n      <td>70.462651</td>\n      <td>2.6859</td>\n      <td>35.0</td>\n      <td>0.347390</td>\n      <td>0.115061</td>\n      <td>0.451738</td>\n      <td>0.039947</td>\n      <td>0.7036</td>\n      <td>14.522862</td>\n      <td>0.655957</td>\n      <td>0.862797</td>\n      <td>0.790158</td>\n      <td>1.617982</td>\n      <td>2.6426</td>\n      <td>1.518158</td>\n      <td>1.901592</td>\n      <td>2.286481</td>\n      <td>NaN</td>\n      <td>6.707809</td>\n      <td>0.760985</td>\n      <td>0.141000</td>\n      <td>0.477248</td>\n      <td>0.150771</td>\n      <td>6.515295</td>\n      <td>0.787637</td>\n      <td>2.5899</td>\n      <td>NaN</td>\n      <td>0.815655</td>\n      <td>4.9810</td>\n      <td>NaN</td>\n      <td>0.792492</td>\n      <td>5.0667</td>\n      <td>NaN</td>\n      <td>1.376020</td>\n      <td>2.5899</td>\n      <td>NaN</td>\n      <td>9.756944</td>\n      <td>1.850699</td>\n      <td>1.680564</td>\n      <td>1.479741</td>\n      <td>2.756999</td>\n      <td>3.498191</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>61.671787</td>\n      <td>-32.352049</td>\n      <td>4.6343</td>\n      <td>43.0</td>\n      <td>-1.213509</td>\n      <td>0.172609</td>\n      <td>0.760632</td>\n      <td>0.308129</td>\n      <td>-6.6447</td>\n      <td>-33.857840</td>\n      <td>0.638863</td>\n      <td>2.236571</td>\n      <td>2.117016</td>\n      <td>0.742146</td>\n      <td>4.6086</td>\n      <td>2.463900</td>\n      <td>2.176849</td>\n      <td>2.204248</td>\n      <td>1.904762</td>\n      <td>5.518971</td>\n      <td>-1.200893</td>\n      <td>0.247001</td>\n      <td>0.827619</td>\n      <td>0.127441</td>\n      <td>5.941799</td>\n      <td>0.515963</td>\n      <td>5.2810</td>\n      <td>0.925926</td>\n      <td>0.498100</td>\n      <td>3.7017</td>\n      <td>0.588235</td>\n      <td>0.580842</td>\n      <td>3.2487</td>\n      <td>0.606061</td>\n      <td>0.757378</td>\n      <td>5.2810</td>\n      <td>1.219512</td>\n      <td>22.609636</td>\n      <td>1.329074</td>\n      <td>1.050572</td>\n      <td>1.054568</td>\n      <td>1.759477</td>\n      <td>1.793722</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>25.852995</td>\n      <td>11.489748</td>\n      <td>5.5624</td>\n      <td>37.0</td>\n      <td>-0.530173</td>\n      <td>0.089027</td>\n      <td>0.666595</td>\n      <td>0.064416</td>\n      <td>13.0835</td>\n      <td>-29.816563</td>\n      <td>2.882095</td>\n      <td>3.386518</td>\n      <td>2.970708</td>\n      <td>0.798584</td>\n      <td>7.1239</td>\n      <td>3.590315</td>\n      <td>5.020900</td>\n      <td>4.294800</td>\n      <td>4.761905</td>\n      <td>4.909726</td>\n      <td>-0.447240</td>\n      <td>0.087903</td>\n      <td>0.690142</td>\n      <td>-0.023398</td>\n      <td>4.960394</td>\n      <td>2.795867</td>\n      <td>13.1339</td>\n      <td>5.405405</td>\n      <td>2.781578</td>\n      <td>9.5028</td>\n      <td>4.651163</td>\n      <td>2.577284</td>\n      <td>5.2402</td>\n      <td>8.571429</td>\n      <td>3.388609</td>\n      <td>13.1339</td>\n      <td>8.000000</td>\n      <td>19.535028</td>\n      <td>2.905208</td>\n      <td>2.753667</td>\n      <td>0.537195</td>\n      <td>2.480957</td>\n      <td>3.135451</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 4. Missing values #\n\ncol_ignore = ['RET']\ncol_cat = ['ind']\ncol_num = [x for x in train.columns if x not in col_ignore+col_cat]\n\nfor col in col_num:\n    train[col] = train[col].fillna(train[col].median())\n    test[col] = test[col].fillna(train[col].median())\n\nfor col in col_cat:\n    train[col] = train[col].fillna(value=-1000)\n    test[col] = test[col].fillna(value=-1000)\n    \ndisplay(train.count())","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:12:06.033535Z","iopub.execute_input":"2022-08-24T23:12:06.034039Z","iopub.status.idle":"2022-08-24T23:12:06.889212Z","shell.execute_reply.started":"2022-08-24T23:12:06.033990Z","shell.execute_reply":"2022-08-24T23:12:06.887957Z"},"trusted":true},"execution_count":90,"outputs":[{"output_type":"display_data","data":{"text/plain":"mom482          450000\nmom242          450000\nRET             450000\nind             450000\nbm              450000\nop              450000\ngp              450000\ninv             450000\nmom11           450000\nmom122          450000\namhd            450000\nivol_capm       450000\nivol_ff5        450000\nbeta_bw         450000\nMAX             450000\nvol1m           450000\nvol6m           450000\nvol12m          450000\nBAspr           450000\nsize            450000\nlbm             450000\nlop             450000\nlgp             450000\nlinv            450000\nllme            450000\nl1amhd          450000\nl1MAX           450000\nl1BAspr         450000\nl3amhd          450000\nl3MAX           450000\nl3BAspr         450000\nl6amhd          450000\nl6MAX           450000\nl6BAspr         450000\nl12amhd         450000\nl12MAX          450000\nl12BAspr        450000\nl12mom122       450000\nl12ivol_capm    450000\nl12ivol_ff5     450000\nl12beta_bw      450000\nl12vol6m        450000\nl12vol12m       450000\namhd_miss       450000\nBAspr_miss      450000\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# [optional] Target Encoding\n","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:12:06.890871Z","iopub.execute_input":"2022-08-24T23:12:06.891533Z","iopub.status.idle":"2022-08-24T23:12:06.896582Z","shell.execute_reply.started":"2022-08-24T23:12:06.891489Z","shell.execute_reply":"2022-08-24T23:12:06.895429Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"X_train = train.copy()\ny_train = X_train.pop('RET')\n\nX_test = test.copy()\ny_test = X_test.pop('RET')","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:12:06.898386Z","iopub.execute_input":"2022-08-24T23:12:06.898811Z","iopub.status.idle":"2022-08-24T23:12:06.971851Z","shell.execute_reply.started":"2022-08-24T23:12:06.898770Z","shell.execute_reply":"2022-08-24T23:12:06.970904Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"# 5. Feature engineering #\n\ntime1 = time.time()\n\nfeature_transformer = ColumnTransformer([\n    (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat),\n    ('num', StandardScaler(), col_num)])\n\nprint('Number of features before transformation: ', X_train.shape)\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ', time.time()-time1)\nprint('Number of features after transformation: ', X_train.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:12:06.973283Z","iopub.execute_input":"2022-08-24T23:12:06.973732Z","iopub.status.idle":"2022-08-24T23:12:08.004939Z","shell.execute_reply.started":"2022-08-24T23:12:06.973691Z","shell.execute_reply":"2022-08-24T23:12:08.003889Z"},"trusted":true},"execution_count":93,"outputs":[{"name":"stdout","text":"Number of features before transformation:  (450000, 44)\ntime to do feature proprocessing:  1.0241260528564453\nNumber of features after transformation:  (450000, 92)\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:12:08.008560Z","iopub.execute_input":"2022-08-24T23:12:08.008857Z","iopub.status.idle":"2022-08-24T23:12:08.134020Z","shell.execute_reply.started":"2022-08-24T23:12:08.008830Z","shell.execute_reply":"2022-08-24T23:12:08.132816Z"},"trusted":true},"execution_count":94,"outputs":[{"execution_count":94,"output_type":"execute_result","data":{"text/plain":"        cat__ind_1.0  cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  \\\n0                0.0           0.0           0.0           0.0           0.0   \n1                0.0           0.0           0.0           0.0           0.0   \n2                0.0           0.0           0.0           0.0           0.0   \n3                0.0           0.0           0.0           0.0           0.0   \n4                0.0           0.0           0.0           0.0           0.0   \n...              ...           ...           ...           ...           ...   \n449995           0.0           0.0           0.0           0.0           1.0   \n449996           0.0           0.0           0.0           0.0           0.0   \n449997           0.0           0.0           0.0           0.0           0.0   \n449998           0.0           0.0           0.0           0.0           0.0   \n449999           0.0           0.0           0.0           0.0           0.0   \n\n        cat__ind_6.0  cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  \\\n0                0.0           0.0           0.0           0.0            0.0   \n1                0.0           0.0           0.0           0.0            0.0   \n2                0.0           0.0           0.0           0.0            0.0   \n3                0.0           0.0           0.0           0.0            0.0   \n4                0.0           0.0           0.0           0.0            0.0   \n...              ...           ...           ...           ...            ...   \n449995           0.0           0.0           0.0           0.0            0.0   \n449996           0.0           0.0           0.0           0.0            0.0   \n449997           0.0           0.0           0.0           0.0            0.0   \n449998           0.0           0.0           0.0           0.0            0.0   \n449999           0.0           0.0           0.0           0.0            0.0   \n\n        cat__ind_11.0  cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n449995            0.0            0.0            0.0            0.0   \n449996            0.0            0.0            0.0            0.0   \n449997            0.0            0.0            0.0            0.0   \n449998            0.0            0.0            0.0            0.0   \n449999            0.0            0.0            0.0            0.0   \n\n        cat__ind_15.0  cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 1.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n449995            0.0            0.0            0.0            0.0   \n449996            0.0            0.0            0.0            0.0   \n449997            0.0            0.0            0.0            0.0   \n449998            0.0            0.0            0.0            0.0   \n449999            0.0            0.0            0.0            0.0   \n\n        cat__ind_19.0  cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n449995            0.0            0.0            0.0            0.0   \n449996            0.0            0.0            0.0            0.0   \n449997            0.0            0.0            0.0            0.0   \n449998            0.0            1.0            0.0            0.0   \n449999            0.0            0.0            0.0            0.0   \n\n        cat__ind_23.0  cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n449995            0.0            0.0            0.0            0.0   \n449996            0.0            0.0            0.0            0.0   \n449997            0.0            0.0            0.0            0.0   \n449998            0.0            0.0            0.0            0.0   \n449999            0.0            0.0            0.0            0.0   \n\n        cat__ind_27.0  cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            1.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n449995            0.0            0.0            0.0            0.0   \n449996            0.0            0.0            0.0            0.0   \n449997            0.0            0.0            0.0            0.0   \n449998            0.0            0.0            0.0            0.0   \n449999            0.0            0.0            0.0            0.0   \n\n        cat__ind_31.0  cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n449995            0.0            0.0            0.0            0.0   \n449996            0.0            1.0            0.0            0.0   \n449997            0.0            0.0            0.0            1.0   \n449998            0.0            0.0            0.0            0.0   \n449999            0.0            0.0            0.0            0.0   \n\n        cat__ind_35.0  cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  \\\n0                 0.0            0.0            1.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 1.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            1.0            0.0            0.0   \n...               ...            ...            ...            ...   \n449995            0.0            0.0            0.0            0.0   \n449996            0.0            0.0            0.0            0.0   \n449997            0.0            0.0            0.0            0.0   \n449998            0.0            0.0            0.0            0.0   \n449999            0.0            0.0            0.0            0.0   \n\n        cat__ind_39.0  cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n449995            0.0            0.0            0.0            0.0   \n449996            0.0            0.0            0.0            0.0   \n449997            0.0            0.0            0.0            0.0   \n449998            0.0            0.0            0.0            0.0   \n449999            1.0            0.0            0.0            0.0   \n\n        cat__ind_43.0  cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n449995            0.0            0.0            0.0            0.0   \n449996            0.0            0.0            0.0            0.0   \n449997            0.0            0.0            0.0            0.0   \n449998            0.0            0.0            0.0            0.0   \n449999            0.0            0.0            0.0            0.0   \n\n        cat__ind_47.0  cat__ind_48.0  cat__ind_49.0  num__mom482  num__mom242  \\\n0                 0.0            0.0            0.0    -0.240646    -0.635248   \n1                 0.0            0.0            0.0    -1.016460    -0.218044   \n2                 0.0            0.0            0.0    -1.229874    -1.369932   \n3                 0.0            0.0            0.0     0.599680     0.912902   \n4                 0.0            0.0            0.0    -0.240646    -1.118020   \n...               ...            ...            ...          ...          ...   \n449995            0.0            0.0            0.0     1.315888     0.897482   \n449996            0.0            0.0            0.0    -0.240646     0.117182   \n449997            0.0            0.0            0.0    -0.240646    -0.483596   \n449998            0.0            0.0            0.0    -0.294051    -0.482389   \n449999            0.0            0.0            0.0    -0.240646    -0.595909   \n\n         num__bm   num__op   num__gp  num__inv  num__mom11  num__mom122  \\\n0      -0.527049 -0.406069 -0.764860 -0.773249   -0.476608    -0.144158   \n1       1.529826 -1.200961 -1.848452 -0.654341    1.557056    -0.268592   \n2      -0.743028 -2.840265 -0.639449 -1.532933   -1.027465     0.949180   \n3      -0.125444  1.206628  0.590711 -0.152859    2.221795     0.501581   \n4       0.421713  0.037479  0.284706 -0.039219   -1.031418    -1.067072   \n...          ...       ...       ...       ...         ...          ...   \n449995 -0.256444  0.297743 -0.757515 -0.486596   -0.048452     0.674240   \n449996  0.134817  0.491549 -0.558503 -0.146420    0.577661    -0.347240   \n449997 -0.622137  0.073549  0.521537  3.020454   -1.191934    -0.159041   \n449998  0.180248 -0.672195 -1.350763  0.772823   -0.946176    -0.218174   \n449999  0.231826  0.628168 -0.701019  0.109948    0.392048    -0.607762   \n\n        num__amhd  num__ivol_capm  num__ivol_ff5  num__beta_bw  num__MAX  \\\n0       -0.142463       -0.944909      -0.902088     -0.866760 -1.013839   \n1       -0.372125        0.916577       0.844947      1.064234  0.895307   \n2        1.528221        1.219169       0.798262      0.941486  2.681038   \n3       -0.030106       -0.259338      -0.223331     -0.524402  0.186396   \n4        1.816061       -0.236898      -0.226901     -2.803847 -0.735317   \n...           ...             ...            ...           ...       ...   \n449995  -2.112626       -0.981807      -0.923759     -0.038707 -0.970784   \n449996  -1.072196       -0.280221      -0.524855      0.386285  0.446779   \n449997   0.572189        0.516987       0.538511     -0.988096  0.021528   \n449998   0.109179       -0.103079      -0.074033      0.193084 -0.794811   \n449999  -0.159908       -0.612382      -0.492889     -0.385014 -0.796175   \n\n        num__vol1m  num__vol6m  num__vol12m  num__BAspr  num__size  num__lbm  \\\n0        -0.969523   -0.882379    -0.806243    0.125563   0.534777 -1.002737   \n1         1.129414    1.232448     0.804749   -0.308849   0.559382  0.977632   \n2         0.000564    2.681210     2.621339    0.078728  -1.014599 -1.627575   \n3        -0.181723   -0.166973    -0.377322   -0.067633   0.627950 -0.104204   \n4        -0.470434    1.196160     0.999394    1.472644  -1.260530  0.327831   \n...            ...         ...          ...         ...        ...       ...   \n449995   -0.998238   -1.169561    -1.208532   -0.746890   2.129434  0.219557   \n449996   -0.237400    0.166416     1.146319   -0.714149   0.949998 -0.299957   \n449997    0.426417    1.082450     1.222030   -0.216454  -0.396376 -0.561976   \n449998   -0.148249   -0.523460    -0.489950   -0.308849   0.034253  0.089611   \n449999   -0.685955   -0.904388    -1.099085   -0.308849   0.923945 -0.077137   \n\n        num__lop  num__lgp  num__linv  num__llme  num__l1amhd  num__l1MAX  \\\n0      -0.454161 -0.925964   2.864368   0.510155    -0.128980   -0.870890   \n1      -0.399222 -1.474584  -0.543267   0.388057    -0.360666    1.323561   \n2      -1.905415 -1.301892   1.706309  -1.211820     1.529655    0.230743   \n3       1.139329  0.613217  -0.253519   0.427970    -0.034526   -0.844919   \n4      -0.505525 -0.129697   2.864368  -1.012383     1.803289    1.120410   \n...          ...       ...        ...        ...          ...         ...   \n449995 -0.176765 -0.821488  -1.535737   2.060959    -2.125719   -0.842998   \n449996  0.149962 -0.869739   1.208811   0.991974    -1.085173   -0.229780   \n449997 -0.055523  0.736006   2.864368  -0.368991     0.577938    1.186278   \n449998 -1.111199 -1.372185  -1.535737   0.246356     0.100926   -0.338160   \n449999  0.737898 -0.584015  -0.145575   1.034571    -0.188982   -0.363358   \n\n        num__l1BAspr  num__l3amhd  num__l3MAX  num__l3BAspr  num__l6amhd  \\\n0           0.414454    -0.110788   -0.859146      0.163348    -0.125365   \n1          -0.307199    -0.287688    1.299383     -0.305689    -0.159842   \n2           1.121315     1.539860    2.862878      0.873381     1.160461   \n3           0.250754    -0.032014    0.595524      0.046651     0.011054   \n4           1.416475     1.779533    0.854988      1.710854     1.778915   \n...              ...          ...         ...           ...          ...   \n449995     -0.749538    -2.134768   -0.633853     -0.733743    -2.163420   \n449996     -0.714417    -1.104680    0.185499     -0.742942    -1.096121   \n449997      0.164812     0.506247    0.857384     -0.276469     0.321943   \n449998     -0.307199     0.051787   -0.411216     -0.305689     0.023342   \n449999     -0.307199    -0.238902   -0.628181     -0.305689    -0.274164   \n\n        num__l6MAX  num__l6BAspr  num__l12amhd  num__l12MAX  num__l12BAspr  \\\n0        -0.372280     -0.091536     -0.193510    -0.870890      -0.152518   \n1         0.721888     -0.300875     -0.047951     1.323561      -0.294550   \n2         1.278479      3.642144      0.842800     0.230743       2.386013   \n3        -0.725997     -0.181339      0.012074    -0.844919       0.320572   \n4        -0.597086      2.113466      1.650196     1.120410       1.838916   \n...            ...           ...           ...          ...            ...   \n449995   -1.081304     -0.733012     -2.256971    -0.842998      -0.776046   \n449996    0.793698     -0.740700     -1.104243    -0.229780      -0.764267   \n449997    2.101103      0.188464      0.141236     1.186278      -0.322785   \n449998   -0.042728     -0.300875      0.113819    -0.338160      -0.294550   \n449999   -1.081304     -0.300875     -0.177655    -0.363358      -0.294550   \n\n        num__l12mom122  num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  \\\n0            -0.343396          -0.686115         -0.578729         0.231450   \n1            -1.220669           1.076149          0.537834         0.560930   \n2            -1.637392           2.827290          2.833406         2.441543   \n3             0.041882          -0.680164         -0.750219        -0.864214   \n4            -0.452692           0.738409          0.673157        -1.543876   \n...                ...                ...               ...              ...   \n449995        0.890425          -1.082554         -1.051521        -0.064716   \n449996        1.346810           0.388836          0.594572        -0.074963   \n449997       -0.428796           1.850562          1.490528         1.317339   \n449998       -0.194195          -0.914682         -0.834364         0.015063   \n449999       -0.426033          -1.055172         -1.037008        -0.109342   \n\n        num__l12vol6m  num__l12vol12m  num__amhd_miss  num__BAspr_miss  \n0           -0.592654       -0.451629       -0.463038        -0.848667  \n1            0.230858        0.174888       -0.463038         1.178318  \n2            2.475681        2.351473       -0.463038        -0.848667  \n3           -0.773355       -0.982240       -0.463038        -0.848667  \n4            0.729433        1.151345       -0.463038        -0.848667  \n...               ...             ...             ...              ...  \n449995      -1.195207       -1.206483       -0.463038        -0.848667  \n449996       0.579000        0.242498       -0.463038        -0.848667  \n449997       1.221495        0.834802       -0.463038        -0.848667  \n449998      -0.705622       -0.748996       -0.463038         1.178318  \n449999      -1.195207       -1.225751       -0.463038         1.178318  \n\n[450000 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.240646</td>\n      <td>-0.635248</td>\n      <td>-0.527049</td>\n      <td>-0.406069</td>\n      <td>-0.764860</td>\n      <td>-0.773249</td>\n      <td>-0.476608</td>\n      <td>-0.144158</td>\n      <td>-0.142463</td>\n      <td>-0.944909</td>\n      <td>-0.902088</td>\n      <td>-0.866760</td>\n      <td>-1.013839</td>\n      <td>-0.969523</td>\n      <td>-0.882379</td>\n      <td>-0.806243</td>\n      <td>0.125563</td>\n      <td>0.534777</td>\n      <td>-1.002737</td>\n      <td>-0.454161</td>\n      <td>-0.925964</td>\n      <td>2.864368</td>\n      <td>0.510155</td>\n      <td>-0.128980</td>\n      <td>-0.870890</td>\n      <td>0.414454</td>\n      <td>-0.110788</td>\n      <td>-0.859146</td>\n      <td>0.163348</td>\n      <td>-0.125365</td>\n      <td>-0.372280</td>\n      <td>-0.091536</td>\n      <td>-0.193510</td>\n      <td>-0.870890</td>\n      <td>-0.152518</td>\n      <td>-0.343396</td>\n      <td>-0.686115</td>\n      <td>-0.578729</td>\n      <td>0.231450</td>\n      <td>-0.592654</td>\n      <td>-0.451629</td>\n      <td>-0.463038</td>\n      <td>-0.848667</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.016460</td>\n      <td>-0.218044</td>\n      <td>1.529826</td>\n      <td>-1.200961</td>\n      <td>-1.848452</td>\n      <td>-0.654341</td>\n      <td>1.557056</td>\n      <td>-0.268592</td>\n      <td>-0.372125</td>\n      <td>0.916577</td>\n      <td>0.844947</td>\n      <td>1.064234</td>\n      <td>0.895307</td>\n      <td>1.129414</td>\n      <td>1.232448</td>\n      <td>0.804749</td>\n      <td>-0.308849</td>\n      <td>0.559382</td>\n      <td>0.977632</td>\n      <td>-0.399222</td>\n      <td>-1.474584</td>\n      <td>-0.543267</td>\n      <td>0.388057</td>\n      <td>-0.360666</td>\n      <td>1.323561</td>\n      <td>-0.307199</td>\n      <td>-0.287688</td>\n      <td>1.299383</td>\n      <td>-0.305689</td>\n      <td>-0.159842</td>\n      <td>0.721888</td>\n      <td>-0.300875</td>\n      <td>-0.047951</td>\n      <td>1.323561</td>\n      <td>-0.294550</td>\n      <td>-1.220669</td>\n      <td>1.076149</td>\n      <td>0.537834</td>\n      <td>0.560930</td>\n      <td>0.230858</td>\n      <td>0.174888</td>\n      <td>-0.463038</td>\n      <td>1.178318</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.229874</td>\n      <td>-1.369932</td>\n      <td>-0.743028</td>\n      <td>-2.840265</td>\n      <td>-0.639449</td>\n      <td>-1.532933</td>\n      <td>-1.027465</td>\n      <td>0.949180</td>\n      <td>1.528221</td>\n      <td>1.219169</td>\n      <td>0.798262</td>\n      <td>0.941486</td>\n      <td>2.681038</td>\n      <td>0.000564</td>\n      <td>2.681210</td>\n      <td>2.621339</td>\n      <td>0.078728</td>\n      <td>-1.014599</td>\n      <td>-1.627575</td>\n      <td>-1.905415</td>\n      <td>-1.301892</td>\n      <td>1.706309</td>\n      <td>-1.211820</td>\n      <td>1.529655</td>\n      <td>0.230743</td>\n      <td>1.121315</td>\n      <td>1.539860</td>\n      <td>2.862878</td>\n      <td>0.873381</td>\n      <td>1.160461</td>\n      <td>1.278479</td>\n      <td>3.642144</td>\n      <td>0.842800</td>\n      <td>0.230743</td>\n      <td>2.386013</td>\n      <td>-1.637392</td>\n      <td>2.827290</td>\n      <td>2.833406</td>\n      <td>2.441543</td>\n      <td>2.475681</td>\n      <td>2.351473</td>\n      <td>-0.463038</td>\n      <td>-0.848667</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.599680</td>\n      <td>0.912902</td>\n      <td>-0.125444</td>\n      <td>1.206628</td>\n      <td>0.590711</td>\n      <td>-0.152859</td>\n      <td>2.221795</td>\n      <td>0.501581</td>\n      <td>-0.030106</td>\n      <td>-0.259338</td>\n      <td>-0.223331</td>\n      <td>-0.524402</td>\n      <td>0.186396</td>\n      <td>-0.181723</td>\n      <td>-0.166973</td>\n      <td>-0.377322</td>\n      <td>-0.067633</td>\n      <td>0.627950</td>\n      <td>-0.104204</td>\n      <td>1.139329</td>\n      <td>0.613217</td>\n      <td>-0.253519</td>\n      <td>0.427970</td>\n      <td>-0.034526</td>\n      <td>-0.844919</td>\n      <td>0.250754</td>\n      <td>-0.032014</td>\n      <td>0.595524</td>\n      <td>0.046651</td>\n      <td>0.011054</td>\n      <td>-0.725997</td>\n      <td>-0.181339</td>\n      <td>0.012074</td>\n      <td>-0.844919</td>\n      <td>0.320572</td>\n      <td>0.041882</td>\n      <td>-0.680164</td>\n      <td>-0.750219</td>\n      <td>-0.864214</td>\n      <td>-0.773355</td>\n      <td>-0.982240</td>\n      <td>-0.463038</td>\n      <td>-0.848667</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.240646</td>\n      <td>-1.118020</td>\n      <td>0.421713</td>\n      <td>0.037479</td>\n      <td>0.284706</td>\n      <td>-0.039219</td>\n      <td>-1.031418</td>\n      <td>-1.067072</td>\n      <td>1.816061</td>\n      <td>-0.236898</td>\n      <td>-0.226901</td>\n      <td>-2.803847</td>\n      <td>-0.735317</td>\n      <td>-0.470434</td>\n      <td>1.196160</td>\n      <td>0.999394</td>\n      <td>1.472644</td>\n      <td>-1.260530</td>\n      <td>0.327831</td>\n      <td>-0.505525</td>\n      <td>-0.129697</td>\n      <td>2.864368</td>\n      <td>-1.012383</td>\n      <td>1.803289</td>\n      <td>1.120410</td>\n      <td>1.416475</td>\n      <td>1.779533</td>\n      <td>0.854988</td>\n      <td>1.710854</td>\n      <td>1.778915</td>\n      <td>-0.597086</td>\n      <td>2.113466</td>\n      <td>1.650196</td>\n      <td>1.120410</td>\n      <td>1.838916</td>\n      <td>-0.452692</td>\n      <td>0.738409</td>\n      <td>0.673157</td>\n      <td>-1.543876</td>\n      <td>0.729433</td>\n      <td>1.151345</td>\n      <td>-0.463038</td>\n      <td>-0.848667</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>449995</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.315888</td>\n      <td>0.897482</td>\n      <td>-0.256444</td>\n      <td>0.297743</td>\n      <td>-0.757515</td>\n      <td>-0.486596</td>\n      <td>-0.048452</td>\n      <td>0.674240</td>\n      <td>-2.112626</td>\n      <td>-0.981807</td>\n      <td>-0.923759</td>\n      <td>-0.038707</td>\n      <td>-0.970784</td>\n      <td>-0.998238</td>\n      <td>-1.169561</td>\n      <td>-1.208532</td>\n      <td>-0.746890</td>\n      <td>2.129434</td>\n      <td>0.219557</td>\n      <td>-0.176765</td>\n      <td>-0.821488</td>\n      <td>-1.535737</td>\n      <td>2.060959</td>\n      <td>-2.125719</td>\n      <td>-0.842998</td>\n      <td>-0.749538</td>\n      <td>-2.134768</td>\n      <td>-0.633853</td>\n      <td>-0.733743</td>\n      <td>-2.163420</td>\n      <td>-1.081304</td>\n      <td>-0.733012</td>\n      <td>-2.256971</td>\n      <td>-0.842998</td>\n      <td>-0.776046</td>\n      <td>0.890425</td>\n      <td>-1.082554</td>\n      <td>-1.051521</td>\n      <td>-0.064716</td>\n      <td>-1.195207</td>\n      <td>-1.206483</td>\n      <td>-0.463038</td>\n      <td>-0.848667</td>\n    </tr>\n    <tr>\n      <th>449996</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.240646</td>\n      <td>0.117182</td>\n      <td>0.134817</td>\n      <td>0.491549</td>\n      <td>-0.558503</td>\n      <td>-0.146420</td>\n      <td>0.577661</td>\n      <td>-0.347240</td>\n      <td>-1.072196</td>\n      <td>-0.280221</td>\n      <td>-0.524855</td>\n      <td>0.386285</td>\n      <td>0.446779</td>\n      <td>-0.237400</td>\n      <td>0.166416</td>\n      <td>1.146319</td>\n      <td>-0.714149</td>\n      <td>0.949998</td>\n      <td>-0.299957</td>\n      <td>0.149962</td>\n      <td>-0.869739</td>\n      <td>1.208811</td>\n      <td>0.991974</td>\n      <td>-1.085173</td>\n      <td>-0.229780</td>\n      <td>-0.714417</td>\n      <td>-1.104680</td>\n      <td>0.185499</td>\n      <td>-0.742942</td>\n      <td>-1.096121</td>\n      <td>0.793698</td>\n      <td>-0.740700</td>\n      <td>-1.104243</td>\n      <td>-0.229780</td>\n      <td>-0.764267</td>\n      <td>1.346810</td>\n      <td>0.388836</td>\n      <td>0.594572</td>\n      <td>-0.074963</td>\n      <td>0.579000</td>\n      <td>0.242498</td>\n      <td>-0.463038</td>\n      <td>-0.848667</td>\n    </tr>\n    <tr>\n      <th>449997</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.240646</td>\n      <td>-0.483596</td>\n      <td>-0.622137</td>\n      <td>0.073549</td>\n      <td>0.521537</td>\n      <td>3.020454</td>\n      <td>-1.191934</td>\n      <td>-0.159041</td>\n      <td>0.572189</td>\n      <td>0.516987</td>\n      <td>0.538511</td>\n      <td>-0.988096</td>\n      <td>0.021528</td>\n      <td>0.426417</td>\n      <td>1.082450</td>\n      <td>1.222030</td>\n      <td>-0.216454</td>\n      <td>-0.396376</td>\n      <td>-0.561976</td>\n      <td>-0.055523</td>\n      <td>0.736006</td>\n      <td>2.864368</td>\n      <td>-0.368991</td>\n      <td>0.577938</td>\n      <td>1.186278</td>\n      <td>0.164812</td>\n      <td>0.506247</td>\n      <td>0.857384</td>\n      <td>-0.276469</td>\n      <td>0.321943</td>\n      <td>2.101103</td>\n      <td>0.188464</td>\n      <td>0.141236</td>\n      <td>1.186278</td>\n      <td>-0.322785</td>\n      <td>-0.428796</td>\n      <td>1.850562</td>\n      <td>1.490528</td>\n      <td>1.317339</td>\n      <td>1.221495</td>\n      <td>0.834802</td>\n      <td>-0.463038</td>\n      <td>-0.848667</td>\n    </tr>\n    <tr>\n      <th>449998</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.294051</td>\n      <td>-0.482389</td>\n      <td>0.180248</td>\n      <td>-0.672195</td>\n      <td>-1.350763</td>\n      <td>0.772823</td>\n      <td>-0.946176</td>\n      <td>-0.218174</td>\n      <td>0.109179</td>\n      <td>-0.103079</td>\n      <td>-0.074033</td>\n      <td>0.193084</td>\n      <td>-0.794811</td>\n      <td>-0.148249</td>\n      <td>-0.523460</td>\n      <td>-0.489950</td>\n      <td>-0.308849</td>\n      <td>0.034253</td>\n      <td>0.089611</td>\n      <td>-1.111199</td>\n      <td>-1.372185</td>\n      <td>-1.535737</td>\n      <td>0.246356</td>\n      <td>0.100926</td>\n      <td>-0.338160</td>\n      <td>-0.307199</td>\n      <td>0.051787</td>\n      <td>-0.411216</td>\n      <td>-0.305689</td>\n      <td>0.023342</td>\n      <td>-0.042728</td>\n      <td>-0.300875</td>\n      <td>0.113819</td>\n      <td>-0.338160</td>\n      <td>-0.294550</td>\n      <td>-0.194195</td>\n      <td>-0.914682</td>\n      <td>-0.834364</td>\n      <td>0.015063</td>\n      <td>-0.705622</td>\n      <td>-0.748996</td>\n      <td>-0.463038</td>\n      <td>1.178318</td>\n    </tr>\n    <tr>\n      <th>449999</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.240646</td>\n      <td>-0.595909</td>\n      <td>0.231826</td>\n      <td>0.628168</td>\n      <td>-0.701019</td>\n      <td>0.109948</td>\n      <td>0.392048</td>\n      <td>-0.607762</td>\n      <td>-0.159908</td>\n      <td>-0.612382</td>\n      <td>-0.492889</td>\n      <td>-0.385014</td>\n      <td>-0.796175</td>\n      <td>-0.685955</td>\n      <td>-0.904388</td>\n      <td>-1.099085</td>\n      <td>-0.308849</td>\n      <td>0.923945</td>\n      <td>-0.077137</td>\n      <td>0.737898</td>\n      <td>-0.584015</td>\n      <td>-0.145575</td>\n      <td>1.034571</td>\n      <td>-0.188982</td>\n      <td>-0.363358</td>\n      <td>-0.307199</td>\n      <td>-0.238902</td>\n      <td>-0.628181</td>\n      <td>-0.305689</td>\n      <td>-0.274164</td>\n      <td>-1.081304</td>\n      <td>-0.300875</td>\n      <td>-0.177655</td>\n      <td>-0.363358</td>\n      <td>-0.294550</td>\n      <td>-0.426033</td>\n      <td>-1.055172</td>\n      <td>-1.037008</td>\n      <td>-0.109342</td>\n      <td>-1.195207</td>\n      <td>-1.225751</td>\n      <td>-0.463038</td>\n      <td>1.178318</td>\n    </tr>\n  </tbody>\n</table>\n<p>450000 rows  92 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Model fitting #\n\n# first, some trivial baselines:\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\ntime1 = time.time()\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=300, max_depth=6, eta=0.05, colsample_bytree=0.6)\nxgb1.fit(X_train, y_train)\nprint('XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:29:23.389788Z","iopub.execute_input":"2022-08-24T23:29:23.390375Z","iopub.status.idle":"2022-08-24T23:29:32.839364Z","shell.execute_reply.started":"2022-08-24T23:29:23.390341Z","shell.execute_reply":"2022-08-24T23:29:32.838496Z"},"trusted":true},"execution_count":99,"outputs":[{"name":"stdout","text":"mae of a constant model 10.389806908094874\nR2 of a constant model 0.0\nXGB train: 10.136495747533294 0.0830074368680972 9.427964210510254\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[300, 500], 'max_depth':[2,4,6], 'eta':[0.02, 0.04, 0.06],\n             'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbm = GridSearchCV(xgb, param_grid, cv=2, verbose=2, scoring='neg_mean_absolute_error')\nxgbm.fit(X_train, y_train)\nprint('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)\n# this runs for 40 min and finds \n# 'eta': 0.02, 'max_depth': 6, 'n_estimators': 500, 0.01095415380877135\nprint('XGB train:', mean_absolute_error(y_train, xgbm.predict(X_train)), r2_score(y_train, xgbm.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:12:17.533915Z","iopub.execute_input":"2022-08-24T23:12:17.536253Z","iopub.status.idle":"2022-08-24T23:14:38.630784Z","shell.execute_reply.started":"2022-08-24T23:12:17.536214Z","shell.execute_reply":"2022-08-24T23:14:38.630018Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 18 candidates, totalling 36 fits\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=300, subsample=0.6; total time=   1.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=300, subsample=0.6; total time=   1.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.8s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=300, subsample=0.6; total time=   3.2s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.6s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.7s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.2s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.7s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.4s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=300, subsample=0.6; total time=   1.9s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=300, subsample=0.6; total time=   2.0s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.6s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.5s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.6s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.6s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.8s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.7s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.4s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.6s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.8s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.4s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=300, subsample=0.6; total time=   1.8s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=300, subsample=0.6; total time=   1.9s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.5s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.5s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.6s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.7s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.7s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.1s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.4s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.5s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.6s\nXGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 4, 'n_estimators': 500, 'subsample': 0.6} -10.366587737626642 135.0766487121582\nXGB train: 10.318687949190421 0.024045170757232892 141.08638501167297\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\n\ndef objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    cv_regularizer=0.01\n    # Usually values between 0.1 and 0.2 work fine.\n\n    params = {\n        \"tree_method\": 'gpu_hist',\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.005, 0.2),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 0.95),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 0.95),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 20.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 150.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 10.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 10)    }\n    # usually it makes sense to resrtict hyperparameter space from some solutions which Optuna will find\n    # e.g., for tmx-joined data only (downsampled tmx), optuna keeps selecting depths of 2 and 3.\n    # for my purposes (smooth left side of prc, close to 1), those solutions are no good.\n\n    temp_out = []\n\n    for i in range(cv_runs):\n\n        X = X_train\n        y = y_train\n\n        model = XGBRegressor(**params, njobs=-1)\n        rkf = KFold(n_splits=n_splits, shuffle=True)\n        X_values = X.values\n        y_values = y.values\n        y_pred = np.zeros_like(y_values)\n        y_pred_train = np.zeros_like(y_values)\n        for train_index, test_index in rkf.split(X_values):\n            X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n            y_A, y_B = y_values[train_index], y_values[test_index]\n            model.fit(X_A, y_A, eval_set=[(X_B, y_B)], verbose = False)\n            y_pred[test_index] += model.predict(X_B)\n                      \n            \n        #score_train = roc_auc_score(y_train, y_pred_train)\n        score_test = mean_absolute_error(y_train, y_pred) \n        #overfit = score_train-score_test\n        #temp_out.append(score_test-cv_regularizer*overfit)\n        temp_out.append(score_test)\n\n    return (np.mean(temp_out))\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=30)\n\nprint('Total time for hypermarameter optimization ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\n\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\n\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X_train, y_train)\nprint('Optuna XGB train:', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:14:38.634335Z","iopub.execute_input":"2022-08-24T23:14:38.636034Z","iopub.status.idle":"2022-08-24T23:21:27.774253Z","shell.execute_reply.started":"2022-08-24T23:14:38.635995Z","shell.execute_reply":"2022-08-24T23:21:27.773474Z"},"trusted":true},"execution_count":97,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2022-08-24 23:14:38,647]\u001b[0m A new study created in memory with name: no-name-3d554597-dc08-4eb3-8ecf-41a006c42517\u001b[0m\n\u001b[32m[I 2022-08-24 23:14:53,480]\u001b[0m Trial 0 finished with value: 10.526722817294694 and parameters: {'n_estimators': 481, 'max_depth': 6, 'learning_rate': 0.15021528161431894, 'colsample_bytree': 0.26610290032367545, 'subsample': 0.9017868285847415, 'alpha': 0.10513064422806374, 'lambda': 1.6870741169989811, 'gamma': 2.9798475690959514e-07, 'min_child_weight': 0.21663360598759243}. Best is trial 0 with value: 10.526722817294694.\u001b[0m\n\u001b[32m[I 2022-08-24 23:15:10,276]\u001b[0m Trial 1 finished with value: 10.566288437335386 and parameters: {'n_estimators': 384, 'max_depth': 7, 'learning_rate': 0.11191867875631323, 'colsample_bytree': 0.6194954204240425, 'subsample': 0.7976609565300435, 'alpha': 0.41479687377488966, 'lambda': 0.12240104563464604, 'gamma': 4.946222361832333e-07, 'min_child_weight': 0.7811800317001102}. Best is trial 0 with value: 10.526722817294694.\u001b[0m\n\u001b[32m[I 2022-08-24 23:15:17,340]\u001b[0m Trial 2 finished with value: 10.412355239957423 and parameters: {'n_estimators': 290, 'max_depth': 4, 'learning_rate': 0.15863211564248217, 'colsample_bytree': 0.7433664976982582, 'subsample': 0.5634423448667082, 'alpha': 5.0735797997710534, 'lambda': 64.52020254945691, 'gamma': 6.784887052604722e-08, 'min_child_weight': 0.2009669187776869}. Best is trial 2 with value: 10.412355239957423.\u001b[0m\n\u001b[32m[I 2022-08-24 23:15:32,169]\u001b[0m Trial 3 finished with value: 10.445093975887609 and parameters: {'n_estimators': 325, 'max_depth': 7, 'learning_rate': 0.09526280783167462, 'colsample_bytree': 0.3215442698123019, 'subsample': 0.8596130918541944, 'alpha': 7.586999872789501, 'lambda': 4.845927235582017, 'gamma': 5.468217881791797e-10, 'min_child_weight': 0.3004741683960491}. Best is trial 2 with value: 10.412355239957423.\u001b[0m\n\u001b[32m[I 2022-08-24 23:15:45,804]\u001b[0m Trial 4 finished with value: 10.509248651731339 and parameters: {'n_estimators': 428, 'max_depth': 6, 'learning_rate': 0.16987263736634614, 'colsample_bytree': 0.28218830682280055, 'subsample': 0.7426055979267445, 'alpha': 0.11061887116702325, 'lambda': 47.843770842711834, 'gamma': 0.06683368553169278, 'min_child_weight': 2.735229144845402}. Best is trial 2 with value: 10.412355239957423.\u001b[0m\n\u001b[32m[I 2022-08-24 23:15:56,593]\u001b[0m Trial 5 finished with value: 10.391476053785592 and parameters: {'n_estimators': 315, 'max_depth': 6, 'learning_rate': 0.06948550478515257, 'colsample_bytree': 0.22831765642927804, 'subsample': 0.779208713163027, 'alpha': 1.7464443481264265, 'lambda': 4.14944292321346, 'gamma': 0.2842410244428049, 'min_child_weight': 1.7592022477808755}. Best is trial 5 with value: 10.391476053785592.\u001b[0m\n\u001b[32m[I 2022-08-24 23:16:01,743]\u001b[0m Trial 6 finished with value: 10.3724878055461 and parameters: {'n_estimators': 221, 'max_depth': 2, 'learning_rate': 0.10529247343970889, 'colsample_bytree': 0.7328601938433336, 'subsample': 0.7938070440226073, 'alpha': 0.1188612644121234, 'lambda': 6.541151395527156, 'gamma': 2.7149982881547734e-05, 'min_child_weight': 0.2792381167348085}. Best is trial 6 with value: 10.3724878055461.\u001b[0m\n\u001b[32m[I 2022-08-24 23:16:26,920]\u001b[0m Trial 7 finished with value: 10.862065273295537 and parameters: {'n_estimators': 171, 'max_depth': 10, 'learning_rate': 0.17474538135348036, 'colsample_bytree': 0.6683932465699128, 'subsample': 0.8005282710788988, 'alpha': 1.29624417240598, 'lambda': 0.24919047214110893, 'gamma': 9.421298009506316e-05, 'min_child_weight': 2.4146498062265884}. Best is trial 6 with value: 10.3724878055461.\u001b[0m\n\u001b[32m[I 2022-08-24 23:16:32,513]\u001b[0m Trial 8 finished with value: 10.377135504651637 and parameters: {'n_estimators': 231, 'max_depth': 3, 'learning_rate': 0.020608359834845994, 'colsample_bytree': 0.9347627790150661, 'subsample': 0.5250047884595312, 'alpha': 0.5763888881127005, 'lambda': 0.22184276368168535, 'gamma': 0.8254595430449668, 'min_child_weight': 0.2596881743257269}. Best is trial 6 with value: 10.3724878055461.\u001b[0m\n\u001b[32m[I 2022-08-24 23:16:55,080]\u001b[0m Trial 9 finished with value: 10.789887348934027 and parameters: {'n_estimators': 392, 'max_depth': 8, 'learning_rate': 0.1595721596954015, 'colsample_bytree': 0.2968643071258381, 'subsample': 0.7359373960129706, 'alpha': 1.0426102978996807, 'lambda': 0.1385312685724703, 'gamma': 2.611996676293e-09, 'min_child_weight': 2.924786063579136}. Best is trial 6 with value: 10.3724878055461.\u001b[0m\n\u001b[32m[I 2022-08-24 23:16:59,324]\u001b[0m Trial 10 finished with value: 10.379954429184467 and parameters: {'n_estimators': 127, 'max_depth': 2, 'learning_rate': 0.0338758608850014, 'colsample_bytree': 0.9041150592748541, 'subsample': 0.636681132412889, 'alpha': 16.0595630055606, 'lambda': 16.769869726427704, 'gamma': 0.0005382419763720687, 'min_child_weight': 0.7543431438607073}. Best is trial 6 with value: 10.3724878055461.\u001b[0m\n\u001b[32m[I 2022-08-24 23:17:04,321]\u001b[0m Trial 11 finished with value: 10.37859028878726 and parameters: {'n_estimators': 220, 'max_depth': 2, 'learning_rate': 0.03418539388034522, 'colsample_bytree': 0.8975296782163642, 'subsample': 0.5014612999354324, 'alpha': 0.3068296352076888, 'lambda': 1.0372006715184088, 'gamma': 0.002446943728357264, 'min_child_weight': 9.408961463667481}. Best is trial 6 with value: 10.3724878055461.\u001b[0m\n\u001b[32m[I 2022-08-24 23:17:10,901]\u001b[0m Trial 12 finished with value: 10.369917368125234 and parameters: {'n_estimators': 234, 'max_depth': 4, 'learning_rate': 0.01170739733514983, 'colsample_bytree': 0.7915044019991332, 'subsample': 0.6833483918781521, 'alpha': 0.27531981497575686, 'lambda': 13.128968793318803, 'gamma': 7.480689969954561, 'min_child_weight': 0.12434391973111497}. Best is trial 12 with value: 10.369917368125234.\u001b[0m\n\u001b[32m[I 2022-08-24 23:17:17,179]\u001b[0m Trial 13 finished with value: 10.388916286608971 and parameters: {'n_estimators': 240, 'max_depth': 4, 'learning_rate': 0.11756518241525046, 'colsample_bytree': 0.49223342980016427, 'subsample': 0.651595065030804, 'alpha': 0.2599631393673622, 'lambda': 13.86284433690236, 'gamma': 9.171477424693002, 'min_child_weight': 0.10967318355680442}. Best is trial 12 with value: 10.369917368125234.\u001b[0m\n\u001b[32m[I 2022-08-24 23:17:22,722]\u001b[0m Trial 14 finished with value: 10.37010507477837 and parameters: {'n_estimators': 175, 'max_depth': 4, 'learning_rate': 0.06350125106850427, 'colsample_bytree': 0.758655523658372, 'subsample': 0.6610372057036412, 'alpha': 0.1962831955162484, 'lambda': 11.71712123425406, 'gamma': 7.916770938815853e-06, 'min_child_weight': 0.11880032133533425}. Best is trial 12 with value: 10.369917368125234.\u001b[0m\n\u001b[32m[I 2022-08-24 23:17:27,332]\u001b[0m Trial 15 finished with value: 10.366723301129893 and parameters: {'n_estimators': 119, 'max_depth': 4, 'learning_rate': 0.06106585821475976, 'colsample_bytree': 0.49953858883341284, 'subsample': 0.653381087202765, 'alpha': 0.23745530679456198, 'lambda': 129.3228336537936, 'gamma': 0.010296570532992565, 'min_child_weight': 0.10070180456628335}. Best is trial 15 with value: 10.366723301129893.\u001b[0m\n\u001b[32m[I 2022-08-24 23:17:32,837]\u001b[0m Trial 16 finished with value: 10.36378794650928 and parameters: {'n_estimators': 123, 'max_depth': 5, 'learning_rate': 0.007266970843085405, 'colsample_bytree': 0.4735517587576551, 'subsample': 0.5889432270080164, 'alpha': 0.6686756309457739, 'lambda': 106.01815188250777, 'gamma': 0.014642157462037968, 'min_child_weight': 0.5717611964882389}. Best is trial 16 with value: 10.36378794650928.\u001b[0m\n\u001b[32m[I 2022-08-24 23:17:38,450]\u001b[0m Trial 17 finished with value: 10.369022543918192 and parameters: {'n_estimators': 119, 'max_depth': 5, 'learning_rate': 0.06175715458342416, 'colsample_bytree': 0.4725518428342641, 'subsample': 0.5882370546552598, 'alpha': 0.7301118984920063, 'lambda': 59.89033173940955, 'gamma': 0.010176541299926928, 'min_child_weight': 0.5022466101293328}. Best is trial 16 with value: 10.36378794650928.\u001b[0m\n\u001b[32m[I 2022-08-24 23:17:43,530]\u001b[0m Trial 18 finished with value: 10.388865660893648 and parameters: {'n_estimators': 101, 'max_depth': 5, 'learning_rate': 0.19636513683961304, 'colsample_bytree': 0.40723866828024896, 'subsample': 0.6140011164249258, 'alpha': 2.5027834343460094, 'lambda': 148.55341768512477, 'gamma': 0.010244660058566193, 'min_child_weight': 1.2499656736532232}. Best is trial 16 with value: 10.36378794650928.\u001b[0m\n\u001b[32m[I 2022-08-24 23:18:01,860]\u001b[0m Trial 19 finished with value: 10.351216404821562 and parameters: {'n_estimators': 178, 'max_depth': 9, 'learning_rate': 0.005045368124716681, 'colsample_bytree': 0.5589097917117927, 'subsample': 0.6913822843602039, 'alpha': 0.5942027248492242, 'lambda': 117.69361633118866, 'gamma': 0.0004512571394868357, 'min_child_weight': 5.63836728178663}. Best is trial 19 with value: 10.351216404821562.\u001b[0m\n\u001b[32m[I 2022-08-24 23:18:21,122]\u001b[0m Trial 20 finished with value: 10.376307276906859 and parameters: {'n_estimators': 174, 'max_depth': 10, 'learning_rate': 0.039517174645983505, 'colsample_bytree': 0.10729102455603706, 'subsample': 0.6947594623041128, 'alpha': 2.8898943528084278, 'lambda': 31.617331179253473, 'gamma': 0.00031270099733445993, 'min_child_weight': 5.853848904881858}. Best is trial 19 with value: 10.351216404821562.\u001b[0m\n\u001b[32m[I 2022-08-24 23:18:37,027]\u001b[0m Trial 21 finished with value: 10.351624979890719 and parameters: {'n_estimators': 143, 'max_depth': 9, 'learning_rate': 0.011883457265230081, 'colsample_bytree': 0.5534749882952535, 'subsample': 0.5665482173561965, 'alpha': 0.6488593909264636, 'lambda': 148.66584249129053, 'gamma': 0.024586116107625636, 'min_child_weight': 0.5114947763135372}. Best is trial 19 with value: 10.351216404821562.\u001b[0m\n\u001b[32m[I 2022-08-24 23:18:53,516]\u001b[0m Trial 22 finished with value: 10.351091544573297 and parameters: {'n_estimators': 147, 'max_depth': 9, 'learning_rate': 0.006479866354655534, 'colsample_bytree': 0.5690544760183198, 'subsample': 0.5646515104706141, 'alpha': 0.7050327134012297, 'lambda': 90.08963570991814, 'gamma': 0.1288306176659676, 'min_child_weight': 0.4510255222909596}. Best is trial 22 with value: 10.351091544573297.\u001b[0m\n\u001b[32m[I 2022-08-24 23:19:11,188]\u001b[0m Trial 23 finished with value: 10.35626595752577 and parameters: {'n_estimators': 163, 'max_depth': 9, 'learning_rate': 0.02281589281956177, 'colsample_bytree': 0.59191331131413, 'subsample': 0.5349249163988549, 'alpha': 0.4840729478098099, 'lambda': 34.464264893803744, 'gamma': 0.22027707872233288, 'min_child_weight': 0.43309147567311906}. Best is trial 22 with value: 10.351091544573297.\u001b[0m\n\u001b[32m[I 2022-08-24 23:19:31,503]\u001b[0m Trial 24 finished with value: 10.368840614838579 and parameters: {'n_estimators': 195, 'max_depth': 9, 'learning_rate': 0.04459394048601259, 'colsample_bytree': 0.6051053821667027, 'subsample': 0.5715692868379559, 'alpha': 0.9628902807360838, 'lambda': 82.43048379819992, 'gamma': 0.000776067843146648, 'min_child_weight': 1.1367637862535984}. Best is trial 22 with value: 10.351091544573297.\u001b[0m\n\u001b[32m[I 2022-08-24 23:19:57,885]\u001b[0m Trial 25 finished with value: 10.353086069367189 and parameters: {'n_estimators': 276, 'max_depth': 9, 'learning_rate': 0.00588410802308467, 'colsample_bytree': 0.40823316643247265, 'subsample': 0.6120144047141483, 'alpha': 0.9023661265600732, 'lambda': 26.28353319381187, 'gamma': 4.381133468729323e-06, 'min_child_weight': 3.7577865280225473}. Best is trial 22 with value: 10.351091544573297.\u001b[0m\n\u001b[32m[I 2022-08-24 23:20:15,574]\u001b[0m Trial 26 finished with value: 10.354701703536117 and parameters: {'n_estimators': 260, 'max_depth': 8, 'learning_rate': 0.020138784951105725, 'colsample_bytree': 0.5553691038031766, 'subsample': 0.5500777386997062, 'alpha': 2.12569197222526, 'lambda': 82.74171440387998, 'gamma': 1.6742027790805567, 'min_child_weight': 1.582588217946493}. Best is trial 22 with value: 10.351091544573297.\u001b[0m\n\u001b[32m[I 2022-08-24 23:20:27,242]\u001b[0m Trial 27 finished with value: 10.38259308400001 and parameters: {'n_estimators': 148, 'max_depth': 8, 'learning_rate': 0.08697828795543544, 'colsample_bytree': 0.6559597711918571, 'subsample': 0.708710106781495, 'alpha': 0.3785282106256236, 'lambda': 145.6974821661835, 'gamma': 0.03971209470866027, 'min_child_weight': 0.410768107576487}. Best is trial 22 with value: 10.351091544573297.\u001b[0m\n\u001b[32m[I 2022-08-24 23:20:55,126]\u001b[0m Trial 28 finished with value: 10.44764154236766 and parameters: {'n_estimators': 205, 'max_depth': 10, 'learning_rate': 0.047466253748340244, 'colsample_bytree': 0.39410430355708215, 'subsample': 0.5005932327528635, 'alpha': 0.17202400844587418, 'lambda': 1.5260959110330878, 'gamma': 0.0019567572011905815, 'min_child_weight': 0.7993858624513998}. Best is trial 22 with value: 10.351091544573297.\u001b[0m\n\u001b[32m[I 2022-08-24 23:21:15,637]\u001b[0m Trial 29 finished with value: 10.581400244714434 and parameters: {'n_estimators': 488, 'max_depth': 7, 'learning_rate': 0.12636953020458289, 'colsample_bytree': 0.6831577410919297, 'subsample': 0.9485119685281389, 'alpha': 1.4411540774604292, 'lambda': 0.621498047763084, 'gamma': 0.00012501534972049702, 'min_child_weight': 6.883164489613659}. Best is trial 22 with value: 10.351091544573297.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  396.99320578575134\n        n_estimators : 147\n           max_depth : 9\n       learning_rate : 0.006479866354655534\n    colsample_bytree : 0.5690544760183198\n           subsample : 0.5646515104706141\n               alpha : 0.7050327134012297\n              lambda : 90.08963570991814\n               gamma : 0.1288306176659676\n    min_child_weight : 0.4510255222909596\nbest objective value : 10.351091544573297\nOptuna XGB train: 10.299659210427196 0.019808488237133992 409.12291526794434\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate performance of XGB models:\n\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\nprint('XGB GS test:', mean_absolute_error(y_test, xgbm.predict(X_test)), r2_score(y_test, xgbm.predict(X_test)))\nprint('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:21:27.777564Z","iopub.execute_input":"2022-08-24T23:21:27.778871Z","iopub.status.idle":"2022-08-24T23:21:29.497031Z","shell.execute_reply.started":"2022-08-24T23:21:27.778827Z","shell.execute_reply":"2022-08-24T23:21:29.496262Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"XGB test: 10.433286907906377 0.00680636279583724\nXGB GS test: 10.43436500298352 0.006703743049378708\nOptuna XGB test: 10.421613860522193 0.004916413442041523\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_size = 0.1\n# df.reset_index(inplace=True, drop=True)\n# #random.seed(2)\n# test_index = random.sample(list(df.index), int(test_size*df.shape[0]))\n# train = df.iloc[list(set(df.index)-set(test_index))]\n# test = df.iloc[test_index]\n# train.reset_index(drop=True, inplace=True)\n# test.reset_index(drop=True, inplace=True)\n# display(train.shape, test.shape, train.head(3), test.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:10:56.715852Z","iopub.status.idle":"2022-08-24T22:10:56.716304Z","shell.execute_reply.started":"2022-08-24T22:10:56.716076Z","shell.execute_reply":"2022-08-24T22:10:56.716096Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
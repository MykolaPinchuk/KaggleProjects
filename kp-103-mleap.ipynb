{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Load libraries and data.\n2. pEDA. Look at feature distribution, fix them if they do not look right.\n3. Train-test split. Most likely couple years into test set. 2015-2018?. Impute missing values.\n4. Transform numerical features, add ohe for inds.\n5. Fit classic models: ols as a baseline, then xgb.\n6. Fir DL.\n\n\nNotes:\nideally, I want to use time-based cross-validation.\nsince I have panel data, it is not a trivial task.\nneed to find some solution online.\ne.g., https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8.\n\nfor now, will try to do siple for loop.\n","metadata":{}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, KFold, PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\npd.set_option('display.max_columns', 110)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:00:02.579361Z","iopub.execute_input":"2022-09-06T15:00:02.579836Z","iopub.status.idle":"2022-09-06T15:00:13.392952Z","shell.execute_reply.started":"2022-09-06T15:00:02.579748Z","shell.execute_reply":"2022-09-06T15:00:13.391579Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:00:13.397024Z","iopub.execute_input":"2022-09-06T15:00:13.398098Z","iopub.status.idle":"2022-09-06T15:00:13.411775Z","shell.execute_reply.started":"2022-09-06T15:00:13.398053Z","shell.execute_reply":"2022-09-06T15:00:13.409956Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:00:13.414375Z","iopub.execute_input":"2022-09-06T15:00:13.415046Z","iopub.status.idle":"2022-09-06T15:00:13.452394Z","shell.execute_reply.started":"2022-09-06T15:00:13.414990Z","shell.execute_reply":"2022-09-06T15:00:13.450921Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"#min_prd_list = range(100, 676, 25)\nmin_prd_list = [150, 250, 350, 450, 550, 650]\n#min_prd = min_prd_list[0]\nwindows_width = 3*12\ncv_regularizer=0.2\noptuna_trials = 10\ntime0 = time.time()\n\nresults = pd.DataFrame(columns = ['min_prd', 'xgbf_train', 'xgbf_val', 'xgbf_test', \n                                  'xgbgs_train', 'xgbgs_val', 'xgbgs_test', \n                                  'xgbo_train', 'xgbo_val', 'xgbo_test'])\nresults.min_prd = min_prd_list\n\nfor min_prd in min_prd_list:\n\n    with open('../input/kaggle-46pkl/IMLEAP_v4.pkl', 'rb') as pickled_one:\n        df = pickle.load(pickled_one)\n    df = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+3))]\n    df_cnt = df.count()\n    empty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\n    df.drop(columns=empty_cols, inplace=True)\n    #display(df.shape, df.head(), df.year.describe(), df.count())\n\n    df = df[(df.RET>-50)&(df.RET<75)]\n    meanret = df.groupby('prd').RET.mean().to_frame().reset_index().rename(columns={'RET':'mRET'})\n    df = pd.merge(df, meanret, on='prd', how='left')\n    df.RET = df.RET-df.mRET\n    df.drop(columns='mRET', inplace=True)\n\n    features_miss_dummies = ['amhd', 'BAspr']\n    for col in features_miss_dummies:\n        if col in df.columns:\n            df[col+'_miss'] = df[col].isnull().astype(int)\n\n    temp_cols = ['PERMNO', 'year', 'prd']\n    df.reset_index(inplace=True, drop=True)\n    X = df.copy()\n    y = X.pop('RET')\n\n    train_indx = X.prd<(min_prd+windows_width-1)\n    val_indx = X.prd==(min_prd+windows_width-1)\n    val_indx_extra = X.prd==(min_prd+windows_width+1)\n    test_indx = X.prd==(min_prd+windows_width)\n\n    X_train = X[train_indx]\n    X_val = X[val_indx]\n    X_val_extra = X[val_indx_extra]\n    X_test = X[test_indx]\n    y_train = y[train_indx]\n    y_val = y[val_indx]\n    y_val_extra = y[val_indx_extra]\n    y_test = y[test_indx]\n\n    #display(X_train.head(3), X_train.tail(3), y_train.head(3), y_train.tail(3))\n    display(X_train.shape, X_val.shape, X_test.shape, X_train.prd.describe(), X_val.prd.describe(), X_test.prd.describe())\n\n    X_train.drop(columns=temp_cols, inplace=True)\n    X_val.drop(columns=temp_cols, inplace=True)\n    X_val_extra.drop(columns=temp_cols, inplace=True)\n    X_test.drop(columns=temp_cols, inplace=True)\n\n    #display(X_train.tail())\n    col_cat = ['ind']\n    col_num = [x for x in X_train.columns if x not in col_cat]\n    for col in col_num:\n        X_train[col] = X_train[col].fillna(X_train[col].median())\n        X_val[col] = X_val[col].fillna(X_train[col].median())\n        X_val_extra[col] = X_val_extra[col].fillna(X_train[col].median())\n        X_test[col] = X_test[col].fillna(X_train[col].median())\n    for col in col_cat:\n        X_train[col] = X_train[col].fillna(value=-1000)\n        X_val[col] = X_val[col].fillna(value=-1000)\n        X_val_extra[col] = X_val_extra[col].fillna(value=-1000)\n        X_test[col] = X_test[col].fillna(value=-1000)\n\n    #display(X_train.tail())\n    feature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                            (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                            remainder=\"passthrough\")\n\n    print('Number of features before transformation: ', X_train.shape)\n    train_index, val_index, val_index_extra, test_index = X_train.index, X_val.index, X_val_extra.index, X_test.index\n    X_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\n    X_val = pd.DataFrame(feature_transformer.transform(X_val), columns=feature_transformer.get_feature_names_out())\n    X_val_extra = pd.DataFrame(feature_transformer.transform(X_val_extra), columns=feature_transformer.get_feature_names_out())\n    X_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\n    print('time to do feature proprocessing: ')\n    print('Number of features after transformation: ', X_train.shape, X_val.shape, X_val_extra.shape, X_test.shape)\n    X_train.index = train_index\n    X_val.index = val_index\n    X_val_extra.index = val_index_extra\n    X_test.index = test_index\n    #display(X_train.tail())\n\n    X = pd.concat([X_train, X_val])\n    y = pd.concat([y_train, y_val])\n    #display(X,y)\n\n    X_ = pd.concat([X_train, X_val, X_val_extra])\n    y_ = pd.concat([y_train, y_val, y_val_extra])\n    #display(X,y, X_,y_)\n\n    print('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n    print('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\n    xgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=400, max_depth=4, eta=0.02, colsample_bytree=0.4, subsample=0.6)\n    xgb1.fit(X_train, y_train)\n    print('fixed XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\n    print('XGB val:', mean_absolute_error(y_val, xgb1.predict(X_val)), r2_score(y_val, xgb1.predict(X_val)))\n    print('XGB val extra:', mean_absolute_error(y_val_extra, xgb1.predict(X_val_extra)), r2_score(y_val_extra, xgb1.predict(X_val_extra)))\n    print('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbf_train':'xgbf_test'] = \\\n    [r2_score(y_train, xgb1.predict(X_train)), \n    r2_score(y_val, xgb1.predict(X_val)),\n    r2_score(y_test, xgb1.predict(X_test))]\n\n    time1 = time.time()\n\n    # Create a list where train data indices are -1 and validation data indices are 0\n    split_index = [-1 if x in X_train.index else 0 for x in X.index]\n    pds = PredefinedSplit(test_fold = split_index)\n\n    xgb = XGBRegressor(tree_method = 'gpu_hist')\n    param_grid = {'n_estimators':[400, 600, 800], 'max_depth':[2,3,4,5], 'eta':[0.006, 0.012, 0.02], \n                  'subsample':[0.6], 'colsample_bytree':[0.6]}\n    xgbgs = GridSearchCV(estimator = xgb, cv=pds, param_grid=param_grid)\n\n    # Fit with all data\n    xgbgs.fit(X_, y_)\n\n    print('gs XGB', xgbgs.best_params_, xgbgs.best_score_, time.time()-time1)\n    print('XGB train:', mean_absolute_error(y_train, xgbgs.predict(X_train)), r2_score(y_train, xgbgs.predict(X_train)))\n    print('XGB validation:', mean_absolute_error(y_val, xgbgs.predict(X_val)), r2_score(y_val, xgbgs.predict(X_val)))\n    print('XGB validation extra:', mean_absolute_error(y_val_extra, xgbgs.predict(X_val_extra)), r2_score(y_val_extra, xgbgs.predict(X_val_extra)))\n    print('XGB test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_score(y_test, xgbgs.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbgs_train':'xgbgs_test'] = \\\n    [r2_score(y_train, xgbgs.predict(X_train)), \n    r2_score(y_val, xgbgs.predict(X_val)),\n    r2_score(y_test, xgbgs.predict(X_test))]\n\n    time1 = time.time()\n    def objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n        params = {\n        \"tree_method\": 'gpu_hist',\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1000),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.0005, 0.03),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.05, 0.95),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 0.95),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 50.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 500.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 100.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 100)    }\n\n        model = XGBRegressor(**params, njobs=-1)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose = False)\n\n        score_train = r2_score(y_train, model.predict(X_train))\n        score_val = r2_score(y_val, model.predict(X_val))\n        score_val_extra = r2_score(y_val_extra, model.predict(X_val_extra)) \n        score_val = (score_val+score_val_extra)/2\n        overfit = np.abs(score_train-score_val)\n\n        return score_val-cv_regularizer*overfit\n\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=optuna_trials)\n    print('Total time for hypermarameter optimization ', time.time()-time1)\n    hp = study.best_params\n    for key, value in hp.items():\n        print(f\"{key:>20s} : {value}\")\n    print(f\"{'best objective value':>20s} : {study.best_value}\")\n    optuna_hyperpars = study.best_params\n    optuna_hyperpars['tree_method']='gpu_hist'\n    optuna_xgb = XGBRegressor(**optuna_hyperpars)\n    optuna_xgb.fit(X, y)\n    print('Optuna XGB train: \\n', \n          mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), '\\nvalidation \\n',\n          mean_absolute_error(y_val, optuna_xgb.predict(X_val)), r2_score(y_val, optuna_xgb.predict(X_val)),\n          mean_absolute_error(y_val_extra, optuna_xgb.predict(X_val_extra)), r2_score(y_val_extra, optuna_xgb.predict(X_val_extra)), '\\ntest \\n',\n          mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbo_train':'xgbo_test'] = \\\n    [r2_score(y_train, optuna_xgb.predict(X_train)), \n    r2_score(y_val, optuna_xgb.predict(X_val)),\n    r2_score(y_test, optuna_xgb.predict(X_test))]\n\n    display(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:00:13.456940Z","iopub.execute_input":"2022-09-06T15:00:13.457344Z","iopub.status.idle":"2022-09-06T15:09:46.738117Z","shell.execute_reply.started":"2022-09-06T15:00:13.457317Z","shell.execute_reply":"2022-09-06T15:09:46.736787Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"(46500, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(1485, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(1485, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    46500.000000\nmean       167.403097\nstd         10.350122\nmin        149.000000\n25%        159.000000\n50%        168.000000\n75%        176.000000\nmax        184.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    1485.0\nmean      185.0\nstd         0.0\nmin       185.0\n25%       185.0\n50%       185.0\n75%       185.0\nmax       185.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    1485.0\nmean      186.0\nstd         0.0\nmin       186.0\n25%       186.0\n50%       186.0\n75%       186.0\nmax       186.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (46500, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (46500, 84) (1485, 84) (1474, 84) (1485, 84)\nmae of a constant model 7.611699806708129\nR2 of a constant model 0.0\nfixed XGB train: 7.1529799389174675 0.07120427640683447\nXGB val: 9.183082887242872 -0.00016061966534830496\nXGB val extra: 8.553188621861128 0.01153515650833925\nXGB test: 8.77751868102996 0.016670313684464477\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.006, 'max_depth': 2, 'n_estimators': 800, 'subsample': 0.6} 0.005150627933506624 54.70077300071716\nXGB train: 7.313471429509544 0.01831733337915975\nXGB validation: 9.106041737477625 0.01610699090269707\nXGB validation extra: 8.521897418727352 0.029404842200184467\nXGB test: 8.765554307865417 0.021497743796950575\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 15:01:20,870]\u001b[0m A new study created in memory with name: no-name-26b7e579-e8e9-4bba-b186-6d198e8a2683\u001b[0m\n\u001b[32m[I 2022-09-06 15:01:21,466]\u001b[0m Trial 0 finished with value: 0.004954209832742728 and parameters: {'n_estimators': 716, 'max_depth': 2, 'learning_rate': 0.02845221243135854, 'colsample_bytree': 0.2074474655431569, 'subsample': 0.5184173505271172, 'alpha': 0.7755045392131564, 'lambda': 1.9782934057405683, 'gamma': 1.3595321715264572, 'min_child_weight': 0.3560084439414596}. Best is trial 0 with value: 0.004954209832742728.\u001b[0m\n\u001b[32m[I 2022-09-06 15:01:25,139]\u001b[0m Trial 1 finished with value: 0.0022559209602501127 and parameters: {'n_estimators': 727, 'max_depth': 5, 'learning_rate': 0.003305178452628404, 'colsample_bytree': 0.32895792713022004, 'subsample': 0.8699498585534569, 'alpha': 20.587774911857935, 'lambda': 21.96204936409525, 'gamma': 2.5751713675922224e-05, 'min_child_weight': 1.4616406093370715}. Best is trial 0 with value: 0.004954209832742728.\u001b[0m\n\u001b[32m[I 2022-09-06 15:01:26,201]\u001b[0m Trial 2 finished with value: 0.004679468225396666 and parameters: {'n_estimators': 765, 'max_depth': 2, 'learning_rate': 0.012703743846216892, 'colsample_bytree': 0.2217461457227632, 'subsample': 0.5856993639175411, 'alpha': 0.37822609876038216, 'lambda': 0.6913018305843541, 'gamma': 2.3927719975480208e-09, 'min_child_weight': 0.5074338579849929}. Best is trial 0 with value: 0.004954209832742728.\u001b[0m\n\u001b[32m[I 2022-09-06 15:01:27,154]\u001b[0m Trial 3 finished with value: 0.0013838378853753717 and parameters: {'n_estimators': 829, 'max_depth': 3, 'learning_rate': 0.012185702423517061, 'colsample_bytree': 0.7254856333539298, 'subsample': 0.283975579942494, 'alpha': 0.4044644477023808, 'lambda': 0.36098620019566313, 'gamma': 1.183904375039508e-06, 'min_child_weight': 0.1456387455899987}. Best is trial 0 with value: 0.004954209832742728.\u001b[0m\n\u001b[32m[I 2022-09-06 15:01:28,237]\u001b[0m Trial 4 finished with value: 0.0035016819765159466 and parameters: {'n_estimators': 784, 'max_depth': 4, 'learning_rate': 0.004197081873690092, 'colsample_bytree': 0.21586007290566572, 'subsample': 0.44813753763397723, 'alpha': 1.0947457886455856, 'lambda': 7.874067820571985, 'gamma': 79.40284215531251, 'min_child_weight': 0.1466568451662812}. Best is trial 0 with value: 0.004954209832742728.\u001b[0m\n\u001b[32m[I 2022-09-06 15:01:28,982]\u001b[0m Trial 5 finished with value: 0.006074037012272604 and parameters: {'n_estimators': 870, 'max_depth': 4, 'learning_rate': 0.015828929497737346, 'colsample_bytree': 0.44974727197516856, 'subsample': 0.23098060732459935, 'alpha': 14.016306937171839, 'lambda': 47.84929866153429, 'gamma': 1.0856917429589176e-08, 'min_child_weight': 0.30210342275790314}. Best is trial 5 with value: 0.006074037012272604.\u001b[0m\n\u001b[32m[I 2022-09-06 15:01:30,629]\u001b[0m Trial 6 finished with value: 0.001632990689006508 and parameters: {'n_estimators': 786, 'max_depth': 5, 'learning_rate': 0.006921138630327088, 'colsample_bytree': 0.8682322148486411, 'subsample': 0.811729352376366, 'alpha': 23.29523334289553, 'lambda': 0.9880021543274137, 'gamma': 1.008404626437763e-06, 'min_child_weight': 9.298664194993123}. Best is trial 5 with value: 0.006074037012272604.\u001b[0m\n\u001b[32m[I 2022-09-06 15:01:31,325]\u001b[0m Trial 7 finished with value: 0.0014392045365094084 and parameters: {'n_estimators': 621, 'max_depth': 5, 'learning_rate': 0.022409299391423464, 'colsample_bytree': 0.6181381769813855, 'subsample': 0.4591209549396722, 'alpha': 0.3935791021619593, 'lambda': 13.168427191014885, 'gamma': 0.0010453292799951915, 'min_child_weight': 0.34701978057785515}. Best is trial 5 with value: 0.006074037012272604.\u001b[0m\n\u001b[32m[I 2022-09-06 15:01:31,738]\u001b[0m Trial 8 finished with value: 0.005792632402310471 and parameters: {'n_estimators': 896, 'max_depth': 2, 'learning_rate': 0.028895094342465875, 'colsample_bytree': 0.842373245533434, 'subsample': 0.17998900406601867, 'alpha': 6.603230421747682, 'lambda': 20.822428610287165, 'gamma': 6.59098897689276e-10, 'min_child_weight': 4.502758550094922}. Best is trial 5 with value: 0.006074037012272604.\u001b[0m\n\u001b[32m[I 2022-09-06 15:01:32,456]\u001b[0m Trial 9 finished with value: 0.00023748022392688917 and parameters: {'n_estimators': 803, 'max_depth': 6, 'learning_rate': 0.018911739807425405, 'colsample_bytree': 0.9264734492733979, 'subsample': 0.25278028197023245, 'alpha': 0.47459238350610183, 'lambda': 0.21340296397572164, 'gamma': 0.004612037108699166, 'min_child_weight': 10.963617272036341}. Best is trial 5 with value: 0.006074037012272604.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  11.58721661567688\n        n_estimators : 870\n           max_depth : 4\n       learning_rate : 0.015828929497737346\n    colsample_bytree : 0.44974727197516856\n           subsample : 0.23098060732459935\n               alpha : 14.016306937171839\n              lambda : 47.84929866153429\n               gamma : 1.0856917429589176e-08\n    min_child_weight : 0.30210342275790314\nbest objective value : 0.006074037012272604\nOptuna XGB train: \n 7.138894261758377 0.06957446590845062 \nvalidation \n 8.87284741762145 0.06938383592931607 8.597723976863982 0.0022773945452720934 \ntest \n 8.799772787510003 0.006478275208486539\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150   0.071204 -0.000161   0.01667    0.018317  0.016107   0.021498   \n1      250        NaN       NaN       NaN         NaN       NaN        NaN   \n2      350        NaN       NaN       NaN         NaN       NaN        NaN   \n3      450        NaN       NaN       NaN         NaN       NaN        NaN   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.069574  0.069384  0.006478  \n1        NaN       NaN       NaN  \n2        NaN       NaN       NaN  \n3        NaN       NaN       NaN  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.071204</td>\n      <td>-0.000161</td>\n      <td>0.01667</td>\n      <td>0.018317</td>\n      <td>0.016107</td>\n      <td>0.021498</td>\n      <td>0.069574</td>\n      <td>0.069384</td>\n      <td>0.006478</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(79162, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2145, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2137, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    79162.000000\nmean       266.337409\nstd         10.396123\nmin        249.000000\n25%        257.000000\n50%        266.000000\n75%        275.000000\nmax        284.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2145.0\nmean      285.0\nstd         0.0\nmin       285.0\n25%       285.0\n50%       285.0\n75%       285.0\nmax       285.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2137.0\nmean      286.0\nstd         0.0\nmin       286.0\n25%       286.0\n50%       286.0\n75%       286.0\nmax       286.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (79162, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (79162, 85) (2145, 85) (2123, 85) (2137, 85)\nmae of a constant model 8.11874542158204\nR2 of a constant model 0.0\nfixed XGB train: 7.989726165770913 0.04574106643165299\nXGB val: 7.230642818671135 0.006387854977991436\nXGB val extra: 8.249036348156604 0.001989263307294542\nXGB test: 7.349965616394131 -0.005745749216557128\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6} 0.006919449234190411 64.74776220321655\nXGB train: 7.896457698630022 0.07503272963024654\nXGB validation: 6.972727102724957 0.09007260841175058\nXGB validation extra: 8.025860380885911 0.07515300032276506\nXGB test: 7.287699865107958 0.011620586071633854\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 15:02:48,578]\u001b[0m A new study created in memory with name: no-name-1f3ac558-93cc-4479-b208-2ab5ebac4734\u001b[0m\n\u001b[32m[I 2022-09-06 15:02:49,109]\u001b[0m Trial 0 finished with value: -0.0003788638633571484 and parameters: {'n_estimators': 648, 'max_depth': 3, 'learning_rate': 0.020199618827427237, 'colsample_bytree': 0.5797454247565685, 'subsample': 0.15791569239102904, 'alpha': 0.9387455348777937, 'lambda': 58.348076030158275, 'gamma': 0.016700863095033984, 'min_child_weight': 8.001756759966453}. Best is trial 0 with value: -0.0003788638633571484.\u001b[0m\n\u001b[32m[I 2022-09-06 15:02:49,673]\u001b[0m Trial 1 finished with value: -0.0008786205281355209 and parameters: {'n_estimators': 688, 'max_depth': 4, 'learning_rate': 0.025114790265582203, 'colsample_bytree': 0.2753521431246099, 'subsample': 0.310819180064134, 'alpha': 0.10107965562999052, 'lambda': 0.1258300544143421, 'gamma': 0.17119026673819812, 'min_child_weight': 69.46300775162064}. Best is trial 0 with value: -0.0003788638633571484.\u001b[0m\n\u001b[32m[I 2022-09-06 15:02:51,866]\u001b[0m Trial 2 finished with value: 8.65233240721075e-06 and parameters: {'n_estimators': 529, 'max_depth': 5, 'learning_rate': 0.0034023517174999697, 'colsample_bytree': 0.414244398320911, 'subsample': 0.6976327551579239, 'alpha': 3.5680589287921407, 'lambda': 57.21915502752976, 'gamma': 0.05832283818191535, 'min_child_weight': 51.72427202282284}. Best is trial 2 with value: 8.65233240721075e-06.\u001b[0m\n\u001b[32m[I 2022-09-06 15:02:54,173]\u001b[0m Trial 3 finished with value: 0.0004310465303988707 and parameters: {'n_estimators': 545, 'max_depth': 4, 'learning_rate': 0.004187962737502871, 'colsample_bytree': 0.8579563285021229, 'subsample': 0.9305959247367368, 'alpha': 2.2515333965268045, 'lambda': 173.67193874350858, 'gamma': 0.38652957694785645, 'min_child_weight': 0.13180629703964103}. Best is trial 3 with value: 0.0004310465303988707.\u001b[0m\n\u001b[32m[I 2022-09-06 15:02:54,827]\u001b[0m Trial 4 finished with value: -0.0007926974794166819 and parameters: {'n_estimators': 848, 'max_depth': 2, 'learning_rate': 0.013129049858657057, 'colsample_bytree': 0.5522272643429724, 'subsample': 0.7722114144055918, 'alpha': 49.136516885352975, 'lambda': 65.58473576339696, 'gamma': 3.3844594137893305e-10, 'min_child_weight': 5.848891692748095}. Best is trial 3 with value: 0.0004310465303988707.\u001b[0m\n\u001b[32m[I 2022-09-06 15:02:55,389]\u001b[0m Trial 5 finished with value: -0.0008716139989834559 and parameters: {'n_estimators': 515, 'max_depth': 2, 'learning_rate': 0.0258438140270115, 'colsample_bytree': 0.29781528740879454, 'subsample': 0.46870731314449166, 'alpha': 6.2923255901933794, 'lambda': 1.7768570910671782, 'gamma': 2.3960564854793155e-05, 'min_child_weight': 22.09039645355105}. Best is trial 3 with value: 0.0004310465303988707.\u001b[0m\n\u001b[32m[I 2022-09-06 15:02:55,915]\u001b[0m Trial 6 finished with value: -0.0007236153511450017 and parameters: {'n_estimators': 881, 'max_depth': 3, 'learning_rate': 0.018648799023279258, 'colsample_bytree': 0.27702034882298004, 'subsample': 0.3075844937243156, 'alpha': 0.2852220293562932, 'lambda': 2.189621915975132, 'gamma': 3.440970206009055e-05, 'min_child_weight': 4.969533435953109}. Best is trial 3 with value: 0.0004310465303988707.\u001b[0m\n\u001b[32m[I 2022-09-06 15:02:57,696]\u001b[0m Trial 7 finished with value: 0.0009427474426427773 and parameters: {'n_estimators': 917, 'max_depth': 6, 'learning_rate': 0.013088980468555096, 'colsample_bytree': 0.8392500345621887, 'subsample': 0.15296776714445537, 'alpha': 11.703110264713567, 'lambda': 2.844869045796562, 'gamma': 0.002160397723590732, 'min_child_weight': 0.3995837929227508}. Best is trial 7 with value: 0.0009427474426427773.\u001b[0m\n\u001b[32m[I 2022-09-06 15:02:58,308]\u001b[0m Trial 8 finished with value: -0.0011106020419361594 and parameters: {'n_estimators': 789, 'max_depth': 2, 'learning_rate': 0.007710804621118382, 'colsample_bytree': 0.41526603447834787, 'subsample': 0.24545693097477933, 'alpha': 15.16657166992899, 'lambda': 1.316934872462206, 'gamma': 1.255613475783925e-08, 'min_child_weight': 1.0675647633369143}. Best is trial 7 with value: 0.0009427474426427773.\u001b[0m\n\u001b[32m[I 2022-09-06 15:03:00,715]\u001b[0m Trial 9 finished with value: 0.00010971473791698827 and parameters: {'n_estimators': 927, 'max_depth': 4, 'learning_rate': 0.006922084191704647, 'colsample_bytree': 0.5985107837114846, 'subsample': 0.8458140793575315, 'alpha': 11.135641387567256, 'lambda': 71.19000214752504, 'gamma': 0.03547261583311711, 'min_child_weight': 1.5631427999670147}. Best is trial 7 with value: 0.0009427474426427773.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  12.138232469558716\n        n_estimators : 917\n           max_depth : 6\n       learning_rate : 0.013088980468555096\n    colsample_bytree : 0.8392500345621887\n           subsample : 0.15296776714445537\n               alpha : 11.703110264713567\n              lambda : 2.844869045796562\n               gamma : 0.002160397723590732\n    min_child_weight : 0.3995837929227508\nbest objective value : 0.0009427474426427773\nOptuna XGB train: \n 7.719609618816063 0.1230089836795667 \nvalidation \n 6.817985337709399 0.13607374753073143 8.230769440297554 0.006467734228338906 \ntest \n 7.363379071245889 -0.01239581447600191\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150   0.071204 -0.000161   0.01667    0.018317  0.016107   0.021498   \n1      250   0.045741  0.006388 -0.005746    0.075033  0.090073   0.011621   \n2      350        NaN       NaN       NaN         NaN       NaN        NaN   \n3      450        NaN       NaN       NaN         NaN       NaN        NaN   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.069574  0.069384  0.006478  \n1   0.123009  0.136074 -0.012396  \n2        NaN       NaN       NaN  \n3        NaN       NaN       NaN  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.071204</td>\n      <td>-0.000161</td>\n      <td>0.01667</td>\n      <td>0.018317</td>\n      <td>0.016107</td>\n      <td>0.021498</td>\n      <td>0.069574</td>\n      <td>0.069384</td>\n      <td>0.006478</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.045741</td>\n      <td>0.006388</td>\n      <td>-0.005746</td>\n      <td>0.075033</td>\n      <td>0.090073</td>\n      <td>0.011621</td>\n      <td>0.123009</td>\n      <td>0.136074</td>\n      <td>-0.012396</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(86525, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2346, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2318, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    86525.000000\nmean       366.510604\nstd         10.388374\nmin        349.000000\n25%        357.000000\n50%        366.000000\n75%        376.000000\nmax        384.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2346.0\nmean      385.0\nstd         0.0\nmin       385.0\n25%       385.0\n50%       385.0\n75%       385.0\nmax       385.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2318.0\nmean      386.0\nstd         0.0\nmin       386.0\n25%       386.0\n50%       386.0\n75%       386.0\nmax       386.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (86525, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (86525, 86) (2346, 86) (2610, 86) (2318, 86)\nmae of a constant model 8.840500354133429\nR2 of a constant model 0.0\nfixed XGB train: 8.562905844495273 0.04670825564653647\nXGB val: 10.527740523699148 0.021300346785617985\nXGB val extra: 9.319794429468487 0.016115242465840285\nXGB test: 8.875064932819427 -0.00879510174767284\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 4, 'n_estimators': 400, 'subsample': 0.6} 0.022875798989792306 70.68659043312073\nXGB train: 8.56238929492026 0.046876368556779124\nXGB validation: 10.32372184879376 0.06349487542977317\nXGB validation extra: 9.146381443809611 0.06443084503742291\nXGB test: 8.877039071378235 -0.008644849102869001\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 15:04:30,360]\u001b[0m A new study created in memory with name: no-name-e8a7c8cf-c889-4d68-8ddc-3beb5c1b51c9\u001b[0m\n\u001b[32m[I 2022-09-06 15:04:35,674]\u001b[0m Trial 0 finished with value: 0.011935690124664045 and parameters: {'n_estimators': 813, 'max_depth': 5, 'learning_rate': 0.00852212516059276, 'colsample_bytree': 0.1834825235108407, 'subsample': 0.3450690703845051, 'alpha': 15.063436752184199, 'lambda': 3.706132349621058, 'gamma': 4.398432406275116e-06, 'min_child_weight': 1.4851455080672094}. Best is trial 0 with value: 0.011935690124664045.\u001b[0m\n\u001b[32m[I 2022-09-06 15:04:38,339]\u001b[0m Trial 1 finished with value: 0.01631634863601541 and parameters: {'n_estimators': 659, 'max_depth': 3, 'learning_rate': 0.02300381543543052, 'colsample_bytree': 0.21536374350287985, 'subsample': 0.9410392852249955, 'alpha': 26.459738568772803, 'lambda': 1.3924853403648574, 'gamma': 0.0006194511973296753, 'min_child_weight': 0.39248552068134956}. Best is trial 1 with value: 0.01631634863601541.\u001b[0m\n\u001b[32m[I 2022-09-06 15:04:40,596]\u001b[0m Trial 2 finished with value: 0.011005066163203648 and parameters: {'n_estimators': 505, 'max_depth': 5, 'learning_rate': 0.027673863414332936, 'colsample_bytree': 0.5128019448626567, 'subsample': 0.8972096873890838, 'alpha': 9.991810890389681, 'lambda': 24.586826856557888, 'gamma': 4.52434557434838e-08, 'min_child_weight': 38.09321122854879}. Best is trial 1 with value: 0.01631634863601541.\u001b[0m\n\u001b[32m[I 2022-09-06 15:04:42,398]\u001b[0m Trial 3 finished with value: 0.015775314420524912 and parameters: {'n_estimators': 755, 'max_depth': 5, 'learning_rate': 0.022614448673646485, 'colsample_bytree': 0.5857787920274803, 'subsample': 0.4942379208403901, 'alpha': 0.2410631937900809, 'lambda': 120.55758065646683, 'gamma': 8.415313206433123e-09, 'min_child_weight': 0.2733808474466595}. Best is trial 1 with value: 0.01631634863601541.\u001b[0m\n\u001b[32m[I 2022-09-06 15:04:44,542]\u001b[0m Trial 4 finished with value: 0.01361415012040652 and parameters: {'n_estimators': 567, 'max_depth': 2, 'learning_rate': 0.013198520359137884, 'colsample_bytree': 0.1158688372651067, 'subsample': 0.5948490818796959, 'alpha': 4.283457076146382, 'lambda': 0.7039386452880051, 'gamma': 0.024505580863709762, 'min_child_weight': 1.0327431394386362}. Best is trial 1 with value: 0.01631634863601541.\u001b[0m\n\u001b[32m[I 2022-09-06 15:04:46,399]\u001b[0m Trial 5 finished with value: 0.010056188554388367 and parameters: {'n_estimators': 765, 'max_depth': 5, 'learning_rate': 0.020280771884990997, 'colsample_bytree': 0.1403432342738724, 'subsample': 0.23593545247773712, 'alpha': 47.84075858286965, 'lambda': 0.38750621826923737, 'gamma': 5.107883188354083e-08, 'min_child_weight': 4.8318474465557575}. Best is trial 1 with value: 0.01631634863601541.\u001b[0m\n\u001b[32m[I 2022-09-06 15:04:49,023]\u001b[0m Trial 6 finished with value: 0.010714933849595033 and parameters: {'n_estimators': 829, 'max_depth': 2, 'learning_rate': 0.003114789041020195, 'colsample_bytree': 0.2663238116275625, 'subsample': 0.9157481348118868, 'alpha': 18.138146998664574, 'lambda': 4.476258656813583, 'gamma': 1.1787032109161188e-10, 'min_child_weight': 0.6629827772085932}. Best is trial 1 with value: 0.01631634863601541.\u001b[0m\n\u001b[32m[I 2022-09-06 15:04:49,972]\u001b[0m Trial 7 finished with value: 0.01524662736015676 and parameters: {'n_estimators': 699, 'max_depth': 3, 'learning_rate': 0.028276100999122037, 'colsample_bytree': 0.25048405369138066, 'subsample': 0.164924540042863, 'alpha': 0.22837453045244926, 'lambda': 52.28178092549184, 'gamma': 4.064426118329803e-08, 'min_child_weight': 91.63076068016592}. Best is trial 1 with value: 0.01631634863601541.\u001b[0m\n\u001b[32m[I 2022-09-06 15:04:51,774]\u001b[0m Trial 8 finished with value: 0.016963065181500037 and parameters: {'n_estimators': 561, 'max_depth': 3, 'learning_rate': 0.026083841042181397, 'colsample_bytree': 0.5885222592448868, 'subsample': 0.9159670486463489, 'alpha': 1.7741776230119974, 'lambda': 0.6956535797873422, 'gamma': 7.706283874861532e-09, 'min_child_weight': 72.13357370028801}. Best is trial 8 with value: 0.016963065181500037.\u001b[0m\n\u001b[32m[I 2022-09-06 15:04:54,250]\u001b[0m Trial 9 finished with value: 0.01716022377136972 and parameters: {'n_estimators': 805, 'max_depth': 3, 'learning_rate': 0.01093444849441119, 'colsample_bytree': 0.6278870569680204, 'subsample': 0.28094732057221317, 'alpha': 1.3642129893923913, 'lambda': 122.8139325004871, 'gamma': 0.14126523161236829, 'min_child_weight': 2.968379629850239}. Best is trial 9 with value: 0.01716022377136972.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  23.89175248146057\n        n_estimators : 805\n           max_depth : 3\n       learning_rate : 0.01093444849441119\n    colsample_bytree : 0.6278870569680204\n           subsample : 0.28094732057221317\n               alpha : 1.3642129893923913\n              lambda : 122.8139325004871\n               gamma : 0.14126523161236829\n    min_child_weight : 2.968379629850239\nbest objective value : 0.01716022377136972\nOptuna XGB train: \n 8.6267691632647 0.025190009663262347 \nvalidation \n 10.418006939435815 0.03941029108709704 9.314531301517311 0.014801066070844304 \ntest \n 8.843554046313812 -0.0037301501119104685\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150   0.071204 -0.000161   0.01667    0.018317  0.016107   0.021498   \n1      250   0.045741  0.006388 -0.005746    0.075033  0.090073   0.011621   \n2      350   0.046708    0.0213 -0.008795    0.046876  0.063495  -0.008645   \n3      450        NaN       NaN       NaN         NaN       NaN        NaN   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.069574  0.069384  0.006478  \n1   0.123009  0.136074 -0.012396  \n2    0.02519   0.03941  -0.00373  \n3        NaN       NaN       NaN  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.071204</td>\n      <td>-0.000161</td>\n      <td>0.01667</td>\n      <td>0.018317</td>\n      <td>0.016107</td>\n      <td>0.021498</td>\n      <td>0.069574</td>\n      <td>0.069384</td>\n      <td>0.006478</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.045741</td>\n      <td>0.006388</td>\n      <td>-0.005746</td>\n      <td>0.075033</td>\n      <td>0.090073</td>\n      <td>0.011621</td>\n      <td>0.123009</td>\n      <td>0.136074</td>\n      <td>-0.012396</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.046708</td>\n      <td>0.0213</td>\n      <td>-0.008795</td>\n      <td>0.046876</td>\n      <td>0.063495</td>\n      <td>-0.008645</td>\n      <td>0.02519</td>\n      <td>0.03941</td>\n      <td>-0.00373</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(99700, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2833, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2787, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    99700.000000\nmean       466.677944\nstd         10.329035\nmin        449.000000\n25%        458.000000\n50%        467.000000\n75%        476.000000\nmax        484.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2833.0\nmean      485.0\nstd         0.0\nmin       485.0\n25%       485.0\n50%       485.0\n75%       485.0\nmax       485.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2787.0\nmean      486.0\nstd         0.0\nmin       486.0\n25%       486.0\n50%       486.0\n75%       486.0\nmax       486.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (99700, 44)\ntime to do feature proprocessing: \nNumber of features after transformation:  (99700, 92) (2833, 92) (2727, 92) (2787, 92)\nmae of a constant model 10.210542205300575\nR2 of a constant model 0.0\nfixed XGB train: 9.670539161825753 0.04047777643589612\nXGB val: 14.119854935793931 -0.011400105894938894\nXGB val extra: 12.667085128683043 -0.009290873117867138\nXGB test: 13.45023251492294 0.0008179448039563608\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.006, 'max_depth': 2, 'n_estimators': 400, 'subsample': 0.6} -0.0014937493919477962 74.19417643547058\nXGB train: 9.804290496236602 0.00898440965303382\nXGB validation: 13.960111789536413 0.010042227685765681\nXGB validation extra: 12.641099390315963 0.003887766344075394\nXGB test: 13.452277073068617 0.008149983693027663\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 15:06:19,164]\u001b[0m A new study created in memory with name: no-name-de4695c3-4869-41e8-924c-088a43038c85\u001b[0m\n\u001b[32m[I 2022-09-06 15:06:19,770]\u001b[0m Trial 0 finished with value: -0.0009118629434726078 and parameters: {'n_estimators': 580, 'max_depth': 3, 'learning_rate': 0.016185463667806457, 'colsample_bytree': 0.06623860402864747, 'subsample': 0.8394009336522216, 'alpha': 9.08819098969852, 'lambda': 34.334643822509, 'gamma': 7.379341454318019e-10, 'min_child_weight': 84.44898377381887}. Best is trial 0 with value: -0.0009118629434726078.\u001b[0m\n\u001b[32m[I 2022-09-06 15:06:20,654]\u001b[0m Trial 1 finished with value: -0.0007673759987424144 and parameters: {'n_estimators': 903, 'max_depth': 6, 'learning_rate': 0.027437604727723615, 'colsample_bytree': 0.7022298463266061, 'subsample': 0.2861569017376816, 'alpha': 3.513887237516328, 'lambda': 185.37789878210128, 'gamma': 0.07614884983909964, 'min_child_weight': 1.4142091458911081}. Best is trial 1 with value: -0.0007673759987424144.\u001b[0m\n\u001b[32m[I 2022-09-06 15:06:21,646]\u001b[0m Trial 2 finished with value: -0.0022329559611386166 and parameters: {'n_estimators': 690, 'max_depth': 2, 'learning_rate': 0.023605109108461943, 'colsample_bytree': 0.820329813777407, 'subsample': 0.6579238952502923, 'alpha': 10.294050953443323, 'lambda': 1.9366075787523203, 'gamma': 0.005963724354291606, 'min_child_weight': 0.12608782090030304}. Best is trial 1 with value: -0.0007673759987424144.\u001b[0m\n\u001b[32m[I 2022-09-06 15:06:22,552]\u001b[0m Trial 3 finished with value: -0.0007327716022554664 and parameters: {'n_estimators': 946, 'max_depth': 5, 'learning_rate': 0.01467254791104184, 'colsample_bytree': 0.3121690901103121, 'subsample': 0.3866222536641589, 'alpha': 0.1312272626256783, 'lambda': 1.172019524450208, 'gamma': 2.914465794257053e-07, 'min_child_weight': 1.289584389288468}. Best is trial 3 with value: -0.0007327716022554664.\u001b[0m\n\u001b[32m[I 2022-09-06 15:06:23,254]\u001b[0m Trial 4 finished with value: -0.0009007513739491379 and parameters: {'n_estimators': 797, 'max_depth': 5, 'learning_rate': 0.006685244669215957, 'colsample_bytree': 0.12387524314589198, 'subsample': 0.6457604416850579, 'alpha': 2.1036416080849705, 'lambda': 0.1637571276988583, 'gamma': 0.0001722379761591069, 'min_child_weight': 4.282624724643453}. Best is trial 3 with value: -0.0007327716022554664.\u001b[0m\n\u001b[32m[I 2022-09-06 15:06:23,967]\u001b[0m Trial 5 finished with value: -0.0007302790931564384 and parameters: {'n_estimators': 668, 'max_depth': 5, 'learning_rate': 0.02670436030022084, 'colsample_bytree': 0.5150631630925622, 'subsample': 0.8823534620162584, 'alpha': 5.933446412272011, 'lambda': 79.25002998954662, 'gamma': 0.0007224528817291634, 'min_child_weight': 0.48375231342324615}. Best is trial 5 with value: -0.0007302790931564384.\u001b[0m\n\u001b[32m[I 2022-09-06 15:06:24,699]\u001b[0m Trial 6 finished with value: -0.0007076579563201513 and parameters: {'n_estimators': 847, 'max_depth': 5, 'learning_rate': 0.02328299876517198, 'colsample_bytree': 0.3562608788795625, 'subsample': 0.6454744448495292, 'alpha': 0.213994965361254, 'lambda': 103.02528612786746, 'gamma': 1.2307653966169751e-08, 'min_child_weight': 0.5822118418462454}. Best is trial 6 with value: -0.0007076579563201513.\u001b[0m\n\u001b[32m[I 2022-09-06 15:06:25,409]\u001b[0m Trial 7 finished with value: -0.0008214794062685549 and parameters: {'n_estimators': 769, 'max_depth': 5, 'learning_rate': 0.019214590510828956, 'colsample_bytree': 0.4995206005911394, 'subsample': 0.46079154265503597, 'alpha': 2.1733182428961393, 'lambda': 0.784486936736519, 'gamma': 0.003854490362192825, 'min_child_weight': 22.511292563454287}. Best is trial 6 with value: -0.0007076579563201513.\u001b[0m\n\u001b[32m[I 2022-09-06 15:06:25,975]\u001b[0m Trial 8 finished with value: -0.0008729497149820631 and parameters: {'n_estimators': 692, 'max_depth': 2, 'learning_rate': 0.020916000131002793, 'colsample_bytree': 0.33137859042517864, 'subsample': 0.8448403975360262, 'alpha': 31.55057614284588, 'lambda': 84.31441684556259, 'gamma': 1.0200264075053826e-06, 'min_child_weight': 0.22225430800295462}. Best is trial 6 with value: -0.0007076579563201513.\u001b[0m\n\u001b[32m[I 2022-09-06 15:06:26,686]\u001b[0m Trial 9 finished with value: -0.0007030946842123242 and parameters: {'n_estimators': 943, 'max_depth': 5, 'learning_rate': 0.012296260060313702, 'colsample_bytree': 0.5138384271723192, 'subsample': 0.6527461367164995, 'alpha': 27.7347701584618, 'lambda': 0.2578674138557917, 'gamma': 1.8844107459098522e-08, 'min_child_weight': 3.9042743092260452}. Best is trial 9 with value: -0.0007030946842123242.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  7.524581432342529\n        n_estimators : 943\n           max_depth : 5\n       learning_rate : 0.012296260060313702\n    colsample_bytree : 0.5138384271723192\n           subsample : 0.6527461367164995\n               alpha : 27.7347701584618\n              lambda : 0.2578674138557917\n               gamma : 1.8844107459098522e-08\n    min_child_weight : 3.9042743092260452\nbest objective value : -0.0007030946842123242\nOptuna XGB train: \n 9.490266739435366 0.08539312808074961 \nvalidation \n 13.347948368890012 0.11778394846065599 12.636732527652521 -0.004354622977884137 \ntest \n 13.368969189034402 0.012547068169859177\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150   0.071204 -0.000161   0.01667    0.018317  0.016107   0.021498   \n1      250   0.045741  0.006388 -0.005746    0.075033  0.090073   0.011621   \n2      350   0.046708    0.0213 -0.008795    0.046876  0.063495  -0.008645   \n3      450   0.040478   -0.0114  0.000818    0.008984  0.010042    0.00815   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.069574  0.069384  0.006478  \n1   0.123009  0.136074 -0.012396  \n2    0.02519   0.03941  -0.00373  \n3   0.085393  0.117784  0.012547  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.071204</td>\n      <td>-0.000161</td>\n      <td>0.01667</td>\n      <td>0.018317</td>\n      <td>0.016107</td>\n      <td>0.021498</td>\n      <td>0.069574</td>\n      <td>0.069384</td>\n      <td>0.006478</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.045741</td>\n      <td>0.006388</td>\n      <td>-0.005746</td>\n      <td>0.075033</td>\n      <td>0.090073</td>\n      <td>0.011621</td>\n      <td>0.123009</td>\n      <td>0.136074</td>\n      <td>-0.012396</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.046708</td>\n      <td>0.0213</td>\n      <td>-0.008795</td>\n      <td>0.046876</td>\n      <td>0.063495</td>\n      <td>-0.008645</td>\n      <td>0.02519</td>\n      <td>0.03941</td>\n      <td>-0.00373</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>0.040478</td>\n      <td>-0.0114</td>\n      <td>0.000818</td>\n      <td>0.008984</td>\n      <td>0.010042</td>\n      <td>0.00815</td>\n      <td>0.085393</td>\n      <td>0.117784</td>\n      <td>0.012547</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(78603, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2065, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2059, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    78603.000000\nmean       566.290854\nstd         10.352088\nmin        549.000000\n25%        557.000000\n50%        566.000000\n75%        575.000000\nmax        584.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2065.0\nmean      585.0\nstd         0.0\nmin       585.0\n25%       585.0\n50%       585.0\n75%       585.0\nmax       585.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2059.0\nmean      586.0\nstd         0.0\nmin       586.0\n25%       586.0\n50%       586.0\n75%       586.0\nmax       586.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (78603, 44)\ntime to do feature proprocessing: \nNumber of features after transformation:  (78603, 92) (2065, 92) (2042, 92) (2059, 92)\nmae of a constant model 8.175653033280861\nR2 of a constant model 0.0\nfixed XGB train: 8.126013299046427 0.0500953377217731\nXGB val: 7.527006619549129 -0.010190099713712542\nXGB val extra: 6.837919375551958 0.018002988465907643\nXGB test: 6.7424119650891425 -0.0009784770822560684\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.006, 'max_depth': 4, 'n_estimators': 400, 'subsample': 0.6} -0.004847000815498381 68.23465657234192\nXGB train: 8.215210209331026 0.0231606141626165\nXGB validation: 7.490845302602235 0.005404060755053175\nXGB validation extra: 6.823230532344146 0.022118759568284685\nXGB test: 6.743501158234606 0.0001022404600998783\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 15:07:51,900]\u001b[0m A new study created in memory with name: no-name-89fdb1d0-967a-41ef-b210-2dce8a4209e4\u001b[0m\n\u001b[32m[I 2022-09-06 15:07:52,539]\u001b[0m Trial 0 finished with value: -0.0012422802919705989 and parameters: {'n_estimators': 868, 'max_depth': 5, 'learning_rate': 0.014949098646758203, 'colsample_bytree': 0.18718986725113357, 'subsample': 0.2417867070220699, 'alpha': 39.5556579921763, 'lambda': 0.18943503129401532, 'gamma': 0.0009149315289874026, 'min_child_weight': 7.353412811775146}. Best is trial 0 with value: -0.0012422802919705989.\u001b[0m\n\u001b[32m[I 2022-09-06 15:07:54,755]\u001b[0m Trial 1 finished with value: -0.0016040635068364395 and parameters: {'n_estimators': 503, 'max_depth': 6, 'learning_rate': 0.0008059429314777244, 'colsample_bytree': 0.16057822186856052, 'subsample': 0.7010187728021973, 'alpha': 0.5319289794404337, 'lambda': 1.8627197183847988, 'gamma': 2.73787123635746e-10, 'min_child_weight': 81.22087766631029}. Best is trial 0 with value: -0.0012422802919705989.\u001b[0m\n\u001b[32m[I 2022-09-06 15:07:55,465]\u001b[0m Trial 2 finished with value: -0.001570774159635624 and parameters: {'n_estimators': 733, 'max_depth': 5, 'learning_rate': 0.010815875888732868, 'colsample_bytree': 0.25008952606591317, 'subsample': 0.7117758667021458, 'alpha': 0.3107941472153967, 'lambda': 424.3141487625491, 'gamma': 4.041507209933045, 'min_child_weight': 26.3804235477796}. Best is trial 0 with value: -0.0012422802919705989.\u001b[0m\n\u001b[32m[I 2022-09-06 15:07:56,071]\u001b[0m Trial 3 finished with value: -0.0002543228295455169 and parameters: {'n_estimators': 514, 'max_depth': 4, 'learning_rate': 0.029659852351687502, 'colsample_bytree': 0.3587673802556699, 'subsample': 0.6903844399555382, 'alpha': 0.23800450350633776, 'lambda': 4.310141765758661, 'gamma': 0.8890627803159612, 'min_child_weight': 4.532180314085571}. Best is trial 3 with value: -0.0002543228295455169.\u001b[0m\n\u001b[32m[I 2022-09-06 15:07:56,637]\u001b[0m Trial 4 finished with value: -0.0019173875565519083 and parameters: {'n_estimators': 635, 'max_depth': 4, 'learning_rate': 0.02680422641925459, 'colsample_bytree': 0.6233304391807195, 'subsample': 0.34194314388063707, 'alpha': 1.3394208568029193, 'lambda': 204.5320213422498, 'gamma': 0.017669858585980476, 'min_child_weight': 3.0572715631591847}. Best is trial 3 with value: -0.0002543228295455169.\u001b[0m\n\u001b[32m[I 2022-09-06 15:07:57,145]\u001b[0m Trial 5 finished with value: -0.0009615697688696523 and parameters: {'n_estimators': 630, 'max_depth': 2, 'learning_rate': 0.01721780821984996, 'colsample_bytree': 0.2013614499545151, 'subsample': 0.507417995576617, 'alpha': 33.099573752678694, 'lambda': 9.48256838113247, 'gamma': 7.968185704929524e-10, 'min_child_weight': 13.73377699070236}. Best is trial 3 with value: -0.0002543228295455169.\u001b[0m\n\u001b[32m[I 2022-09-06 15:07:58,076]\u001b[0m Trial 6 finished with value: -0.0018044690888699311 and parameters: {'n_estimators': 905, 'max_depth': 6, 'learning_rate': 0.003951516771742814, 'colsample_bytree': 0.3126364152475318, 'subsample': 0.14304638149256169, 'alpha': 32.81675885730501, 'lambda': 24.835147626558225, 'gamma': 4.882363591623538e-10, 'min_child_weight': 0.2834549160785554}. Best is trial 3 with value: -0.0002543228295455169.\u001b[0m\n\u001b[32m[I 2022-09-06 15:07:58,558]\u001b[0m Trial 7 finished with value: -0.00206962042688561 and parameters: {'n_estimators': 750, 'max_depth': 2, 'learning_rate': 0.017149693523898005, 'colsample_bytree': 0.43434174311996865, 'subsample': 0.38084845418390345, 'alpha': 12.853470215314882, 'lambda': 208.3504078288868, 'gamma': 6.644355769842061e-08, 'min_child_weight': 0.1816407021174221}. Best is trial 3 with value: -0.0002543228295455169.\u001b[0m\n\u001b[32m[I 2022-09-06 15:07:59,090]\u001b[0m Trial 8 finished with value: -0.0016576829956009397 and parameters: {'n_estimators': 1000, 'max_depth': 3, 'learning_rate': 0.009544528290047222, 'colsample_bytree': 0.8933740633381545, 'subsample': 0.3024375565852454, 'alpha': 5.149534483444407, 'lambda': 0.5963026520089076, 'gamma': 28.920663698735453, 'min_child_weight': 72.85938500781506}. Best is trial 3 with value: -0.0002543228295455169.\u001b[0m\n\u001b[32m[I 2022-09-06 15:08:00,318]\u001b[0m Trial 9 finished with value: -0.0009206807533223227 and parameters: {'n_estimators': 921, 'max_depth': 4, 'learning_rate': 0.0017143117536545442, 'colsample_bytree': 0.3493114756788217, 'subsample': 0.18880212683145997, 'alpha': 0.5531197612862694, 'lambda': 6.649428191268787, 'gamma': 8.892996902975396e-06, 'min_child_weight': 1.4245810527566132}. Best is trial 3 with value: -0.0002543228295455169.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  8.422085285186768\n        n_estimators : 514\n           max_depth : 4\n       learning_rate : 0.029659852351687502\n    colsample_bytree : 0.3587673802556699\n           subsample : 0.6903844399555382\n               alpha : 0.23800450350633776\n              lambda : 4.310141765758661\n               gamma : 0.8890627803159612\n    min_child_weight : 4.532180314085571\nbest objective value : -0.0002543228295455169\nOptuna XGB train: \n 8.052535911334903 0.07155090230855266 \nvalidation \n 7.390018962046558 0.042917896481972195 6.849807871552835 0.0135600172157051 \ntest \n 6.74633487131517 -0.00291603384339445\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150   0.071204 -0.000161   0.01667    0.018317  0.016107   0.021498   \n1      250   0.045741  0.006388 -0.005746    0.075033  0.090073   0.011621   \n2      350   0.046708    0.0213 -0.008795    0.046876  0.063495  -0.008645   \n3      450   0.040478   -0.0114  0.000818    0.008984  0.010042    0.00815   \n4      550   0.050095  -0.01019 -0.000978    0.023161  0.005404   0.000102   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.069574  0.069384  0.006478  \n1   0.123009  0.136074 -0.012396  \n2    0.02519   0.03941  -0.00373  \n3   0.085393  0.117784  0.012547  \n4   0.071551  0.042918 -0.002916  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.071204</td>\n      <td>-0.000161</td>\n      <td>0.01667</td>\n      <td>0.018317</td>\n      <td>0.016107</td>\n      <td>0.021498</td>\n      <td>0.069574</td>\n      <td>0.069384</td>\n      <td>0.006478</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.045741</td>\n      <td>0.006388</td>\n      <td>-0.005746</td>\n      <td>0.075033</td>\n      <td>0.090073</td>\n      <td>0.011621</td>\n      <td>0.123009</td>\n      <td>0.136074</td>\n      <td>-0.012396</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.046708</td>\n      <td>0.0213</td>\n      <td>-0.008795</td>\n      <td>0.046876</td>\n      <td>0.063495</td>\n      <td>-0.008645</td>\n      <td>0.02519</td>\n      <td>0.03941</td>\n      <td>-0.00373</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>0.040478</td>\n      <td>-0.0114</td>\n      <td>0.000818</td>\n      <td>0.008984</td>\n      <td>0.010042</td>\n      <td>0.00815</td>\n      <td>0.085393</td>\n      <td>0.117784</td>\n      <td>0.012547</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>0.050095</td>\n      <td>-0.01019</td>\n      <td>-0.000978</td>\n      <td>0.023161</td>\n      <td>0.005404</td>\n      <td>0.000102</td>\n      <td>0.071551</td>\n      <td>0.042918</td>\n      <td>-0.002916</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(61151, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(1594, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(1582, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    61151.000000\nmean       666.117741\nstd         10.398283\nmin        649.000000\n25%        657.000000\n50%        666.000000\n75%        675.000000\nmax        684.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    1594.0\nmean      685.0\nstd         0.0\nmin       685.0\n25%       685.0\n50%       685.0\n75%       685.0\nmax       685.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    1582.0\nmean      686.0\nstd         0.0\nmin       686.0\n25%       686.0\n50%       686.0\n75%       686.0\nmax       686.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (61151, 44)\ntime to do feature proprocessing: \nNumber of features after transformation:  (61151, 92) (1594, 92) (1585, 92) (1582, 92)\nmae of a constant model 7.439980835436514\nR2 of a constant model 0.0\nfixed XGB train: 7.242620590812273 0.06591094654206109\nXGB val: 7.311524541484795 0.04426722318751797\nXGB val extra: 8.622495234842779 0.028798702071412996\nXGB test: 6.383956690981623 0.020157123887624362\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 5, 'n_estimators': 800, 'subsample': 0.6} 0.052306665171114375 63.4166419506073\nXGB train: 6.914571290350368 0.18130090829550216\nXGB validation: 6.63119104606503 0.242366089505619\nXGB validation extra: 7.8670786180098675 0.2254812738814418\nXGB test: 6.359357823955069 0.039765939632249725\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 15:09:14,748]\u001b[0m A new study created in memory with name: no-name-133d8841-b4d4-4526-95fa-276c747e3e35\u001b[0m\n\u001b[32m[I 2022-09-06 15:09:17,323]\u001b[0m Trial 0 finished with value: 0.02417644940252479 and parameters: {'n_estimators': 864, 'max_depth': 6, 'learning_rate': 0.021065140494538167, 'colsample_bytree': 0.9343819634949804, 'subsample': 0.6010937548026669, 'alpha': 8.820030375509061, 'lambda': 7.1199879818289205, 'gamma': 1.1183677107609582e-10, 'min_child_weight': 0.3141126108896247}. Best is trial 0 with value: 0.02417644940252479.\u001b[0m\n\u001b[32m[I 2022-09-06 15:09:19,152]\u001b[0m Trial 1 finished with value: 0.032388123111512956 and parameters: {'n_estimators': 992, 'max_depth': 3, 'learning_rate': 0.016351504818016244, 'colsample_bytree': 0.481732828184353, 'subsample': 0.3375724679183383, 'alpha': 1.5529889825254963, 'lambda': 22.109561592043377, 'gamma': 2.57023304346311e-05, 'min_child_weight': 6.194342481966519}. Best is trial 1 with value: 0.032388123111512956.\u001b[0m\n\u001b[32m[I 2022-09-06 15:09:20,417]\u001b[0m Trial 2 finished with value: 0.0284727601093586 and parameters: {'n_estimators': 893, 'max_depth': 6, 'learning_rate': 0.027238833497764684, 'colsample_bytree': 0.766579638084428, 'subsample': 0.2965257814394511, 'alpha': 0.16403450074524945, 'lambda': 19.80699000980457, 'gamma': 6.090916228267536e-10, 'min_child_weight': 0.11919390079616365}. Best is trial 1 with value: 0.032388123111512956.\u001b[0m\n\u001b[32m[I 2022-09-06 15:09:21,554]\u001b[0m Trial 3 finished with value: 0.028562069235791056 and parameters: {'n_estimators': 626, 'max_depth': 2, 'learning_rate': 0.010972285338762038, 'colsample_bytree': 0.9113257232502079, 'subsample': 0.18920653160009598, 'alpha': 0.48071493054888764, 'lambda': 2.2499595573407793, 'gamma': 2.472813109181202e-07, 'min_child_weight': 23.7986339195395}. Best is trial 1 with value: 0.032388123111512956.\u001b[0m\n\u001b[32m[I 2022-09-06 15:09:25,750]\u001b[0m Trial 4 finished with value: 0.03645394520532948 and parameters: {'n_estimators': 907, 'max_depth': 4, 'learning_rate': 0.011018328197605915, 'colsample_bytree': 0.08131703530808249, 'subsample': 0.855374550479251, 'alpha': 17.342092730170776, 'lambda': 89.62822040609346, 'gamma': 0.0022161248465408434, 'min_child_weight': 18.81264841634753}. Best is trial 4 with value: 0.03645394520532948.\u001b[0m\n\u001b[32m[I 2022-09-06 15:09:29,638]\u001b[0m Trial 5 finished with value: 0.029086004245923912 and parameters: {'n_estimators': 700, 'max_depth': 6, 'learning_rate': 0.010808461905834617, 'colsample_bytree': 0.409866445291302, 'subsample': 0.5979656354935149, 'alpha': 3.620088824262036, 'lambda': 42.26782200622095, 'gamma': 20.444186366302482, 'min_child_weight': 0.12072204141574948}. Best is trial 4 with value: 0.03645394520532948.\u001b[0m\n\u001b[32m[I 2022-09-06 15:09:32,664]\u001b[0m Trial 6 finished with value: 0.023039206032728778 and parameters: {'n_estimators': 663, 'max_depth': 4, 'learning_rate': 0.0022018795601293053, 'colsample_bytree': 0.6557667314364158, 'subsample': 0.2813822489038258, 'alpha': 5.196129491505046, 'lambda': 5.718431872535543, 'gamma': 7.712136486249842e-05, 'min_child_weight': 3.1543362799104324}. Best is trial 4 with value: 0.03645394520532948.\u001b[0m\n\u001b[32m[I 2022-09-06 15:09:34,304]\u001b[0m Trial 7 finished with value: 0.034173717748682345 and parameters: {'n_estimators': 667, 'max_depth': 4, 'learning_rate': 0.023795292489975, 'colsample_bytree': 0.5284454487213753, 'subsample': 0.49769424412944174, 'alpha': 0.1583290618891951, 'lambda': 11.971105464823403, 'gamma': 1.6231402191042556e-05, 'min_child_weight': 0.9333023847268445}. Best is trial 4 with value: 0.03645394520532948.\u001b[0m\n\u001b[32m[I 2022-09-06 15:09:36,993]\u001b[0m Trial 8 finished with value: 0.030880842189300827 and parameters: {'n_estimators': 794, 'max_depth': 4, 'learning_rate': 0.00679523692781832, 'colsample_bytree': 0.14984725972001045, 'subsample': 0.46361291401132865, 'alpha': 13.64961709654253, 'lambda': 5.544337170965518, 'gamma': 6.488258595776546e-08, 'min_child_weight': 0.16015283119034018}. Best is trial 4 with value: 0.03645394520532948.\u001b[0m\n\u001b[32m[I 2022-09-06 15:09:41,702]\u001b[0m Trial 9 finished with value: 0.02428766632899504 and parameters: {'n_estimators': 520, 'max_depth': 6, 'learning_rate': 0.004095472378182427, 'colsample_bytree': 0.39407987260146676, 'subsample': 0.33293110797642284, 'alpha': 0.11827542340207182, 'lambda': 0.22100713233468022, 'gamma': 0.9455169466164429, 'min_child_weight': 0.3894658587733227}. Best is trial 4 with value: 0.03645394520532948.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  26.95559859275818\n        n_estimators : 907\n           max_depth : 4\n       learning_rate : 0.011018328197605915\n    colsample_bytree : 0.08131703530808249\n           subsample : 0.855374550479251\n               alpha : 17.342092730170776\n              lambda : 89.62822040609346\n               gamma : 0.0022161248465408434\n    min_child_weight : 18.81264841634753\nbest objective value : 0.03645394520532948\nOptuna XGB train: \n 7.3122127902638825 0.03437051270490843 \nvalidation \n 7.190935402825914 0.0763504307802142 8.557179077682974 0.04401624547281158 \ntest \n 6.325561150782183 0.044897411335958703\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150   0.071204 -0.000161   0.01667    0.018317  0.016107   0.021498   \n1      250   0.045741  0.006388 -0.005746    0.075033  0.090073   0.011621   \n2      350   0.046708    0.0213 -0.008795    0.046876  0.063495  -0.008645   \n3      450   0.040478   -0.0114  0.000818    0.008984  0.010042    0.00815   \n4      550   0.050095  -0.01019 -0.000978    0.023161  0.005404   0.000102   \n5      650   0.065911  0.044267  0.020157    0.181301  0.242366   0.039766   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.069574  0.069384  0.006478  \n1   0.123009  0.136074 -0.012396  \n2    0.02519   0.03941  -0.00373  \n3   0.085393  0.117784  0.012547  \n4   0.071551  0.042918 -0.002916  \n5   0.034371   0.07635  0.044897  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.071204</td>\n      <td>-0.000161</td>\n      <td>0.01667</td>\n      <td>0.018317</td>\n      <td>0.016107</td>\n      <td>0.021498</td>\n      <td>0.069574</td>\n      <td>0.069384</td>\n      <td>0.006478</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.045741</td>\n      <td>0.006388</td>\n      <td>-0.005746</td>\n      <td>0.075033</td>\n      <td>0.090073</td>\n      <td>0.011621</td>\n      <td>0.123009</td>\n      <td>0.136074</td>\n      <td>-0.012396</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.046708</td>\n      <td>0.0213</td>\n      <td>-0.008795</td>\n      <td>0.046876</td>\n      <td>0.063495</td>\n      <td>-0.008645</td>\n      <td>0.02519</td>\n      <td>0.03941</td>\n      <td>-0.00373</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>0.040478</td>\n      <td>-0.0114</td>\n      <td>0.000818</td>\n      <td>0.008984</td>\n      <td>0.010042</td>\n      <td>0.00815</td>\n      <td>0.085393</td>\n      <td>0.117784</td>\n      <td>0.012547</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>0.050095</td>\n      <td>-0.01019</td>\n      <td>-0.000978</td>\n      <td>0.023161</td>\n      <td>0.005404</td>\n      <td>0.000102</td>\n      <td>0.071551</td>\n      <td>0.042918</td>\n      <td>-0.002916</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>0.065911</td>\n      <td>0.044267</td>\n      <td>0.020157</td>\n      <td>0.181301</td>\n      <td>0.242366</td>\n      <td>0.039766</td>\n      <td>0.034371</td>\n      <td>0.07635</td>\n      <td>0.044897</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"results.iloc[:,1:].mean()\n# cv_regularizer = 0.5\n# optuna_trials = 80\nprint(time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:09:46.741194Z","iopub.execute_input":"2022-09-06T15:09:46.742229Z","iopub.status.idle":"2022-09-06T15:09:46.754530Z","shell.execute_reply.started":"2022-09-06T15:09:46.742185Z","shell.execute_reply":"2022-09-06T15:09:46.752949Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"573.2465569972992\n","output_type":"stream"}]},{"cell_type":"code","source":"# general point:\n# compared to NN, xgb is harder to regularize\n# in NN, you can simply shrink coefficient towards constant prediction.\n# in xgb, you can not do that. the only way to regularize is via hyperparameters.\n# in other words, by tweaking hyperpars, in NN you can approach R^2=0.0 prediction from a constant model arbitrarily close\n# in xgb, you can not do that.\n# by setting eta as low as 0.1% you can bring r2 down to 0.1%, but lowering eta further actyally increases abs(r2).\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:09:46.756446Z","iopub.execute_input":"2022-09-06T15:09:46.757316Z","iopub.status.idle":"2022-09-06T15:09:46.769002Z","shell.execute_reply.started":"2022-09-06T15:09:46.757260Z","shell.execute_reply":"2022-09-06T15:09:46.767476Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate performance of XGB models:\nr2_xgb1 = r2_score(y_test, xgb1.predict(X_test))\nr2_xgbgs = r2_score(y_test, xgbgs.predict(X_test))\nr2_xgbo = r2_score(y_test, optuna_xgb.predict(X_test))\n\nprint('Min_prd: ', min_prd)\nprint('Constant guess: ', mean_absolute_error(y_test, np.ones(len(y_test))*y_test.mean()), \n      r2_score(y_test, np.ones(len(y_test))*y_test.mean()))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_xgb1)\nprint('XGB GS test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_xgbgs)\nprint('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_xgbo)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:09:46.770519Z","iopub.execute_input":"2022-09-06T15:09:46.772026Z","iopub.status.idle":"2022-09-06T15:09:46.940082Z","shell.execute_reply.started":"2022-09-06T15:09:46.771985Z","shell.execute_reply":"2022-09-06T15:09:46.938774Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Min_prd:  650\nConstant guess:  6.455628673143056 0.0\nXGB test: 6.383956690981623 0.020157123887624362\nXGB GS test: 6.359357823955069 0.039765939632249725\nOptuna XGB test: 6.325561150782183 0.044897411335958703\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_ignore = ['RET', 'prd']\ncol_cat = ['ind']\ncol_num = [x for x in train.columns if x not in col_ignore+col_cat]\nfor col in col_num:\n    train[col] = train[col].fillna(train[col].median())\n    test[col] = test[col].fillna(train[col].median())\nfor col in col_cat:\n    train[col] = train[col].fillna(value=-1000)\n    test[col] = test[col].fillna(value=-1000)\n\nX_train = train.copy()\ny_train = X_train.pop('RET')\nX_test = test.copy()\ny_test = X_test.pop('RET')\ny_train.reset_index(inplace=True, drop=True)\n\nfeature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                        (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                        remainder=\"passthrough\")\n\nprint('Number of features before transformation: ', X_train.shape)\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ')\nprint('Number of features after transformation: ', X_train.shape)\n\nX_train0 = X_train.copy()\ny_train0 = y_train.copy()\n\nX_train.drop(columns=['remainder__prd'], inplace=True)\nX_test.drop(columns=['remainder__prd'], inplace=True)\n\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=300, max_depth=5, eta=0.03, colsample_bytree=0.6)\nxgb1.fit(X_train, y_train)\nprint('XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\n\ntime1 = time.time()\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[400, 700], 'max_depth':[2,3,4], 'eta':[0.006, 0.012, 0.02], 'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbm = GridSearchCV(xgb, param_grid, cv=2, verbose=2, scoring='r2')\nxgbm.fit(X_train, y_train)\nprint('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)\nprint('XGB train:', mean_absolute_error(y_train, xgbm.predict(X_train)), r2_score(y_train, xgbm.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.121283Z","iopub.status.idle":"2022-09-06T00:36:53.121922Z","shell.execute_reply.started":"2022-09-06T00:36:53.121669Z","shell.execute_reply":"2022-09-06T00:36:53.121693Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time1 = time.time()\ndef objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    params = {\n    \"tree_method\": 'gpu_hist',\n    \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n    \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1000),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n    \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.001, 0.05),\n    \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 0.95),\n    \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 0.95),\n    \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 30.0),\n    \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 200.0),\n    \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 10.0),\n    \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n    temp_out = []\n\n    for i in range(cv_runs):\n\n        X = X_train\n        y = y_train\n        model = XGBRegressor(**params, njobs=-1)\n        rkf = KFold(n_splits=n_splits, shuffle=True)\n        X_values = X.values\n        y_values = y.values\n        y_pred = np.zeros_like(y_values)\n        y_pred_train = np.zeros_like(y_values)\n        for train_index, test_index in rkf.split(X_values):\n            X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n            y_A, y_B = y_values[train_index], y_values[test_index]\n            model.fit(X_A, y_A, eval_set=[(X_B, y_B)], verbose = False)\n            y_pred[test_index] = model.predict(X_B)\n            y_pred_train[train_index] = model.predict(X_A)\n\n        score_train = r2_score(y_train, y_pred_train)\n        score_test = r2_score(y_train, y_pred) \n        overfit = (score_train-score_test)\n        temp_out.append(score_test-cv_regularizer*overfit)\n\n    return (np.mean(temp_out))\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=optuna_trials)\nprint('Total time for hypermarameter optimization ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X_train, y_train)\nprint('Optuna XGB train:', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), time.time()-time1)\n\n# Evaluate performance of XGB models:\nr2_xgb1 = r2_score(y_test, xgb1.predict(X_test))\nr2_xgbgs = r2_score(y_test, xgbm.predict(X_test))\nr2_xgbo = r2_score(y_test, optuna_xgb.predict(X_test))\n\nprint('Min_prd: ', min_prd)\nprint('Constant guess: ', mean_absolute_error(y_test, np.ones(len(y_test))*y_test.mean()), \n      r2_score(y_test, np.ones(len(y_test))*y_test.mean()))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_xgb1)\nprint('XGB GS test:', mean_absolute_error(y_test, xgbm.predict(X_test)), r2_xgbgs)\nprint('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_xgbo)\n\nresults.loc[results.min_prd==min_prd,'xgbf':'xgbo'] = r2_xgb1, r2_xgbgs, r2_xgbo","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.123533Z","iopub.status.idle":"2022-09-06T00:36:53.124198Z","shell.execute_reply.started":"2022-09-06T00:36:53.123921Z","shell.execute_reply":"2022-09-06T00:36:53.123944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for loop to see appx performance over the whole sample with some rolling window\n\ntime0 = time.time()\n\n#min_prd_list = range(100, 676, 25)\nmin_prd_list = [125]\nwindows_width = 40*12\ncv_regularizer=0.05\noptuna_trials = 2\n\nmin_prd = 100\n    \nwith open('../input/kaggle-46pkl/IMLEAP_v4.pkl', 'rb') as pickled_one:\n    df = pickle.load(pickled_one)\ndf = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+2))]\ndf_cnt = df.count()\nempty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\ndf.drop(columns=empty_cols, inplace=True)\ndisplay(df.shape, df.head(), df.year.describe(), df.count())\n\ndf = df[(df.RET>-50)&(df.RET<75)]\nmeanret = df.groupby('prd').RET.mean().to_frame().reset_index().rename(columns={'RET':'mRET'})\ndf = pd.merge(df, meanret, on='prd', how='left')\ndf.RET = df.RET-df.mRET\ndf.drop(columns='mRET', inplace=True)\n\nfeatures_miss_dummies = ['amhd', 'BAspr']\nfor col in features_miss_dummies:\n    if col in df.columns:\n        df[col+'_miss'] = df[col].isnull().astype(int)\n\ndf.reset_index(inplace=True, drop=True)\ntemp_cols = ['PERMNO', 'year']\ntrain = df[df.prd<(min_prd+windows_width)]\ntest = df[df.prd==(min_prd+windows_width)]\ntrain.drop(columns=temp_cols, inplace=True)\ntest.drop(columns=temp_cols, inplace=True)\n\ncol_ignore = ['RET', 'prd']\ncol_cat = ['ind']\ncol_num = [x for x in train.columns if x not in col_ignore+col_cat]\nfor col in col_num:\n    train[col] = train[col].fillna(train[col].median())\n    test[col] = test[col].fillna(train[col].median())\nfor col in col_cat:\n    train[col] = train[col].fillna(value=-1000)\n    test[col] = test[col].fillna(value=-1000)\n\nX_train = train.copy()\ny_train = X_train.pop('RET')\nX_test = test.copy()\ny_test = X_test.pop('RET')\ny_train.reset_index(inplace=True, drop=True)\n\nfeature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                        (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                        remainder=\"passthrough\")\n\nprint('Number of features before transformation: ', X_train.shape)\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ')\nprint('Number of features after transformation: ', X_train.shape)\n\nX_train0 = X_train.copy()\ny_train0 = y_train.copy()\n\nX_train.drop(columns=['remainder__prd'], inplace=True)\nX_test.drop(columns=['remainder__prd'], inplace=True)\n\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=300, max_depth=5, eta=0.03, colsample_bytree=0.6)\nxgb1.fit(X_train, y_train)\nprint('XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\n\ntime1 = time.time()\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[400, 700], 'max_depth':[2,3,4], 'eta':[0.006, 0.012, 0.02], 'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbm = GridSearchCV(xgb, param_grid, cv=2, verbose=2, scoring='r2')\nxgbm.fit(X_train, y_train)\nprint('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)\nprint('XGB train:', mean_absolute_error(y_train, xgbm.predict(X_train)), r2_score(y_train, xgbm.predict(X_train)), time.time()-time1)\n\ntime1 = time.time()\ndef objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    params = {\n    \"tree_method\": 'gpu_hist',\n    \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n    \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1000),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n    \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.001, 0.03),\n    \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 0.95),\n    \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 0.95),\n    \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 30.0),\n    \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 200.0),\n    \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 10.0),\n    \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n    temp_out = []\n\n    for i in range(cv_runs):\n\n        X = X_train\n        y = y_train\n        model = XGBRegressor(**params, njobs=-1)\n        rkf = KFold(n_splits=n_splits, shuffle=True)\n        X_values = X.values\n        y_values = y.values\n        y_pred = np.zeros_like(y_values)\n        y_pred_train = np.zeros_like(y_values)\n        for train_index, test_index in rkf.split(X_values):\n            X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n            y_A, y_B = y_values[train_index], y_values[test_index]\n            model.fit(X_A, y_A, eval_set=[(X_B, y_B)], verbose = False)\n            y_pred[test_index] = model.predict(X_B)\n            y_pred_train[train_index] = model.predict(X_A)\n\n        score_train = r2_score(y_train, y_pred_train)\n        score_test = r2_score(y_train, y_pred) \n        overfit = (score_train-score_test)\n        temp_out.append(score_test-cv_regularizer*overfit)\n\n    return (np.mean(temp_out))\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=optuna_trials)\nprint('Total time for hypermarameter optimization ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X_train, y_train)\nprint('Optuna XGB train:', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), time.time()-time1)\n\n# Evaluate performance of XGB models:\nr2_xgb1 = r2_score(y_test, xgb1.predict(X_test))\nr2_xgbgs = r2_score(y_test, xgbm.predict(X_test))\nr2_xgbo = r2_score(y_test, optuna_xgb.predict(X_test))\n\nprint('Min_prd: ', min_prd)\nprint('Constant guess: ', mean_absolute_error(y_test, np.ones(len(y_test))*y_test.mean()), \n      r2_score(y_test, np.ones(len(y_test))*y_test.mean()))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_xgb1)\nprint('XGB GS test:', mean_absolute_error(y_test, xgbm.predict(X_test)), r2_xgbgs)\nprint('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_xgbo)\n\nresults.loc[results.min_prd==min_prd,'xgbf':'xgbo'] = r2_xgb1, r2_xgbgs, r2_xgbo\n    \nprint(time.time()-time0, results)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T02:03:46.871933Z","iopub.execute_input":"2022-09-06T02:03:46.872329Z","iopub.status.idle":"2022-09-06T02:03:55.932852Z","shell.execute_reply.started":"2022-09-06T02:03:46.872294Z","shell.execute_reply":"2022-09-06T02:03:55.931124Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"(1015788, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"   PERMNO  prd  mom482     mom242  year      RET   ind        bm        op  \\\n0   10005  375     NaN -72.706514  1989  -0.7000  30.0  0.490174 -0.214332   \n1   10005  376     NaN -68.539010  1989 -20.7400  30.0  0.490174 -0.214332   \n2   10005  377     NaN -60.041983  1989 -25.6500  30.0  0.490174 -0.214332   \n3   10005  378     NaN -67.083392  1989  -0.6800  30.0  0.490174 -0.214332   \n4   10005  379     NaN -70.826677  1989  32.6433  30.0  0.490174 -0.214332   \n\n    gp       inv      mom11     mom122  amhd  ivol_capm  ivol_ff5   beta_bw  \\\n0  0.0 -0.230583  -0.710000 -33.516946   NaN   0.765013  0.647098  0.843440   \n1  0.0 -0.230583  -0.700000 -48.453613   NaN   0.765013  0.647098  0.569757   \n2  0.0 -0.230583 -20.740000 -48.510651   NaN   4.169273  3.399652  0.580613   \n3  0.0 -0.230583 -22.380465 -38.209694   NaN   5.565847  5.250996  0.663164   \n4  0.0 -0.230583  -0.680000 -53.776947   NaN   0.765013  0.647098  0.372045   \n\n      MAX     vol1m     vol6m    vol12m      size      lbm       lop  \\\n0  1.4066  0.866870  1.485001  3.167929 -0.424011  0.75014 -0.084639   \n1  1.4066  0.866870  1.484870  2.598143 -0.424011  0.75014 -0.084639   \n2  1.4066  4.364358  2.310132  2.882070 -0.647218  0.75014 -0.084639   \n3  1.4066  6.813720  3.190628  2.799597 -0.934794  0.75014 -0.084639   \n4  1.4066  0.866870  2.840962  2.265160 -0.934794  0.75014 -0.084639   \n\n        lgp      linv      llme  l1amhd   l1MAX  l3amhd   l3MAX  l6amhd  \\\n0  0.015282  0.306039 -0.087557     NaN  1.4066     NaN  1.4066     NaN   \n1  0.015282  0.306039  0.163748     NaN  1.4066     NaN  1.4066     NaN   \n2  0.015282  0.306039  0.163748     NaN  1.4066     NaN  1.4066     NaN   \n3  0.015282  0.306039 -0.241753     NaN  1.4066     NaN  1.4066     NaN   \n4  0.015282  0.306039 -0.241753     NaN  1.4066     NaN  1.4066     NaN   \n\n    l6MAX  l12amhd  l12MAX  l12mom122  l12ivol_capm  l12ivol_ff5  l12beta_bw  \\\n0  1.4066      NaN  1.4066 -52.647854      0.765013     0.647098    0.380573   \n1  1.4066      NaN  1.4066 -61.937195      5.362759     5.279563    0.441740   \n2  1.4066      NaN  1.4066 -43.100755      0.765013     0.647098    0.456509   \n3  1.4066      NaN  1.4066 -22.391633      5.852977     5.112499    0.452695   \n4  1.4066      NaN  1.4066 -36.888819      4.932686     3.778440    0.664521   \n\n   l12vol6m  l12vol12m  \n0  2.621943   6.827920  \n1  3.313448   4.436441  \n2  3.313442   4.197256  \n3  4.107071   4.246072  \n4  4.378863   3.604576  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10005</td>\n      <td>375</td>\n      <td>NaN</td>\n      <td>-72.706514</td>\n      <td>1989</td>\n      <td>-0.7000</td>\n      <td>30.0</td>\n      <td>0.490174</td>\n      <td>-0.214332</td>\n      <td>0.0</td>\n      <td>-0.230583</td>\n      <td>-0.710000</td>\n      <td>-33.516946</td>\n      <td>NaN</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>0.843440</td>\n      <td>1.4066</td>\n      <td>0.866870</td>\n      <td>1.485001</td>\n      <td>3.167929</td>\n      <td>-0.424011</td>\n      <td>0.75014</td>\n      <td>-0.084639</td>\n      <td>0.015282</td>\n      <td>0.306039</td>\n      <td>-0.087557</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>-52.647854</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>0.380573</td>\n      <td>2.621943</td>\n      <td>6.827920</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10005</td>\n      <td>376</td>\n      <td>NaN</td>\n      <td>-68.539010</td>\n      <td>1989</td>\n      <td>-20.7400</td>\n      <td>30.0</td>\n      <td>0.490174</td>\n      <td>-0.214332</td>\n      <td>0.0</td>\n      <td>-0.230583</td>\n      <td>-0.700000</td>\n      <td>-48.453613</td>\n      <td>NaN</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>0.569757</td>\n      <td>1.4066</td>\n      <td>0.866870</td>\n      <td>1.484870</td>\n      <td>2.598143</td>\n      <td>-0.424011</td>\n      <td>0.75014</td>\n      <td>-0.084639</td>\n      <td>0.015282</td>\n      <td>0.306039</td>\n      <td>0.163748</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>-61.937195</td>\n      <td>5.362759</td>\n      <td>5.279563</td>\n      <td>0.441740</td>\n      <td>3.313448</td>\n      <td>4.436441</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10005</td>\n      <td>377</td>\n      <td>NaN</td>\n      <td>-60.041983</td>\n      <td>1989</td>\n      <td>-25.6500</td>\n      <td>30.0</td>\n      <td>0.490174</td>\n      <td>-0.214332</td>\n      <td>0.0</td>\n      <td>-0.230583</td>\n      <td>-20.740000</td>\n      <td>-48.510651</td>\n      <td>NaN</td>\n      <td>4.169273</td>\n      <td>3.399652</td>\n      <td>0.580613</td>\n      <td>1.4066</td>\n      <td>4.364358</td>\n      <td>2.310132</td>\n      <td>2.882070</td>\n      <td>-0.647218</td>\n      <td>0.75014</td>\n      <td>-0.084639</td>\n      <td>0.015282</td>\n      <td>0.306039</td>\n      <td>0.163748</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>-43.100755</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>0.456509</td>\n      <td>3.313442</td>\n      <td>4.197256</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10005</td>\n      <td>378</td>\n      <td>NaN</td>\n      <td>-67.083392</td>\n      <td>1989</td>\n      <td>-0.6800</td>\n      <td>30.0</td>\n      <td>0.490174</td>\n      <td>-0.214332</td>\n      <td>0.0</td>\n      <td>-0.230583</td>\n      <td>-22.380465</td>\n      <td>-38.209694</td>\n      <td>NaN</td>\n      <td>5.565847</td>\n      <td>5.250996</td>\n      <td>0.663164</td>\n      <td>1.4066</td>\n      <td>6.813720</td>\n      <td>3.190628</td>\n      <td>2.799597</td>\n      <td>-0.934794</td>\n      <td>0.75014</td>\n      <td>-0.084639</td>\n      <td>0.015282</td>\n      <td>0.306039</td>\n      <td>-0.241753</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>-22.391633</td>\n      <td>5.852977</td>\n      <td>5.112499</td>\n      <td>0.452695</td>\n      <td>4.107071</td>\n      <td>4.246072</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10005</td>\n      <td>379</td>\n      <td>NaN</td>\n      <td>-70.826677</td>\n      <td>1989</td>\n      <td>32.6433</td>\n      <td>30.0</td>\n      <td>0.490174</td>\n      <td>-0.214332</td>\n      <td>0.0</td>\n      <td>-0.230583</td>\n      <td>-0.680000</td>\n      <td>-53.776947</td>\n      <td>NaN</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>0.372045</td>\n      <td>1.4066</td>\n      <td>0.866870</td>\n      <td>2.840962</td>\n      <td>2.265160</td>\n      <td>-0.934794</td>\n      <td>0.75014</td>\n      <td>-0.084639</td>\n      <td>0.015282</td>\n      <td>0.306039</td>\n      <td>-0.241753</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>NaN</td>\n      <td>1.4066</td>\n      <td>-36.888819</td>\n      <td>4.932686</td>\n      <td>3.778440</td>\n      <td>0.664521</td>\n      <td>4.378863</td>\n      <td>3.604576</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    1.015788e+06\nmean     1.988628e+03\nstd      1.049190e+01\nmin      1.966000e+03\n25%      1.980000e+03\n50%      1.989000e+03\n75%      1.997000e+03\nmax      2.006000e+03\nName: year, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"PERMNO          1015788\nprd             1015788\nmom482           854012\nmom242           998350\nyear            1015788\nRET             1015788\nind             1015788\nbm              1015788\nop              1015788\ngp              1015788\ninv             1015169\nmom11           1015788\nmom122          1015788\namhd             795671\nivol_capm       1015718\nivol_ff5        1015718\nbeta_bw         1015788\nMAX             1015788\nvol1m           1015574\nvol6m           1014887\nvol12m          1013580\nsize            1015788\nlbm             1015788\nlop             1015788\nlgp             1015788\nlinv            1015788\nllme            1015788\nl1amhd           795002\nl1MAX           1015750\nl3amhd           793555\nl3MAX           1015548\nl6amhd           791187\nl6MAX           1015354\nl12amhd          785514\nl12MAX          1015750\nl12mom122       1007361\nl12ivol_capm    1014694\nl12ivol_ff5     1014694\nl12beta_bw      1015073\nl12vol6m        1012562\nl12vol12m       1002306\ndtype: int64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (1005133, 39)\ntime to do feature proprocessing: \nNumber of features after transformation:  (1005133, 87)\nmae of a constant model 9.223010182458546\nR2 of a constant model 0.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/2112378575.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0mxgb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpu_hist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolsample_bytree\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mxgb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'XGB train:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_absolute_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr2_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    970\u001b[0m             \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m             \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 972\u001b[0;31m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    973\u001b[0m         )\n\u001b[1;32m    974\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1733\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1734\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1736\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    201\u001b[0m     \"\"\"\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mXGBoostError\u001b[0m: [02:03:55] ../src/tree/updater_gpu_hist.cu:712: Exception in gpu_hist: [02:03:55] ../src/c_api/../data/../common/device_helpers.cuh:428: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 43778048\n- Requested memory: 19727712\n\nStack trace:\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x38f939) [0x7f52271c4939]\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x393d4b) [0x7f52271c8d4b]\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x402cba) [0x7f5227237cba]\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x403278) [0x7f5227238278]\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x3f536c) [0x7f522722a36c]\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x3a234f) [0x7f52271d734f]\n  [bt] (6) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x3a5395) [0x7f52271da395]\n  [bt] (7) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x416809) [0x7f522724b809]\n  [bt] (8) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x416ece) [0x7f522724bece]\n\n\n\nStack trace:\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x637999) [0x7f522746c999]\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x658cc5) [0x7f522748dcc5]\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1d2e43) [0x7f5227007e43]\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1d39f7) [0x7f52270089f7]\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x20fd12) [0x7f5227044d12]\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7f5226ede688]\n  [bt] (6) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7f5289e82a4a]\n  [bt] (7) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.8(+0x5fea) [0x7f5289e81fea]\n  [bt] (8) /opt/conda/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2f4) [0x7f5289e98784]\n\n"],"ename":"XGBoostError","evalue":"[02:03:55] ../src/tree/updater_gpu_hist.cu:712: Exception in gpu_hist: [02:03:55] ../src/c_api/../data/../common/device_helpers.cuh:428: Memory allocation error on worker 0: std::bad_alloc: cudaErrorMemoryAllocation: out of memory\n- Free memory: 43778048\n- Requested memory: 19727712\n\nStack trace:\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x38f939) [0x7f52271c4939]\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x393d4b) [0x7f52271c8d4b]\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x402cba) [0x7f5227237cba]\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x403278) [0x7f5227238278]\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x3f536c) [0x7f522722a36c]\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x3a234f) [0x7f52271d734f]\n  [bt] (6) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x3a5395) [0x7f52271da395]\n  [bt] (7) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x416809) [0x7f522724b809]\n  [bt] (8) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x416ece) [0x7f522724bece]\n\n\n\nStack trace:\n  [bt] (0) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x637999) [0x7f522746c999]\n  [bt] (1) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x658cc5) [0x7f522748dcc5]\n  [bt] (2) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1d2e43) [0x7f5227007e43]\n  [bt] (3) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x1d39f7) [0x7f52270089f7]\n  [bt] (4) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x20fd12) [0x7f5227044d12]\n  [bt] (5) /opt/conda/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x68) [0x7f5226ede688]\n  [bt] (6) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.8(+0x6a4a) [0x7f5289e82a4a]\n  [bt] (7) /opt/conda/lib/python3.7/lib-dynload/../../libffi.so.8(+0x5fea) [0x7f5289e81fea]\n  [bt] (8) /opt/conda/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2f4) [0x7f5289e98784]\n\n","output_type":"error"}]},{"cell_type":"code","source":"df.skew()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T02:04:00.475156Z","iopub.execute_input":"2022-09-06T02:04:00.475842Z","iopub.status.idle":"2022-09-06T02:04:01.129536Z","shell.execute_reply.started":"2022-09-06T02:04:00.475807Z","shell.execute_reply":"2022-09-06T02:04:01.128404Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"PERMNO          0.087933\nprd            -0.198148\nmom482          1.193992\nmom242          0.890617\nyear           -0.198149\nRET             0.792396\nind            -0.131010\nbm             -0.590142\nop             -0.827249\ngp              0.446368\ninv             1.261316\nmom11           0.275975\nmom122          0.651943\namhd           -0.376405\nivol_capm       1.226450\nivol_ff5        1.236885\nbeta_bw         0.457922\nMAX             1.345936\nvol1m           1.211599\nvol6m           1.144974\nvol12m          1.088969\nsize            0.379107\nlbm            -0.595647\nlop            -0.846113\nlgp             0.433002\nlinv            1.218153\nllme            0.419597\nl1amhd         -0.378887\nl1MAX           1.346688\nl3amhd         -0.383315\nl3MAX           1.348446\nl6amhd         -0.388782\nl6MAX           1.347230\nl12amhd        -0.399100\nl12MAX          1.346688\nl12mom122       0.660565\nl12ivol_capm    1.225928\nl12ivol_ff5     1.237615\nl12beta_bw      0.494384\nl12vol6m        1.136219\nl12vol12m       1.079234\namhd_miss       1.377965\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-09-06T03:22:40.089070Z","iopub.execute_input":"2022-09-06T03:22:40.089617Z","iopub.status.idle":"2022-09-06T03:22:40.167922Z","shell.execute_reply.started":"2022-09-06T03:22:40.089572Z","shell.execute_reply":"2022-09-06T03:22:40.166956Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"       PERMNO  prd      mom482      mom242  year        RET   ind        bm  \\\n0       10025  649   30.604529   47.614289  2012   6.600503  15.0 -0.953163   \n1       10025  650   97.106725   44.386493  2012  22.413854  15.0 -0.953163   \n2       10025  651  138.786753   50.502876  2012   9.260382  15.0 -0.899741   \n3       10025  652  162.913720   87.364374  2012   4.114279  15.0 -0.899741   \n4       10025  653  174.780357  113.102274  2012  16.416518  15.0 -0.899741   \n...       ...  ...         ...         ...   ...        ...   ...       ...   \n48154   93423  672         NaN   85.363516  2014   2.891556   7.0 -1.226522   \n48155   93423  673         NaN   95.330763  2014   1.794075   7.0 -1.226522   \n48156   93423  674         NaN   73.564394  2014   1.531531   7.0 -1.226522   \n48157   93423  675         NaN   65.099184  2014  -5.751390   7.0 -1.909081   \n48158   93423  676         NaN   52.233824  2014  -7.425820   7.0 -1.909081   \n\n             op        gp       inv    mom11     mom122      amhd  ivol_capm  \\\n0      0.060639  0.373348 -0.025756   0.1724  14.759414  1.259688   1.204700   \n1      0.060639  0.373348 -0.025756  -0.9851  13.571797  1.141539   0.765013   \n2      0.072943  0.362002  0.184931  26.1222  18.270241  1.103795   1.517394   \n3      0.072943  0.362002  0.184931   7.8760  60.787559  1.047262   0.937883   \n4      0.072943  0.362002  0.184931   7.1633  72.766685  0.949094   1.120278   \n...         ...       ...       ...      ...        ...       ...        ...   \n48154  0.100139  0.189565  0.154149  -1.5931  18.300911 -3.226472   0.779887   \n48155  0.100139  0.189565  0.154149  -0.0249  15.793176 -3.219131   0.952419   \n48156  0.100139  0.189565  0.154149   1.9681  11.776170 -3.241532   0.914882   \n48157  0.130961  0.232296 -0.146767   5.1656  20.864465 -3.238086   0.837567   \n48158  0.130961  0.232296 -0.146767 -10.1763  21.476203 -3.247683   1.268193   \n\n       ivol_ff5   beta_bw     MAX     vol1m     vol6m    vol12m     BAspr  \\\n0      0.923609  1.058419  2.5839  1.533648  2.224711  2.413616  0.404975   \n1      0.647098  1.026432  1.4066  0.866870  1.761031  2.337123  0.579206   \n2      1.511676  1.007201  5.7938  1.525676  1.485825  2.334760  0.136519   \n3      0.768468  0.991882  3.1364  0.989203  1.371582  2.299344  0.578902   \n4      0.963537  1.038772  4.3231  1.269428  1.271617  2.116573  0.335306   \n...         ...       ...     ...       ...       ...       ...       ...   \n48154  0.759569  0.902643  1.4066  0.908718  1.377351  1.451315  0.029163   \n48155  0.914672  0.949362  2.1480  1.275081  1.248010  1.359812  0.029163   \n48156  0.855551  0.932978  2.3892  0.997929  1.240860  1.341014  0.029163   \n48157  0.751698  0.990565  2.4722  0.881300  1.221605  1.319646  0.047048   \n48158  1.108560  1.007570  2.7474  1.434456  1.280950  1.308248  0.029163   \n\n           size       lbm       lop       lgp      linv      llme    l1amhd  \\\n0      5.260005 -1.100725  0.176879  0.498498 -0.078728  5.226473  1.278464   \n1      5.250206 -1.100725  0.176879  0.498498 -0.078728  5.238599  1.259688   \n2      5.482288 -0.953163  0.060639  0.373348 -0.025756  5.188325  1.141539   \n3      5.558824 -0.953163  0.060639  0.373348 -0.025756  5.001724  1.103795   \n4      5.628101 -0.953163  0.060639  0.373348 -0.025756  5.005778  1.047262   \n...         ...       ...       ...       ...       ...       ...       ...   \n48154  8.244170 -0.835300  0.096950  0.203164 -0.031126  8.179936 -3.222051   \n48155  8.247775 -0.835300  0.096950  0.203164 -0.031126  8.155703 -3.226472   \n48156  8.255716 -0.835300  0.096950  0.203164 -0.031126  8.178869 -3.219131   \n48157  8.306082 -1.226522  0.100139  0.189565  0.154149  8.120186 -3.241532   \n48158  8.200147 -1.226522  0.100139  0.189565  0.154149  8.166935 -3.238086   \n\n        l1MAX   l1BAspr    l3amhd   l3MAX   l3BAspr    l6amhd   l6MAX  \\\n0      2.9882  0.371854  1.383731  4.1328  0.513906  1.510489  8.7149   \n1      2.5839  0.404975  1.369098  3.7563  0.490055  1.482684  9.2927   \n2      1.4066  0.579206  1.278464  2.9882  0.371854  1.460316  8.6057   \n3      5.7938  0.136519  1.259688  2.5839  0.404975  1.383731  4.1328   \n4      3.1364  0.578902  1.141539  1.4066  0.579206  1.369098  3.7563   \n...       ...       ...       ...     ...       ...       ...     ...   \n48154  4.6275  0.029163 -3.315129  1.8033  0.081566 -3.386130  2.4529   \n48155  1.4066  0.029163 -3.255144  3.6344  0.029163 -3.355495  6.5831   \n48156  2.1480  0.029163 -3.222051  4.6275  0.029163 -3.388181  2.9005   \n48157  2.3892  0.029163 -3.226472  1.4066  0.029163 -3.315129  1.8033   \n48158  2.4722  0.047048 -3.219131  2.1480  0.029163 -3.255144  3.6344   \n\n        l6BAspr   l12amhd  l12MAX  l12BAspr  l12mom122  l12ivol_capm  \\\n0      0.558214  1.431610  2.9882  0.397878   7.446355      1.532176   \n1      0.468883  1.488235  2.5839  0.579216  20.525370      1.116961   \n2      0.567577  1.494722  1.4066  0.307377  28.446093      1.852721   \n3      0.513906  1.504704  5.7938  0.367918   0.922674      1.208446   \n4      0.490055  1.523562  3.1364  0.681302  14.425587      2.072178   \n...         ...       ...     ...       ...        ...           ...   \n48154  0.029481 -3.092530  4.6275  0.029163  50.350497      0.765013   \n48155  0.029163 -3.181713  1.4066  0.029163  59.229231      2.117291   \n48156  0.029163 -3.257544  2.1480  0.029163  65.920814      0.979222   \n48157  0.081566 -3.317072  2.3892  0.029163  44.723091      1.057337   \n48158  0.029163 -3.357783  2.4722  0.029163  28.349347      1.602867   \n\n       l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  amhd_miss  BAspr_miss  \n0         1.429862    1.297452  2.318278   2.736604          0           0  \n1         0.998634    1.245832  2.151214   2.495713          0           0  \n2         1.680914    1.262682  2.178625   2.343239          0           0  \n3         0.908065    1.307383  2.056547   2.325010          0           0  \n4         1.968630    1.081815  2.335976   2.367897          0           0  \n...            ...         ...       ...        ...        ...         ...  \n48154     0.647098    0.792883  1.553267   1.526357          0           0  \n48155     1.899401    0.771122  1.459107   1.609127          0           0  \n48156     0.920754    0.772685  1.454548   1.602632          0           0  \n48157     0.845541    0.695503  1.379176   1.518435          0           0  \n48158     1.509123    0.694050  1.499910   1.509470          0           0  \n\n[48159 rows x 48 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10025</td>\n      <td>649</td>\n      <td>30.604529</td>\n      <td>47.614289</td>\n      <td>2012</td>\n      <td>6.600503</td>\n      <td>15.0</td>\n      <td>-0.953163</td>\n      <td>0.060639</td>\n      <td>0.373348</td>\n      <td>-0.025756</td>\n      <td>0.1724</td>\n      <td>14.759414</td>\n      <td>1.259688</td>\n      <td>1.204700</td>\n      <td>0.923609</td>\n      <td>1.058419</td>\n      <td>2.5839</td>\n      <td>1.533648</td>\n      <td>2.224711</td>\n      <td>2.413616</td>\n      <td>0.404975</td>\n      <td>5.260005</td>\n      <td>-1.100725</td>\n      <td>0.176879</td>\n      <td>0.498498</td>\n      <td>-0.078728</td>\n      <td>5.226473</td>\n      <td>1.278464</td>\n      <td>2.9882</td>\n      <td>0.371854</td>\n      <td>1.383731</td>\n      <td>4.1328</td>\n      <td>0.513906</td>\n      <td>1.510489</td>\n      <td>8.7149</td>\n      <td>0.558214</td>\n      <td>1.431610</td>\n      <td>2.9882</td>\n      <td>0.397878</td>\n      <td>7.446355</td>\n      <td>1.532176</td>\n      <td>1.429862</td>\n      <td>1.297452</td>\n      <td>2.318278</td>\n      <td>2.736604</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10025</td>\n      <td>650</td>\n      <td>97.106725</td>\n      <td>44.386493</td>\n      <td>2012</td>\n      <td>22.413854</td>\n      <td>15.0</td>\n      <td>-0.953163</td>\n      <td>0.060639</td>\n      <td>0.373348</td>\n      <td>-0.025756</td>\n      <td>-0.9851</td>\n      <td>13.571797</td>\n      <td>1.141539</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>1.026432</td>\n      <td>1.4066</td>\n      <td>0.866870</td>\n      <td>1.761031</td>\n      <td>2.337123</td>\n      <td>0.579206</td>\n      <td>5.250206</td>\n      <td>-1.100725</td>\n      <td>0.176879</td>\n      <td>0.498498</td>\n      <td>-0.078728</td>\n      <td>5.238599</td>\n      <td>1.259688</td>\n      <td>2.5839</td>\n      <td>0.404975</td>\n      <td>1.369098</td>\n      <td>3.7563</td>\n      <td>0.490055</td>\n      <td>1.482684</td>\n      <td>9.2927</td>\n      <td>0.468883</td>\n      <td>1.488235</td>\n      <td>2.5839</td>\n      <td>0.579216</td>\n      <td>20.525370</td>\n      <td>1.116961</td>\n      <td>0.998634</td>\n      <td>1.245832</td>\n      <td>2.151214</td>\n      <td>2.495713</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10025</td>\n      <td>651</td>\n      <td>138.786753</td>\n      <td>50.502876</td>\n      <td>2012</td>\n      <td>9.260382</td>\n      <td>15.0</td>\n      <td>-0.899741</td>\n      <td>0.072943</td>\n      <td>0.362002</td>\n      <td>0.184931</td>\n      <td>26.1222</td>\n      <td>18.270241</td>\n      <td>1.103795</td>\n      <td>1.517394</td>\n      <td>1.511676</td>\n      <td>1.007201</td>\n      <td>5.7938</td>\n      <td>1.525676</td>\n      <td>1.485825</td>\n      <td>2.334760</td>\n      <td>0.136519</td>\n      <td>5.482288</td>\n      <td>-0.953163</td>\n      <td>0.060639</td>\n      <td>0.373348</td>\n      <td>-0.025756</td>\n      <td>5.188325</td>\n      <td>1.141539</td>\n      <td>1.4066</td>\n      <td>0.579206</td>\n      <td>1.278464</td>\n      <td>2.9882</td>\n      <td>0.371854</td>\n      <td>1.460316</td>\n      <td>8.6057</td>\n      <td>0.567577</td>\n      <td>1.494722</td>\n      <td>1.4066</td>\n      <td>0.307377</td>\n      <td>28.446093</td>\n      <td>1.852721</td>\n      <td>1.680914</td>\n      <td>1.262682</td>\n      <td>2.178625</td>\n      <td>2.343239</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10025</td>\n      <td>652</td>\n      <td>162.913720</td>\n      <td>87.364374</td>\n      <td>2012</td>\n      <td>4.114279</td>\n      <td>15.0</td>\n      <td>-0.899741</td>\n      <td>0.072943</td>\n      <td>0.362002</td>\n      <td>0.184931</td>\n      <td>7.8760</td>\n      <td>60.787559</td>\n      <td>1.047262</td>\n      <td>0.937883</td>\n      <td>0.768468</td>\n      <td>0.991882</td>\n      <td>3.1364</td>\n      <td>0.989203</td>\n      <td>1.371582</td>\n      <td>2.299344</td>\n      <td>0.578902</td>\n      <td>5.558824</td>\n      <td>-0.953163</td>\n      <td>0.060639</td>\n      <td>0.373348</td>\n      <td>-0.025756</td>\n      <td>5.001724</td>\n      <td>1.103795</td>\n      <td>5.7938</td>\n      <td>0.136519</td>\n      <td>1.259688</td>\n      <td>2.5839</td>\n      <td>0.404975</td>\n      <td>1.383731</td>\n      <td>4.1328</td>\n      <td>0.513906</td>\n      <td>1.504704</td>\n      <td>5.7938</td>\n      <td>0.367918</td>\n      <td>0.922674</td>\n      <td>1.208446</td>\n      <td>0.908065</td>\n      <td>1.307383</td>\n      <td>2.056547</td>\n      <td>2.325010</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10025</td>\n      <td>653</td>\n      <td>174.780357</td>\n      <td>113.102274</td>\n      <td>2012</td>\n      <td>16.416518</td>\n      <td>15.0</td>\n      <td>-0.899741</td>\n      <td>0.072943</td>\n      <td>0.362002</td>\n      <td>0.184931</td>\n      <td>7.1633</td>\n      <td>72.766685</td>\n      <td>0.949094</td>\n      <td>1.120278</td>\n      <td>0.963537</td>\n      <td>1.038772</td>\n      <td>4.3231</td>\n      <td>1.269428</td>\n      <td>1.271617</td>\n      <td>2.116573</td>\n      <td>0.335306</td>\n      <td>5.628101</td>\n      <td>-0.953163</td>\n      <td>0.060639</td>\n      <td>0.373348</td>\n      <td>-0.025756</td>\n      <td>5.005778</td>\n      <td>1.047262</td>\n      <td>3.1364</td>\n      <td>0.578902</td>\n      <td>1.141539</td>\n      <td>1.4066</td>\n      <td>0.579206</td>\n      <td>1.369098</td>\n      <td>3.7563</td>\n      <td>0.490055</td>\n      <td>1.523562</td>\n      <td>3.1364</td>\n      <td>0.681302</td>\n      <td>14.425587</td>\n      <td>2.072178</td>\n      <td>1.968630</td>\n      <td>1.081815</td>\n      <td>2.335976</td>\n      <td>2.367897</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48154</th>\n      <td>93423</td>\n      <td>672</td>\n      <td>NaN</td>\n      <td>85.363516</td>\n      <td>2014</td>\n      <td>2.891556</td>\n      <td>7.0</td>\n      <td>-1.226522</td>\n      <td>0.100139</td>\n      <td>0.189565</td>\n      <td>0.154149</td>\n      <td>-1.5931</td>\n      <td>18.300911</td>\n      <td>-3.226472</td>\n      <td>0.779887</td>\n      <td>0.759569</td>\n      <td>0.902643</td>\n      <td>1.4066</td>\n      <td>0.908718</td>\n      <td>1.377351</td>\n      <td>1.451315</td>\n      <td>0.029163</td>\n      <td>8.244170</td>\n      <td>-0.835300</td>\n      <td>0.096950</td>\n      <td>0.203164</td>\n      <td>-0.031126</td>\n      <td>8.179936</td>\n      <td>-3.222051</td>\n      <td>4.6275</td>\n      <td>0.029163</td>\n      <td>-3.315129</td>\n      <td>1.8033</td>\n      <td>0.081566</td>\n      <td>-3.386130</td>\n      <td>2.4529</td>\n      <td>0.029481</td>\n      <td>-3.092530</td>\n      <td>4.6275</td>\n      <td>0.029163</td>\n      <td>50.350497</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>0.792883</td>\n      <td>1.553267</td>\n      <td>1.526357</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48155</th>\n      <td>93423</td>\n      <td>673</td>\n      <td>NaN</td>\n      <td>95.330763</td>\n      <td>2014</td>\n      <td>1.794075</td>\n      <td>7.0</td>\n      <td>-1.226522</td>\n      <td>0.100139</td>\n      <td>0.189565</td>\n      <td>0.154149</td>\n      <td>-0.0249</td>\n      <td>15.793176</td>\n      <td>-3.219131</td>\n      <td>0.952419</td>\n      <td>0.914672</td>\n      <td>0.949362</td>\n      <td>2.1480</td>\n      <td>1.275081</td>\n      <td>1.248010</td>\n      <td>1.359812</td>\n      <td>0.029163</td>\n      <td>8.247775</td>\n      <td>-0.835300</td>\n      <td>0.096950</td>\n      <td>0.203164</td>\n      <td>-0.031126</td>\n      <td>8.155703</td>\n      <td>-3.226472</td>\n      <td>1.4066</td>\n      <td>0.029163</td>\n      <td>-3.255144</td>\n      <td>3.6344</td>\n      <td>0.029163</td>\n      <td>-3.355495</td>\n      <td>6.5831</td>\n      <td>0.029163</td>\n      <td>-3.181713</td>\n      <td>1.4066</td>\n      <td>0.029163</td>\n      <td>59.229231</td>\n      <td>2.117291</td>\n      <td>1.899401</td>\n      <td>0.771122</td>\n      <td>1.459107</td>\n      <td>1.609127</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48156</th>\n      <td>93423</td>\n      <td>674</td>\n      <td>NaN</td>\n      <td>73.564394</td>\n      <td>2014</td>\n      <td>1.531531</td>\n      <td>7.0</td>\n      <td>-1.226522</td>\n      <td>0.100139</td>\n      <td>0.189565</td>\n      <td>0.154149</td>\n      <td>1.9681</td>\n      <td>11.776170</td>\n      <td>-3.241532</td>\n      <td>0.914882</td>\n      <td>0.855551</td>\n      <td>0.932978</td>\n      <td>2.3892</td>\n      <td>0.997929</td>\n      <td>1.240860</td>\n      <td>1.341014</td>\n      <td>0.029163</td>\n      <td>8.255716</td>\n      <td>-0.835300</td>\n      <td>0.096950</td>\n      <td>0.203164</td>\n      <td>-0.031126</td>\n      <td>8.178869</td>\n      <td>-3.219131</td>\n      <td>2.1480</td>\n      <td>0.029163</td>\n      <td>-3.222051</td>\n      <td>4.6275</td>\n      <td>0.029163</td>\n      <td>-3.388181</td>\n      <td>2.9005</td>\n      <td>0.029163</td>\n      <td>-3.257544</td>\n      <td>2.1480</td>\n      <td>0.029163</td>\n      <td>65.920814</td>\n      <td>0.979222</td>\n      <td>0.920754</td>\n      <td>0.772685</td>\n      <td>1.454548</td>\n      <td>1.602632</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48157</th>\n      <td>93423</td>\n      <td>675</td>\n      <td>NaN</td>\n      <td>65.099184</td>\n      <td>2014</td>\n      <td>-5.751390</td>\n      <td>7.0</td>\n      <td>-1.909081</td>\n      <td>0.130961</td>\n      <td>0.232296</td>\n      <td>-0.146767</td>\n      <td>5.1656</td>\n      <td>20.864465</td>\n      <td>-3.238086</td>\n      <td>0.837567</td>\n      <td>0.751698</td>\n      <td>0.990565</td>\n      <td>2.4722</td>\n      <td>0.881300</td>\n      <td>1.221605</td>\n      <td>1.319646</td>\n      <td>0.047048</td>\n      <td>8.306082</td>\n      <td>-1.226522</td>\n      <td>0.100139</td>\n      <td>0.189565</td>\n      <td>0.154149</td>\n      <td>8.120186</td>\n      <td>-3.241532</td>\n      <td>2.3892</td>\n      <td>0.029163</td>\n      <td>-3.226472</td>\n      <td>1.4066</td>\n      <td>0.029163</td>\n      <td>-3.315129</td>\n      <td>1.8033</td>\n      <td>0.081566</td>\n      <td>-3.317072</td>\n      <td>2.3892</td>\n      <td>0.029163</td>\n      <td>44.723091</td>\n      <td>1.057337</td>\n      <td>0.845541</td>\n      <td>0.695503</td>\n      <td>1.379176</td>\n      <td>1.518435</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48158</th>\n      <td>93423</td>\n      <td>676</td>\n      <td>NaN</td>\n      <td>52.233824</td>\n      <td>2014</td>\n      <td>-7.425820</td>\n      <td>7.0</td>\n      <td>-1.909081</td>\n      <td>0.130961</td>\n      <td>0.232296</td>\n      <td>-0.146767</td>\n      <td>-10.1763</td>\n      <td>21.476203</td>\n      <td>-3.247683</td>\n      <td>1.268193</td>\n      <td>1.108560</td>\n      <td>1.007570</td>\n      <td>2.7474</td>\n      <td>1.434456</td>\n      <td>1.280950</td>\n      <td>1.308248</td>\n      <td>0.029163</td>\n      <td>8.200147</td>\n      <td>-1.226522</td>\n      <td>0.100139</td>\n      <td>0.189565</td>\n      <td>0.154149</td>\n      <td>8.166935</td>\n      <td>-3.238086</td>\n      <td>2.4722</td>\n      <td>0.047048</td>\n      <td>-3.219131</td>\n      <td>2.1480</td>\n      <td>0.029163</td>\n      <td>-3.255144</td>\n      <td>3.6344</td>\n      <td>0.029163</td>\n      <td>-3.357783</td>\n      <td>2.4722</td>\n      <td>0.029163</td>\n      <td>28.349347</td>\n      <td>1.602867</td>\n      <td>1.509123</td>\n      <td>0.694050</td>\n      <td>1.499910</td>\n      <td>1.509470</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>48159 rows  48 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.128921Z","iopub.status.idle":"2022-09-06T00:36:53.129664Z","shell.execute_reply.started":"2022-09-06T00:36:53.129409Z","shell.execute_reply":"2022-09-06T00:36:53.129433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total time for a script: ', time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.131169Z","iopub.status.idle":"2022-09-06T00:36:53.131976Z","shell.execute_reply.started":"2022-09-06T00:36:53.131718Z","shell.execute_reply":"2022-09-06T00:36:53.131742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.iloc[:,1:].mean()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.133368Z","iopub.status.idle":"2022-09-06T00:36:53.134132Z","shell.execute_reply.started":"2022-09-06T00:36:53.133849Z","shell.execute_reply":"2022-09-06T00:36:53.133875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3yr window, trials=20, cv_reg=0.03: 0.88%. runs 1 hr.\n# 3yr, t=40, cv_reg=0.04: 0.96%.\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.135461Z","iopub.status.idle":"2022-09-06T00:36:53.136216Z","shell.execute_reply.started":"2022-09-06T00:36:53.135933Z","shell.execute_reply":"2022-09-06T00:36:53.135958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(X_train, X_val, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:45:09.378763Z","iopub.execute_input":"2022-09-06T00:45:09.379158Z","iopub.status.idle":"2022-09-06T00:45:09.707560Z","shell.execute_reply.started":"2022-09-06T00:45:09.379127Z","shell.execute_reply":"2022-09-06T00:45:09.706650Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n0         2.440252     0.030032 -1.497142  1.080837  0.724789  0.010338   \n1         1.943299    -0.222745 -1.497142  1.080837  0.724789  0.010338   \n2         2.107274    -0.246470 -1.497142  1.080837  0.724789  0.010338   \n3         1.685620    -0.374008 -1.497142  1.080837  0.724789  0.010338   \n4         0.690976    -0.960773 -1.116726  0.846425  0.659979 -0.425881   \n...            ...          ...       ...       ...       ...       ...   \n76883    -0.566013    -0.667712  0.154976 -0.449011  0.225954 -0.367814   \n76884    -0.611810    -0.723772  0.154976 -0.449011  0.225954 -0.367814   \n76885    -0.605239    -0.659530  0.154976 -0.449011  0.225954 -0.367814   \n76886    -0.636869     0.194059  0.154976 -0.449011  0.225954 -0.367814   \n76887    -0.612396     1.516430  0.154976 -0.449011  0.225954 -0.367814   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n0       -0.001164    -0.769063   0.061164       -0.521796      -0.613313   \n1       -0.101503    -0.679058   0.111973       -0.476189      -0.779485   \n2       -0.589553    -0.665144   0.154523       -0.408752      -0.524974   \n3       -0.618918    -0.822612   0.211327       -0.498429      -0.506168   \n4       -1.749070    -0.921894   0.244523        0.492161       0.500216   \n...           ...          ...        ...             ...            ...   \n76883    0.612983     0.285701   1.814529        0.666135       0.386534   \n76884   -0.496413    -0.259853   1.815901       -0.143222      -0.002847   \n76885   -0.174600    -0.146213   1.836054       -0.038214       0.183720   \n76886   -0.157636    -0.246570   1.827610       -0.591596      -0.602846   \n76887   -0.217149    -0.008820   1.811085       -0.820075      -0.854234   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n0          0.588274 -0.300375   -0.346009   -0.753732    -0.858156   \n1          0.796765  0.336568    0.151111   -0.611420    -0.768969   \n2          0.808655 -0.458075   -0.365669   -0.517451    -0.719170   \n3          0.804064 -0.440318   -0.506501   -0.577175    -0.709441   \n4          0.802548 -0.766582    0.249959   -0.365264    -0.586161   \n...             ...       ...         ...         ...          ...   \n76883     -1.700196  0.520511    0.372631   -0.104971    -0.283154   \n76884     -1.605932 -0.449028   -0.208101   -0.059078    -0.341420   \n76885     -1.478466 -0.567504   -0.317410   -0.098314    -0.326505   \n76886     -1.377626 -0.812586   -0.835152   -0.279218    -0.371848   \n76887     -1.399491 -1.068270   -1.062484   -0.517437    -0.444693   \n\n       num__BAspr  num__size  num__lbm  num__lop  num__lgp  num__linv  \\\n0       -0.246214  -0.287970 -2.285884  0.420012  0.545504  -1.453573   \n1       -0.067952  -0.289231 -2.285884  0.420012  0.545504  -1.453573   \n2       -0.391055  -0.327156 -2.285884  0.420012  0.545504  -1.453573   \n3        0.015571  -0.361888 -2.285884  0.420012  0.545504  -1.453573   \n4       -0.355706  -0.554908 -1.350873  1.033453  0.734261  -0.196466   \n...           ...        ...       ...       ...       ...        ...   \n76883    0.578188  -1.096146 -0.204261 -0.003571  0.471093  -0.542937   \n76884    1.810455  -1.123628 -0.204261 -0.003571  0.471093  -0.542937   \n76885    4.784910  -1.130472 -0.204261 -0.003571  0.471093  -0.542937   \n76886    0.264961  -1.136254 -0.204261 -0.003571  0.471093  -0.542937   \n76887   -0.264628  -1.145775 -0.204261 -0.003571  0.471093  -0.542937   \n\n       num__llme  num__l1amhd  num__l1MAX  num__l1BAspr  num__l3amhd  \\\n0      -0.079910     0.006677   -0.437054     -0.263244    -0.017419   \n1      -0.103939     0.066074   -0.314577     -0.248255    -0.011362   \n2      -0.115354     0.117049    0.323205     -0.070163     0.018450   \n3      -0.092495     0.159737   -0.472485     -0.392958     0.078257   \n4      -0.107086     0.216727   -0.454704      0.013279     0.129583   \n...          ...          ...         ...           ...          ...   \n76883  -1.281176     1.768226   -0.382531      4.403380     1.779250   \n76884  -1.115603     1.825157    0.507389      0.575359     1.787660   \n76885  -1.172635     1.826533   -0.463426      1.806448     1.792151   \n76886  -1.154434     1.846752   -0.582058      4.778059     1.849475   \n76887  -1.218224     1.838280   -0.827462      0.262431     1.850861   \n\n       num__l3MAX  num__l3BAspr  num__l6amhd  num__l6MAX  num__l6BAspr  \\\n0       -0.263946     -0.307586    -0.080405    0.615517     -0.196028   \n1       -1.055385     -0.403408    -0.066179   -0.733851     -0.412131   \n2       -0.444897     -0.264606    -0.061915   -0.784494     -0.392271   \n3       -0.321323     -0.249523     0.002885   -0.268268     -0.303054   \n4        0.322169     -0.070310     0.009010   -1.065697     -0.400682   \n...           ...           ...          ...         ...           ...   \n76883    0.919108      0.244168     1.925952   -0.366884      2.330492   \n76884    1.129089     -0.080817     1.849914   -0.779174      0.290555   \n76885   -0.389885      4.431401     1.829081   -0.505598      0.306861   \n76886    0.508003      0.579277     1.819658    0.923738      0.259099   \n76887   -0.471504      1.818117     1.828163    1.135308     -0.072012   \n\n       num__l12amhd  num__l12MAX  num__l12BAspr  num__l12mom122  \\\n0         -0.055520    -0.437054      -0.300325        1.723461   \n1         -0.085070    -0.314577      -0.258891        0.909845   \n2         -0.064350     0.323205      -0.377451        0.554145   \n3         -0.075501    -0.472485      -0.344694        0.702872   \n4         -0.079914    -0.454704      -0.286153        0.816731   \n...             ...          ...            ...             ...   \n76883      1.981934    -0.382531       2.973730       -1.310315   \n76884      1.990553     0.507389       0.907904       -1.211626   \n76885      1.990080    -0.463426       1.463785       -0.708503   \n76886      1.995408    -0.582058       1.706591       -0.915727   \n76887      2.011370    -0.827462       0.550428        0.462225   \n\n       num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n0              -0.612851         -0.587156        -0.248782      -0.897198   \n1              -0.555438         -0.378270        -0.329453      -0.907155   \n2              -0.862790         -0.815012        -0.193126      -0.992504   \n3              -0.546726         -0.572587         0.058540      -0.900193   \n4              -0.796444         -0.772408         0.374460      -0.883001   \n...                  ...               ...              ...            ...   \n76883           0.503558          0.788956        -1.184905       0.752818   \n76884           0.495542          0.588720        -1.014403       0.106444   \n76885          -0.311360         -0.419508        -0.842612       0.006417   \n76886           0.107819          0.304902        -0.620406      -0.061611   \n76887           0.356451          0.487998        -0.286047      -0.059314   \n\n       num__l12vol12m  num__amhd_miss  num__BAspr_miss  cat__ind_1.0  \\\n0           -0.999512       -0.178017        -0.078007           0.0   \n1           -0.987610       -0.178017        -0.078007           0.0   \n2           -0.974490       -0.178017        -0.078007           0.0   \n3           -0.950343       -0.178017        -0.078007           0.0   \n4           -0.930604       -0.178017        -0.078007           0.0   \n...               ...             ...              ...           ...   \n76883        1.244635       -0.178017        -0.078007           0.0   \n76884        1.229502       -0.178017        -0.078007           0.0   \n76885        1.229408       -0.178017        -0.078007           0.0   \n76886        1.250421       -0.178017        -0.078007           0.0   \n76887        1.007059       -0.178017        -0.078007           0.0   \n\n       cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  \\\n0               0.0           0.0           0.0           0.0           0.0   \n1               0.0           0.0           0.0           0.0           0.0   \n2               0.0           0.0           0.0           0.0           0.0   \n3               0.0           0.0           0.0           0.0           0.0   \n4               0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n76883           0.0           0.0           0.0           0.0           0.0   \n76884           0.0           0.0           0.0           0.0           0.0   \n76885           0.0           0.0           0.0           0.0           0.0   \n76886           0.0           0.0           0.0           0.0           0.0   \n76887           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n0               0.0           0.0           0.0            0.0            0.0   \n1               0.0           0.0           0.0            0.0            0.0   \n2               0.0           0.0           0.0            0.0            0.0   \n3               0.0           0.0           0.0            0.0            0.0   \n4               0.0           0.0           0.0            0.0            0.0   \n...             ...           ...           ...            ...            ...   \n76883           0.0           0.0           0.0            0.0            0.0   \n76884           0.0           0.0           0.0            0.0            0.0   \n76885           0.0           0.0           0.0            0.0            0.0   \n76886           0.0           0.0           0.0            0.0            0.0   \n76887           0.0           0.0           0.0            0.0            0.0   \n\n       cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n0                0.0            0.0            0.0            1.0   \n1                0.0            0.0            0.0            1.0   \n2                0.0            0.0            0.0            1.0   \n3                0.0            0.0            0.0            1.0   \n4                0.0            0.0            0.0            1.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            1.0            0.0   \n76884            0.0            0.0            1.0            0.0   \n76885            0.0            0.0            1.0            0.0   \n76886            0.0            0.0            1.0            0.0   \n76887            0.0            0.0            1.0            0.0   \n\n       cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  cat__ind_47.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_48.0  cat__ind_49.0  \n0                0.0            0.0  \n1                0.0            0.0  \n2                0.0            0.0  \n3                0.0            0.0  \n4                0.0            0.0  \n...              ...            ...  \n76883            0.0            0.0  \n76884            0.0            0.0  \n76885            0.0            0.0  \n76886            0.0            0.0  \n76887            0.0            0.0  \n\n[69441 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.440252</td>\n      <td>0.030032</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.001164</td>\n      <td>-0.769063</td>\n      <td>0.061164</td>\n      <td>-0.521796</td>\n      <td>-0.613313</td>\n      <td>0.588274</td>\n      <td>-0.300375</td>\n      <td>-0.346009</td>\n      <td>-0.753732</td>\n      <td>-0.858156</td>\n      <td>-0.246214</td>\n      <td>-0.287970</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.079910</td>\n      <td>0.006677</td>\n      <td>-0.437054</td>\n      <td>-0.263244</td>\n      <td>-0.017419</td>\n      <td>-0.263946</td>\n      <td>-0.307586</td>\n      <td>-0.080405</td>\n      <td>0.615517</td>\n      <td>-0.196028</td>\n      <td>-0.055520</td>\n      <td>-0.437054</td>\n      <td>-0.300325</td>\n      <td>1.723461</td>\n      <td>-0.612851</td>\n      <td>-0.587156</td>\n      <td>-0.248782</td>\n      <td>-0.897198</td>\n      <td>-0.999512</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.943299</td>\n      <td>-0.222745</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.101503</td>\n      <td>-0.679058</td>\n      <td>0.111973</td>\n      <td>-0.476189</td>\n      <td>-0.779485</td>\n      <td>0.796765</td>\n      <td>0.336568</td>\n      <td>0.151111</td>\n      <td>-0.611420</td>\n      <td>-0.768969</td>\n      <td>-0.067952</td>\n      <td>-0.289231</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.103939</td>\n      <td>0.066074</td>\n      <td>-0.314577</td>\n      <td>-0.248255</td>\n      <td>-0.011362</td>\n      <td>-1.055385</td>\n      <td>-0.403408</td>\n      <td>-0.066179</td>\n      <td>-0.733851</td>\n      <td>-0.412131</td>\n      <td>-0.085070</td>\n      <td>-0.314577</td>\n      <td>-0.258891</td>\n      <td>0.909845</td>\n      <td>-0.555438</td>\n      <td>-0.378270</td>\n      <td>-0.329453</td>\n      <td>-0.907155</td>\n      <td>-0.987610</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.107274</td>\n      <td>-0.246470</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.589553</td>\n      <td>-0.665144</td>\n      <td>0.154523</td>\n      <td>-0.408752</td>\n      <td>-0.524974</td>\n      <td>0.808655</td>\n      <td>-0.458075</td>\n      <td>-0.365669</td>\n      <td>-0.517451</td>\n      <td>-0.719170</td>\n      <td>-0.391055</td>\n      <td>-0.327156</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.115354</td>\n      <td>0.117049</td>\n      <td>0.323205</td>\n      <td>-0.070163</td>\n      <td>0.018450</td>\n      <td>-0.444897</td>\n      <td>-0.264606</td>\n      <td>-0.061915</td>\n      <td>-0.784494</td>\n      <td>-0.392271</td>\n      <td>-0.064350</td>\n      <td>0.323205</td>\n      <td>-0.377451</td>\n      <td>0.554145</td>\n      <td>-0.862790</td>\n      <td>-0.815012</td>\n      <td>-0.193126</td>\n      <td>-0.992504</td>\n      <td>-0.974490</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.685620</td>\n      <td>-0.374008</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.618918</td>\n      <td>-0.822612</td>\n      <td>0.211327</td>\n      <td>-0.498429</td>\n      <td>-0.506168</td>\n      <td>0.804064</td>\n      <td>-0.440318</td>\n      <td>-0.506501</td>\n      <td>-0.577175</td>\n      <td>-0.709441</td>\n      <td>0.015571</td>\n      <td>-0.361888</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.092495</td>\n      <td>0.159737</td>\n      <td>-0.472485</td>\n      <td>-0.392958</td>\n      <td>0.078257</td>\n      <td>-0.321323</td>\n      <td>-0.249523</td>\n      <td>0.002885</td>\n      <td>-0.268268</td>\n      <td>-0.303054</td>\n      <td>-0.075501</td>\n      <td>-0.472485</td>\n      <td>-0.344694</td>\n      <td>0.702872</td>\n      <td>-0.546726</td>\n      <td>-0.572587</td>\n      <td>0.058540</td>\n      <td>-0.900193</td>\n      <td>-0.950343</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.690976</td>\n      <td>-0.960773</td>\n      <td>-1.116726</td>\n      <td>0.846425</td>\n      <td>0.659979</td>\n      <td>-0.425881</td>\n      <td>-1.749070</td>\n      <td>-0.921894</td>\n      <td>0.244523</td>\n      <td>0.492161</td>\n      <td>0.500216</td>\n      <td>0.802548</td>\n      <td>-0.766582</td>\n      <td>0.249959</td>\n      <td>-0.365264</td>\n      <td>-0.586161</td>\n      <td>-0.355706</td>\n      <td>-0.554908</td>\n      <td>-1.350873</td>\n      <td>1.033453</td>\n      <td>0.734261</td>\n      <td>-0.196466</td>\n      <td>-0.107086</td>\n      <td>0.216727</td>\n      <td>-0.454704</td>\n      <td>0.013279</td>\n      <td>0.129583</td>\n      <td>0.322169</td>\n      <td>-0.070310</td>\n      <td>0.009010</td>\n      <td>-1.065697</td>\n      <td>-0.400682</td>\n      <td>-0.079914</td>\n      <td>-0.454704</td>\n      <td>-0.286153</td>\n      <td>0.816731</td>\n      <td>-0.796444</td>\n      <td>-0.772408</td>\n      <td>0.374460</td>\n      <td>-0.883001</td>\n      <td>-0.930604</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76883</th>\n      <td>-0.566013</td>\n      <td>-0.667712</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>0.612983</td>\n      <td>0.285701</td>\n      <td>1.814529</td>\n      <td>0.666135</td>\n      <td>0.386534</td>\n      <td>-1.700196</td>\n      <td>0.520511</td>\n      <td>0.372631</td>\n      <td>-0.104971</td>\n      <td>-0.283154</td>\n      <td>0.578188</td>\n      <td>-1.096146</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.281176</td>\n      <td>1.768226</td>\n      <td>-0.382531</td>\n      <td>4.403380</td>\n      <td>1.779250</td>\n      <td>0.919108</td>\n      <td>0.244168</td>\n      <td>1.925952</td>\n      <td>-0.366884</td>\n      <td>2.330492</td>\n      <td>1.981934</td>\n      <td>-0.382531</td>\n      <td>2.973730</td>\n      <td>-1.310315</td>\n      <td>0.503558</td>\n      <td>0.788956</td>\n      <td>-1.184905</td>\n      <td>0.752818</td>\n      <td>1.244635</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76884</th>\n      <td>-0.611810</td>\n      <td>-0.723772</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.496413</td>\n      <td>-0.259853</td>\n      <td>1.815901</td>\n      <td>-0.143222</td>\n      <td>-0.002847</td>\n      <td>-1.605932</td>\n      <td>-0.449028</td>\n      <td>-0.208101</td>\n      <td>-0.059078</td>\n      <td>-0.341420</td>\n      <td>1.810455</td>\n      <td>-1.123628</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.115603</td>\n      <td>1.825157</td>\n      <td>0.507389</td>\n      <td>0.575359</td>\n      <td>1.787660</td>\n      <td>1.129089</td>\n      <td>-0.080817</td>\n      <td>1.849914</td>\n      <td>-0.779174</td>\n      <td>0.290555</td>\n      <td>1.990553</td>\n      <td>0.507389</td>\n      <td>0.907904</td>\n      <td>-1.211626</td>\n      <td>0.495542</td>\n      <td>0.588720</td>\n      <td>-1.014403</td>\n      <td>0.106444</td>\n      <td>1.229502</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76885</th>\n      <td>-0.605239</td>\n      <td>-0.659530</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.174600</td>\n      <td>-0.146213</td>\n      <td>1.836054</td>\n      <td>-0.038214</td>\n      <td>0.183720</td>\n      <td>-1.478466</td>\n      <td>-0.567504</td>\n      <td>-0.317410</td>\n      <td>-0.098314</td>\n      <td>-0.326505</td>\n      <td>4.784910</td>\n      <td>-1.130472</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.172635</td>\n      <td>1.826533</td>\n      <td>-0.463426</td>\n      <td>1.806448</td>\n      <td>1.792151</td>\n      <td>-0.389885</td>\n      <td>4.431401</td>\n      <td>1.829081</td>\n      <td>-0.505598</td>\n      <td>0.306861</td>\n      <td>1.990080</td>\n      <td>-0.463426</td>\n      <td>1.463785</td>\n      <td>-0.708503</td>\n      <td>-0.311360</td>\n      <td>-0.419508</td>\n      <td>-0.842612</td>\n      <td>0.006417</td>\n      <td>1.229408</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76886</th>\n      <td>-0.636869</td>\n      <td>0.194059</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.157636</td>\n      <td>-0.246570</td>\n      <td>1.827610</td>\n      <td>-0.591596</td>\n      <td>-0.602846</td>\n      <td>-1.377626</td>\n      <td>-0.812586</td>\n      <td>-0.835152</td>\n      <td>-0.279218</td>\n      <td>-0.371848</td>\n      <td>0.264961</td>\n      <td>-1.136254</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.154434</td>\n      <td>1.846752</td>\n      <td>-0.582058</td>\n      <td>4.778059</td>\n      <td>1.849475</td>\n      <td>0.508003</td>\n      <td>0.579277</td>\n      <td>1.819658</td>\n      <td>0.923738</td>\n      <td>0.259099</td>\n      <td>1.995408</td>\n      <td>-0.582058</td>\n      <td>1.706591</td>\n      <td>-0.915727</td>\n      <td>0.107819</td>\n      <td>0.304902</td>\n      <td>-0.620406</td>\n      <td>-0.061611</td>\n      <td>1.250421</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76887</th>\n      <td>-0.612396</td>\n      <td>1.516430</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.217149</td>\n      <td>-0.008820</td>\n      <td>1.811085</td>\n      <td>-0.820075</td>\n      <td>-0.854234</td>\n      <td>-1.399491</td>\n      <td>-1.068270</td>\n      <td>-1.062484</td>\n      <td>-0.517437</td>\n      <td>-0.444693</td>\n      <td>-0.264628</td>\n      <td>-1.145775</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.218224</td>\n      <td>1.838280</td>\n      <td>-0.827462</td>\n      <td>0.262431</td>\n      <td>1.850861</td>\n      <td>-0.471504</td>\n      <td>1.818117</td>\n      <td>1.828163</td>\n      <td>1.135308</td>\n      <td>-0.072012</td>\n      <td>2.011370</td>\n      <td>-0.827462</td>\n      <td>0.550428</td>\n      <td>0.462225</td>\n      <td>0.356451</td>\n      <td>0.487998</td>\n      <td>-0.286047</td>\n      <td>-0.059314</td>\n      <td>1.007059</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>69441 rows  92 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n36       -0.447636     1.503621 -0.616017  0.807145  0.566597 -0.676510   \n76        1.082342     2.932444  0.409891 -0.407999  0.054079 -0.291606   \n116      -0.038815     1.490517 -0.764228  1.661493  1.180223 -0.148263   \n183       0.725718     2.932444  1.004495 -1.707367 -0.945570 -0.468686   \n223       1.156386     1.442577 -1.101432  1.080693  0.088267 -0.321219   \n...            ...          ...       ...       ...       ...       ...   \n76808    -0.208849     1.877030 -0.097743  0.250530 -0.844100  1.878685   \n76820    -0.208849     0.180506 -0.479475  0.136375 -0.134496 -0.308538   \n76832    -0.208849    -0.030012 -0.391261  0.852059 -0.361299  1.687970   \n76848    -0.208849     2.667662 -1.806515 -1.257885  1.031139 -0.502160   \n76888    -0.681134     0.851401  0.154976 -0.449011  0.225954 -0.367814   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n36       0.041370    -0.444748   0.286301       -0.897920      -0.885205   \n76      -0.494304     2.201976   1.496388       -0.425977      -0.353926   \n116      0.073340     0.476343   0.792124       -1.047382      -0.969301   \n183      1.596658     2.201976   1.897724        1.685851       0.595604   \n223      0.139841     0.631494  -2.086450       -1.158357      -1.062442   \n...           ...          ...        ...             ...            ...   \n76808   -0.188176     1.104220  -0.374849       -0.970493      -0.937521   \n76820   -1.749070    -0.208232  -0.673038        2.482681       2.569855   \n76832    0.340067    -0.707754  -0.263601       -0.838144      -0.726073   \n76848    0.796485     2.201976  -0.277872       -0.478127      -0.643053   \n76888   -0.197249    -0.371359   1.771866       -0.237781      -0.182437   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n36         0.669968 -0.996079   -0.899538   -0.823745    -0.714925   \n76        -0.845733 -0.672869   -0.675338   -0.443324    -0.122958   \n116       -1.318185 -1.001774   -1.186375   -1.471881    -1.437878   \n183       -0.920109  2.095057    1.318192    2.145037     2.084527   \n223       -0.191328 -0.798725   -0.987976   -1.260964    -1.413439   \n...             ...       ...         ...         ...          ...   \n76808      0.150749 -0.914616   -1.141697   -0.557570    -0.777044   \n76820     -0.799717 -0.861063    1.967085   -0.098038    -0.578841   \n76832      0.518787 -1.026050   -0.958595   -0.389652    -0.598841   \n76848      0.846803 -0.448241   -0.343692   -0.448423    -0.564333   \n76888     -1.507184 -0.333492   -0.526458   -0.562515    -0.479081   \n\n       num__BAspr  num__size  num__lbm  num__lop  num__lgp  num__linv  \\\n36      -0.357029  -0.373732 -0.394250 -0.569229 -0.440817   0.242727   \n76       0.004582  -1.042218  0.421057  0.116165  0.326541  -1.125382   \n116     -0.154287  -0.845688 -0.283119  1.622306  1.928796  -1.026000   \n183     -0.322355  -1.567221  1.564946 -2.080395 -1.770265  -0.728135   \n223     -0.466201   2.825176 -0.664197  0.890849 -0.008299   0.969864   \n...           ...        ...       ...       ...       ...        ...   \n76808   -0.434216   0.249006  0.140510  0.324503 -0.716776   1.592674   \n76820   -0.449947   0.357319 -0.000264 -0.144985 -0.379309   1.387099   \n76832   -0.409310  -0.068315  0.117335  1.076196 -0.028161   1.083701   \n76848   -0.464551   0.750689 -0.923249 -1.272632  0.706670  -0.675277   \n76888    0.067450  -1.151990 -0.204261 -0.003571  0.471093  -0.542937   \n\n       num__llme  num__l1amhd  num__l1MAX  num__l1BAspr  num__l3amhd  \\\n36     -0.275573     0.267270   -0.269506     -0.318411     0.205912   \n76     -1.573113     1.527156   -0.594924      0.712950     1.761770   \n116    -1.010973     0.805248   -0.393897     -0.194811     0.822153   \n183    -2.319771     1.908623    0.307918      0.395609     2.123751   \n223     2.717759    -2.082807   -1.103496     -0.468662    -2.064685   \n...          ...          ...         ...           ...          ...   \n76808   0.001962    -0.378625   -0.626435     -0.437696    -0.401772   \n76820   0.508948    -0.659527   -0.753338     -0.460048    -0.628558   \n76832   0.048485    -0.272724   -0.908995     -0.445693    -0.278341   \n76848   0.285313    -0.255770   -0.190112     -0.466064    -0.178502   \n76888  -1.137309     1.821702   -1.083483     -0.266651     1.871219   \n\n       num__l3MAX  num__l3BAspr  num__l6amhd  num__l6MAX  num__l6BAspr  \\\n36      -0.104132     -0.331265     0.072720   -0.946680     -0.412905   \n76      -0.679498      0.038660     1.822095    0.159410      0.331043   \n116     -1.143382     -0.218875     0.889428   -0.854871     -0.437584   \n183      2.460112      0.569178     2.168014    2.476404      4.909389   \n223     -1.156894     -0.468742    -2.049128   -1.029393     -0.463778   \n...           ...           ...          ...         ...           ...   \n76808   -0.636862     -0.263074    -0.374597   -0.687537     -0.456005   \n76820    0.006270     -0.462151    -0.576820   -0.657888     -0.354463   \n76832   -0.623313     -0.452715    -0.304512   -0.314697     -0.448004   \n76848    0.670976     -0.467300    -0.090091    0.268836     -0.460818   \n76888   -0.591198      4.808439     1.832704   -0.395160      4.525245   \n\n       num__l12amhd  num__l12MAX  num__l12BAspr  num__l12mom122  \\\n36         0.484859    -0.269506      -0.423203        2.672025   \n76         2.221642    -0.594924       0.995690        1.607994   \n116        1.100826    -0.393897      -0.315926        1.487194   \n183       -0.140420     0.307918       4.010015        2.157728   \n223       -1.998789    -1.103496      -0.456639        1.337993   \n...             ...          ...            ...             ...   \n76808     -0.054501    -0.626435      -0.255555        1.189582   \n76820     -0.463467    -0.753338      -0.432884        1.033342   \n76832     -0.192267    -0.908995      -0.424018        2.672025   \n76848      0.050297    -0.190112      -0.402299        1.856043   \n76888      2.050753    -1.083483       2.233048        2.518535   \n\n       num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n36             -0.595621         -0.602738         0.655205      -0.705329   \n76             -0.186524         -0.124067        -2.448014       0.095269   \n116            -1.219011         -1.187009        -0.936426      -1.256309   \n183             1.577745          1.776789         3.967493       2.218228   \n223            -1.117885         -1.013970        -0.382679      -1.246534   \n...                  ...               ...              ...            ...   \n76808          -0.540612         -0.551645         0.953966      -0.311244   \n76820          -0.835113         -0.701370        -0.737806      -0.829634   \n76832          -0.765681         -0.780979         0.172734      -0.440386   \n76848           0.576474          0.139332         0.663355      -0.276019   \n76888           0.155802          0.049098        -0.276814      -0.146158   \n\n       num__l12vol12m  num__amhd_miss  num__BAspr_miss  cat__ind_1.0  \\\n36          -0.016263       -0.178017        -0.078007           0.0   \n76           0.861902       -0.178017        -0.078007           0.0   \n116         -0.357296       -0.178017        -0.078007           0.0   \n183          2.230892       -0.178017        -0.078007           0.0   \n223         -1.062468       -0.178017        -0.078007           0.0   \n...               ...             ...              ...           ...   \n76808        0.013890       -0.178017        -0.078007           0.0   \n76820       -0.770382       -0.178017        -0.078007           0.0   \n76832       -0.020364       -0.178017        -0.078007           0.0   \n76848        0.117571       -0.178017        -0.078007           0.0   \n76888        0.813927       -0.178017        -0.078007           0.0   \n\n       cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  \\\n36              0.0           0.0           0.0           0.0           0.0   \n76              0.0           0.0           0.0           0.0           0.0   \n116             1.0           0.0           0.0           0.0           0.0   \n183             0.0           0.0           0.0           0.0           0.0   \n223             0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n76808           0.0           0.0           0.0           0.0           0.0   \n76820           0.0           0.0           0.0           0.0           0.0   \n76832           0.0           0.0           0.0           0.0           0.0   \n76848           0.0           0.0           0.0           0.0           0.0   \n76888           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n36              0.0           0.0           0.0            0.0            0.0   \n76              0.0           0.0           0.0            0.0            0.0   \n116             0.0           0.0           0.0            0.0            0.0   \n183             0.0           0.0           0.0            0.0            0.0   \n223             0.0           0.0           0.0            0.0            0.0   \n...             ...           ...           ...            ...            ...   \n76808           0.0           0.0           0.0            0.0            0.0   \n76820           0.0           0.0           0.0            0.0            0.0   \n76832           0.0           0.0           0.0            0.0            0.0   \n76848           0.0           0.0           0.0            0.0            0.0   \n76888           0.0           0.0           0.0            0.0            0.0   \n\n       cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n36               0.0            0.0            0.0            1.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            1.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            1.0            0.0   \n\n       cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            1.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            1.0            0.0   \n76820            0.0            0.0            1.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              1.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            1.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            1.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  cat__ind_47.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_48.0  cat__ind_49.0  \n36               0.0            0.0  \n76               0.0            0.0  \n116              0.0            0.0  \n183              0.0            0.0  \n223              0.0            0.0  \n...              ...            ...  \n76808            0.0            0.0  \n76820            0.0            0.0  \n76832            0.0            0.0  \n76848            0.0            0.0  \n76888            0.0            0.0  \n\n[1881 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36</th>\n      <td>-0.447636</td>\n      <td>1.503621</td>\n      <td>-0.616017</td>\n      <td>0.807145</td>\n      <td>0.566597</td>\n      <td>-0.676510</td>\n      <td>0.041370</td>\n      <td>-0.444748</td>\n      <td>0.286301</td>\n      <td>-0.897920</td>\n      <td>-0.885205</td>\n      <td>0.669968</td>\n      <td>-0.996079</td>\n      <td>-0.899538</td>\n      <td>-0.823745</td>\n      <td>-0.714925</td>\n      <td>-0.357029</td>\n      <td>-0.373732</td>\n      <td>-0.394250</td>\n      <td>-0.569229</td>\n      <td>-0.440817</td>\n      <td>0.242727</td>\n      <td>-0.275573</td>\n      <td>0.267270</td>\n      <td>-0.269506</td>\n      <td>-0.318411</td>\n      <td>0.205912</td>\n      <td>-0.104132</td>\n      <td>-0.331265</td>\n      <td>0.072720</td>\n      <td>-0.946680</td>\n      <td>-0.412905</td>\n      <td>0.484859</td>\n      <td>-0.269506</td>\n      <td>-0.423203</td>\n      <td>2.672025</td>\n      <td>-0.595621</td>\n      <td>-0.602738</td>\n      <td>0.655205</td>\n      <td>-0.705329</td>\n      <td>-0.016263</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>1.082342</td>\n      <td>2.932444</td>\n      <td>0.409891</td>\n      <td>-0.407999</td>\n      <td>0.054079</td>\n      <td>-0.291606</td>\n      <td>-0.494304</td>\n      <td>2.201976</td>\n      <td>1.496388</td>\n      <td>-0.425977</td>\n      <td>-0.353926</td>\n      <td>-0.845733</td>\n      <td>-0.672869</td>\n      <td>-0.675338</td>\n      <td>-0.443324</td>\n      <td>-0.122958</td>\n      <td>0.004582</td>\n      <td>-1.042218</td>\n      <td>0.421057</td>\n      <td>0.116165</td>\n      <td>0.326541</td>\n      <td>-1.125382</td>\n      <td>-1.573113</td>\n      <td>1.527156</td>\n      <td>-0.594924</td>\n      <td>0.712950</td>\n      <td>1.761770</td>\n      <td>-0.679498</td>\n      <td>0.038660</td>\n      <td>1.822095</td>\n      <td>0.159410</td>\n      <td>0.331043</td>\n      <td>2.221642</td>\n      <td>-0.594924</td>\n      <td>0.995690</td>\n      <td>1.607994</td>\n      <td>-0.186524</td>\n      <td>-0.124067</td>\n      <td>-2.448014</td>\n      <td>0.095269</td>\n      <td>0.861902</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>-0.038815</td>\n      <td>1.490517</td>\n      <td>-0.764228</td>\n      <td>1.661493</td>\n      <td>1.180223</td>\n      <td>-0.148263</td>\n      <td>0.073340</td>\n      <td>0.476343</td>\n      <td>0.792124</td>\n      <td>-1.047382</td>\n      <td>-0.969301</td>\n      <td>-1.318185</td>\n      <td>-1.001774</td>\n      <td>-1.186375</td>\n      <td>-1.471881</td>\n      <td>-1.437878</td>\n      <td>-0.154287</td>\n      <td>-0.845688</td>\n      <td>-0.283119</td>\n      <td>1.622306</td>\n      <td>1.928796</td>\n      <td>-1.026000</td>\n      <td>-1.010973</td>\n      <td>0.805248</td>\n      <td>-0.393897</td>\n      <td>-0.194811</td>\n      <td>0.822153</td>\n      <td>-1.143382</td>\n      <td>-0.218875</td>\n      <td>0.889428</td>\n      <td>-0.854871</td>\n      <td>-0.437584</td>\n      <td>1.100826</td>\n      <td>-0.393897</td>\n      <td>-0.315926</td>\n      <td>1.487194</td>\n      <td>-1.219011</td>\n      <td>-1.187009</td>\n      <td>-0.936426</td>\n      <td>-1.256309</td>\n      <td>-0.357296</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>0.725718</td>\n      <td>2.932444</td>\n      <td>1.004495</td>\n      <td>-1.707367</td>\n      <td>-0.945570</td>\n      <td>-0.468686</td>\n      <td>1.596658</td>\n      <td>2.201976</td>\n      <td>1.897724</td>\n      <td>1.685851</td>\n      <td>0.595604</td>\n      <td>-0.920109</td>\n      <td>2.095057</td>\n      <td>1.318192</td>\n      <td>2.145037</td>\n      <td>2.084527</td>\n      <td>-0.322355</td>\n      <td>-1.567221</td>\n      <td>1.564946</td>\n      <td>-2.080395</td>\n      <td>-1.770265</td>\n      <td>-0.728135</td>\n      <td>-2.319771</td>\n      <td>1.908623</td>\n      <td>0.307918</td>\n      <td>0.395609</td>\n      <td>2.123751</td>\n      <td>2.460112</td>\n      <td>0.569178</td>\n      <td>2.168014</td>\n      <td>2.476404</td>\n      <td>4.909389</td>\n      <td>-0.140420</td>\n      <td>0.307918</td>\n      <td>4.010015</td>\n      <td>2.157728</td>\n      <td>1.577745</td>\n      <td>1.776789</td>\n      <td>3.967493</td>\n      <td>2.218228</td>\n      <td>2.230892</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>1.156386</td>\n      <td>1.442577</td>\n      <td>-1.101432</td>\n      <td>1.080693</td>\n      <td>0.088267</td>\n      <td>-0.321219</td>\n      <td>0.139841</td>\n      <td>0.631494</td>\n      <td>-2.086450</td>\n      <td>-1.158357</td>\n      <td>-1.062442</td>\n      <td>-0.191328</td>\n      <td>-0.798725</td>\n      <td>-0.987976</td>\n      <td>-1.260964</td>\n      <td>-1.413439</td>\n      <td>-0.466201</td>\n      <td>2.825176</td>\n      <td>-0.664197</td>\n      <td>0.890849</td>\n      <td>-0.008299</td>\n      <td>0.969864</td>\n      <td>2.717759</td>\n      <td>-2.082807</td>\n      <td>-1.103496</td>\n      <td>-0.468662</td>\n      <td>-2.064685</td>\n      <td>-1.156894</td>\n      <td>-0.468742</td>\n      <td>-2.049128</td>\n      <td>-1.029393</td>\n      <td>-0.463778</td>\n      <td>-1.998789</td>\n      <td>-1.103496</td>\n      <td>-0.456639</td>\n      <td>1.337993</td>\n      <td>-1.117885</td>\n      <td>-1.013970</td>\n      <td>-0.382679</td>\n      <td>-1.246534</td>\n      <td>-1.062468</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76808</th>\n      <td>-0.208849</td>\n      <td>1.877030</td>\n      <td>-0.097743</td>\n      <td>0.250530</td>\n      <td>-0.844100</td>\n      <td>1.878685</td>\n      <td>-0.188176</td>\n      <td>1.104220</td>\n      <td>-0.374849</td>\n      <td>-0.970493</td>\n      <td>-0.937521</td>\n      <td>0.150749</td>\n      <td>-0.914616</td>\n      <td>-1.141697</td>\n      <td>-0.557570</td>\n      <td>-0.777044</td>\n      <td>-0.434216</td>\n      <td>0.249006</td>\n      <td>0.140510</td>\n      <td>0.324503</td>\n      <td>-0.716776</td>\n      <td>1.592674</td>\n      <td>0.001962</td>\n      <td>-0.378625</td>\n      <td>-0.626435</td>\n      <td>-0.437696</td>\n      <td>-0.401772</td>\n      <td>-0.636862</td>\n      <td>-0.263074</td>\n      <td>-0.374597</td>\n      <td>-0.687537</td>\n      <td>-0.456005</td>\n      <td>-0.054501</td>\n      <td>-0.626435</td>\n      <td>-0.255555</td>\n      <td>1.189582</td>\n      <td>-0.540612</td>\n      <td>-0.551645</td>\n      <td>0.953966</td>\n      <td>-0.311244</td>\n      <td>0.013890</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76820</th>\n      <td>-0.208849</td>\n      <td>0.180506</td>\n      <td>-0.479475</td>\n      <td>0.136375</td>\n      <td>-0.134496</td>\n      <td>-0.308538</td>\n      <td>-1.749070</td>\n      <td>-0.208232</td>\n      <td>-0.673038</td>\n      <td>2.482681</td>\n      <td>2.569855</td>\n      <td>-0.799717</td>\n      <td>-0.861063</td>\n      <td>1.967085</td>\n      <td>-0.098038</td>\n      <td>-0.578841</td>\n      <td>-0.449947</td>\n      <td>0.357319</td>\n      <td>-0.000264</td>\n      <td>-0.144985</td>\n      <td>-0.379309</td>\n      <td>1.387099</td>\n      <td>0.508948</td>\n      <td>-0.659527</td>\n      <td>-0.753338</td>\n      <td>-0.460048</td>\n      <td>-0.628558</td>\n      <td>0.006270</td>\n      <td>-0.462151</td>\n      <td>-0.576820</td>\n      <td>-0.657888</td>\n      <td>-0.354463</td>\n      <td>-0.463467</td>\n      <td>-0.753338</td>\n      <td>-0.432884</td>\n      <td>1.033342</td>\n      <td>-0.835113</td>\n      <td>-0.701370</td>\n      <td>-0.737806</td>\n      <td>-0.829634</td>\n      <td>-0.770382</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76832</th>\n      <td>-0.208849</td>\n      <td>-0.030012</td>\n      <td>-0.391261</td>\n      <td>0.852059</td>\n      <td>-0.361299</td>\n      <td>1.687970</td>\n      <td>0.340067</td>\n      <td>-0.707754</td>\n      <td>-0.263601</td>\n      <td>-0.838144</td>\n      <td>-0.726073</td>\n      <td>0.518787</td>\n      <td>-1.026050</td>\n      <td>-0.958595</td>\n      <td>-0.389652</td>\n      <td>-0.598841</td>\n      <td>-0.409310</td>\n      <td>-0.068315</td>\n      <td>0.117335</td>\n      <td>1.076196</td>\n      <td>-0.028161</td>\n      <td>1.083701</td>\n      <td>0.048485</td>\n      <td>-0.272724</td>\n      <td>-0.908995</td>\n      <td>-0.445693</td>\n      <td>-0.278341</td>\n      <td>-0.623313</td>\n      <td>-0.452715</td>\n      <td>-0.304512</td>\n      <td>-0.314697</td>\n      <td>-0.448004</td>\n      <td>-0.192267</td>\n      <td>-0.908995</td>\n      <td>-0.424018</td>\n      <td>2.672025</td>\n      <td>-0.765681</td>\n      <td>-0.780979</td>\n      <td>0.172734</td>\n      <td>-0.440386</td>\n      <td>-0.020364</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76848</th>\n      <td>-0.208849</td>\n      <td>2.667662</td>\n      <td>-1.806515</td>\n      <td>-1.257885</td>\n      <td>1.031139</td>\n      <td>-0.502160</td>\n      <td>0.796485</td>\n      <td>2.201976</td>\n      <td>-0.277872</td>\n      <td>-0.478127</td>\n      <td>-0.643053</td>\n      <td>0.846803</td>\n      <td>-0.448241</td>\n      <td>-0.343692</td>\n      <td>-0.448423</td>\n      <td>-0.564333</td>\n      <td>-0.464551</td>\n      <td>0.750689</td>\n      <td>-0.923249</td>\n      <td>-1.272632</td>\n      <td>0.706670</td>\n      <td>-0.675277</td>\n      <td>0.285313</td>\n      <td>-0.255770</td>\n      <td>-0.190112</td>\n      <td>-0.466064</td>\n      <td>-0.178502</td>\n      <td>0.670976</td>\n      <td>-0.467300</td>\n      <td>-0.090091</td>\n      <td>0.268836</td>\n      <td>-0.460818</td>\n      <td>0.050297</td>\n      <td>-0.190112</td>\n      <td>-0.402299</td>\n      <td>1.856043</td>\n      <td>0.576474</td>\n      <td>0.139332</td>\n      <td>0.663355</td>\n      <td>-0.276019</td>\n      <td>0.117571</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76888</th>\n      <td>-0.681134</td>\n      <td>0.851401</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.197249</td>\n      <td>-0.371359</td>\n      <td>1.771866</td>\n      <td>-0.237781</td>\n      <td>-0.182437</td>\n      <td>-1.507184</td>\n      <td>-0.333492</td>\n      <td>-0.526458</td>\n      <td>-0.562515</td>\n      <td>-0.479081</td>\n      <td>0.067450</td>\n      <td>-1.151990</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.137309</td>\n      <td>1.821702</td>\n      <td>-1.083483</td>\n      <td>-0.266651</td>\n      <td>1.871219</td>\n      <td>-0.591198</td>\n      <td>4.808439</td>\n      <td>1.832704</td>\n      <td>-0.395160</td>\n      <td>4.525245</td>\n      <td>2.050753</td>\n      <td>-1.083483</td>\n      <td>2.233048</td>\n      <td>2.518535</td>\n      <td>0.155802</td>\n      <td>0.049098</td>\n      <td>-0.276814</td>\n      <td>-0.146158</td>\n      <td>0.813927</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1881 rows  92 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n37       -0.370711     1.045747 -0.616017  0.807145  0.566597 -0.676510   \n77        1.562862     2.932444  0.409891 -0.407999  0.054079 -0.291606   \n117      -0.017971     1.548951 -0.764228  1.661493  1.180223 -0.148263   \n184       0.995702     2.932444  1.004495 -1.707367 -0.945570 -0.468686   \n224       1.147943     1.282408 -1.101432  1.080693  0.088267 -0.321219   \n...            ...          ...       ...       ...       ...       ...   \n76809    -0.208849     1.361380 -0.097743  0.250530 -0.844100  1.878685   \n76821    -0.208849    -0.112372 -0.479475  0.136375 -0.134496 -0.308538   \n76833    -0.208849    -0.300522 -0.391261  0.852059 -0.361299  1.687970   \n76849    -0.208849     1.938339 -1.806515 -1.257885  1.031139 -0.502160   \n76889    -0.690620     0.644627  0.154976 -0.449011  0.225954 -0.367814   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n37       0.375087     0.151844   0.324364       -0.926429      -0.835087   \n77       1.619434     1.765912   1.497371       -0.321803      -0.310475   \n117      0.001117     0.335290   0.784002       -1.173535      -1.074915   \n184     -0.118384     2.201976   1.897724        1.946351       2.146835   \n224      0.057190     0.589992  -2.076221       -1.190013      -1.156021   \n...           ...          ...        ...             ...            ...   \n76809   -1.457988     0.954821  -0.377764       -0.554896      -0.484308   \n76821    0.503895    -0.692448  -0.679762       -0.529885      -0.387674   \n76833   -1.046258    -0.672714  -0.253326       -0.697715      -0.685183   \n76849   -0.298017     2.201976  -0.300283       -0.712674      -0.693444   \n76889   -0.064676    -0.604371   1.771012       -0.826214      -0.853646   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n37         0.819344 -0.767050   -0.415347   -0.837416    -0.742606   \n77        -0.819265 -0.310528   -0.604683   -0.665054    -0.198205   \n117       -1.418253 -1.138869   -1.358698   -1.498212    -1.529875   \n184       -0.986489  1.522755    1.681061    2.134220     2.084527   \n224       -0.079273 -0.976936   -0.943820   -1.358742    -1.372436   \n...             ...       ...         ...         ...          ...   \n76809      0.210192 -0.860239   -0.538398   -0.528166    -0.760821   \n76821     -0.777245 -0.624373   -0.747129   -0.121647    -0.547000   \n76833      0.464065 -1.147242   -0.860877   -0.522436    -0.591345   \n76849      0.865974 -0.488813   -0.815114   -0.553735    -0.551772   \n76889     -1.525174 -0.806854   -1.030321   -0.850105    -0.520813   \n\n       num__BAspr  num__size  num__lbm  num__lop  num__lgp  num__linv  \\\n37      -0.273727  -0.347390 -0.394250 -0.569229 -0.440817   0.242727   \n77       0.019889  -0.948273  0.421057  0.116165  0.326541  -1.125382   \n117     -0.196580  -0.841639 -0.283119  1.622306  1.928796  -1.026000   \n184      2.409377  -1.570553  1.564946 -2.080395 -1.770265  -0.728135   \n224     -0.454743   2.832604 -0.664197  0.890849 -0.008299   0.969864   \n...           ...        ...       ...       ...       ...        ...   \n76809   -0.397030   0.215362  0.140510  0.324503 -0.716776   1.592674   \n76821   -0.452278   0.391971 -0.000264 -0.144985 -0.379309   1.387099   \n76833   -0.440970  -0.131511  0.117335  1.076196 -0.028161   1.083701   \n76849   -0.451074   0.746059 -0.923249 -1.272632  0.706670  -0.675277   \n76889    1.154986  -1.151990 -0.204261 -0.003571  0.471093  -0.542937   \n\n       num__llme  num__l1amhd  num__l1MAX  num__l1BAspr  num__l3amhd  \\\n37     -0.416237     0.291945   -1.011197     -0.358964     0.243817   \n77     -1.415715     1.505979   -0.687561      0.002301     1.688294   \n117    -0.975310     0.799418   -1.016899     -0.156416     0.818801   \n184    -2.259244     1.908623    2.084007     -0.324324     1.933517   \n224     2.738320    -2.088544   -0.813583     -0.468032    -2.077690   \n...          ...          ...         ...           ...          ...   \n76809   0.017774    -0.371362   -0.929627     -0.436078    -0.384307   \n76821   0.496130    -0.670522   -0.876003     -0.451794    -0.642843   \n76833   0.063224    -0.259750   -1.041207     -0.411195    -0.267628   \n76849   0.369738    -0.274068   -0.462638     -0.466384    -0.209352   \n76889  -1.076504     1.782355   -0.347738      0.065109     1.862689   \n\n       num__l3MAX  num__l3BAspr  num__l6amhd  num__l6MAX  num__l6BAspr  \\\n37      -0.839387     -0.324342     0.095959   -0.731315     -0.357240   \n77      -0.429360      0.854002     1.819425    0.480177      0.086252   \n117     -0.878843     -0.312308     0.904621   -1.013453     -0.255808   \n184      1.854294      0.172582     2.168014    2.476404      4.909389   \n224     -0.794177     -0.457275    -2.050656    0.040278     -0.467010   \n...           ...           ...          ...         ...           ...   \n76809    0.216762     -0.442030    -0.394194   -0.249067     -0.408983   \n76821   -0.738654     -0.463502    -0.585459   -0.954593     -0.462559   \n76833   -0.419104     -0.448052    -0.286616    0.417637     -0.447920   \n76849   -0.783334     -0.467451    -0.108760    0.803386     -0.464383   \n76889   -0.838800      0.264378     1.890670    0.509522      0.600523   \n\n       num__l12amhd  num__l12MAX  num__l12BAspr  num__l12mom122  \\\n37         0.260356    -1.011197      -0.440167        2.672025   \n77         2.144114    -0.687561       2.133388        2.306243   \n117        1.068031    -1.016899      -0.067369        1.237983   \n184       -0.140420     2.084007       3.674272        0.206004   \n224       -2.036770    -0.813583      -0.457157        1.051903   \n...             ...          ...            ...             ...   \n76809     -0.169345    -0.929627      -0.440086        1.073954   \n76821     -0.505047    -0.876003      -0.409439        1.389443   \n76833     -0.268217    -1.041207      -0.449478        0.965275   \n76849      0.019749    -0.462638      -0.443034        0.389688   \n76889      2.019325    -0.347738       2.482332        1.956218   \n\n       num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n37             -0.081076         -0.058999         0.166738      -0.602469   \n77              1.035512          1.048353        -2.348474       0.271762   \n117            -0.498111         -0.379121        -1.343626      -1.164610   \n184             2.463395          2.423646        -0.487464       2.218228   \n224            -1.203442         -1.138263        -0.489305      -1.280598   \n...                  ...               ...              ...            ...   \n76809          -0.449619         -0.438896         1.020797      -0.401676   \n76821          -1.141501         -1.187009        -0.780925      -0.876849   \n76833          -0.818476         -0.690079         0.571615      -0.709454   \n76849          -0.734271         -0.657172         0.494761      -0.346837   \n76889          -0.317879         -0.206623        -1.289419      -0.263619   \n\n       num__l12vol12m  num__amhd_miss  num__BAspr_miss  cat__ind_1.0  \\\n37          -0.391493       -0.178017        -0.078007           0.0   \n77           0.884246       -0.178017        -0.078007           0.0   \n117         -0.710352       -0.178017        -0.078007           0.0   \n184          2.230892       -0.178017        -0.078007           0.0   \n224         -1.270395       -0.178017        -0.078007           0.0   \n...               ...             ...              ...           ...   \n76809       -0.174816       -0.178017        -0.078007           0.0   \n76821       -0.868594       -0.178017        -0.078007           0.0   \n76833       -0.189070       -0.178017        -0.078007           0.0   \n76849       -0.123037       -0.178017        -0.078007           0.0   \n76889        0.330423       -0.178017        -0.078007           0.0   \n\n       cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  \\\n37              0.0           0.0           0.0           0.0           0.0   \n77              0.0           0.0           0.0           0.0           0.0   \n117             1.0           0.0           0.0           0.0           0.0   \n184             0.0           0.0           0.0           0.0           0.0   \n224             0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n76809           0.0           0.0           0.0           0.0           0.0   \n76821           0.0           0.0           0.0           0.0           0.0   \n76833           0.0           0.0           0.0           0.0           0.0   \n76849           0.0           0.0           0.0           0.0           0.0   \n76889           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n37              0.0           0.0           0.0            0.0            0.0   \n77              0.0           0.0           0.0            0.0            0.0   \n117             0.0           0.0           0.0            0.0            0.0   \n184             0.0           0.0           0.0            0.0            0.0   \n224             0.0           0.0           0.0            0.0            0.0   \n...             ...           ...           ...            ...            ...   \n76809           0.0           0.0           0.0            0.0            0.0   \n76821           0.0           0.0           0.0            0.0            0.0   \n76833           0.0           0.0           0.0            0.0            0.0   \n76849           0.0           0.0           0.0            0.0            0.0   \n76889           0.0           0.0           0.0            0.0            0.0   \n\n       cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n37               0.0            0.0            0.0            1.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            1.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            1.0            0.0   \n\n       cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            1.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            1.0            0.0   \n76821            0.0            0.0            1.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              1.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            1.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            1.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  cat__ind_47.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_48.0  cat__ind_49.0  \n37               0.0            0.0  \n77               0.0            0.0  \n117              0.0            0.0  \n184              0.0            0.0  \n224              0.0            0.0  \n...              ...            ...  \n76809            0.0            0.0  \n76821            0.0            0.0  \n76833            0.0            0.0  \n76849            0.0            0.0  \n76889            0.0            0.0  \n\n[1873 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37</th>\n      <td>-0.370711</td>\n      <td>1.045747</td>\n      <td>-0.616017</td>\n      <td>0.807145</td>\n      <td>0.566597</td>\n      <td>-0.676510</td>\n      <td>0.375087</td>\n      <td>0.151844</td>\n      <td>0.324364</td>\n      <td>-0.926429</td>\n      <td>-0.835087</td>\n      <td>0.819344</td>\n      <td>-0.767050</td>\n      <td>-0.415347</td>\n      <td>-0.837416</td>\n      <td>-0.742606</td>\n      <td>-0.273727</td>\n      <td>-0.347390</td>\n      <td>-0.394250</td>\n      <td>-0.569229</td>\n      <td>-0.440817</td>\n      <td>0.242727</td>\n      <td>-0.416237</td>\n      <td>0.291945</td>\n      <td>-1.011197</td>\n      <td>-0.358964</td>\n      <td>0.243817</td>\n      <td>-0.839387</td>\n      <td>-0.324342</td>\n      <td>0.095959</td>\n      <td>-0.731315</td>\n      <td>-0.357240</td>\n      <td>0.260356</td>\n      <td>-1.011197</td>\n      <td>-0.440167</td>\n      <td>2.672025</td>\n      <td>-0.081076</td>\n      <td>-0.058999</td>\n      <td>0.166738</td>\n      <td>-0.602469</td>\n      <td>-0.391493</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>1.562862</td>\n      <td>2.932444</td>\n      <td>0.409891</td>\n      <td>-0.407999</td>\n      <td>0.054079</td>\n      <td>-0.291606</td>\n      <td>1.619434</td>\n      <td>1.765912</td>\n      <td>1.497371</td>\n      <td>-0.321803</td>\n      <td>-0.310475</td>\n      <td>-0.819265</td>\n      <td>-0.310528</td>\n      <td>-0.604683</td>\n      <td>-0.665054</td>\n      <td>-0.198205</td>\n      <td>0.019889</td>\n      <td>-0.948273</td>\n      <td>0.421057</td>\n      <td>0.116165</td>\n      <td>0.326541</td>\n      <td>-1.125382</td>\n      <td>-1.415715</td>\n      <td>1.505979</td>\n      <td>-0.687561</td>\n      <td>0.002301</td>\n      <td>1.688294</td>\n      <td>-0.429360</td>\n      <td>0.854002</td>\n      <td>1.819425</td>\n      <td>0.480177</td>\n      <td>0.086252</td>\n      <td>2.144114</td>\n      <td>-0.687561</td>\n      <td>2.133388</td>\n      <td>2.306243</td>\n      <td>1.035512</td>\n      <td>1.048353</td>\n      <td>-2.348474</td>\n      <td>0.271762</td>\n      <td>0.884246</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>-0.017971</td>\n      <td>1.548951</td>\n      <td>-0.764228</td>\n      <td>1.661493</td>\n      <td>1.180223</td>\n      <td>-0.148263</td>\n      <td>0.001117</td>\n      <td>0.335290</td>\n      <td>0.784002</td>\n      <td>-1.173535</td>\n      <td>-1.074915</td>\n      <td>-1.418253</td>\n      <td>-1.138869</td>\n      <td>-1.358698</td>\n      <td>-1.498212</td>\n      <td>-1.529875</td>\n      <td>-0.196580</td>\n      <td>-0.841639</td>\n      <td>-0.283119</td>\n      <td>1.622306</td>\n      <td>1.928796</td>\n      <td>-1.026000</td>\n      <td>-0.975310</td>\n      <td>0.799418</td>\n      <td>-1.016899</td>\n      <td>-0.156416</td>\n      <td>0.818801</td>\n      <td>-0.878843</td>\n      <td>-0.312308</td>\n      <td>0.904621</td>\n      <td>-1.013453</td>\n      <td>-0.255808</td>\n      <td>1.068031</td>\n      <td>-1.016899</td>\n      <td>-0.067369</td>\n      <td>1.237983</td>\n      <td>-0.498111</td>\n      <td>-0.379121</td>\n      <td>-1.343626</td>\n      <td>-1.164610</td>\n      <td>-0.710352</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>0.995702</td>\n      <td>2.932444</td>\n      <td>1.004495</td>\n      <td>-1.707367</td>\n      <td>-0.945570</td>\n      <td>-0.468686</td>\n      <td>-0.118384</td>\n      <td>2.201976</td>\n      <td>1.897724</td>\n      <td>1.946351</td>\n      <td>2.146835</td>\n      <td>-0.986489</td>\n      <td>1.522755</td>\n      <td>1.681061</td>\n      <td>2.134220</td>\n      <td>2.084527</td>\n      <td>2.409377</td>\n      <td>-1.570553</td>\n      <td>1.564946</td>\n      <td>-2.080395</td>\n      <td>-1.770265</td>\n      <td>-0.728135</td>\n      <td>-2.259244</td>\n      <td>1.908623</td>\n      <td>2.084007</td>\n      <td>-0.324324</td>\n      <td>1.933517</td>\n      <td>1.854294</td>\n      <td>0.172582</td>\n      <td>2.168014</td>\n      <td>2.476404</td>\n      <td>4.909389</td>\n      <td>-0.140420</td>\n      <td>2.084007</td>\n      <td>3.674272</td>\n      <td>0.206004</td>\n      <td>2.463395</td>\n      <td>2.423646</td>\n      <td>-0.487464</td>\n      <td>2.218228</td>\n      <td>2.230892</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>224</th>\n      <td>1.147943</td>\n      <td>1.282408</td>\n      <td>-1.101432</td>\n      <td>1.080693</td>\n      <td>0.088267</td>\n      <td>-0.321219</td>\n      <td>0.057190</td>\n      <td>0.589992</td>\n      <td>-2.076221</td>\n      <td>-1.190013</td>\n      <td>-1.156021</td>\n      <td>-0.079273</td>\n      <td>-0.976936</td>\n      <td>-0.943820</td>\n      <td>-1.358742</td>\n      <td>-1.372436</td>\n      <td>-0.454743</td>\n      <td>2.832604</td>\n      <td>-0.664197</td>\n      <td>0.890849</td>\n      <td>-0.008299</td>\n      <td>0.969864</td>\n      <td>2.738320</td>\n      <td>-2.088544</td>\n      <td>-0.813583</td>\n      <td>-0.468032</td>\n      <td>-2.077690</td>\n      <td>-0.794177</td>\n      <td>-0.457275</td>\n      <td>-2.050656</td>\n      <td>0.040278</td>\n      <td>-0.467010</td>\n      <td>-2.036770</td>\n      <td>-0.813583</td>\n      <td>-0.457157</td>\n      <td>1.051903</td>\n      <td>-1.203442</td>\n      <td>-1.138263</td>\n      <td>-0.489305</td>\n      <td>-1.280598</td>\n      <td>-1.270395</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76809</th>\n      <td>-0.208849</td>\n      <td>1.361380</td>\n      <td>-0.097743</td>\n      <td>0.250530</td>\n      <td>-0.844100</td>\n      <td>1.878685</td>\n      <td>-1.457988</td>\n      <td>0.954821</td>\n      <td>-0.377764</td>\n      <td>-0.554896</td>\n      <td>-0.484308</td>\n      <td>0.210192</td>\n      <td>-0.860239</td>\n      <td>-0.538398</td>\n      <td>-0.528166</td>\n      <td>-0.760821</td>\n      <td>-0.397030</td>\n      <td>0.215362</td>\n      <td>0.140510</td>\n      <td>0.324503</td>\n      <td>-0.716776</td>\n      <td>1.592674</td>\n      <td>0.017774</td>\n      <td>-0.371362</td>\n      <td>-0.929627</td>\n      <td>-0.436078</td>\n      <td>-0.384307</td>\n      <td>0.216762</td>\n      <td>-0.442030</td>\n      <td>-0.394194</td>\n      <td>-0.249067</td>\n      <td>-0.408983</td>\n      <td>-0.169345</td>\n      <td>-0.929627</td>\n      <td>-0.440086</td>\n      <td>1.073954</td>\n      <td>-0.449619</td>\n      <td>-0.438896</td>\n      <td>1.020797</td>\n      <td>-0.401676</td>\n      <td>-0.174816</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76821</th>\n      <td>-0.208849</td>\n      <td>-0.112372</td>\n      <td>-0.479475</td>\n      <td>0.136375</td>\n      <td>-0.134496</td>\n      <td>-0.308538</td>\n      <td>0.503895</td>\n      <td>-0.692448</td>\n      <td>-0.679762</td>\n      <td>-0.529885</td>\n      <td>-0.387674</td>\n      <td>-0.777245</td>\n      <td>-0.624373</td>\n      <td>-0.747129</td>\n      <td>-0.121647</td>\n      <td>-0.547000</td>\n      <td>-0.452278</td>\n      <td>0.391971</td>\n      <td>-0.000264</td>\n      <td>-0.144985</td>\n      <td>-0.379309</td>\n      <td>1.387099</td>\n      <td>0.496130</td>\n      <td>-0.670522</td>\n      <td>-0.876003</td>\n      <td>-0.451794</td>\n      <td>-0.642843</td>\n      <td>-0.738654</td>\n      <td>-0.463502</td>\n      <td>-0.585459</td>\n      <td>-0.954593</td>\n      <td>-0.462559</td>\n      <td>-0.505047</td>\n      <td>-0.876003</td>\n      <td>-0.409439</td>\n      <td>1.389443</td>\n      <td>-1.141501</td>\n      <td>-1.187009</td>\n      <td>-0.780925</td>\n      <td>-0.876849</td>\n      <td>-0.868594</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76833</th>\n      <td>-0.208849</td>\n      <td>-0.300522</td>\n      <td>-0.391261</td>\n      <td>0.852059</td>\n      <td>-0.361299</td>\n      <td>1.687970</td>\n      <td>-1.046258</td>\n      <td>-0.672714</td>\n      <td>-0.253326</td>\n      <td>-0.697715</td>\n      <td>-0.685183</td>\n      <td>0.464065</td>\n      <td>-1.147242</td>\n      <td>-0.860877</td>\n      <td>-0.522436</td>\n      <td>-0.591345</td>\n      <td>-0.440970</td>\n      <td>-0.131511</td>\n      <td>0.117335</td>\n      <td>1.076196</td>\n      <td>-0.028161</td>\n      <td>1.083701</td>\n      <td>0.063224</td>\n      <td>-0.259750</td>\n      <td>-1.041207</td>\n      <td>-0.411195</td>\n      <td>-0.267628</td>\n      <td>-0.419104</td>\n      <td>-0.448052</td>\n      <td>-0.286616</td>\n      <td>0.417637</td>\n      <td>-0.447920</td>\n      <td>-0.268217</td>\n      <td>-1.041207</td>\n      <td>-0.449478</td>\n      <td>0.965275</td>\n      <td>-0.818476</td>\n      <td>-0.690079</td>\n      <td>0.571615</td>\n      <td>-0.709454</td>\n      <td>-0.189070</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76849</th>\n      <td>-0.208849</td>\n      <td>1.938339</td>\n      <td>-1.806515</td>\n      <td>-1.257885</td>\n      <td>1.031139</td>\n      <td>-0.502160</td>\n      <td>-0.298017</td>\n      <td>2.201976</td>\n      <td>-0.300283</td>\n      <td>-0.712674</td>\n      <td>-0.693444</td>\n      <td>0.865974</td>\n      <td>-0.488813</td>\n      <td>-0.815114</td>\n      <td>-0.553735</td>\n      <td>-0.551772</td>\n      <td>-0.451074</td>\n      <td>0.746059</td>\n      <td>-0.923249</td>\n      <td>-1.272632</td>\n      <td>0.706670</td>\n      <td>-0.675277</td>\n      <td>0.369738</td>\n      <td>-0.274068</td>\n      <td>-0.462638</td>\n      <td>-0.466384</td>\n      <td>-0.209352</td>\n      <td>-0.783334</td>\n      <td>-0.467451</td>\n      <td>-0.108760</td>\n      <td>0.803386</td>\n      <td>-0.464383</td>\n      <td>0.019749</td>\n      <td>-0.462638</td>\n      <td>-0.443034</td>\n      <td>0.389688</td>\n      <td>-0.734271</td>\n      <td>-0.657172</td>\n      <td>0.494761</td>\n      <td>-0.346837</td>\n      <td>-0.123037</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76889</th>\n      <td>-0.690620</td>\n      <td>0.644627</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.064676</td>\n      <td>-0.604371</td>\n      <td>1.771012</td>\n      <td>-0.826214</td>\n      <td>-0.853646</td>\n      <td>-1.525174</td>\n      <td>-0.806854</td>\n      <td>-1.030321</td>\n      <td>-0.850105</td>\n      <td>-0.520813</td>\n      <td>1.154986</td>\n      <td>-1.151990</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.076504</td>\n      <td>1.782355</td>\n      <td>-0.347738</td>\n      <td>0.065109</td>\n      <td>1.862689</td>\n      <td>-0.838800</td>\n      <td>0.264378</td>\n      <td>1.890670</td>\n      <td>0.509522</td>\n      <td>0.600523</td>\n      <td>2.019325</td>\n      <td>-0.347738</td>\n      <td>2.482332</td>\n      <td>1.956218</td>\n      <td>-0.317879</td>\n      <td>-0.206623</td>\n      <td>-1.289419</td>\n      <td>-0.263619</td>\n      <td>0.330423</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1873 rows  92 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results0","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.137582Z","iopub.status.idle":"2022-09-06T00:36:53.138490Z","shell.execute_reply.started":"2022-09-06T00:36:53.138181Z","shell.execute_reply":"2022-09-06T00:36:53.138210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create validation set \nval_indx = X_train0.remainder__prd == min_prd+windows_width-1\nX_val = X_train0[val_indx]\nX_val.drop(columns='remainder__prd', inplace=True)\ny_val = y_train0[val_indx]\ndisplay(X_val, y_val)\ntrain_indx = X_train0.remainder__prd < min_prd+windows_width-1\nX_train = X_train0[train_indx]\nX_train.drop(columns='remainder__prd', inplace=True)\ny_train = y_train0[train_indx]\ndisplay(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:43:37.704986Z","iopub.execute_input":"2022-09-06T00:43:37.705414Z","iopub.status.idle":"2022-09-06T00:43:37.736895Z","shell.execute_reply.started":"2022-09-06T00:43:37.705381Z","shell.execute_reply":"2022-09-06T00:43:37.735112Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_16/751690025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremainder__prd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmin_prd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindows_width\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_indx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'remainder__prd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_indx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_train0' is not defined"],"ename":"NameError","evalue":"name 'X_train0' is not defined","output_type":"error"}]},{"cell_type":"code","source":"neurons_base = 16\ndropout_rate = 0.05\n# n_b=8 was ok with small overfit.\n# n_b=32 starts clearly overfitting. \n# 128 fits clearly slower than 64 and becomes somewhat unstable. regularization could make it work, but i see no reason to go wider.\n# 64 seems to have nice balance of flexibility and runtime, but its variance may be too large. dropout makes variance vene worse.\n# 6 hidden layers is probably most this architecture can hold\n\n# in this framework the optimal model seems to have width of 16 or 32, somehow regularized. try l1/l2?\n# w32 can take at most 0.03 dropout.\n# w16 looks good w/o dropout.\n\n# more general point:\n# main drawback of dropout is in incresing variance\n# for textbook problems with high s/n ratio (e.g., mnist) this may be ok.\n# for application like this with very low s/n ratio dropout may be a bad idea.\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*32, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train.shape[1:]),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*16, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    Dense(1)])\n\nprint(model_snn.count_params())","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:45:30.311529Z","iopub.execute_input":"2022-09-06T00:45:30.312230Z","iopub.status.idle":"2022-09-06T00:45:30.385460Z","shell.execute_reply.started":"2022-09-06T00:45:30.312193Z","shell.execute_reply":"2022-09-06T00:45:30.384376Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"222721\n","output_type":"stream"}]},{"cell_type":"code","source":"neurons_base = 16\ndropout_rate = 0.05\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*27, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train.shape[1:]),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*9, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*3, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    Dense(1)])\n\nprint(model_snn.count_params())\n\n# similar problem as before: model seems ok in terms of flexibility and variance, but adding dropout breaks it before i can fix overfitting.\n# the solution is to either use smaller models or to use laternative regularizers (which do not increase variance.)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T01:56:18.293248Z","iopub.execute_input":"2022-09-06T01:56:18.293809Z","iopub.status.idle":"2022-09-06T01:56:23.218663Z","shell.execute_reply.started":"2022-09-06T01:56:18.293759Z","shell.execute_reply":"2022-09-06T01:56:23.217686Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2022-09-06 01:56:18.346069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:18.351748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:18.352749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:18.354627: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-09-06 01:56:18.354980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:18.356110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:18.357102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:22.788783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:22.789725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from S","output_type":"stream"},{"name":"stdout","text":"110289\n","output_type":"stream"},{"name":"stderr","text":"ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:22.790430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:22.791027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14879 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping50 = EarlyStopping(patience=50, restore_best_weights=True)\ntime1 = time.time()\noptimizer_adam = tf.keras.optimizers.Adam()\nmodel_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_snn.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=2, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nprint([r2_score(y_train, model_snn.predict(X_train)), \n       r2_score(y_val, model_snn.predict(X_val)),\n       r2_score(y_test, model_snn.predict(X_test))])\nprint(time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T01:56:23.220689Z","iopub.execute_input":"2022-09-06T01:56:23.221370Z","iopub.status.idle":"2022-09-06T01:56:35.380012Z","shell.execute_reply.started":"2022-09-06T01:56:23.221331Z","shell.execute_reply":"2022-09-06T01:56:35.379062Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2022-09-06 01:56:23.446034: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1000\n34/34 - 2s - loss: 203.5653 - mean_squared_error: 203.5653 - val_loss: 134.1529 - val_mean_squared_error: 134.1529\nEpoch 2/1000\n34/34 - 0s - loss: 202.2396 - mean_squared_error: 202.2396 - val_loss: 133.9866 - val_mean_squared_error: 133.9866\nEpoch 3/1000\n34/34 - 0s - loss: 201.9889 - mean_squared_error: 201.9889 - val_loss: 133.8872 - val_mean_squared_error: 133.8872\nEpoch 4/1000\n34/34 - 0s - loss: 201.7041 - mean_squared_error: 201.7041 - val_loss: 133.9403 - val_mean_squared_error: 133.9403\nEpoch 5/1000\n34/34 - 0s - loss: 201.2448 - mean_squared_error: 201.2448 - val_loss: 133.7487 - val_mean_squared_error: 133.7487\nEpoch 6/1000\n34/34 - 0s - loss: 200.6950 - mean_squared_error: 200.6950 - val_loss: 134.1508 - val_mean_squared_error: 134.1508\nEpoch 7/1000\n34/34 - 0s - loss: 200.2834 - mean_squared_error: 200.2834 - val_loss: 134.3009 - val_mean_squared_error: 134.3009\nEpoch 8/1000\n34/34 - 0s - loss: 200.2230 - mean_squared_error: 200.2230 - val_loss: 134.6417 - val_mean_squared_error: 134.6417\nEpoch 9/1000\n34/34 - 0s - loss: 199.7150 - mean_squared_error: 199.7150 - val_loss: 135.4830 - val_mean_squared_error: 135.4830\nEpoch 10/1000\n34/34 - 0s - loss: 199.6890 - mean_squared_error: 199.6890 - val_loss: 134.2659 - val_mean_squared_error: 134.2659\nEpoch 11/1000\n34/34 - 0s - loss: 199.4605 - mean_squared_error: 199.4605 - val_loss: 134.4543 - val_mean_squared_error: 134.4543\nEpoch 12/1000\n34/34 - 0s - loss: 198.7751 - mean_squared_error: 198.7751 - val_loss: 136.4794 - val_mean_squared_error: 136.4794\nEpoch 13/1000\n34/34 - 0s - loss: 198.8705 - mean_squared_error: 198.8705 - val_loss: 135.3572 - val_mean_squared_error: 135.3572\nEpoch 14/1000\n34/34 - 0s - loss: 198.3818 - mean_squared_error: 198.3818 - val_loss: 135.1687 - val_mean_squared_error: 135.1687\nEpoch 15/1000\n34/34 - 0s - loss: 197.8335 - mean_squared_error: 197.8335 - val_loss: 136.0797 - val_mean_squared_error: 136.0797\nEpoch 16/1000\n34/34 - 0s - loss: 197.5053 - mean_squared_error: 197.5053 - val_loss: 135.6482 - val_mean_squared_error: 135.6482\nEpoch 17/1000\n34/34 - 0s - loss: 196.9620 - mean_squared_error: 196.9620 - val_loss: 137.2255 - val_mean_squared_error: 137.2255\nEpoch 18/1000\n34/34 - 0s - loss: 196.6492 - mean_squared_error: 196.6492 - val_loss: 137.1179 - val_mean_squared_error: 137.1179\nEpoch 19/1000\n34/34 - 0s - loss: 196.6281 - mean_squared_error: 196.6281 - val_loss: 136.7719 - val_mean_squared_error: 136.7719\nEpoch 20/1000\n34/34 - 0s - loss: 196.1639 - mean_squared_error: 196.1639 - val_loss: 136.6230 - val_mean_squared_error: 136.6230\nEpoch 21/1000\n34/34 - 0s - loss: 195.6301 - mean_squared_error: 195.6301 - val_loss: 137.6051 - val_mean_squared_error: 137.6051\nEpoch 22/1000\n34/34 - 0s - loss: 195.3080 - mean_squared_error: 195.3080 - val_loss: 137.0344 - val_mean_squared_error: 137.0344\nEpoch 23/1000\n34/34 - 0s - loss: 194.7576 - mean_squared_error: 194.7576 - val_loss: 137.2127 - val_mean_squared_error: 137.2127\nEpoch 24/1000\n34/34 - 0s - loss: 194.3277 - mean_squared_error: 194.3277 - val_loss: 138.1346 - val_mean_squared_error: 138.1346\nEpoch 25/1000\n34/34 - 0s - loss: 194.0926 - mean_squared_error: 194.0926 - val_loss: 139.3853 - val_mean_squared_error: 139.3853\nEpoch 26/1000\n34/34 - 0s - loss: 193.5556 - mean_squared_error: 193.5556 - val_loss: 136.8358 - val_mean_squared_error: 136.8358\nEpoch 27/1000\n34/34 - 0s - loss: 193.3162 - mean_squared_error: 193.3162 - val_loss: 141.0429 - val_mean_squared_error: 141.0429\nEpoch 28/1000\n34/34 - 0s - loss: 192.9642 - mean_squared_error: 192.9642 - val_loss: 139.2817 - val_mean_squared_error: 139.2817\nEpoch 29/1000\n34/34 - 0s - loss: 192.5963 - mean_squared_error: 192.5963 - val_loss: 139.8153 - val_mean_squared_error: 139.8153\nEpoch 30/1000\n34/34 - 0s - loss: 191.9129 - mean_squared_error: 191.9129 - val_loss: 141.0102 - val_mean_squared_error: 141.0102\nEpoch 31/1000\n34/34 - 0s - loss: 192.1046 - mean_squared_error: 192.1046 - val_loss: 138.9361 - val_mean_squared_error: 138.9361\nEpoch 32/1000\n34/34 - 0s - loss: 190.7665 - mean_squared_error: 190.7665 - val_loss: 140.3271 - val_mean_squared_error: 140.3271\nEpoch 33/1000\n34/34 - 0s - loss: 190.8974 - mean_squared_error: 190.8974 - val_loss: 140.6469 - val_mean_squared_error: 140.6469\nEpoch 34/1000\n34/34 - 0s - loss: 190.4319 - mean_squared_error: 190.4319 - val_loss: 140.6880 - val_mean_squared_error: 140.6880\nEpoch 35/1000\n34/34 - 0s - loss: 190.2489 - mean_squared_error: 190.2489 - val_loss: 140.8828 - val_mean_squared_error: 140.8828\nEpoch 36/1000\n34/34 - 0s - loss: 189.7810 - mean_squared_error: 189.7810 - val_loss: 139.9942 - val_mean_squared_error: 139.9942\nEpoch 37/1000\n34/34 - 0s - loss: 189.4464 - mean_squared_error: 189.4464 - val_loss: 142.8586 - val_mean_squared_error: 142.8586\nEpoch 38/1000\n34/34 - 0s - loss: 188.9634 - mean_squared_error: 188.9634 - val_loss: 141.4897 - val_mean_squared_error: 141.4897\nEpoch 39/1000\n34/34 - 0s - loss: 188.0409 - mean_squared_error: 188.0409 - val_loss: 140.2587 - val_mean_squared_error: 140.2587\nEpoch 40/1000\n34/34 - 0s - loss: 187.8784 - mean_squared_error: 187.8784 - val_loss: 141.7381 - val_mean_squared_error: 141.7381\nEpoch 41/1000\n34/34 - 0s - loss: 187.8020 - mean_squared_error: 187.8020 - val_loss: 142.5590 - val_mean_squared_error: 142.5590\nEpoch 42/1000\n34/34 - 0s - loss: 186.9467 - mean_squared_error: 186.9467 - val_loss: 141.5645 - val_mean_squared_error: 141.5645\nEpoch 43/1000\n34/34 - 0s - loss: 186.8272 - mean_squared_error: 186.8272 - val_loss: 139.7713 - val_mean_squared_error: 139.7713\nEpoch 44/1000\n34/34 - 0s - loss: 185.7294 - mean_squared_error: 185.7294 - val_loss: 141.3277 - val_mean_squared_error: 141.3277\nEpoch 45/1000\n34/34 - 0s - loss: 185.4902 - mean_squared_error: 185.4902 - val_loss: 141.0635 - val_mean_squared_error: 141.0635\nEpoch 46/1000\n34/34 - 0s - loss: 185.1499 - mean_squared_error: 185.1499 - val_loss: 139.8159 - val_mean_squared_error: 139.8159\nEpoch 47/1000\n34/34 - 0s - loss: 184.3077 - mean_squared_error: 184.3077 - val_loss: 141.8927 - val_mean_squared_error: 141.8927\nEpoch 48/1000\n34/34 - 0s - loss: 184.3616 - mean_squared_error: 184.3616 - val_loss: 141.0018 - val_mean_squared_error: 141.0018\nEpoch 49/1000\n34/34 - 0s - loss: 184.0969 - mean_squared_error: 184.0969 - val_loss: 140.8256 - val_mean_squared_error: 140.8256\nEpoch 50/1000\n34/34 - 0s - loss: 183.6438 - mean_squared_error: 183.6438 - val_loss: 144.9551 - val_mean_squared_error: 144.9551\nEpoch 51/1000\n34/34 - 0s - loss: 182.8962 - mean_squared_error: 182.8962 - val_loss: 140.7123 - val_mean_squared_error: 140.7123\nEpoch 52/1000\n34/34 - 0s - loss: 182.7739 - mean_squared_error: 182.7739 - val_loss: 144.1363 - val_mean_squared_error: 144.1363\nEpoch 53/1000\n34/34 - 0s - loss: 182.4254 - mean_squared_error: 182.4254 - val_loss: 141.3552 - val_mean_squared_error: 141.3552\nEpoch 54/1000\n34/34 - 0s - loss: 182.3056 - mean_squared_error: 182.3056 - val_loss: 142.5166 - val_mean_squared_error: 142.5166\nEpoch 55/1000\n34/34 - 0s - loss: 180.9704 - mean_squared_error: 180.9704 - val_loss: 142.7140 - val_mean_squared_error: 142.7140\nMinimum Validation Loss: 133.7487\n[0.015923566593638583, 0.010245079730373696, -0.003292317941543388]\n12.024835586547852\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"X_train.skew()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T01:58:14.615470Z","iopub.execute_input":"2022-09-06T01:58:14.616043Z","iopub.status.idle":"2022-09-06T01:58:14.787794Z","shell.execute_reply.started":"2022-09-06T01:58:14.615998Z","shell.execute_reply":"2022-09-06T01:58:14.786906Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"num__mom482       2.014820\nnum__mom242       1.343044\nnum__bm           0.122894\nnum__op          -0.848916\nnum__gp           0.607772\n                   ...    \ncat__ind_45.0    19.090080\ncat__ind_46.0    79.436081\ncat__ind_47.0    12.898892\ncat__ind_48.0    20.318332\ncat__ind_49.0    13.162803\nLength: 92, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"X_val","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.150674Z","iopub.status.idle":"2022-09-06T00:36:53.151429Z","shell.execute_reply.started":"2022-09-06T00:36:53.151166Z","shell.execute_reply":"2022-09-06T00:36:53.151191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# on nns:\n# - try classic regularizers\n# - try differene architecture (not snnn)\n","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Define a problem, describe business context.\n2. Load data, preclean it.\n3. EDA: target, features.\n4. Descibe train/test split strategy. Show main results and discuss them.\n5. Run evth for a few periods. Show feature importance and error analysis.","metadata":{}},{"cell_type":"markdown","source":"### 1. Business problem\n\n#### Objective:\n\n#### Metric:\n\n#### Summary of results:\n","metadata":{}},{"cell_type":"markdown","source":"### 2. Load data and preclean it","metadata":{"execution":{"iopub.status.busy":"2022-09-26T15:43:15.362648Z","iopub.execute_input":"2022-09-26T15:43:15.363102Z","iopub.status.idle":"2022-09-26T15:43:15.385837Z","shell.execute_reply.started":"2022-09-26T15:43:15.363009Z","shell.execute_reply":"2022-09-26T15:43:15.384918Z"}}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle, shap\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, KFold, PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\nfrom optuna.integration import TFKerasPruningCallback\nfrom optuna.trial import TrialState\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\noptuna.logging.set_verbosity(optuna.logging.WARNING)\npd.set_option('display.max_columns', 150)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-09-26T16:00:56.518403Z","iopub.execute_input":"2022-09-26T16:00:56.518762Z","iopub.status.idle":"2022-09-26T16:01:06.917315Z","shell.execute_reply.started":"2022-09-26T16:00:56.518731Z","shell.execute_reply":"2022-09-26T16:01:06.916345Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-09-26T16:01:06.920445Z","iopub.execute_input":"2022-09-26T16:01:06.921409Z","iopub.status.idle":"2022-09-26T16:01:06.931977Z","shell.execute_reply.started":"2022-09-26T16:01:06.921371Z","shell.execute_reply":"2022-09-26T16:01:06.931021Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-09-26T16:01:06.934331Z","iopub.execute_input":"2022-09-26T16:01:06.935001Z","iopub.status.idle":"2022-09-26T16:01:06.964035Z","shell.execute_reply.started":"2022-09-26T16:01:06.934934Z","shell.execute_reply":"2022-09-26T16:01:06.963164Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Functions for Optuna NNs\n\ncv_nn_regularizer = 0.075\n\ndef create_snnn4_model_hyperpars(neurons_base=4, l1_reg_rate=0.1, l2_reg_rate=0.1):\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\ndef create_snnn4_model(trial):\n\n    # hyperparameter space\n    neurons_base = trial.suggest_int(\"neurons_base\", 4, 24, 2)\n    l2_reg_rate = trial.suggest_float('l2_regularizer', 0, 0.9)\n    l1_reg_rate = trial.suggest_float('l1_regularizer', 0, 0.9)\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\n\ndef create_snnn6_model_hyperpars(neurons_base=4, l1_reg_rate=0.1, l2_reg_rate=0.1):\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\ndef create_snnn6_model(trial):\n\n    # hyperparameter space\n    neurons_base = trial.suggest_int(\"neurons_base\", 4, 24, 2)\n    l2_reg_rate = trial.suggest_float('l2_regularizer', 0, 0.9)\n    l1_reg_rate = trial.suggest_float('l1_regularizer', 0, 0.9)\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\n\ndef objective_nn4(trial):\n    \n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_snnn4_model(trial)\n\n        callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=40),\n        TFKerasPruningCallback(trial, \"val_loss\"),\n    ]\n\n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n        history = model.fit(X_train, y_train, \n                                validation_data=(X_val, y_val),\n                                batch_size=2048, \n                                epochs=500, \n                                verbose=0, \n                                callbacks=callbacks)\n\n        # Evaluate the model accuracy on the validation set.\n        loss_train = model.evaluate(X_train, y_train, verbose=0)\n        loss_val = model.evaluate(X_val_extra, y_val_extra, verbose=0)\n        overfit = max(loss_val[1]-loss_train[1], 0)\n        return loss_val[1] + cv_nn_regularizer*overfit\n    \n    \ndef objective_nn6(trial):\n    \n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_snnn6_model(trial)\n\n        callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=40),\n        TFKerasPruningCallback(trial, \"val_loss\"),\n    ]\n\n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n        history = model.fit(X_train, y_train, \n                                validation_data=(X_val, y_val),\n                                batch_size=2048, \n                                epochs=500, \n                                verbose=0, \n                                callbacks=callbacks)\n\n        # Evaluate the model accuracy on the validation set.\n        loss_train = model.evaluate(X_train, y_train, verbose=0)\n        loss_val = model.evaluate(X_val_extra, y_val_extra, verbose=0)\n        overfit = max(loss_val[1]-loss_train[1], 0)\n        return loss_val[1] + cv_nn_regularizer*overfit\n","metadata":{"execution":{"iopub.status.busy":"2022-09-26T16:01:06.966962Z","iopub.execute_input":"2022-09-26T16:01:06.967339Z","iopub.status.idle":"2022-09-26T16:01:06.998779Z","shell.execute_reply.started":"2022-09-26T16:01:06.967300Z","shell.execute_reply":"2022-09-26T16:01:06.997814Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"time0 = time.time()\n\nmin_prd = 500\nwindows_width = 5*12\ncv_xgb_regularizer=0.2\noptuna_xgb_trials = 80\noptuna_nn_trials = 100\n\n\nwith open('../input/mleap-46-preprocessed/MLEAP_46_v7.pkl', 'rb') as pickled_one:\n    df = pickle.load(pickled_one)\ndf = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+10))]\ndf_cnt = df.count()\nempty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\ndf.drop(columns=empty_cols, inplace=True)\n#display(df.shape, df.head(), df.year.describe(), df.count())\n\nfeatures_miss_dummies = ['amhd', 'BAspr']\nfor col in features_miss_dummies:\n    if col in df.columns:\n        df[col+'_miss'] = df[col].isnull().astype(int)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-26T16:01:07.000074Z","iopub.execute_input":"2022-09-26T16:01:07.000946Z","iopub.status.idle":"2022-09-26T16:01:11.410866Z","shell.execute_reply.started":"2022-09-26T16:01:07.000909Z","shell.execute_reply":"2022-09-26T16:01:11.409922Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### 3. EDA","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Train/test split strategy","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Main results","metadata":{}},{"cell_type":"code","source":"results_df = pd.read_csv('../input/mleap-v49-results/temp_models_reg005_1.csv')\nresults_df","metadata":{"execution":{"iopub.status.busy":"2022-09-26T16:01:11.412426Z","iopub.execute_input":"2022-09-26T16:01:11.412774Z","iopub.status.idle":"2022-09-26T16:01:11.457693Z","shell.execute_reply.started":"2022-09-26T16:01:11.412740Z","shell.execute_reply":"2022-09-26T16:01:11.456829Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"    Unnamed: 0  min_prd  xgbf_train  xgbf_val  xgbf_test  xgbgs_train  \\\n0            0      100    0.110633 -0.010679   0.005897     0.032282   \n1            1      125    0.066434  0.004399   0.015985     0.040227   \n2            2      150    0.064235  0.028400  -0.007106     0.108114   \n3            3      175    0.073734  0.019790  -0.011655     0.093350   \n4            4      200    0.056176  0.048308   0.034120     0.111365   \n5            5      225    0.064410 -0.005404   0.012234     0.083176   \n6            6      250    0.061997 -0.017393   0.027061     0.018820   \n7            7      275    0.084432  0.128375   0.097800     0.109516   \n8            8      300    0.109596  0.041026   0.083551     0.097602   \n9            9      325    0.111924  0.042263  -0.038793     0.133787   \n10          10      350    0.064108  0.020667   0.031485     0.055273   \n11          11      375    0.045452  0.020881   0.013222     0.063410   \n12          12      400    0.041836  0.023006   0.023431     0.041640   \n13          13      425    0.043315  0.023428   0.021350     0.042503   \n14          14      450    0.044173  0.004679   0.008707     0.057491   \n15          15      475    0.042778  0.002243   0.002169     0.038150   \n16          16      500    0.042942  0.039946   0.045640     0.092373   \n17          17      525    0.055984  0.008138   0.037555     0.068378   \n18          18      550    0.079068 -0.007977  -0.002293     0.036324   \n19          19      575    0.061489  0.046520   0.011930     0.084962   \n20          20      600    0.053373 -0.018035  -0.004766     0.014890   \n21          21      625         NaN       NaN        NaN          NaN   \n22          22      650         NaN       NaN        NaN          NaN   \n\n    xgbgs_val  xgbgs_test  xgbo_train  xgbo_val  xgbo_test  nn4_train  \\\n0    0.009775    0.016553    0.142414  0.097214   0.017316   0.092751   \n1    0.053275    0.020740    0.081517  0.077877   0.018348   0.020937   \n2    0.136225   -0.040268    0.102913  0.141299  -0.040073   0.039123   \n3    0.065665    0.000453    0.017994  0.013669  -0.002192   0.057370   \n4    0.141292    0.040536    0.057042  0.089245   0.040346   0.041166   \n5    0.067360    0.026466    0.070134  0.033220   0.020088   0.055780   \n6   -0.001580    0.013451    0.189037  0.155622   0.031786   0.048662   \n7    0.192547    0.108320    0.102921  0.176972   0.108397   0.073269   \n8    0.058866    0.086605    0.098751  0.054814   0.084547   0.098849   \n9    0.091224   -0.039374    0.112554  0.067039  -0.034580   0.106888   \n10   0.033387    0.033121    0.059194  0.035949   0.034516   0.058240   \n11   0.054559    0.020181    0.059622  0.051217   0.016368   0.035642   \n12   0.041122    0.028316    0.033655  0.031300   0.027806   0.029551   \n13   0.047076    0.017911    0.034322  0.037631   0.016773   0.031853   \n14   0.063234    0.021913    0.031195  0.029232   0.012268   0.034023   \n15   0.026061    0.009934    0.022383  0.010773   0.004530   0.026574   \n16   0.106835    0.065450    0.056338  0.078596   0.058214   0.033054   \n17   0.034044    0.045484    0.049435  0.033478   0.042042   0.038853   \n18   0.007323    0.014142    0.059759  0.017997   0.009193   0.065788   \n19   0.089735    0.022645    0.049855  0.059204   0.020475   0.049639   \n20  -0.000217    0.005112    0.036381 -0.004054   0.002889   0.011194   \n21        NaN         NaN         NaN       NaN        NaN        NaN   \n22        NaN         NaN         NaN       NaN        NaN        NaN   \n\n     nn4_val  nn4_test  nn6_train   nn6_val  nn6_test  nn4opt_train  \\\n0  -0.004731  0.004806   0.077033  0.003310  0.007623  2.368052e-02   \n1   0.023310  0.014654   0.039157  0.018810  0.009496 -1.010410e-06   \n2   0.041242 -0.044962   0.046364  0.044968 -0.055987  4.366676e-02   \n3   0.021523 -0.006702   0.064954  0.017231 -0.010337 -4.252701e-06   \n4   0.071331  0.022324   0.041645  0.073893  0.025533  3.709691e-02   \n5   0.008508  0.012525   0.052652  0.008472  0.013793 -5.871936e-07   \n6  -0.007640  0.024796   0.048832 -0.016709 -0.007211 -1.719546e-09   \n7   0.144908  0.110993   0.070322  0.143302  0.112889  7.233643e-02   \n8   0.040880  0.086885   0.095496  0.042473  0.086713  1.023098e-01   \n9   0.049632 -0.052411   0.104488  0.049346 -0.055748  9.684091e-02   \n10  0.030566  0.022642   0.055391  0.030115  0.026270  5.876430e-02   \n11  0.024271  0.005288   0.034158  0.025277  0.002064  3.371769e-02   \n12  0.025455  0.034004   0.025434  0.024934  0.036392  2.552223e-02   \n13  0.030089  0.025648   0.029592  0.030158  0.021560  3.133441e-02   \n14  0.031850  0.015509   0.035800  0.034246  0.007337 -1.435706e-07   \n15  0.011256  0.011812   0.033069  0.012727 -0.002586 -3.848525e-07   \n16  0.044643  0.043081   0.035159  0.043644  0.044818  2.918796e-02   \n17  0.012892  0.031937   0.042841  0.010648  0.029456  3.686924e-02   \n18  0.005629  0.016282   0.063776  0.008980  0.021159 -9.343118e-07   \n19  0.055342  0.008340   0.045969  0.055721  0.019505  3.947695e-02   \n20 -0.003839  0.004517   0.031915 -0.025365 -0.014393 -1.116158e-07   \n21       NaN       NaN        NaN       NaN       NaN           NaN   \n22       NaN       NaN        NaN       NaN       NaN           NaN   \n\n    nn4opt_val   nn4opt_test  nn6opt_train    nn6opt_val  nn6opt_test  \n0    -0.006378  4.413288e-03  2.373109e-02 -1.185371e-03     0.006157  \n1    -0.000038 -1.945688e-05 -7.328776e-07 -3.910448e-05    -0.000020  \n2     0.040878 -4.501329e-02  5.041909e-02  4.294926e-02    -0.043427  \n3    -0.000010 -4.954723e-07 -7.191610e-09 -6.917556e-07    -0.000008  \n4     0.065155  2.521314e-02  3.844909e-02  6.439698e-02     0.024244  \n5    -0.000169 -1.014969e-04 -5.182621e-06 -1.285523e-04    -0.000073  \n6    -0.000007 -2.775066e-06 -8.389908e-07 -2.850800e-06    -0.000007  \n7     0.144175  1.100276e-01  7.211387e-02  1.423667e-01     0.106699  \n8     0.039196  8.664547e-02  9.784490e-02  3.844309e-02     0.087651  \n9     0.041965 -6.858033e-02  1.009017e-01  4.594199e-02    -0.066538  \n10    0.030567  2.703620e-02  5.610774e-02  2.959755e-02     0.028619  \n11    0.022140  5.191548e-03  3.490645e-02  2.254672e-02    -0.003187  \n12    0.021759  3.121626e-02  3.332331e-02  2.432268e-02     0.030985  \n13    0.028021  2.200129e-02  2.661165e-02  3.152557e-02     0.012151  \n14   -0.000024 -1.837915e-05 -1.779765e-08 -2.865202e-05    -0.000022  \n15   -0.000045 -3.566047e-05 -3.725332e-07 -4.539190e-05    -0.000036  \n16    0.039344  4.251399e-02  5.253141e-02  5.219970e-02     0.037235  \n17    0.013139  3.232509e-02 -1.155932e-06 -8.184362e-05    -0.000014  \n18   -0.000007 -1.136215e-05 -8.872024e-08 -9.620223e-06    -0.000015  \n19    0.050070  1.513632e-02  5.294881e-02  5.275299e-02     0.013744  \n20   -0.000010 -2.508825e-05 -3.862022e-12 -1.229114e-05    -0.000029  \n21         NaN           NaN           NaN           NaN          NaN  \n22         NaN           NaN           NaN           NaN          NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n      <th>nn4_train</th>\n      <th>nn4_val</th>\n      <th>nn4_test</th>\n      <th>nn6_train</th>\n      <th>nn6_val</th>\n      <th>nn6_test</th>\n      <th>nn4opt_train</th>\n      <th>nn4opt_val</th>\n      <th>nn4opt_test</th>\n      <th>nn6opt_train</th>\n      <th>nn6opt_val</th>\n      <th>nn6opt_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>100</td>\n      <td>0.110633</td>\n      <td>-0.010679</td>\n      <td>0.005897</td>\n      <td>0.032282</td>\n      <td>0.009775</td>\n      <td>0.016553</td>\n      <td>0.142414</td>\n      <td>0.097214</td>\n      <td>0.017316</td>\n      <td>0.092751</td>\n      <td>-0.004731</td>\n      <td>0.004806</td>\n      <td>0.077033</td>\n      <td>0.003310</td>\n      <td>0.007623</td>\n      <td>2.368052e-02</td>\n      <td>-0.006378</td>\n      <td>4.413288e-03</td>\n      <td>2.373109e-02</td>\n      <td>-1.185371e-03</td>\n      <td>0.006157</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>125</td>\n      <td>0.066434</td>\n      <td>0.004399</td>\n      <td>0.015985</td>\n      <td>0.040227</td>\n      <td>0.053275</td>\n      <td>0.020740</td>\n      <td>0.081517</td>\n      <td>0.077877</td>\n      <td>0.018348</td>\n      <td>0.020937</td>\n      <td>0.023310</td>\n      <td>0.014654</td>\n      <td>0.039157</td>\n      <td>0.018810</td>\n      <td>0.009496</td>\n      <td>-1.010410e-06</td>\n      <td>-0.000038</td>\n      <td>-1.945688e-05</td>\n      <td>-7.328776e-07</td>\n      <td>-3.910448e-05</td>\n      <td>-0.000020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>150</td>\n      <td>0.064235</td>\n      <td>0.028400</td>\n      <td>-0.007106</td>\n      <td>0.108114</td>\n      <td>0.136225</td>\n      <td>-0.040268</td>\n      <td>0.102913</td>\n      <td>0.141299</td>\n      <td>-0.040073</td>\n      <td>0.039123</td>\n      <td>0.041242</td>\n      <td>-0.044962</td>\n      <td>0.046364</td>\n      <td>0.044968</td>\n      <td>-0.055987</td>\n      <td>4.366676e-02</td>\n      <td>0.040878</td>\n      <td>-4.501329e-02</td>\n      <td>5.041909e-02</td>\n      <td>4.294926e-02</td>\n      <td>-0.043427</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>175</td>\n      <td>0.073734</td>\n      <td>0.019790</td>\n      <td>-0.011655</td>\n      <td>0.093350</td>\n      <td>0.065665</td>\n      <td>0.000453</td>\n      <td>0.017994</td>\n      <td>0.013669</td>\n      <td>-0.002192</td>\n      <td>0.057370</td>\n      <td>0.021523</td>\n      <td>-0.006702</td>\n      <td>0.064954</td>\n      <td>0.017231</td>\n      <td>-0.010337</td>\n      <td>-4.252701e-06</td>\n      <td>-0.000010</td>\n      <td>-4.954723e-07</td>\n      <td>-7.191610e-09</td>\n      <td>-6.917556e-07</td>\n      <td>-0.000008</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>200</td>\n      <td>0.056176</td>\n      <td>0.048308</td>\n      <td>0.034120</td>\n      <td>0.111365</td>\n      <td>0.141292</td>\n      <td>0.040536</td>\n      <td>0.057042</td>\n      <td>0.089245</td>\n      <td>0.040346</td>\n      <td>0.041166</td>\n      <td>0.071331</td>\n      <td>0.022324</td>\n      <td>0.041645</td>\n      <td>0.073893</td>\n      <td>0.025533</td>\n      <td>3.709691e-02</td>\n      <td>0.065155</td>\n      <td>2.521314e-02</td>\n      <td>3.844909e-02</td>\n      <td>6.439698e-02</td>\n      <td>0.024244</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>225</td>\n      <td>0.064410</td>\n      <td>-0.005404</td>\n      <td>0.012234</td>\n      <td>0.083176</td>\n      <td>0.067360</td>\n      <td>0.026466</td>\n      <td>0.070134</td>\n      <td>0.033220</td>\n      <td>0.020088</td>\n      <td>0.055780</td>\n      <td>0.008508</td>\n      <td>0.012525</td>\n      <td>0.052652</td>\n      <td>0.008472</td>\n      <td>0.013793</td>\n      <td>-5.871936e-07</td>\n      <td>-0.000169</td>\n      <td>-1.014969e-04</td>\n      <td>-5.182621e-06</td>\n      <td>-1.285523e-04</td>\n      <td>-0.000073</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>250</td>\n      <td>0.061997</td>\n      <td>-0.017393</td>\n      <td>0.027061</td>\n      <td>0.018820</td>\n      <td>-0.001580</td>\n      <td>0.013451</td>\n      <td>0.189037</td>\n      <td>0.155622</td>\n      <td>0.031786</td>\n      <td>0.048662</td>\n      <td>-0.007640</td>\n      <td>0.024796</td>\n      <td>0.048832</td>\n      <td>-0.016709</td>\n      <td>-0.007211</td>\n      <td>-1.719546e-09</td>\n      <td>-0.000007</td>\n      <td>-2.775066e-06</td>\n      <td>-8.389908e-07</td>\n      <td>-2.850800e-06</td>\n      <td>-0.000007</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>275</td>\n      <td>0.084432</td>\n      <td>0.128375</td>\n      <td>0.097800</td>\n      <td>0.109516</td>\n      <td>0.192547</td>\n      <td>0.108320</td>\n      <td>0.102921</td>\n      <td>0.176972</td>\n      <td>0.108397</td>\n      <td>0.073269</td>\n      <td>0.144908</td>\n      <td>0.110993</td>\n      <td>0.070322</td>\n      <td>0.143302</td>\n      <td>0.112889</td>\n      <td>7.233643e-02</td>\n      <td>0.144175</td>\n      <td>1.100276e-01</td>\n      <td>7.211387e-02</td>\n      <td>1.423667e-01</td>\n      <td>0.106699</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>300</td>\n      <td>0.109596</td>\n      <td>0.041026</td>\n      <td>0.083551</td>\n      <td>0.097602</td>\n      <td>0.058866</td>\n      <td>0.086605</td>\n      <td>0.098751</td>\n      <td>0.054814</td>\n      <td>0.084547</td>\n      <td>0.098849</td>\n      <td>0.040880</td>\n      <td>0.086885</td>\n      <td>0.095496</td>\n      <td>0.042473</td>\n      <td>0.086713</td>\n      <td>1.023098e-01</td>\n      <td>0.039196</td>\n      <td>8.664547e-02</td>\n      <td>9.784490e-02</td>\n      <td>3.844309e-02</td>\n      <td>0.087651</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>325</td>\n      <td>0.111924</td>\n      <td>0.042263</td>\n      <td>-0.038793</td>\n      <td>0.133787</td>\n      <td>0.091224</td>\n      <td>-0.039374</td>\n      <td>0.112554</td>\n      <td>0.067039</td>\n      <td>-0.034580</td>\n      <td>0.106888</td>\n      <td>0.049632</td>\n      <td>-0.052411</td>\n      <td>0.104488</td>\n      <td>0.049346</td>\n      <td>-0.055748</td>\n      <td>9.684091e-02</td>\n      <td>0.041965</td>\n      <td>-6.858033e-02</td>\n      <td>1.009017e-01</td>\n      <td>4.594199e-02</td>\n      <td>-0.066538</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>350</td>\n      <td>0.064108</td>\n      <td>0.020667</td>\n      <td>0.031485</td>\n      <td>0.055273</td>\n      <td>0.033387</td>\n      <td>0.033121</td>\n      <td>0.059194</td>\n      <td>0.035949</td>\n      <td>0.034516</td>\n      <td>0.058240</td>\n      <td>0.030566</td>\n      <td>0.022642</td>\n      <td>0.055391</td>\n      <td>0.030115</td>\n      <td>0.026270</td>\n      <td>5.876430e-02</td>\n      <td>0.030567</td>\n      <td>2.703620e-02</td>\n      <td>5.610774e-02</td>\n      <td>2.959755e-02</td>\n      <td>0.028619</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>375</td>\n      <td>0.045452</td>\n      <td>0.020881</td>\n      <td>0.013222</td>\n      <td>0.063410</td>\n      <td>0.054559</td>\n      <td>0.020181</td>\n      <td>0.059622</td>\n      <td>0.051217</td>\n      <td>0.016368</td>\n      <td>0.035642</td>\n      <td>0.024271</td>\n      <td>0.005288</td>\n      <td>0.034158</td>\n      <td>0.025277</td>\n      <td>0.002064</td>\n      <td>3.371769e-02</td>\n      <td>0.022140</td>\n      <td>5.191548e-03</td>\n      <td>3.490645e-02</td>\n      <td>2.254672e-02</td>\n      <td>-0.003187</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>400</td>\n      <td>0.041836</td>\n      <td>0.023006</td>\n      <td>0.023431</td>\n      <td>0.041640</td>\n      <td>0.041122</td>\n      <td>0.028316</td>\n      <td>0.033655</td>\n      <td>0.031300</td>\n      <td>0.027806</td>\n      <td>0.029551</td>\n      <td>0.025455</td>\n      <td>0.034004</td>\n      <td>0.025434</td>\n      <td>0.024934</td>\n      <td>0.036392</td>\n      <td>2.552223e-02</td>\n      <td>0.021759</td>\n      <td>3.121626e-02</td>\n      <td>3.332331e-02</td>\n      <td>2.432268e-02</td>\n      <td>0.030985</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>425</td>\n      <td>0.043315</td>\n      <td>0.023428</td>\n      <td>0.021350</td>\n      <td>0.042503</td>\n      <td>0.047076</td>\n      <td>0.017911</td>\n      <td>0.034322</td>\n      <td>0.037631</td>\n      <td>0.016773</td>\n      <td>0.031853</td>\n      <td>0.030089</td>\n      <td>0.025648</td>\n      <td>0.029592</td>\n      <td>0.030158</td>\n      <td>0.021560</td>\n      <td>3.133441e-02</td>\n      <td>0.028021</td>\n      <td>2.200129e-02</td>\n      <td>2.661165e-02</td>\n      <td>3.152557e-02</td>\n      <td>0.012151</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>450</td>\n      <td>0.044173</td>\n      <td>0.004679</td>\n      <td>0.008707</td>\n      <td>0.057491</td>\n      <td>0.063234</td>\n      <td>0.021913</td>\n      <td>0.031195</td>\n      <td>0.029232</td>\n      <td>0.012268</td>\n      <td>0.034023</td>\n      <td>0.031850</td>\n      <td>0.015509</td>\n      <td>0.035800</td>\n      <td>0.034246</td>\n      <td>0.007337</td>\n      <td>-1.435706e-07</td>\n      <td>-0.000024</td>\n      <td>-1.837915e-05</td>\n      <td>-1.779765e-08</td>\n      <td>-2.865202e-05</td>\n      <td>-0.000022</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>475</td>\n      <td>0.042778</td>\n      <td>0.002243</td>\n      <td>0.002169</td>\n      <td>0.038150</td>\n      <td>0.026061</td>\n      <td>0.009934</td>\n      <td>0.022383</td>\n      <td>0.010773</td>\n      <td>0.004530</td>\n      <td>0.026574</td>\n      <td>0.011256</td>\n      <td>0.011812</td>\n      <td>0.033069</td>\n      <td>0.012727</td>\n      <td>-0.002586</td>\n      <td>-3.848525e-07</td>\n      <td>-0.000045</td>\n      <td>-3.566047e-05</td>\n      <td>-3.725332e-07</td>\n      <td>-4.539190e-05</td>\n      <td>-0.000036</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>500</td>\n      <td>0.042942</td>\n      <td>0.039946</td>\n      <td>0.045640</td>\n      <td>0.092373</td>\n      <td>0.106835</td>\n      <td>0.065450</td>\n      <td>0.056338</td>\n      <td>0.078596</td>\n      <td>0.058214</td>\n      <td>0.033054</td>\n      <td>0.044643</td>\n      <td>0.043081</td>\n      <td>0.035159</td>\n      <td>0.043644</td>\n      <td>0.044818</td>\n      <td>2.918796e-02</td>\n      <td>0.039344</td>\n      <td>4.251399e-02</td>\n      <td>5.253141e-02</td>\n      <td>5.219970e-02</td>\n      <td>0.037235</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>525</td>\n      <td>0.055984</td>\n      <td>0.008138</td>\n      <td>0.037555</td>\n      <td>0.068378</td>\n      <td>0.034044</td>\n      <td>0.045484</td>\n      <td>0.049435</td>\n      <td>0.033478</td>\n      <td>0.042042</td>\n      <td>0.038853</td>\n      <td>0.012892</td>\n      <td>0.031937</td>\n      <td>0.042841</td>\n      <td>0.010648</td>\n      <td>0.029456</td>\n      <td>3.686924e-02</td>\n      <td>0.013139</td>\n      <td>3.232509e-02</td>\n      <td>-1.155932e-06</td>\n      <td>-8.184362e-05</td>\n      <td>-0.000014</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>550</td>\n      <td>0.079068</td>\n      <td>-0.007977</td>\n      <td>-0.002293</td>\n      <td>0.036324</td>\n      <td>0.007323</td>\n      <td>0.014142</td>\n      <td>0.059759</td>\n      <td>0.017997</td>\n      <td>0.009193</td>\n      <td>0.065788</td>\n      <td>0.005629</td>\n      <td>0.016282</td>\n      <td>0.063776</td>\n      <td>0.008980</td>\n      <td>0.021159</td>\n      <td>-9.343118e-07</td>\n      <td>-0.000007</td>\n      <td>-1.136215e-05</td>\n      <td>-8.872024e-08</td>\n      <td>-9.620223e-06</td>\n      <td>-0.000015</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>575</td>\n      <td>0.061489</td>\n      <td>0.046520</td>\n      <td>0.011930</td>\n      <td>0.084962</td>\n      <td>0.089735</td>\n      <td>0.022645</td>\n      <td>0.049855</td>\n      <td>0.059204</td>\n      <td>0.020475</td>\n      <td>0.049639</td>\n      <td>0.055342</td>\n      <td>0.008340</td>\n      <td>0.045969</td>\n      <td>0.055721</td>\n      <td>0.019505</td>\n      <td>3.947695e-02</td>\n      <td>0.050070</td>\n      <td>1.513632e-02</td>\n      <td>5.294881e-02</td>\n      <td>5.275299e-02</td>\n      <td>0.013744</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>600</td>\n      <td>0.053373</td>\n      <td>-0.018035</td>\n      <td>-0.004766</td>\n      <td>0.014890</td>\n      <td>-0.000217</td>\n      <td>0.005112</td>\n      <td>0.036381</td>\n      <td>-0.004054</td>\n      <td>0.002889</td>\n      <td>0.011194</td>\n      <td>-0.003839</td>\n      <td>0.004517</td>\n      <td>0.031915</td>\n      <td>-0.025365</td>\n      <td>-0.014393</td>\n      <td>-1.116158e-07</td>\n      <td>-0.000010</td>\n      <td>-2.508825e-05</td>\n      <td>-3.862022e-12</td>\n      <td>-1.229114e-05</td>\n      <td>-0.000029</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>625</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Fit the model(s) for one window and explore results","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntemp_cols = ['PERMNO', 'year', 'prd']\ndf.reset_index(inplace=True, drop=True)\nX = df.copy()\ny = X.pop('RET')\n\ntrain_indx = X.prd<(min_prd+windows_width-1)\nval_indx = X['prd'].isin(range(min_prd+windows_width-1, min_prd+windows_width+2))\nval_indx_extra = X['prd'].isin(range(min_prd+windows_width+5, min_prd+windows_width+8))\ntest_indx = X['prd'].isin(range(min_prd+windows_width+2, min_prd+windows_width+5))\n\nX_train = X[train_indx]\nX_val = X[val_indx]\nX_val_extra = X[val_indx_extra]\nX_test = X[test_indx]\ny_train = y[train_indx]\ny_val = y[val_indx]\ny_val_extra = y[val_indx_extra]\ny_test = y[test_indx]\n\n#display(X_train.head(3), X_train.tail(3), y_train.head(3), y_train.tail(3))\n#display(X_train.prd.describe(), X_val.prd.describe(), X_test.prd.describe())\ndisplay(X_train.shape, X_val.shape, X_test.shape)\n\nX_train.drop(columns=temp_cols, inplace=True)\nX_val.drop(columns=temp_cols, inplace=True)\nX_val_extra.drop(columns=temp_cols, inplace=True)\nX_test.drop(columns=temp_cols, inplace=True)\n\n#display(X_train.tail())\ncol_cat = ['ind']\ncol_num = [x for x in X_train.columns if x not in col_cat]\nfor col in col_num:\n    X_train[col] = X_train[col].fillna(X_train[col].median())\n    X_val[col] = X_val[col].fillna(X_train[col].median())\n    X_val_extra[col] = X_val_extra[col].fillna(X_train[col].median())\n    X_test[col] = X_test[col].fillna(X_train[col].median())\nfor col in col_cat:\n    X_train[col] = X_train[col].fillna(value=-1000)\n    X_val[col] = X_val[col].fillna(value=-1000)\n    X_val_extra[col] = X_val_extra[col].fillna(value=-1000)\n    X_test[col] = X_test[col].fillna(value=-1000)\n\n#display(X_train.tail())\nfeature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                        (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                        remainder=\"passthrough\")\n\nprint('Number of features before transformation: ', X_train.shape)\ntrain_index, val_index, val_index_extra, test_index = X_train.index, X_val.index, X_val_extra.index, X_test.index\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_val = pd.DataFrame(feature_transformer.transform(X_val), columns=feature_transformer.get_feature_names_out())\nX_val_extra = pd.DataFrame(feature_transformer.transform(X_val_extra), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ')\nprint('Number of features after transformation: ', X_train.shape, X_val.shape, X_val_extra.shape, X_test.shape)\nX_train.index = train_index\nX_val.index = val_index\nX_val_extra.index = val_index_extra\nX_test.index = test_index\n#display(X_train.tail())\n\nX = pd.concat([X_train, X_val])\ny = pd.concat([y_train, y_val])\n#display(X,y)\n\nX_ = pd.concat([X_train, X_val, X_val_extra])\ny_ = pd.concat([y_train, y_val, y_val_extra])\n#display(X,y, X_,y_)\n\n\nresults = pd.DataFrame(columns = ['min_prd', 'xgbf_train', 'xgbf_val', 'xgbf_test', \n                                  'xgbgs_train', 'xgbgs_val', 'xgbgs_test', \n                                  'xgbo_train', 'xgbo_val', 'xgbo_test',\n                                  'nn4_train', 'nn4_val', 'nn4_test',\n                                 'nn6_train', 'nn6_val', 'nn6_test',\n                                 'nn4opt_train', 'nn4opt_val', 'nn4opt_test',\n                                 'nn6opt_train', 'nn6opt_val', 'nn6opt_test'])\n\n\n\n### Modeling part ###\n\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=400, max_depth=4, eta=0.02, colsample_bytree=0.4, subsample=0.6)\nxgb1.fit(X_train, y_train)\nprint('fixed XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\nprint('XGB val:', mean_absolute_error(y_val, xgb1.predict(X_val)), r2_score(y_val, xgb1.predict(X_val)))\nprint('XGB val extra:', mean_absolute_error(y_val_extra, xgb1.predict(X_val_extra)), r2_score(y_val_extra, xgb1.predict(X_val_extra)))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbf_train':'xgbf_test'] = \\\n[r2_score(y_train, xgb1.predict(X_train)), \nr2_score(y_val, xgb1.predict(X_val)),\nr2_score(y_test, xgb1.predict(X_test))]\n\ntime1 = time.time()\n\n# Create a list where train data indices are -1 and validation data indices are 0\nsplit_index = [-1 if x in X_train.index else 0 for x in X.index]\npds = PredefinedSplit(test_fold = split_index)\n\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[400, 600, 800], 'max_depth':[2,3,4,5], 'eta':[0.006, 0.012, 0.02], \n              'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbgs = GridSearchCV(estimator = xgb, cv=pds, param_grid=param_grid)\n\n# Fit with all data\nxgbgs.fit(X_, y_)\n\nprint('gs XGB', xgbgs.best_params_, xgbgs.best_score_, time.time()-time1)\nprint('XGB train:', mean_absolute_error(y_train, xgbgs.predict(X_train)), r2_score(y_train, xgbgs.predict(X_train)))\nprint('XGB validation:', mean_absolute_error(y_val, xgbgs.predict(X_val)), r2_score(y_val, xgbgs.predict(X_val)))\nprint('XGB validation extra:', mean_absolute_error(y_val_extra, xgbgs.predict(X_val_extra)), r2_score(y_val_extra, xgbgs.predict(X_val_extra)))\nprint('XGB test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_score(y_test, xgbgs.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbgs_train':'xgbgs_test'] = \\\n[r2_score(y_train, xgbgs.predict(X_train)), \nr2_score(y_val, xgbgs.predict(X_val)),\nr2_score(y_test, xgbgs.predict(X_test))]\n\ntime1 = time.time()\ndef objective_xgb(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    params = {\n    \"tree_method\": 'gpu_hist',\n    \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n    \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 1500),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n    \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.0005, 0.03),\n    \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.05, 0.95),\n    \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 0.95),\n    \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 50.0),\n    \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 500.0),\n    \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 100.0),\n    \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n    model = XGBRegressor(**params, njobs=-1)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose = False)\n\n    score_train = r2_score(y_train, model.predict(X_train))\n    score_val = r2_score(y_val, model.predict(X_val))\n    score_val_extra = r2_score(y_val_extra, model.predict(X_val_extra)) \n    score_val = (score_val+score_val_extra)/2\n    overfit = np.abs(score_train-score_val)\n\n    return score_val-cv_xgb_regularizer*overfit\n\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective_xgb, n_trials=optuna_xgb_trials)\nprint('Total time for hypermarameter optimization, XGB: ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X, y)\nprint('Optuna XGB train: \\n', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), '\\nvalidation \\n',\n      mean_absolute_error(y_val, optuna_xgb.predict(X_val)), r2_score(y_val, optuna_xgb.predict(X_val)),\n      mean_absolute_error(y_val_extra, optuna_xgb.predict(X_val_extra)), r2_score(y_val_extra, optuna_xgb.predict(X_val_extra)), '\\ntest \\n',\n      mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbo_train':'xgbo_test'] = \\\n[r2_score(y_train, optuna_xgb.predict(X_train)), \nr2_score(y_val, optuna_xgb.predict(X_val)),\nr2_score(y_test, optuna_xgb.predict(X_test))]\n\n###########\n### NNs ###\n###########\n\nneurons_base = 8\nl2_reg_rate = 0.5\n\nmodel_snn6 = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn6.count_params())\n\nearly_stopping50 = EarlyStopping(patience=50, restore_best_weights=True)\noptimizer_adam = tf.keras.optimizers.Adam()\n\nmodel_snn6.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n\ntime1 = time.time()\nhistory = model_snn6.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=0, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nresults.loc[results.min_prd==min_prd,'nn6_train':'nn6_test'] = \\\n[r2_score(y_train, model_snn6.predict(X_train)), \nr2_score(y_val, model_snn6.predict(X_val)),\nr2_score(y_test, model_snn6.predict(X_test))]\n\n\n\nneurons_base = 8\nl2_reg_rate = 0.3\n\nmodel_snn4 = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn4.count_params())\n\ntime1 = time.time()\nmodel_snn4.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_snn4.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=0, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nresults.loc[results.min_prd==min_prd,'nn4_train':'nn4_test'] = \\\n[r2_score(y_train, model_snn4.predict(X_train)), \nr2_score(y_val, model_snn4.predict(X_val)),\nr2_score(y_test, model_snn4.predict(X_test))]\n\n\n\n# try optuna, using this kaggle notebook: https://www.kaggle.com/code/mistag/keras-model-tuning-with-optuna\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"minimize\", \n                            sampler=optuna.samplers.TPESampler(), \n                            pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective_nn4, n_trials=optuna_nn_trials)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\ntemp = study.best_params\ndisplay('Optuna NN4', study.best_params, time.time()-time1)\n\noptimal_hyperpars = list(temp.values())\ndisplay(optimal_hyperpars)\nprint('Time for hyperparameter optimization: ',time.time()-time1, optimal_hyperpars)\n\noptuna_nn = create_snnn4_model_hyperpars(neurons_base=optimal_hyperpars[0], \n                                         l2_reg_rate=optimal_hyperpars[1],\n                                        l1_reg_rate=optimal_hyperpars[2])\nhistory = optuna_nn.fit(X_train, y_train, \n                        validation_data=(X_val, y_val),\n                        batch_size=2048, \n                        epochs=1000,\n                        verbose=0, \n                        callbacks=[early_stopping50])\n\nresults.loc[results.min_prd==min_prd,'nn4opt_train':'nn4opt_test'] = \\\n[r2_score(y_train, optuna_nn.predict(X_train)), \nr2_score(y_val, optuna_nn.predict(X_val)),\nr2_score(y_test, optuna_nn.predict(X_test))]\n\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"minimize\", \n                            sampler=optuna.samplers.TPESampler(), \n                            pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective_nn6, n_trials=optuna_nn_trials)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\ntemp = study.best_params\ndisplay('Optuna NN6', study.best_params, time.time()-time1)\n\noptimal_hyperpars = list(temp.values())\ndisplay(optimal_hyperpars)\nprint('Time for hyperparameter optimization: ', time.time()-time1, optimal_hyperpars)\n\noptuna_nn = create_snnn6_model_hyperpars(neurons_base=optimal_hyperpars[0], \n                                         l2_reg_rate=optimal_hyperpars[1],\n                                        l1_reg_rate=optimal_hyperpars[2])\nhistory = optuna_nn.fit(X_train, y_train, \n                        validation_data=(X_val, y_val),\n                        batch_size=2048, \n                        epochs=1000,\n                        verbose=0, \n                        callbacks=[early_stopping50])\n\nresults.loc[results.min_prd==min_prd,'nn6opt_train':'nn6opt_test'] = \\\n[r2_score(y_train, optuna_nn.predict(X_train)), \nr2_score(y_val, optuna_nn.predict(X_val)),\nr2_score(y_test, optuna_nn.predict(X_test))]\n\n\nprint('total time for the script: ', time.time()-time0)\ndisplay(results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Feature Importance","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Error Analysis","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general point:\n# compared to NN, xgb is harder to regularize\n# in NN, you can simply shrink coefficient towards constant prediction.\n# in xgb, you can not do that. the only way to regularize is via hyperparameters.\n# in other words, by tweaking hyperpars, in NN you can approach R^2=0.0 prediction from a constant model arbitrarily close\n# in xgb, you can not do that.\n# by setting eta as low as 0.1% you can bring r2 down to 0.1%, but lowering eta further actyally increases abs(r2).\n","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.131295Z","iopub.execute_input":"2022-09-23T02:35:36.131717Z","iopub.status.idle":"2022-09-23T02:35:36.136439Z","shell.execute_reply.started":"2022-09-23T02:35:36.131676Z","shell.execute_reply":"2022-09-23T02:35:36.135211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('total time for the script: ', time.time()-time0)\ndisplay(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.138217Z","iopub.execute_input":"2022-09-23T02:35:36.138634Z","iopub.status.idle":"2022-09-23T02:35:36.167667Z","shell.execute_reply.started":"2022-09-23T02:35:36.138598Z","shell.execute_reply":"2022-09-23T02:35:36.166507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" \n# def objective_nn(trial):\n    \n#     tf.keras.backend.clear_session()\n    \n#     with strategy.scope():\n#         # Generate our trial model.\n#         model = create_snnn_model(trial)\n\n#         callbacks = [\n#         tf.keras.callbacks.EarlyStopping(patience=40),\n#         TFKerasPruningCallback(trial, \"val_loss\"),\n#     ]\n\n#         # Fit the model on the training data.\n#         # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n#         history = model.fit(X_train, y_train, \n#                                 validation_data=(X_val, y_val),\n#                                 batch_size=2048, \n#                                 epochs=500, \n#                                 verbose=1, \n#                                 callbacks=callbacks)\n\n#         # Evaluate the model accuracy on the validation set.\n#         score = model.evaluate(X_val, y_val, verbose=0)\n#         return score[1]\n\n# trials = 50\n\n# study = optuna.create_study(direction=\"minimize\", \n#                             sampler=optuna.samplers.TPESampler(), \n#                             pruner=optuna.pruners.HyperbandPruner())\n# study.optimize(objective_nn, n_trials=trials)\n# pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n# complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n# temp = study.best_params\n# display(study.best_params, time.time()-time1)\n\n# optimal_hyperpars = list(temp.values())\n# display(optimal_hyperpars)\n# print(time.time()-time1, optimal_hyperpars)\n\n# optuna_nn = create_snnn_model_hyperpars(neurons_base=optimal_hyperpars[0], l2_reg_rate=optimal_hyperpars[1])\n# history = optuna_nn.fit(X_train, y_train, \n#                         validation_data=(X_val, y_val),\n#                         batch_size=2048, \n#                         epochs=1000,\n#                         verbose=1, \n#                         callbacks=[early_stopping50])\n\n# results.loc[results.min_prd==min_prd,'nn4opt_train':'nn4opt_test'] = \\\n# [r2_score(y_train, optuna_nn.predict(X_train)), \n# r2_score(y_val, optuna_nn.predict(X_val)),\n# r2_score(y_test, optuna_nn.predict(X_test))]","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.169234Z","iopub.execute_input":"2022-09-23T02:35:36.169576Z","iopub.status.idle":"2022-09-23T02:35:36.178963Z","shell.execute_reply.started":"2022-09-23T02:35:36.169542Z","shell.execute_reply":"2022-09-23T02:35:36.177802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # try optuna for NN:\n\n# def objective(trial):\n\n#     n_layers = trial.suggest_int('n_layers', 1, 3)\n#     model = tf.keras.Sequential()\n#     for i in range(n_layers):\n#         num_hidden = trial.suggest_int(f'n_units_l{i}', 4, 128, log=True)\n#         model.add(tf.keras.layers.Dense(num_hidden, activation='relu'))\n#     model.add(tf.keras.layers.Dense(1))\n#     display(model.summary())\n#     return accuracy\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=10)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.196466Z","iopub.execute_input":"2022-09-23T02:35:36.196749Z","iopub.status.idle":"2022-09-23T02:35:36.204089Z","shell.execute_reply.started":"2022-09-23T02:35:36.196725Z","shell.execute_reply":"2022-09-23T02:35:36.202999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# on nns:\n# - try classic regularizers (l1, l2 etc)\n# - try different architecture (not snnn)\n# classic architecture:\n# He initialization, elu activation, batch norm, l2 reg, adam.\n\n# - try exotic architecture, e.g., wide'n'deep\n# \n","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.625436Z","iopub.status.idle":"2022-09-23T02:35:36.626010Z","shell.execute_reply.started":"2022-09-23T02:35:36.625745Z","shell.execute_reply":"2022-09-23T02:35:36.625769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# usually self-norm seems better: it overfits less and runs faster\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.628108Z","iopub.status.idle":"2022-09-23T02:35:36.628600Z","shell.execute_reply.started":"2022-09-23T02:35:36.628350Z","shell.execute_reply":"2022-09-23T02:35:36.628374Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
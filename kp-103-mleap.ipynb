{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Define a problem, describe business context.\n2. Load data, preclean it.\n3. EDA: target, features.\n4. Descibe train/test split strategy. Show main results and discuss them.\n5. Run evth for a few periods. Show feature importance and error analysis.","metadata":{}},{"cell_type":"markdown","source":"### 1. Business problem\n\n#### Objective:\n\n#### Metric:\n\n#### Summary of results:\n","metadata":{}},{"cell_type":"markdown","source":"### 2. Load data and preclean it","metadata":{"execution":{"iopub.status.busy":"2022-09-26T15:43:15.362648Z","iopub.execute_input":"2022-09-26T15:43:15.363102Z","iopub.status.idle":"2022-09-26T15:43:15.385837Z","shell.execute_reply.started":"2022-09-26T15:43:15.363009Z","shell.execute_reply":"2022-09-26T15:43:15.384918Z"}}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle, shap\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, KFold, PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\nfrom optuna.integration import TFKerasPruningCallback\nfrom optuna.trial import TrialState\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\noptuna.logging.set_verbosity(optuna.logging.WARNING)\npd.set_option('display.max_columns', 150)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:00:33.522318Z","iopub.execute_input":"2022-09-27T22:00:33.523335Z","iopub.status.idle":"2022-09-27T22:00:44.761405Z","shell.execute_reply.started":"2022-09-27T22:00:33.522714Z","shell.execute_reply":"2022-09-27T22:00:44.760450Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:00:44.763337Z","iopub.execute_input":"2022-09-27T22:00:44.763970Z","iopub.status.idle":"2022-09-27T22:00:44.778661Z","shell.execute_reply.started":"2022-09-27T22:00:44.763932Z","shell.execute_reply":"2022-09-27T22:00:44.777247Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:00:44.780564Z","iopub.execute_input":"2022-09-27T22:00:44.781547Z","iopub.status.idle":"2022-09-27T22:00:44.804296Z","shell.execute_reply.started":"2022-09-27T22:00:44.781505Z","shell.execute_reply":"2022-09-27T22:00:44.803249Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"# Functions for Optuna NNs\n\ncv_nn_regularizer = 0.075\n\ndef create_snnn4_model_hyperpars(neurons_base=4, l1_reg_rate=0.1, l2_reg_rate=0.1):\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\ndef create_snnn4_model(trial):\n\n    # hyperparameter space\n    neurons_base = trial.suggest_int(\"neurons_base\", 4, 24, 2)\n    l2_reg_rate = trial.suggest_float('l2_regularizer', 0, 0.9)\n    l1_reg_rate = trial.suggest_float('l1_regularizer', 0, 0.9)\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\n\ndef create_snnn6_model_hyperpars(neurons_base=4, l1_reg_rate=0.1, l2_reg_rate=0.1):\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\ndef create_snnn6_model(trial):\n\n    # hyperparameter space\n    neurons_base = trial.suggest_int(\"neurons_base\", 4, 24, 2)\n    l2_reg_rate = trial.suggest_float('l2_regularizer', 0, 0.9)\n    l1_reg_rate = trial.suggest_float('l1_regularizer', 0, 0.9)\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\n\ndef objective_nn4(trial):\n    \n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_snnn4_model(trial)\n\n        callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=40),\n        TFKerasPruningCallback(trial, \"val_loss\"),\n    ]\n\n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n        history = model.fit(X_train, y_train, \n                                validation_data=(X_val, y_val),\n                                batch_size=2048, \n                                epochs=500, \n                                verbose=0, \n                                callbacks=callbacks)\n\n        # Evaluate the model accuracy on the validation set.\n        loss_train = model.evaluate(X_train, y_train, verbose=0)\n        loss_val = model.evaluate(X_val_extra, y_val_extra, verbose=0)\n        overfit = max(loss_val[1]-loss_train[1], 0)\n        return loss_val[1] + cv_nn_regularizer*overfit\n    \n    \ndef objective_nn6(trial):\n    \n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_snnn6_model(trial)\n\n        callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=40),\n        TFKerasPruningCallback(trial, \"val_loss\"),\n    ]\n\n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n        history = model.fit(X_train, y_train, \n                                validation_data=(X_val, y_val),\n                                batch_size=2048, \n                                epochs=500, \n                                verbose=0, \n                                callbacks=callbacks)\n\n        # Evaluate the model accuracy on the validation set.\n        loss_train = model.evaluate(X_train, y_train, verbose=0)\n        loss_val = model.evaluate(X_val_extra, y_val_extra, verbose=0)\n        overfit = max(loss_val[1]-loss_train[1], 0)\n        return loss_val[1] + cv_nn_regularizer*overfit\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:00:44.807626Z","iopub.execute_input":"2022-09-27T22:00:44.807981Z","iopub.status.idle":"2022-09-27T22:00:44.840481Z","shell.execute_reply.started":"2022-09-27T22:00:44.807947Z","shell.execute_reply":"2022-09-27T22:00:44.839538Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"time0 = time.time()\n\nmin_prd = 350\nwindows_width = 5*12\ncv_xgb_regularizer=0.2\noptuna_xgb_trials = 80\noptuna_nn_trials = 100\n\n\nwith open('../input/mleap-46-preprocessed/MLEAP_46_v7.pkl', 'rb') as pickled_one:\n    df = pickle.load(pickled_one)\ndf = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+10))]\ndf_cnt = df.count()\nempty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\ndf.drop(columns=empty_cols, inplace=True)\n#display(df.shape, df.head(), df.year.describe(), df.count())\n\nfeatures_miss_dummies = ['amhd', 'BAspr']\nfor col in features_miss_dummies:\n    if col in df.columns:\n        df[col+'_miss'] = df[col].isnull().astype(int)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:00:44.841610Z","iopub.execute_input":"2022-09-27T22:00:44.844419Z","iopub.status.idle":"2022-09-27T22:00:48.401036Z","shell.execute_reply.started":"2022-09-27T22:00:44.844382Z","shell.execute_reply":"2022-09-27T22:00:48.399982Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### 3. EDA","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Train/test split strategy","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Main results","metadata":{}},{"cell_type":"code","source":"results_df = pd.read_csv('../input/mleap-v49-results/temp_models_reg005_1.csv')\nresults_df","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:00:48.402418Z","iopub.execute_input":"2022-09-27T22:00:48.405177Z","iopub.status.idle":"2022-09-27T22:00:48.457100Z","shell.execute_reply.started":"2022-09-27T22:00:48.405147Z","shell.execute_reply":"2022-09-27T22:00:48.456112Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"    Unnamed: 0  min_prd  xgbf_train  xgbf_val  xgbf_test  xgbgs_train  \\\n0            0      100    0.110633 -0.010679   0.005897     0.032282   \n1            1      125    0.066434  0.004399   0.015985     0.040227   \n2            2      150    0.064235  0.028400  -0.007106     0.108114   \n3            3      175    0.073734  0.019790  -0.011655     0.093350   \n4            4      200    0.056176  0.048308   0.034120     0.111365   \n5            5      225    0.064410 -0.005404   0.012234     0.083176   \n6            6      250    0.061997 -0.017393   0.027061     0.018820   \n7            7      275    0.084432  0.128375   0.097800     0.109516   \n8            8      300    0.109596  0.041026   0.083551     0.097602   \n9            9      325    0.111924  0.042263  -0.038793     0.133787   \n10          10      350    0.064108  0.020667   0.031485     0.055273   \n11          11      375    0.045452  0.020881   0.013222     0.063410   \n12          12      400    0.041836  0.023006   0.023431     0.041640   \n13          13      425    0.043315  0.023428   0.021350     0.042503   \n14          14      450    0.044173  0.004679   0.008707     0.057491   \n15          15      475    0.042778  0.002243   0.002169     0.038150   \n16          16      500    0.042942  0.039946   0.045640     0.092373   \n17          17      525    0.055984  0.008138   0.037555     0.068378   \n18          18      550    0.079068 -0.007977  -0.002293     0.036324   \n19          19      575    0.061489  0.046520   0.011930     0.084962   \n20          20      600    0.053373 -0.018035  -0.004766     0.014890   \n21          21      625         NaN       NaN        NaN          NaN   \n22          22      650         NaN       NaN        NaN          NaN   \n\n    xgbgs_val  xgbgs_test  xgbo_train  xgbo_val  xgbo_test  nn4_train  \\\n0    0.009775    0.016553    0.142414  0.097214   0.017316   0.092751   \n1    0.053275    0.020740    0.081517  0.077877   0.018348   0.020937   \n2    0.136225   -0.040268    0.102913  0.141299  -0.040073   0.039123   \n3    0.065665    0.000453    0.017994  0.013669  -0.002192   0.057370   \n4    0.141292    0.040536    0.057042  0.089245   0.040346   0.041166   \n5    0.067360    0.026466    0.070134  0.033220   0.020088   0.055780   \n6   -0.001580    0.013451    0.189037  0.155622   0.031786   0.048662   \n7    0.192547    0.108320    0.102921  0.176972   0.108397   0.073269   \n8    0.058866    0.086605    0.098751  0.054814   0.084547   0.098849   \n9    0.091224   -0.039374    0.112554  0.067039  -0.034580   0.106888   \n10   0.033387    0.033121    0.059194  0.035949   0.034516   0.058240   \n11   0.054559    0.020181    0.059622  0.051217   0.016368   0.035642   \n12   0.041122    0.028316    0.033655  0.031300   0.027806   0.029551   \n13   0.047076    0.017911    0.034322  0.037631   0.016773   0.031853   \n14   0.063234    0.021913    0.031195  0.029232   0.012268   0.034023   \n15   0.026061    0.009934    0.022383  0.010773   0.004530   0.026574   \n16   0.106835    0.065450    0.056338  0.078596   0.058214   0.033054   \n17   0.034044    0.045484    0.049435  0.033478   0.042042   0.038853   \n18   0.007323    0.014142    0.059759  0.017997   0.009193   0.065788   \n19   0.089735    0.022645    0.049855  0.059204   0.020475   0.049639   \n20  -0.000217    0.005112    0.036381 -0.004054   0.002889   0.011194   \n21        NaN         NaN         NaN       NaN        NaN        NaN   \n22        NaN         NaN         NaN       NaN        NaN        NaN   \n\n     nn4_val  nn4_test  nn6_train   nn6_val  nn6_test  nn4opt_train  \\\n0  -0.004731  0.004806   0.077033  0.003310  0.007623  2.368052e-02   \n1   0.023310  0.014654   0.039157  0.018810  0.009496 -1.010410e-06   \n2   0.041242 -0.044962   0.046364  0.044968 -0.055987  4.366676e-02   \n3   0.021523 -0.006702   0.064954  0.017231 -0.010337 -4.252701e-06   \n4   0.071331  0.022324   0.041645  0.073893  0.025533  3.709691e-02   \n5   0.008508  0.012525   0.052652  0.008472  0.013793 -5.871936e-07   \n6  -0.007640  0.024796   0.048832 -0.016709 -0.007211 -1.719546e-09   \n7   0.144908  0.110993   0.070322  0.143302  0.112889  7.233643e-02   \n8   0.040880  0.086885   0.095496  0.042473  0.086713  1.023098e-01   \n9   0.049632 -0.052411   0.104488  0.049346 -0.055748  9.684091e-02   \n10  0.030566  0.022642   0.055391  0.030115  0.026270  5.876430e-02   \n11  0.024271  0.005288   0.034158  0.025277  0.002064  3.371769e-02   \n12  0.025455  0.034004   0.025434  0.024934  0.036392  2.552223e-02   \n13  0.030089  0.025648   0.029592  0.030158  0.021560  3.133441e-02   \n14  0.031850  0.015509   0.035800  0.034246  0.007337 -1.435706e-07   \n15  0.011256  0.011812   0.033069  0.012727 -0.002586 -3.848525e-07   \n16  0.044643  0.043081   0.035159  0.043644  0.044818  2.918796e-02   \n17  0.012892  0.031937   0.042841  0.010648  0.029456  3.686924e-02   \n18  0.005629  0.016282   0.063776  0.008980  0.021159 -9.343118e-07   \n19  0.055342  0.008340   0.045969  0.055721  0.019505  3.947695e-02   \n20 -0.003839  0.004517   0.031915 -0.025365 -0.014393 -1.116158e-07   \n21       NaN       NaN        NaN       NaN       NaN           NaN   \n22       NaN       NaN        NaN       NaN       NaN           NaN   \n\n    nn4opt_val   nn4opt_test  nn6opt_train    nn6opt_val  nn6opt_test  \n0    -0.006378  4.413288e-03  2.373109e-02 -1.185371e-03     0.006157  \n1    -0.000038 -1.945688e-05 -7.328776e-07 -3.910448e-05    -0.000020  \n2     0.040878 -4.501329e-02  5.041909e-02  4.294926e-02    -0.043427  \n3    -0.000010 -4.954723e-07 -7.191610e-09 -6.917556e-07    -0.000008  \n4     0.065155  2.521314e-02  3.844909e-02  6.439698e-02     0.024244  \n5    -0.000169 -1.014969e-04 -5.182621e-06 -1.285523e-04    -0.000073  \n6    -0.000007 -2.775066e-06 -8.389908e-07 -2.850800e-06    -0.000007  \n7     0.144175  1.100276e-01  7.211387e-02  1.423667e-01     0.106699  \n8     0.039196  8.664547e-02  9.784490e-02  3.844309e-02     0.087651  \n9     0.041965 -6.858033e-02  1.009017e-01  4.594199e-02    -0.066538  \n10    0.030567  2.703620e-02  5.610774e-02  2.959755e-02     0.028619  \n11    0.022140  5.191548e-03  3.490645e-02  2.254672e-02    -0.003187  \n12    0.021759  3.121626e-02  3.332331e-02  2.432268e-02     0.030985  \n13    0.028021  2.200129e-02  2.661165e-02  3.152557e-02     0.012151  \n14   -0.000024 -1.837915e-05 -1.779765e-08 -2.865202e-05    -0.000022  \n15   -0.000045 -3.566047e-05 -3.725332e-07 -4.539190e-05    -0.000036  \n16    0.039344  4.251399e-02  5.253141e-02  5.219970e-02     0.037235  \n17    0.013139  3.232509e-02 -1.155932e-06 -8.184362e-05    -0.000014  \n18   -0.000007 -1.136215e-05 -8.872024e-08 -9.620223e-06    -0.000015  \n19    0.050070  1.513632e-02  5.294881e-02  5.275299e-02     0.013744  \n20   -0.000010 -2.508825e-05 -3.862022e-12 -1.229114e-05    -0.000029  \n21         NaN           NaN           NaN           NaN          NaN  \n22         NaN           NaN           NaN           NaN          NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n      <th>nn4_train</th>\n      <th>nn4_val</th>\n      <th>nn4_test</th>\n      <th>nn6_train</th>\n      <th>nn6_val</th>\n      <th>nn6_test</th>\n      <th>nn4opt_train</th>\n      <th>nn4opt_val</th>\n      <th>nn4opt_test</th>\n      <th>nn6opt_train</th>\n      <th>nn6opt_val</th>\n      <th>nn6opt_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>100</td>\n      <td>0.110633</td>\n      <td>-0.010679</td>\n      <td>0.005897</td>\n      <td>0.032282</td>\n      <td>0.009775</td>\n      <td>0.016553</td>\n      <td>0.142414</td>\n      <td>0.097214</td>\n      <td>0.017316</td>\n      <td>0.092751</td>\n      <td>-0.004731</td>\n      <td>0.004806</td>\n      <td>0.077033</td>\n      <td>0.003310</td>\n      <td>0.007623</td>\n      <td>2.368052e-02</td>\n      <td>-0.006378</td>\n      <td>4.413288e-03</td>\n      <td>2.373109e-02</td>\n      <td>-1.185371e-03</td>\n      <td>0.006157</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>125</td>\n      <td>0.066434</td>\n      <td>0.004399</td>\n      <td>0.015985</td>\n      <td>0.040227</td>\n      <td>0.053275</td>\n      <td>0.020740</td>\n      <td>0.081517</td>\n      <td>0.077877</td>\n      <td>0.018348</td>\n      <td>0.020937</td>\n      <td>0.023310</td>\n      <td>0.014654</td>\n      <td>0.039157</td>\n      <td>0.018810</td>\n      <td>0.009496</td>\n      <td>-1.010410e-06</td>\n      <td>-0.000038</td>\n      <td>-1.945688e-05</td>\n      <td>-7.328776e-07</td>\n      <td>-3.910448e-05</td>\n      <td>-0.000020</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>150</td>\n      <td>0.064235</td>\n      <td>0.028400</td>\n      <td>-0.007106</td>\n      <td>0.108114</td>\n      <td>0.136225</td>\n      <td>-0.040268</td>\n      <td>0.102913</td>\n      <td>0.141299</td>\n      <td>-0.040073</td>\n      <td>0.039123</td>\n      <td>0.041242</td>\n      <td>-0.044962</td>\n      <td>0.046364</td>\n      <td>0.044968</td>\n      <td>-0.055987</td>\n      <td>4.366676e-02</td>\n      <td>0.040878</td>\n      <td>-4.501329e-02</td>\n      <td>5.041909e-02</td>\n      <td>4.294926e-02</td>\n      <td>-0.043427</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>175</td>\n      <td>0.073734</td>\n      <td>0.019790</td>\n      <td>-0.011655</td>\n      <td>0.093350</td>\n      <td>0.065665</td>\n      <td>0.000453</td>\n      <td>0.017994</td>\n      <td>0.013669</td>\n      <td>-0.002192</td>\n      <td>0.057370</td>\n      <td>0.021523</td>\n      <td>-0.006702</td>\n      <td>0.064954</td>\n      <td>0.017231</td>\n      <td>-0.010337</td>\n      <td>-4.252701e-06</td>\n      <td>-0.000010</td>\n      <td>-4.954723e-07</td>\n      <td>-7.191610e-09</td>\n      <td>-6.917556e-07</td>\n      <td>-0.000008</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>200</td>\n      <td>0.056176</td>\n      <td>0.048308</td>\n      <td>0.034120</td>\n      <td>0.111365</td>\n      <td>0.141292</td>\n      <td>0.040536</td>\n      <td>0.057042</td>\n      <td>0.089245</td>\n      <td>0.040346</td>\n      <td>0.041166</td>\n      <td>0.071331</td>\n      <td>0.022324</td>\n      <td>0.041645</td>\n      <td>0.073893</td>\n      <td>0.025533</td>\n      <td>3.709691e-02</td>\n      <td>0.065155</td>\n      <td>2.521314e-02</td>\n      <td>3.844909e-02</td>\n      <td>6.439698e-02</td>\n      <td>0.024244</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>225</td>\n      <td>0.064410</td>\n      <td>-0.005404</td>\n      <td>0.012234</td>\n      <td>0.083176</td>\n      <td>0.067360</td>\n      <td>0.026466</td>\n      <td>0.070134</td>\n      <td>0.033220</td>\n      <td>0.020088</td>\n      <td>0.055780</td>\n      <td>0.008508</td>\n      <td>0.012525</td>\n      <td>0.052652</td>\n      <td>0.008472</td>\n      <td>0.013793</td>\n      <td>-5.871936e-07</td>\n      <td>-0.000169</td>\n      <td>-1.014969e-04</td>\n      <td>-5.182621e-06</td>\n      <td>-1.285523e-04</td>\n      <td>-0.000073</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>250</td>\n      <td>0.061997</td>\n      <td>-0.017393</td>\n      <td>0.027061</td>\n      <td>0.018820</td>\n      <td>-0.001580</td>\n      <td>0.013451</td>\n      <td>0.189037</td>\n      <td>0.155622</td>\n      <td>0.031786</td>\n      <td>0.048662</td>\n      <td>-0.007640</td>\n      <td>0.024796</td>\n      <td>0.048832</td>\n      <td>-0.016709</td>\n      <td>-0.007211</td>\n      <td>-1.719546e-09</td>\n      <td>-0.000007</td>\n      <td>-2.775066e-06</td>\n      <td>-8.389908e-07</td>\n      <td>-2.850800e-06</td>\n      <td>-0.000007</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>275</td>\n      <td>0.084432</td>\n      <td>0.128375</td>\n      <td>0.097800</td>\n      <td>0.109516</td>\n      <td>0.192547</td>\n      <td>0.108320</td>\n      <td>0.102921</td>\n      <td>0.176972</td>\n      <td>0.108397</td>\n      <td>0.073269</td>\n      <td>0.144908</td>\n      <td>0.110993</td>\n      <td>0.070322</td>\n      <td>0.143302</td>\n      <td>0.112889</td>\n      <td>7.233643e-02</td>\n      <td>0.144175</td>\n      <td>1.100276e-01</td>\n      <td>7.211387e-02</td>\n      <td>1.423667e-01</td>\n      <td>0.106699</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>300</td>\n      <td>0.109596</td>\n      <td>0.041026</td>\n      <td>0.083551</td>\n      <td>0.097602</td>\n      <td>0.058866</td>\n      <td>0.086605</td>\n      <td>0.098751</td>\n      <td>0.054814</td>\n      <td>0.084547</td>\n      <td>0.098849</td>\n      <td>0.040880</td>\n      <td>0.086885</td>\n      <td>0.095496</td>\n      <td>0.042473</td>\n      <td>0.086713</td>\n      <td>1.023098e-01</td>\n      <td>0.039196</td>\n      <td>8.664547e-02</td>\n      <td>9.784490e-02</td>\n      <td>3.844309e-02</td>\n      <td>0.087651</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>325</td>\n      <td>0.111924</td>\n      <td>0.042263</td>\n      <td>-0.038793</td>\n      <td>0.133787</td>\n      <td>0.091224</td>\n      <td>-0.039374</td>\n      <td>0.112554</td>\n      <td>0.067039</td>\n      <td>-0.034580</td>\n      <td>0.106888</td>\n      <td>0.049632</td>\n      <td>-0.052411</td>\n      <td>0.104488</td>\n      <td>0.049346</td>\n      <td>-0.055748</td>\n      <td>9.684091e-02</td>\n      <td>0.041965</td>\n      <td>-6.858033e-02</td>\n      <td>1.009017e-01</td>\n      <td>4.594199e-02</td>\n      <td>-0.066538</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>350</td>\n      <td>0.064108</td>\n      <td>0.020667</td>\n      <td>0.031485</td>\n      <td>0.055273</td>\n      <td>0.033387</td>\n      <td>0.033121</td>\n      <td>0.059194</td>\n      <td>0.035949</td>\n      <td>0.034516</td>\n      <td>0.058240</td>\n      <td>0.030566</td>\n      <td>0.022642</td>\n      <td>0.055391</td>\n      <td>0.030115</td>\n      <td>0.026270</td>\n      <td>5.876430e-02</td>\n      <td>0.030567</td>\n      <td>2.703620e-02</td>\n      <td>5.610774e-02</td>\n      <td>2.959755e-02</td>\n      <td>0.028619</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>375</td>\n      <td>0.045452</td>\n      <td>0.020881</td>\n      <td>0.013222</td>\n      <td>0.063410</td>\n      <td>0.054559</td>\n      <td>0.020181</td>\n      <td>0.059622</td>\n      <td>0.051217</td>\n      <td>0.016368</td>\n      <td>0.035642</td>\n      <td>0.024271</td>\n      <td>0.005288</td>\n      <td>0.034158</td>\n      <td>0.025277</td>\n      <td>0.002064</td>\n      <td>3.371769e-02</td>\n      <td>0.022140</td>\n      <td>5.191548e-03</td>\n      <td>3.490645e-02</td>\n      <td>2.254672e-02</td>\n      <td>-0.003187</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>400</td>\n      <td>0.041836</td>\n      <td>0.023006</td>\n      <td>0.023431</td>\n      <td>0.041640</td>\n      <td>0.041122</td>\n      <td>0.028316</td>\n      <td>0.033655</td>\n      <td>0.031300</td>\n      <td>0.027806</td>\n      <td>0.029551</td>\n      <td>0.025455</td>\n      <td>0.034004</td>\n      <td>0.025434</td>\n      <td>0.024934</td>\n      <td>0.036392</td>\n      <td>2.552223e-02</td>\n      <td>0.021759</td>\n      <td>3.121626e-02</td>\n      <td>3.332331e-02</td>\n      <td>2.432268e-02</td>\n      <td>0.030985</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>425</td>\n      <td>0.043315</td>\n      <td>0.023428</td>\n      <td>0.021350</td>\n      <td>0.042503</td>\n      <td>0.047076</td>\n      <td>0.017911</td>\n      <td>0.034322</td>\n      <td>0.037631</td>\n      <td>0.016773</td>\n      <td>0.031853</td>\n      <td>0.030089</td>\n      <td>0.025648</td>\n      <td>0.029592</td>\n      <td>0.030158</td>\n      <td>0.021560</td>\n      <td>3.133441e-02</td>\n      <td>0.028021</td>\n      <td>2.200129e-02</td>\n      <td>2.661165e-02</td>\n      <td>3.152557e-02</td>\n      <td>0.012151</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>450</td>\n      <td>0.044173</td>\n      <td>0.004679</td>\n      <td>0.008707</td>\n      <td>0.057491</td>\n      <td>0.063234</td>\n      <td>0.021913</td>\n      <td>0.031195</td>\n      <td>0.029232</td>\n      <td>0.012268</td>\n      <td>0.034023</td>\n      <td>0.031850</td>\n      <td>0.015509</td>\n      <td>0.035800</td>\n      <td>0.034246</td>\n      <td>0.007337</td>\n      <td>-1.435706e-07</td>\n      <td>-0.000024</td>\n      <td>-1.837915e-05</td>\n      <td>-1.779765e-08</td>\n      <td>-2.865202e-05</td>\n      <td>-0.000022</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>15</td>\n      <td>475</td>\n      <td>0.042778</td>\n      <td>0.002243</td>\n      <td>0.002169</td>\n      <td>0.038150</td>\n      <td>0.026061</td>\n      <td>0.009934</td>\n      <td>0.022383</td>\n      <td>0.010773</td>\n      <td>0.004530</td>\n      <td>0.026574</td>\n      <td>0.011256</td>\n      <td>0.011812</td>\n      <td>0.033069</td>\n      <td>0.012727</td>\n      <td>-0.002586</td>\n      <td>-3.848525e-07</td>\n      <td>-0.000045</td>\n      <td>-3.566047e-05</td>\n      <td>-3.725332e-07</td>\n      <td>-4.539190e-05</td>\n      <td>-0.000036</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>16</td>\n      <td>500</td>\n      <td>0.042942</td>\n      <td>0.039946</td>\n      <td>0.045640</td>\n      <td>0.092373</td>\n      <td>0.106835</td>\n      <td>0.065450</td>\n      <td>0.056338</td>\n      <td>0.078596</td>\n      <td>0.058214</td>\n      <td>0.033054</td>\n      <td>0.044643</td>\n      <td>0.043081</td>\n      <td>0.035159</td>\n      <td>0.043644</td>\n      <td>0.044818</td>\n      <td>2.918796e-02</td>\n      <td>0.039344</td>\n      <td>4.251399e-02</td>\n      <td>5.253141e-02</td>\n      <td>5.219970e-02</td>\n      <td>0.037235</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>17</td>\n      <td>525</td>\n      <td>0.055984</td>\n      <td>0.008138</td>\n      <td>0.037555</td>\n      <td>0.068378</td>\n      <td>0.034044</td>\n      <td>0.045484</td>\n      <td>0.049435</td>\n      <td>0.033478</td>\n      <td>0.042042</td>\n      <td>0.038853</td>\n      <td>0.012892</td>\n      <td>0.031937</td>\n      <td>0.042841</td>\n      <td>0.010648</td>\n      <td>0.029456</td>\n      <td>3.686924e-02</td>\n      <td>0.013139</td>\n      <td>3.232509e-02</td>\n      <td>-1.155932e-06</td>\n      <td>-8.184362e-05</td>\n      <td>-0.000014</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>18</td>\n      <td>550</td>\n      <td>0.079068</td>\n      <td>-0.007977</td>\n      <td>-0.002293</td>\n      <td>0.036324</td>\n      <td>0.007323</td>\n      <td>0.014142</td>\n      <td>0.059759</td>\n      <td>0.017997</td>\n      <td>0.009193</td>\n      <td>0.065788</td>\n      <td>0.005629</td>\n      <td>0.016282</td>\n      <td>0.063776</td>\n      <td>0.008980</td>\n      <td>0.021159</td>\n      <td>-9.343118e-07</td>\n      <td>-0.000007</td>\n      <td>-1.136215e-05</td>\n      <td>-8.872024e-08</td>\n      <td>-9.620223e-06</td>\n      <td>-0.000015</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>19</td>\n      <td>575</td>\n      <td>0.061489</td>\n      <td>0.046520</td>\n      <td>0.011930</td>\n      <td>0.084962</td>\n      <td>0.089735</td>\n      <td>0.022645</td>\n      <td>0.049855</td>\n      <td>0.059204</td>\n      <td>0.020475</td>\n      <td>0.049639</td>\n      <td>0.055342</td>\n      <td>0.008340</td>\n      <td>0.045969</td>\n      <td>0.055721</td>\n      <td>0.019505</td>\n      <td>3.947695e-02</td>\n      <td>0.050070</td>\n      <td>1.513632e-02</td>\n      <td>5.294881e-02</td>\n      <td>5.275299e-02</td>\n      <td>0.013744</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>20</td>\n      <td>600</td>\n      <td>0.053373</td>\n      <td>-0.018035</td>\n      <td>-0.004766</td>\n      <td>0.014890</td>\n      <td>-0.000217</td>\n      <td>0.005112</td>\n      <td>0.036381</td>\n      <td>-0.004054</td>\n      <td>0.002889</td>\n      <td>0.011194</td>\n      <td>-0.003839</td>\n      <td>0.004517</td>\n      <td>0.031915</td>\n      <td>-0.025365</td>\n      <td>-0.014393</td>\n      <td>-1.116158e-07</td>\n      <td>-0.000010</td>\n      <td>-2.508825e-05</td>\n      <td>-3.862022e-12</td>\n      <td>-1.229114e-05</td>\n      <td>-0.000029</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>21</td>\n      <td>625</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>22</td>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"results_df.iloc[:,2:].mean()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Fit the model(s) for one window and explore results","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_cols = ['PERMNO', 'year', 'prd']\ndf.reset_index(inplace=True, drop=True)\nX = df.copy()\ny = X.pop('RET')\n\ntrain_indx = X.prd<(min_prd+windows_width-1)\nval_indx = X['prd'].isin(range(min_prd+windows_width-1, min_prd+windows_width+2))\nval_indx_extra = X['prd'].isin(range(min_prd+windows_width+5, min_prd+windows_width+8))\ntest_indx = X['prd'].isin(range(min_prd+windows_width+2, min_prd+windows_width+5))\n\nX_train = X[train_indx]\nX_val = X[val_indx]\nX_val_extra = X[val_indx_extra]\nX_test = X[test_indx]\ny_train = y[train_indx]\ny_val = y[val_indx]\ny_val_extra = y[val_indx_extra]\ny_test = y[test_indx]\n\n#display(X_train.head(3), X_train.tail(3), y_train.head(3), y_train.tail(3))\n#display(X_train.prd.describe(), X_val.prd.describe(), X_test.prd.describe())\ndisplay(X_train.shape, X_val.shape, X_test.shape)\n\nX_train.drop(columns=temp_cols, inplace=True)\nX_val.drop(columns=temp_cols, inplace=True)\nX_val_extra.drop(columns=temp_cols, inplace=True)\nX_test.drop(columns=temp_cols, inplace=True)\n\n#display(X_train.tail())\ncol_cat = ['ind']\ncol_num = [x for x in X_train.columns if x not in col_cat]\nfor col in col_num:\n    X_train[col] = X_train[col].fillna(X_train[col].median())\n    X_val[col] = X_val[col].fillna(X_train[col].median())\n    X_val_extra[col] = X_val_extra[col].fillna(X_train[col].median())\n    X_test[col] = X_test[col].fillna(X_train[col].median())\nfor col in col_cat:\n    X_train[col] = X_train[col].fillna(value=-1000)\n    X_val[col] = X_val[col].fillna(value=-1000)\n    X_val_extra[col] = X_val_extra[col].fillna(value=-1000)\n    X_test[col] = X_test[col].fillna(value=-1000)\n\n#display(X_train.tail())\nfeature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                        (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                        remainder=\"passthrough\")\n\nprint('Number of features before transformation: ', X_train.shape)\ntrain_index, val_index, val_index_extra, test_index = X_train.index, X_val.index, X_val_extra.index, X_test.index\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_val = pd.DataFrame(feature_transformer.transform(X_val), columns=feature_transformer.get_feature_names_out())\nX_val_extra = pd.DataFrame(feature_transformer.transform(X_val_extra), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ')\nprint('Number of features after transformation: ', X_train.shape, X_val.shape, X_val_extra.shape, X_test.shape)\nX_train.index = train_index\nX_val.index = val_index\nX_val_extra.index = val_index_extra\nX_test.index = test_index\n#display(X_train.tail())\n\nX = pd.concat([X_train, X_val])\ny = pd.concat([y_train, y_val])\n#display(X,y)\n\nX_ = pd.concat([X_train, X_val, X_val_extra])\ny_ = pd.concat([y_train, y_val, y_val_extra])\n#display(X,y, X_,y_)\n\n\nresults = pd.DataFrame(columns = ['min_prd', 'xgbf_train', 'xgbf_val', 'xgbf_test', \n                                  'xgbgs_train', 'xgbgs_val', 'xgbgs_test', \n                                  'xgbo_train', 'xgbo_val', 'xgbo_test',\n                                  'nn4_train', 'nn4_val', 'nn4_test',\n                                 'nn6_train', 'nn6_val', 'nn6_test',\n                                 'nn4opt_train', 'nn4opt_val', 'nn4opt_test',\n                                 'nn6opt_train', 'nn6opt_val', 'nn6opt_test'])\n\nresults['min_prd'] = [min_prd]\n\n\n### Modeling part ###\n\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=400, max_depth=4, eta=0.02, colsample_bytree=0.4, subsample=0.6)\nxgb1.fit(X_train, y_train)\nprint('fixed XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\nprint('XGB val:', mean_absolute_error(y_val, xgb1.predict(X_val)), r2_score(y_val, xgb1.predict(X_val)))\nprint('XGB val extra:', mean_absolute_error(y_val_extra, xgb1.predict(X_val_extra)), r2_score(y_val_extra, xgb1.predict(X_val_extra)))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbf_train':'xgbf_test'] = \\\n[r2_score(y_train, xgb1.predict(X_train)), \nr2_score(y_val, xgb1.predict(X_val)),\nr2_score(y_test, xgb1.predict(X_test))]\n\ntime1 = time.time()\n\n# Create a list where train data indices are -1 and validation data indices are 0\nsplit_index = [-1 if x in X_train.index else 0 for x in X.index]\npds = PredefinedSplit(test_fold = split_index)\n\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[400, 600, 800], 'max_depth':[2,3,4,5], 'eta':[0.006, 0.012, 0.02], \n              'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbgs = GridSearchCV(estimator = xgb, cv=pds, param_grid=param_grid)\n\n# Fit with all data\nxgbgs.fit(X_, y_)\n\nprint('gs XGB', xgbgs.best_params_, xgbgs.best_score_, time.time()-time1)\nprint('XGB train:', mean_absolute_error(y_train, xgbgs.predict(X_train)), r2_score(y_train, xgbgs.predict(X_train)))\nprint('XGB validation:', mean_absolute_error(y_val, xgbgs.predict(X_val)), r2_score(y_val, xgbgs.predict(X_val)))\nprint('XGB validation extra:', mean_absolute_error(y_val_extra, xgbgs.predict(X_val_extra)), r2_score(y_val_extra, xgbgs.predict(X_val_extra)))\nprint('XGB test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_score(y_test, xgbgs.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbgs_train':'xgbgs_test'] = \\\n[r2_score(y_train, xgbgs.predict(X_train)), \nr2_score(y_val, xgbgs.predict(X_val)),\nr2_score(y_test, xgbgs.predict(X_test))]\n\ntime1 = time.time()\ndef objective_xgb(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    params = {\n    \"tree_method\": 'gpu_hist',\n    \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n    \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 1500),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n    \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.0005, 0.03),\n    \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.05, 0.95),\n    \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 0.95),\n    \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 50.0),\n    \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 500.0),\n    \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 100.0),\n    \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n    model = XGBRegressor(**params, njobs=-1)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose = False)\n\n    score_train = r2_score(y_train, model.predict(X_train))\n    score_val = r2_score(y_val, model.predict(X_val))\n    score_val_extra = r2_score(y_val_extra, model.predict(X_val_extra)) \n    score_val = (score_val+score_val_extra)/2\n    overfit = np.abs(score_train-score_val)\n\n    return score_val-cv_xgb_regularizer*overfit\n\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective_xgb, n_trials=optuna_xgb_trials)\nprint('Total time for hypermarameter optimization, XGB: ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X, y)\nprint('Optuna XGB train: \\n', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), '\\nvalidation \\n',\n      mean_absolute_error(y_val, optuna_xgb.predict(X_val)), r2_score(y_val, optuna_xgb.predict(X_val)),\n      mean_absolute_error(y_val_extra, optuna_xgb.predict(X_val_extra)), r2_score(y_val_extra, optuna_xgb.predict(X_val_extra)), '\\ntest \\n',\n      mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbo_train':'xgbo_test'] = \\\n[r2_score(y_train, optuna_xgb.predict(X_train)), \nr2_score(y_val, optuna_xgb.predict(X_val)),\nr2_score(y_test, optuna_xgb.predict(X_test))]\n\n###########\n### NNs ###\n###########\n\nneurons_base = 8\nl2_reg_rate = 0.5\n\nmodel_snn6 = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn6.count_params())\n\nearly_stopping50 = EarlyStopping(patience=50, restore_best_weights=True)\noptimizer_adam = tf.keras.optimizers.Adam()\n\nmodel_snn6.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n\ntime1 = time.time()\nhistory = model_snn6.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=0, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nresults.loc[results.min_prd==min_prd,'nn6_train':'nn6_test'] = \\\n[r2_score(y_train, model_snn6.predict(X_train)), \nr2_score(y_val, model_snn6.predict(X_val)),\nr2_score(y_test, model_snn6.predict(X_test))]\n\n\n\nneurons_base = 8\nl2_reg_rate = 0.3\n\nmodel_snn4 = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn4.count_params())\n\ntime1 = time.time()\nmodel_snn4.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_snn4.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=0, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nresults.loc[results.min_prd==min_prd,'nn4_train':'nn4_test'] = \\\n[r2_score(y_train, model_snn4.predict(X_train)), \nr2_score(y_val, model_snn4.predict(X_val)),\nr2_score(y_test, model_snn4.predict(X_test))]\n\n\n\n# try optuna, using this kaggle notebook: https://www.kaggle.com/code/mistag/keras-model-tuning-with-optuna\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"minimize\", \n                            sampler=optuna.samplers.TPESampler(), \n                            pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective_nn4, n_trials=optuna_nn_trials)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\ntemp = study.best_params\ndisplay('Optuna NN4', study.best_params, time.time()-time1)\n\noptimal_hyperpars = list(temp.values())\ndisplay(optimal_hyperpars)\nprint('Time for hyperparameter optimization: ',time.time()-time1, optimal_hyperpars)\n\noptuna_nn = create_snnn4_model_hyperpars(neurons_base=optimal_hyperpars[0], \n                                         l2_reg_rate=optimal_hyperpars[1],\n                                        l1_reg_rate=optimal_hyperpars[2])\nhistory = optuna_nn.fit(X_train, y_train, \n                        validation_data=(X_val, y_val),\n                        batch_size=2048, \n                        epochs=1000,\n                        verbose=0, \n                        callbacks=[early_stopping50])\n\nresults.loc[results.min_prd==min_prd,'nn4opt_train':'nn4opt_test'] = \\\n[r2_score(y_train, optuna_nn.predict(X_train)), \nr2_score(y_val, optuna_nn.predict(X_val)),\nr2_score(y_test, optuna_nn.predict(X_test))]\n\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"minimize\", \n                            sampler=optuna.samplers.TPESampler(), \n                            pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective_nn6, n_trials=optuna_nn_trials)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\ntemp = study.best_params\ndisplay('Optuna NN6', study.best_params, time.time()-time1)\n\noptimal_hyperpars = list(temp.values())\ndisplay(optimal_hyperpars)\nprint('Time for hyperparameter optimization: ', time.time()-time1, optimal_hyperpars)\n\noptuna_nn = create_snnn6_model_hyperpars(neurons_base=optimal_hyperpars[0], \n                                         l2_reg_rate=optimal_hyperpars[1],\n                                        l1_reg_rate=optimal_hyperpars[2])\nhistory = optuna_nn.fit(X_train, y_train, \n                        validation_data=(X_val, y_val),\n                        batch_size=2048, \n                        epochs=1000,\n                        verbose=0, \n                        callbacks=[early_stopping50])\n\nresults.loc[results.min_prd==min_prd,'nn6opt_train':'nn6opt_test'] = \\\n[r2_score(y_train, optuna_nn.predict(X_train)), \nr2_score(y_val, optuna_nn.predict(X_val)),\nr2_score(y_test, optuna_nn.predict(X_test))]\n\n\nprint('total time for the script: ', time.time()-time0)\ndisplay(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:00:48.458546Z","iopub.execute_input":"2022-09-27T22:00:48.458914Z","iopub.status.idle":"2022-09-27T22:44:59.259882Z","shell.execute_reply.started":"2022-09-27T22:00:48.458878Z","shell.execute_reply":"2022-09-27T22:44:59.258889Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"(146944, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(7507, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(7561, 41)"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (146944, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (146944, 86) (7507, 86) (7431, 86) (7561, 86)\nmae of a constant model 10.01529988807284\nR2 of a constant model 0.0\nfixed XGB train: 9.55921006173112 0.06410827554684584\nXGB val: 9.271005226317717 0.02066692926133007\nXGB val extra: 10.901656205339957 0.01356724415096211\nXGB test: 9.4228780692512 0.031484648991487285\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.012, 'max_depth': 3, 'n_estimators': 800, 'subsample': 0.6} 0.021518235581785672 77.83851099014282\nXGB train: 9.602715421644447 0.055272575410556635\nXGB validation: 9.214616030425278 0.03338672541856802\nXGB validation extra: 10.826646916675854 0.026527574479032645\nXGB test: 9.409131718392727 0.03312134954357382\nTotal time for hypermarameter optimization, XGB:  303.8176779747009\n        n_estimators : 1228\n           max_depth : 2\n       learning_rate : 0.02532408614249929\n    colsample_bytree : 0.34999163536765127\n           subsample : 0.6641265517733469\n               alpha : 7.296610812166446\n              lambda : 133.64763526465083\n               gamma : 0.0017913619279596189\n    min_child_weight : 0.49041214710141906\nbest objective value : 0.011874494890156927\nOptuna XGB train: \n 9.570676374168936 0.05729427993842828 \nvalidation \n 9.212560135693588 0.03395126914705948 10.897576378876176 0.013717049359052713 \ntest \n 9.416044347172072 0.033454453968188025\n","output_type":"stream"},{"name":"stderr","text":"2022-09-27 22:07:28.054114: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 22:07:28.055405: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 22:07:28.056094: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 22:07:28.056993: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-09-27 22:07:28.057324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 22:07:28.058000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 22:07:28.058657: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 22:07:34.304159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 22:07:34.305289: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 22:07:34.306295: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-27 22:07:34.307106: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14605 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"name":"stdout","text":"3833\n","output_type":"stream"},{"name":"stderr","text":"2022-09-27 22:07:35.218488: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Minimum Validation Loss: 172.4513\n3489\nMinimum Validation Loss: 171.8334\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'Optuna NN4'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"{'neurons_base': 4,\n 'l2_regularizer': 0.07806616636689395,\n 'l1_regularizer': 0.0037095100533800426}"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"877.071754693985"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"[4, 0.07806616636689395, 0.0037095100533800426]"},"metadata":{}},{"name":"stdout","text":"Time for hyperparameter optimization:  877.0810322761536 [4, 0.07806616636689395, 0.0037095100533800426]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"'Optuna NN6'"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"{'neurons_base': 8,\n 'l2_regularizer': 0.3557163172283674,\n 'l1_regularizer': 0.009506742389685213}"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"1115.5410511493683"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"[8, 0.3557163172283674, 0.009506742389685213]"},"metadata":{}},{"name":"stdout","text":"Time for hyperparameter optimization:  1115.5491471290588 [8, 0.3557163172283674, 0.009506742389685213]\ntotal time for the script:  2654.072046279907\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      350   0.064108  0.020667  0.031485    0.055273  0.033387   0.033121   \n\n  xgbo_train  xgbo_val xgbo_test nn4_train   nn4_val  nn4_test nn6_train  \\\n0   0.057294  0.033951  0.033454  0.058521  0.028758  0.027059  0.058904   \n\n    nn6_val  nn6_test nn4opt_train nn4opt_val nn4opt_test nn6opt_train  \\\n0  0.029009  0.026564     0.059182   0.029553    0.029556     0.056602   \n\n  nn6opt_val nn6opt_test  \n0   0.029238     0.02899  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n      <th>nn4_train</th>\n      <th>nn4_val</th>\n      <th>nn4_test</th>\n      <th>nn6_train</th>\n      <th>nn6_val</th>\n      <th>nn6_test</th>\n      <th>nn4opt_train</th>\n      <th>nn4opt_val</th>\n      <th>nn4opt_test</th>\n      <th>nn6opt_train</th>\n      <th>nn6opt_val</th>\n      <th>nn6opt_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>350</td>\n      <td>0.064108</td>\n      <td>0.020667</td>\n      <td>0.031485</td>\n      <td>0.055273</td>\n      <td>0.033387</td>\n      <td>0.033121</td>\n      <td>0.057294</td>\n      <td>0.033951</td>\n      <td>0.033454</td>\n      <td>0.058521</td>\n      <td>0.028758</td>\n      <td>0.027059</td>\n      <td>0.058904</td>\n      <td>0.029009</td>\n      <td>0.026564</td>\n      <td>0.059182</td>\n      <td>0.029553</td>\n      <td>0.029556</td>\n      <td>0.056602</td>\n      <td>0.029238</td>\n      <td>0.02899</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"iVBORw0KGgoAAAANSUhEUgAAAXMAAAD1CAYAAACiJBXjAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA4f0lEQVR4nO3deXiU5dn38e+syYSsZJkACcEgUXbUoCIoNZCAhBgEd0SflFZLfUSKb6uAj1JRkKrVaivFaltbQWkRSCFQ0LQsLsgiyL4TIJBMQhKyJ7Pd7x8XmRADBEjIOMn5OQ6OhJnJzHnPJL/rus9702mapiGEEMKn6b1dgBBCiOaTMBdCiDZAwlwIIdoACXMhhGgDJMyFEKINMHrjRWtqati1axeRkZEYDAZvlCCEED7H5XJRWFhInz598Pf3b3CfV8J8165djB8/3hsvLYQQPm/BggUkJiY2uM0rYR4ZGekpKDo62hslCCGEz8nPz2f8+PGeDD2XV8K8rrUSHR1NTEyMN0oQQgifdb72tGwAFUKINkDCXAgh2gAJcyGEaAOa7Jnn5eXxq1/9iqKiInQ6Hffffz+PPfYYb731FtnZ2ej1esLDw5kzZw5WqxVN03jllVdYt24d/v7+vPrqq/Tu3bs1lkUIIdqtJmfmBoOB5557jpUrV7Jo0SIWLlzIoUOH+MlPfsLy5cvJzMzkRz/6EX/4wx8AWL9+PTk5OaxZs4ZZs2Yxc+bMq70MQgjR7jUZ5lFRUZ6ZdWBgIPHx8dhsNgIDAz2Pqa6uRqfTAZCdnc2YMWPQ6XQMGDCAsrIyCgoKrlL5Qggh4DJ75rm5uezdu5f+/fsD8OabbzJ06FCWL1/O008/DYDNZmuw73h0dDQ2m63FCp66aDtvZx9ssecTQrQvN9xwg7dLuCouOcwrKyuZPHky06dP98zKf/GLX7Bu3TrS0tL46KOPrlqR59qTV8auk6Wt8lpCCOErLinMHQ4HkydPJi0tjZSUlEb3p6WlsWbNGgCsViv5+fme+/Lz87FarS1ULpiNehwud4s9nxCifdI0jblz5zJ69GjS0tJYuXIlAAUFBYwfP5709HRGjx7Nli1bcLlcPPfcc57H/vWvf/Vu8efR5N4smqYxY8YM4uPjycjI8Nyek5NDt27dANUnj4+PByApKYmPPvqI1NRUvvvuO4KCgoiKimqxgk0GPQ6XXOlOCF/36dZc/rHlRIs+5/2JsYy76dKOKl+zZg379u0jMzOTkpIS7r33XhITE1mxYgVDhgxh0qRJuFwuqqur2bt3LzabjRUrVgBQVlbWonW3hCbDfOvWrWRmZpKQkEB6ejoAU6dOZfHixRw9ehSdTkeXLl349a9/DcDQoUNZt24dycnJWCwWZs+e3aIFmww67DIzF0I009atW0lNTcVgMBAREcHAgQPZuXMnffv2Zfr06TidToYPH07Pnj2JjY3lxIkTzJo1i6FDhzJkyBBvl99Ik2GemJjI/v37G90+dOjQ8z5ep9Px4osvNr+yCzAZ9FTUOq/a8wshWse4m2IueRbdmgYOHMhHH33EunXreO6558jIyGDMmDFkZmbyxRdf8Mknn7Bq1SrmzJnj7VIb8LkjQFWbRWbmQojmSUxMZNWqVbhcLoqLi9myZQv9+vXj5MmTREREcP/993Pfffexe/duiouL0TSNESNGMGXKFPbs2ePt8hvxylkTm8Nk0OFwSs9cCNE8ycnJbNu2jfT0dHQ6Hb/85S+JjIxk6dKlfPDBBxiNRgICApg7dy4FBQVMmzYNt1tNJKdOnerl6hvzwTDX43DLzFwIcWW2bdsGqJbws88+y7PPPtvg/nvuuYd77rmn0c8tXbq0Veq7Uj7XZjFLm0UIIRrxuTA3GfTSZhFCiO/xvTA36mRmLoQQ3+N7YW7Qy37mQgjxPT4X5tIzF0KIxnwuzOVwfiGEaMwnw9zl1nC5JdCFEKKOz4W50aAugiGtFiFEa7jY+c9zc3MZPXp0K1ZzYT4X5maDKlnCXAgh6vngEaB1M3Npswjh07Z/DNta+KI2NzwCAx666ENef/11OnXqxPjx4wF45513MBgMfPPNN5SVleF0Onn66acZPnz4Zb10bW0tM2fOZNeuXZ5rJ996660cPHiQadOm4XA4cLvdvPPOO0RFRTFlyhTy8/Nxu938/Oc/Z9SoUVe82OCLYW5UM3OnzMyFEFdg1KhRzJ492xPmq1at4oMPPuDRRx8lMDCQ4uJiHnjgAYYNG+a5tvGlWLBgAQDLly/n8OHDTJw4kdWrV/PJJ5/w6KOPcvfdd2O323G73axbt46oqCjee+89AMrLy5u9XL4X5mfbLLKvuRA+bsBDTc6ir4ZevXpRVFSEzWajpKSE4OBgIiIimDNnDps3b0av12Oz2Th9+jSRkZGX/Lxbt27lkUceAaB79+507tyZo0ePMmDAAP74xz+Sn59PSkoK3bp1IyEhgblz5/Laa69x5513kpiY2Ozl8uGeubRZhBBXZuTIkaxevZqVK1cyatQoli9fTnFxMUuWLCEzM5OIiAhqa2tb5LXS0tKYN28e/v7+PP7443z99ddcc801LFmyhISEBN566y1+//vfN/t1fC7MTbIBVAjRTKNGjWLlypWsXr2akSNHUl5eTnh4OCaTiY0bN3Ly5MnLfs7ExESWL18OwNGjR8nLyyM+Pp4TJ04QGxvLo48+yrBhw9i/fz82mw2LxUJ6ejoTJ05skfOj+2CbRfWw7E4JcyHElenRoweVlZVERUURFRVFWloakyZNIi0tjT59+niuaXw5Hn74YWbOnElaWhoGg4E5c+ZgNptZtWoVmZmZGI1GIiIieOKJJ9i5cye/+c1v0Ov1GI1GZs6c2exl0mma1ur9itzcXIYNG0Z2djYxMZd32aj/7i8g4y+bWfrz27iha9hVqlAIIX54LpadPtdmkZ65EEI05oNtFumZCyFa1/79+/nVr37V4Daz2cw///lPL1XUmM+Fed3h/LJrohCitVx33XVkZmZ6u4yL8t02i2wAFUIID58Lc5P0zIUQohEfDHPVZnG6ZWYuhBB1mgzzvLw8JkyYwKhRo0hNTeXDDz8EYO7cuYwcOZK0tDSefPJJysrKPD8zf/58kpOTGTFiBBs2bGjRgj2H80ubRQghPJoM87qzf61cuZJFixaxcOFCDh06xODBg1mxYgXLly+nW7duzJ8/H4BDhw6RlZVFVlYW77//Pr/+9a9xuVwtVrDZKG0WIYT4vibDPCoqit69ewMQGBhIfHw8NpuNIUOGYDSqnWEGDBhAfn4+ANnZ2aSmpmI2m4mNjSUuLo4dO3a0WMGya6IQQjR2WT3z3Nxc9u7dS//+/Rvc/umnn3LHHXcAYLPZiI6O9txntVqx2WwtUKpikisNCSFEI5cc5pWVlUyePJnp06cTGBjouX3evHkYDAbuvvvuq1Lg98kpcIUQorFLOmjI4XAwefJk0tLSSElJ8dy+ZMkS1q5dy1//+lfPSdytVqun5QJqpm61WlusYE+bxSk9cyGEqNPkzFzTNGbMmEF8fDwZGRme29evX8/777/PvHnzsFgsntuTkpLIysrCbrdz4sQJcnJy6NevX4sVbNDr0OukzSKEEOdqcma+detWMjMzSUhIID09HYCpU6fy8ssvY7fbPQHfv39/XnrpJXr06MFdd93FqFGjMBgMvPDCCxgMhhYt2mTQS5gLIcQ5mgzzxMRE9u/f3+j2oUOHXvBnJk2axKRJk5pX2UWYDXrpmQshxDl87ghQUBd1lpm5EELU880wN+hwykFDQgjh4aNhLm0WIYQ4l0+Gudmgl8P5hRDiHD4Z5iaDXs5nLoQQ5/DNMDfqZAOoEEKcwzfDXHrmQgjRgM+GuczMhRCino+GuU42gAohxDl8NMxlZi6EEOfy2TCXy8YJIUQ9nwxzs8zMhRCiAZ8Mc+mZCyFEQz4Z5n5GA7XOlrtItBBC+DqfDHOL2UC1XcJcCCHq+GSY+5sM1MgGUCGE8PDRMFd7s7jc0jcXQgjw0TC3mNRl6Goc0moRQgjw1TA3qzCvljAXQgjAR8PcX2bmQgjRgIS5EEK0AT4Z5nU982q77NEihBDg42FeIwcOCSEE4KNh7m9SZcuBQ0IIofhomMveLEIIcS6fDPO6XRNlA6gQQihNhnleXh4TJkxg1KhRpKam8uGHHwKwatUqUlNTuf7669m5c2eDn5k/fz7JycmMGDGCDRs2tHjRctCQEEI0ZGzqAQaDgeeee47evXtTUVHBuHHjGDx4MAkJCbzzzju8+OKLDR5/6NAhsrKyyMrKwmazkZGRwerVqzEYDC1WtKfNIj1zIYQALmFmHhUVRe/evQEIDAwkPj4em81G9+7diY+Pb/T47OxsUlNTMZvNxMbGEhcXx44dO1q0aM+uiQ7ZNVEIIeAye+a5ubns3buX/v37X/AxNpuN6Ohoz/+tVis2m+3KKzwPP+PZvVmkzSKEEMBlhHllZSWTJ09m+vTpBAYGXs2amqTX6/Az6qmVMBdCCOASw9zhcDB58mTS0tJISUm56GOtViv5+fme/9tsNqxWa/OqPA+L2SAzcyGEOKvJMNc0jRkzZhAfH09GRkaTT5iUlERWVhZ2u50TJ06Qk5NDv379WqTYc1lMcrUhIYSo0+TeLFu3biUzM5OEhATS09MBmDp1Kna7nVmzZlFcXMwTTzxBz549+eCDD+jRowd33XUXo0aNwmAw8MILL7Tonix1LCaZmQshRJ0mwzwxMZH9+/ef977k5OTz3j5p0iQmTZrUvMqa4GcyUCN7swghBOCjR4ACWEx6OWhICCHO8t0wlw2gQgjh4bNh7m80yMxcCCHO8t0wl5m5EEJ4+GyYW0wGamTXRCGEAHw8zGVmLoQQis+Gub9JL7smCiHEWT4b5nUzc03TvF2KEEJ4nc+Guf/Zqw3VOmV2LoQQPhvmdec0r5KNoEII4bthHmIxAVBW7fByJUII4X0+G+ahASrMS6rsXq5ECCG8z2fDPMRiBuCMzMyFEMJ3w7xuZl5aJWEuhBC+G+Zne+ZnpM0ihBC+G+Z1G0ClzSKEED4c5kaDniB/I2ekzSKEEL4b5qD65qUyMxdCCB8Pc4tZeuZCCIGvh3mASXrmQgiBj4d5iMUkuyYKIQQ+HuYyMxdCCMW3w/xsz9ztltPgCiHaN98O8wATbg3Ka53eLkUIIbzKx8NcnZ9F+uZCiPauyTDPy8tjwoQJjBo1itTUVD788EMAzpw5Q0ZGBikpKWRkZFBaWgqApmm8/PLLJCcnk5aWxu7du69a8Z5D+qtl90QhRPvWZJgbDAaee+45Vq5cyaJFi1i4cCGHDh3ivffeY9CgQaxZs4ZBgwbx3nvvAbB+/XpycnJYs2YNs2bNYubMmVet+LAOKsyLKiXMhRDtW5NhHhUVRe/evQEIDAwkPj4em81GdnY2Y8aMAWDMmDF8/vnnAJ7bdTodAwYMoKysjIKCgpar+OOH4LMXAOgSGgBAbkl1yz2/EEL4oMvqmefm5rJ371769+9PUVERUVFRAERGRlJUVASAzWYjOjra8zPR0dHYbLaWq7j0BBQeACAqyA8/o57jRZUt9/xCCOGDLjnMKysrmTx5MtOnTycwMLDBfTqdDp1O1+LFnZfRH5w1AOj1OmI7BnC8uKp1XlsIIX6gLinMHQ4HkydPJi0tjZSUFADCw8M97ZOCggI6duwIgNVqJT8/3/Oz+fn5WK3WlqvY6A/OWs9/4zoGcKxIwlwI0b41GeaapjFjxgzi4+PJyMjw3J6UlMSyZcsAWLZsGcOGDWtwu6ZpbN++naCgIE87pkWcMzMH6BquZuaaJgcOCSHaL2NTD9i6dSuZmZkkJCSQnp4OwNSpU3n88ceZMmUKixcvpnPnzrz11lsADB06lHXr1pGcnIzFYmH27NktXLFfwzDvGECV3UVRpZ2IQL+WfS0hhPARTYZ5YmIi+/fvP+99dfucn0un0/Hiiy82v7IL+d7MPC5c7dFyrKhKwlwI0W753hGg3+uZd+2owvx4sezRIoRov3wvzE0NZ+YxYQHodXC0UMJcCNF++V6YG/3BUR/m/iYD10YFsutUmReLEkII7/LBMG+4ARSgT+cQdp0s9VJBQgjhfT4Y5v6gucBVf9rbPl1CKCivpaCs5iI/KIQQbZdvhjk0mJ336RICwE6ZnQsh2qk2Eea9Ogej08Guk9I3F0K0Tz4Y5mf3JT8nzAP9jMRHdGBH7hnv1CSEEF7mg2FeNzOvbXDzTXFhbD1eItcDFUK0S74X5qbGbRaAxG4dOVPl4HBhhReKEkII7/K9MK+bmTsahvnAbuqsjZtzSlq7IiGE8DofDPPGPXOAbuEBRASa2ZJT7IWihBDCu3wwzM/fZtHpdCTGdeTrI0W4pG8uhGhnfDDM62bmtY3uGt2/E3mlNaw/UNjKRQkhhHf5YJhb1Fdn44s4p/SKJiLQj79vPNbKRQkhhHf5YJhfeGZuNup56OZY/ru/QM7VIoRoV3wwzM/fM6/zkyHxRAT68avFO3C43K1YmBBCeI8PhvmFZ+YAIQEmZqX3YU9eGfPXHW7FwoQQwnt8L8xNdT3zC58hcWSfaFL7duLt7ENsPSa7Kgoh2j7fC3PD2Zm54+Knu515d29CA0yMm/c1L2TuQtNkd0UhRNvle2Gu14PBfNGZOUBkkB+fTR3K/9zWjb99fYw5q/ZRbXe1UpFCCNG6fC/ModFFnS8kxGLixbRePJAYy3vrj3D7b/7D6t35rVCgEEK0Lh8O80u7qpBOp+PVcX35xxODiA7x54m/byXpjbV8tPGYHCkqhGgz2nyYgwr0m6/pyJJJg3l5TB9CLSaeX7aLwa/+h+lLd5K91yYtGCGETzN6u4Arcp6LOl8Ks1HPI7fGMf6WrqzZY2PZtpNkbjvJwm+O42fUM/jaCK6LDgKgg9nAuJti6BRiaenqhRCixTUZ5tOmTWPt2rWEh4ezYsUKAPbt28eLL75IVVUVXbp04fXXXycwMBCA+fPns3jxYvR6Pc8//zy33377Vaj60nrmF6LT6RjRO5oRvaOpdbrYfLSEz/faWHeg0HNeF5em8bvsg0wcEk/6gM7odGAy6InrGIDR4JsrNEKItqvJMB87diyPPPIIzz77rOe2GTNm8Oyzz3LzzTezePFi3n//faZMmcKhQ4fIysoiKysLm81GRkYGq1evxmAwtGzVpstrs1yMn9HAkB4RDOkRAeDZhfFEcTW/yz7IH9cd5o/nHHwU5GfkR9dHMbxnFJ1CLMR2tBAd7I9Op2uReoQQ4ko0GeYDBw4kNze3wW05OTkMHDgQgMGDBzNx4kSmTJlCdnY2qampmM1mYmNjiYuLY8eOHdxwww0tXLV/k/uZX6m6UO4aHsAb9/dn8rBr2X7iDAa9jhqHm01Hi8jeW8Dy7055fqZLqIWYMAt7TpURbDExul8n7kuMoXtkIDqdjpNnqvnwqxz255eT3MvKI7fGXZXahRDt1xX1zHv06EF2djbDhw/n3//+N3l5eQDYbDb69+/veZzVasVms7VMpecy+kFV6xzZGRfegbjwDp7/33tTDC63xt68Moor7Rw9XcmGg4XkldZw94DOFJbX8qcNR5i//ggWk4Fgi5HTFXYMOh2dQv15ftkulm07idmox2TQM6xnFDtzSzldUcs1EYGk9e9EsMXE14eL6OBn4Nb48Evq25dWOQi2GGUNQYh26orC/JVXXuGVV17h3XffJSkpCbPZ3NJ1XVwze+bNZdDr6NMlBIA7EiJ57LZuDe4/eaaaDQcKOWCroKLWgTXYnwdv7kp0sD9vfX6ADQdPo3O5OXWmmhcydxPkZyQuIoAvDxXx5y+PNnqtzqH+nC63Ex5o5sGBsYRYTJTXOgn0MxJiMfHN0WIWbT7BnddF8cb9/Qn2N3KmykFeaQ3us22jayI60MGv/uOudbow6fXo9RL+QrQFVxTm3bt3589//jMAR48eZe3atYCaiefn1x+UY7PZsFqtza/y+65wb5bW0iXUwoM3dz3vfc+kXMczKdcBqj+/L7+cuPAAAsxGiivtfHOkiJIqB7fEd8Tp0liyLZfckmqig/05YCvn9TUHGj2nQa9j2PVRZO8r4IaX1hBgNlJR62zwmIhAP27oGsqeU2VEBJrZk1eGTqcjJtRC7y4h3HldJPllNeScriSsg5kQiwm9Tofd6WbjkSKC/I3c0DWMO6+LIibMQlGFndiOFs+aQK3TRX5pDdZgf/xNl7aNpKC8hk+3nmRoQiS9OgdfzlsshPieKwrzoqIiwsPDcbvdzJs3jwcffBCApKQknnnmGTIyMrDZbOTk5NCvX78WLRgAcyDUlrX887YynU5Hz071Idaxg5m7+nZq8Jhpd/Vs8P+c05VYzAZCLCbKahyUVTvoFGKhg5+R706cIXuvjbIaJzFhFrqEWjAa9Nidbv785VG2HT/DLdd0pLCilv+5rRs6nY4TxVV8cbDQsw0gItBMWbUT+zmnD+7dOZj8shpW77bx6qp9nttjwiz4GfUkWIPYebKU3JJqjHodA2JDCfAzYg3yo4OfEbvLTViAiRqHm2NFVQT5G+ke2YFPvz3J0dOVzP33Ph5IjOX+gTGUVjvQoaOo0u65ffC1ERSU1+Bn1KPT6SivcTKoezhdQi2sO1DIkm9zKa120LVjAGn9O/P14SJqHC4evyOejUeKuTaqA90jA6modXL0dCX+JgMJ1qDzfiaapqHT6XC43LyQuYvjxVW8O/4mQiymZn/eQlxNOq2JM1BNnTqVTZs2UVJSQnh4OE899RRVVVUsXLgQgOTkZJ555hnPDG3evHl8+umnGAwGpk+fztChQxs9Z25uLsOGDSM7O5uYmJjLr/rzX8NXb8P/nQbpETdbrdPFkcJKYsIsBPmbcLrcON0aTreGw+kmrINqo+WVVrN2fyGF5bUE+RvZeKQIgG+PnyEy0I+Hb+lKbkk1m3OKcZxtI9U63JiNekqq7PgZDcSEWaisdXKqtIYgPyNvP3wD3xwpbrDHUJ3OIf6cKj3/GpheB9HB6v6IQD+swX7knK6k8pyDv4x6Hc6zR/mGBpioqHF6/j/s+iiqHS5MBj0BZgNRQX4Y9HoWbjqGn9GAn1FPQXktBr2O6GB/OvgZ6NkpmMhAPwx6HXq9jg5mA90iOuDWIDbMQrDFRM7pSrafOMORwkqGXhfJmSo71XY3XcMtdOzgR4jFxL+2nyIq2I9h10dhMRvYl1eOQa/DZNBjNuopq3ZgNOi4vUckBr0Ol1vD7nTjb9J7Bppqh4vjRVWU1ThwujTOVDsorbIzul9n1h4o4NtjZzAb9WpPrWsjqLK7CPIzetpqRRW1fPptLtZgf5J7WQkwGz0D2dWwOaeYWofbs9fYhVTUOgkwGaT9dwEXy84mw7y1C7okX70Da56H546Df0jLFyiuuspaJzodBJjVyuGuk2ojcGiAGbvTTXFlLcN6Wvn6cBEOl5u48A7YnW5cbg2TUcfKnfnkllSRYA0iY3A3/IwGKmqdLN12km7hARRV2FmzJ58HBnbFVlrDt8dLCAkwcVPXMHaeLGXBN8eJDbPg0jRqHW5yS6qpdrhI69+ZsAATRZV2kq6LIjTAxAdfHMXfZGD3qVLKzw4I7rOD3fkY9Do6djBTWH7+7Tomgw6Hq+k/u4hAM2EBZo4VV2F3uukSaqFbRABfHS7iQn+1deEfdHaNqNbp9gxq/iY9N3YNI7FbR1bsOMWRwkpADZpGg56iilr6xYTSLyaEtfsLuaFrKP1iQjlRUsX+/HJOnammf0woUcF+/HtXPqXVDtL6dyYqyI/8shoqatSaT4CfkeE9o9iSU0L/2FCy99r46nAReh3Me+QmBnbryKdbcwn0NxITZsGtwZHCCv69K59NOcWEd1AtwYhAP2LCLAzs1pFKu5PenYIpqrSzalc+ZdUOMgZ3IzrEn/355Ww4eJrC8loiAs2s3V/I3QM6k96/CwXlNVTUOomPCOTI6Qq6hFkor3FSXGnH7nRTbXfRNyaE/NIaCsprKa9xUF7jxK1pjOyjLkNp1OsaDXKapvHt8RI2Hilmb14ZJVV2nrzzWm7r3niwKq128Maa/ViD/enVOZjiCjvjbrqC3KMthvn2hbBsEkzeBh3jW75A0e5U212UVjuIDvG/5J+psjvJOV2F0aDjSGEFNQ43nUMt9OkSjMVk4NvjZ7AG+xHewY+80moKy2s5eaaaOxIiqba7+OZoMZW1TvrGhKADHC41A+/gZ8BWVsOaPTYqapzEhQcQGmDmy0OnOXmmmpG9owkPNBMbFkBYBzNGvY5AfyM1DjeLNh/n9h6R3NUnmlqnmw0HT/PNkSIizwbul4dOc7CgghCLiXfH3wgavPn5AfyMBq6J6MBXh09zuLCS/rGh7M0rw+50YzLo6B4ZSFSwP9uOl1Be46Rnp2Cig/3YcPA0TrdGgNlAsL+JLmEWjhVVcrrCTpC/kfIaJxGBfvxsaDz/+u4UO3IvfDnHBGsgw3taOV5cxaGCCgrLaymqtHvurxsE9TowGvQ4XO4Gg5qfUU/t2UHv5JnG1wi+UpFBfkQE+qFpGt2jAjldXsvR05UUnB2sY8IsuN0ap0prMBv0oIPwDmaujw4iwM/IxsNFlFTZqRv7e3UKJmvykCtaC7pYdvrm4fyWjuprVQl09G4pom2wmA1YzJd3cFuA2ejZcHu+HvxNcWGe7+MjA4mPDGxwf2zHgIs+/8g+DbefPHnntU3WNCA21PO9v8lAci8ryb0a7oRQF4JmozqS+bZr62eTmqZRXusk2N9EabWDaruLjh3MnscCuNwahrNtELdbo9Ku9qyqC6eKWifHiirpGR3MkdOVdAm1YDEbGHdjDEu2naSs2sHofp0wGfQUVtSi10FUkP9534/iSjtbj5XQwc/A2v2FdDAbeey2OOxONws3HUevUwNNYrcwwgLMFFbU0jnEn8/3FnC8uIqwABP+JgNHT1fSPbIDuSXVhFhMRIf4Y9TrMRp0fHfiDF1CLXQJsxDsb/IMQqt25WN3uskpqqSs2oFL09iZW4o12I8h10Zwa3w4I/pEE2IxUWV38tHGYxRXOtA0DVtZDQdsFZTVVHBrfDhPDI2nyu6iqMJOci/rVWln+ebM/MRm+GA4PPxPSEhp+QKFEOIH6GLZ6ZsnGQk4Ox2vlkvCCSEE+HqYt9JRoEII8UPnm2HuFwI6PVQVebsSIYT4QfDNMNfr1UZQabMIIQTgq2EOqtUiM3MhhAB8OszDpWcuhBBn+W6YWzpKmAshxFm+G+YB0jMXQog6vh3mVUVc8CQVQgjRjvhwmIeDyw72Cm9XIoQQXue7YR509rwVZXnerUMIIX4AfDfMQ2LV19Lj3q1DCCF+AHw3zEPPhvkZCXMhhPDdMA/qBHojnDnh7UqEEMLrfDfM9QYI7gKlEuZCCOG7YQ4Q2lXaLEIIQZsIc5mZCyGEb4d5SCyU54HT3vRjhRCiDfPtMA+NBTQoy/V2JUII4VU+HuZd1VdptQgh2jnfDvMQ2ddcCCHA18M8uIu6fJzsniiEaOd8O8yNZnXwkLRZhBDtXJNhPm3aNAYNGsTo0aM9t+3du5f777+f9PR0xo4dy44dOwDQNI2XX36Z5ORk0tLS2L1799WrvE5IrLRZhBDtXpNhPnbsWN5///0Gt7322ms8+eSTZGZm8vTTT/Paa68BsH79enJyclizZg2zZs1i5syZV6XoBkK7ysm2hBDtXpNhPnDgQEJCQhrcptPpqKysBKC8vJyoqCgAsrOzGTNmDDqdjgEDBlBWVkZBQcFVKPscobFQehJczqv7OkII8QNmvJIfmj59OhMnTmTu3Lm43W4++eQTAGw2G9HR0Z7HRUdHY7PZPGF/VYTEguZSBw/VnUlRCCHamSvaAPrxxx8zbdo01q1bx7Rp05gxY0ZL13Xp6vY1lz1ahBDt2BWF+dKlS0lJSQHgrrvu8mwAtVqt5Ofnex6Xn5+P1WptgTIvwnPgkPTNhRDt1xWFeVRUFJs2bQJg48aNdOvWDYCkpCSWLVuGpmls376doKCgq9tiAQiJUec1P7X96r6OEEL8gDXZM586dSqbNm2ipKSEO+64g6eeeopZs2Yxe/ZsnE4nfn5+vPTSSwAMHTqUdevWkZycjMViYfbs2Vd9ATBZoFc6bF8ISc+DX+DVf00hhPiBaTLMf/vb35739iVLljS6TafT8eKLLza/qst1yyTY9Sl89zHc/NPWf30hhPAy3z4CtE7sQOhyE2ycB263t6sRQohW1zbCHODWn0PxYTj0ubcrEUKIVtd2wrxXujpPy39fgYK93q5GCCFaVdsJc4MJkl+C0wdg/h1QeMDbFQkhRKtpO2EO0O9+eGorGMxqhi6EEO1E2wpzgODOqn++Zxnk7/J2NUII0SraXpgDDPq5mp1v+/ulPd7tgt3LwFFzVcsSQlyEo9rbFfi0thnmljC47i7YuRhcjvrb3S61t8v3Q3vju/DPx2DX4vM/X/FRsFdevXqFaO+KDsOcGDi1zduV+Ky2GeYA/R6EqtPw3o/gk/Gw/WP4x6Pw0Tj4+IH6cC46DP95WX1/fGPj57FXwR+HwH9b4WhWIdqr/B3gdoJtj7cr8VltN8yvHQ6xt6jD/Y9vhGU/g31Z0P8hOLoe1qsLarD1L2rG3vlGyN3c+HmOrgN7BRzKbt36m6v4KPzhVjj2lbcraTsKD4Cz1ttV/LDYq9REx17VvOcpOaa+luc1v6Z2qu2GudEME9fATz6HqXvhyc3wi11wzx8h/k7Ykwmapr7G/wiuHwWF+6C6pOHzHFitvhbuhYoLXGhj30pY8vgP6+jTg2tUzZ88DMVHvF2N76stV2toX73j7Up+WA5nw7q5cOiz5j1P3VlPy/Mv/jhxQW03zM9lNENkgjrDIkDPNBVw332sfol6patZPKhZe3UJ5HwJX78LB/4NoXHqvpwNajXwy9+pjTXLfg57V8A382DHIvVvwxtX/gt5+D/w0b1QktPsReb4RgiIUNsHvv5D85+vNVWehv/O+WHNgosOgatWramJenUz6ubuOeYJc5mZX6krutKQz7s+FVb8AlY9CzqD+r/BDHqT6qt/X9rbsHoGrH4eKmzqykY7/gG2XXDim/pf6GU/U1/zd8KwF9RXvyAVqtbeoDfUP6ejBvatgG5DIOjs1Zk2va9mOO8Ph3v/DJ/PVLXd/sz5l+Ob+Woj72P/Uu2kc534Bq65A1x2teYw6nXQ6aC24uJnljxzAgKtagC8HJoGmrvhMl6pr/8AX/wWovtCz9FNP/5y5G5V7bRbnlDvx6UqOqy+ntgMTvulvz9LfwYdIiDl5cuv1RecOfu7b2tumNe1WWRmfqXaZ5gHRqmQzN+h/sgCOqrbJywB224VTCFd1Mm7io9C3G2qBXNkLfQdB1XFsH0B+IeqGRvAwJ/Ctx9C10GweynsXwXOc/aaCQiHu3+vzr3urIGTW9QMX6eH7klw5ww18+8+TL3Wh2nq58rzVT9/y59hzLz6IN69FFb9Sn1/8DMoOwXd74TI69Qsp+wkdL0V/ILVoHFqm1rrWPsqPLRQ7Yp502NqGeuUnYLfJ6ozTwZa1Z4/Dy4Ec4eLv5+5W2HlM2qN5mdfqAHsSrmc6nTGAAdWXTzMS3PV7D28e/1tR9aqQdTaW72v5/ruE8j8X3A7oPMA9f5cqrowd1ZD3nfQqb862ji6z4V/xlkLu5aAOQCG//rKBzrbHnURlh/i6Z0vZ2buqIGCPdDlxoa3a1r9zLzC1rL1NcXthp3/VG3W5vze/gC0zzAHeHBB49uuuUP9O1dda2bknPrb7FUQdg30uhvmD1Wz+pFz1MDgdsK7g9QfX/JLamZcmgtfvwP/mKDuBxXivdIhvAds/Sv8bQzUlsGNEyC6H2Q+qWbsu5fCpz9Re+ZYwuDut+HEJljyhGoNFR1Saxjlp9SAEdSp/tw0XW9V10jVGVSv9+AaFUZ/v0fdX5qrBjC3S+3C+c18NdBs+ata+3BUwb+egnEf1M9izxyHcps6U6XbpQa1FVPVgFhhg/Wvw7AX1e0V+WqwiL9TvQ96o1rWnA1qgBo8ueF77XKqDdIV+RAYrbZX/G1Mfeju+ReM+YPaGN3tdlicATVlcOc0tfZhCVUDV51B/wtJ/wcmfzj+jQryrreqMN7yl8sL8+LD4BcCtaVw7EvYvxK+eFMNXucL9Kri+tZMda26eErMTY0fdy5NU89r6Qhxg1Qrb1+W+vxvnAB3/wD79XUz6tLjUH1GfQYX8tXbsHYOTNlZ/3cFaluUs0a9v+X5KmD1LdQBdrvUc19oQrI3E5Y+DsNnwpBftMxrekn7DfPmMAfA0F+q73/0rPojNJjUPzh7SgFTw9X4a4fBogkQdT3UlMLRDTDqDQiMhNibYeH9gA6uGaqC8cf/hsoitYG26rRqOXz7IRj9VW8+pAs8+DH8Z5YKwIjr1O6W9gq44RH1C2zto2aDNz2mZvY6A4z9kwqhoGjVo//0p2r/ep1BPdbaF2w71WBz42PqNXumqT+KqmL1x1hbDg99otYMig+rYH3g76oV9dU7akZ/7mq3f6gaqIJj1B+9paMapPyDVWhvfFetmaybqzaoRSTA4CmQ+XM48l/1D1SN7/2o/nn1JhUeq6erQStvO9z8OAx9DtbOhq9/r9ZAgjvDqW/VYx74O/znFfj2b9D3PrXWZQ6AY1+rgSb992o5F41Xg2fqG2pgLjqkZvMVBeo9qTwNaGp5U2apNSB7hRoUy07BB8Mhtm6w0Kn3pC7MNU1tb/EPhrvmqtvcbjXY71sB5iC1dvTl79SgqjOo34NRb1x6e6dwv5o4hHZVg3T+DvUeO6pV7T2G1z921xI1cCeMVC2hOpqmJg65m9V7ccP4hq9RN6OOuA5O71efebchF65pT6ZqxR1Z1/C56mblMYnq8z+To34vzAENf37XEvU+n1t7U1bPUJ/X7VPBHAgDxqv3HdR7vv519f2+lU2HefER1TYrzVUD67XDLv74cpsawG75WatcbF7CvLnO9wtwvj+4gI6QkVX/f5cTDGff/h4paldKR019ywegQzh0vU39oWSsguVTYNN8tVYwYam6f8DDsO0jGPWamqkbTI1X50e/CdeNUgHfM02dw6YkB37XH3b+Q+2T7x8Mh/8L98xToRrUCUbMgZNb1Z46Lrt6rpCuKiQWjFN/4Pf+BXrerZZl5Bz1B3P4PzByLtz4qJqhn9oOAWHq653Toe+9qo20/On6Gk9uVWGY9H9w22QVLjvugMSJqv/vqILEH8NXv4d+D6ije7snqYHwULYKP4O5fgBNfUMt69fvqucd9KRqhVnC4NZJsPdfahkAbpigXuP0AbD2gq0fnt0Qp4O3+kLcEDh9SNU99Feqds2tBt4di2DHJ2AKUG0V/xA149fccPwr9VlZwtSA2eVGNSjUlMJ3Z1tJ19yhWn67l6ggH/S/aq3hi9/CtcmQMEINhkt+oga1+B+plkZ0HzD61b9/Jzarn0l5Wc1uF9wHaGpAz92s3hf/ULVWVHxELVPHa9R7tPjH6rGmABV6tz6pBtLcrWrw9fw+RqiJwNBnVUuv8rT6XK5PhS/2q5DuNkR9Hv+dDdfcrtp7A38KUT3rB/ij678X5mdn97G3qDD/UxLEDa5fey7LUxds/3SiqjFllhqMuw2B0pPq/e2epJZh30q1RhwYqQbVLR+oWXndsSSOarjpf9T3xzeqmqx91XtUUaAGi6rTDdcc6mx4A/J2qAnE5y+q1yw6rHaS6D2m4c9oGiyfrO7bsQge+Ei1XitsaiCom/i1IJ2maVqLP2sTcnNzGTZsGNnZ2cTEnOdNa49cDvUL8P2BoDRXtXUiE9RMYs9SNRMOPOfaqo7qxhtAL8W/p6ufS3r+whsDj32t2jK3PQX9H1Qhv+0jWPM8jP+HCpcrYa9Uf/Quu+rP/+1u1ZL58ZqWW8W+GGet2tZw6DM1IwfVpqoqUoPCY8vVsn73Cax7VYXziNlqUNj+sVojSfyxmql1vVVtL9DpYdN76rEdu6vH9H9IDdSZTzbchhJ7qxpkKgrgf7Jg4X1qRv7EehXqh/+jZu1GP7XB9fVr1YZ0t0PNZP1DYdz76j3L2aAG+upi1aqwl6vXdzvUoH33OypAdy9VA3GPEer3qviwqqlDpAqbr95Rr919mArVXunq8+2RAn+8/eyah139fM/RKpz2r1Rrad99DAfWwP9uVjsR2Hapx/oFq9fodrt6zpibVTBP3asGT79gtRa181N4+JP6bUV6E/zykBpkF96vbgu0qvfZZVc1V56G4C5qra+2rP69De+hBvnCfWoC8dRW9fu9+Mfqc3fZ1d+MJVS16e7/G7w3VIX88W/U+/LwIjBaoFM/tVZVmgvZL6m/ga6DYOkTauKx9a9qefQmeHSZev7QrpC7Re0QccskOLi64e7BN0xQa4BX4GLZKWEumna+vTfsVY1Xg5sjdwuEdWu4mt8aNE21qips0DNdHR08+s362RuoWd3612D8YuiRfPHnW/EL1dJ6fK0aCPqMU8FSWaRaPREJai0kbrAK3/eHq4FNp4NHlqgZ7/ls+TN8+3c10Ax4GDb9SYWO3qiCLNAKo9+CDa+r4Bz8tApR225V84HVKhRNHeD/HVAbU49vVP345Jegz1g1oZg3WK0J9kiB8f+sf/11v1FnIu0QqdpQBrPa/gLw841qjez3A1VLq/iw2nuq731qYPv7PSrcE0aqgW3FFOh7v7rUY3h3tSZxfapaa3urr1omt1M9xxdvqtl411tV+zB3i5rt3v83dbvJX63lHlyj1niCotXAWTcx6jNO7VkGai+p1dMbvq9pv1PtxI/GqgHUEqb+1YVvQISaqdeZ9JVqK30wXK11mALUnmerZ6jH1ZSqAdVlV2tijy1Xt2VNhajeaka+f6U6BuYKSJgLcanOtxHP5VSzq4SRTe+R4qiGk99Ct8GX9noHVquBIvkl1b+/VOU2FZKhXVVwd+p/8cHV5YA3+8B1I1WAXUjOF/CvyWrbgrV3/e32KtUqsvZWbRC9UbWZjn0FvzqiXnvfSljyU/Ue/WJP/d43mqb+6fWq7o8fVHscxQxUrSiARzPVjPfVOEjMUP31spNqbefHa9QG9+YqPQlv9oJOA9TgtW+lCtu6icqZE6rdWFOqLkFp7a0G0ZiB6lxP5Tbo/4B6rNulBkO/IDV7z90Cfx6pHmfbrQbSx9c2XINuARLmQgg1ozZ3aNhrvxILH1Ab5Ic+p/aiCu1af1/JMbWmYe118eeo22Plk/Fq1v7Ut2oQKDwAYXFq4/WOf6htQd/fw6w5ti9UYd5UfVeiukS1v5w1ambuH9LiL3Gx7JQNoEK0F+duXG+OhxfVf39ukIMK4ktRt13k3r+o8Ktb44lMUF9vf+bCB8s1x4CHW/4561jC1FeT5cq2YTWThLkQwnuM5ss/2licV/s4N4sQQrRxEuZCCNEGNNlmmTZtGmvXriU8PJwVK9Sh0lOmTOHo0aMAlJeXExQURGZmJgDz589n8eLF6PV6nn/+eW6//farWL4QQgi4hDAfO3YsjzzyCM8++6zntrfeesvz/auvvkpgoNoF6dChQ2RlZZGVlYXNZiMjI4PVq1djMLTAmfSEEEJcUJNtloEDBxIScv5dbDRNY9WqVYwerc5sl52dTWpqKmazmdjYWOLi4tixY0fLViyEEKKRZvXMt2zZQnh4ON26dQPAZrMRHR3tud9qtWKztfIpLYUQoh1q1q6JK1as8MzKL4fL5QIgP19ORC+EEJeqLjPrMvRcVxzmTqeTzz77jCVLlnhus1qtDQLaZrNhtVob/WxhYSEA48ePb3SfEEKIiyssLCQuruEBWlcc5l999RXx8fEN2ipJSUk888wzZGRkYLPZyMnJoV+/fo1+tk+fPixYsIDIyEjZOCqEEJfI5XJRWFhInz6NL4jSZJhPnTqVTZs2UVJSwh133MFTTz3Ffffdx8qVK0lNTW3w2B49enDXXXcxatQoDAYDL7zwwnnD2t/fn8TExGYskhBCtE/fn5HX8cqJtoQQQrQsnzoCdP369YwYMYLk5GTee+89b5dz1SQlJZGWlkZ6ejpjx44F4MyZM2RkZJCSkkJGRgalpaVervLKTZs2jUGDBjXYeH6h5dM0jZdffpnk5GTS0tLYvXu3t8q+Yudb3nfeeYfbb7+d9PR00tPTWbdunee++fPnk5yczIgRI9iwYYM3Sm6WvLw8JkyYwKhRo0hNTeXDDz8E2u5nfKHlbfXPWPMRTqdTGzZsmHb8+HGttrZWS0tL0w4ePOjtsq6KO++8UysqKmpw29y5c7X58+drmqZp8+fP137zm994o7QWsWnTJm3Xrl1aamqq57YLLd/atWu1iRMnam63W9u2bZt27733eqXm5jjf8r799tva+++/3+ixBw8e1NLS0rTa2lrt+PHj2rBhwzSn09ma5TabzWbTdu3apWmappWXl2spKSnawYMH2+xnfKHlbe3P2Gdm5jt27CAuLo7Y2FjMZjOpqalkZ2d7u6xWk52dzZgxYwAYM2YMn3/+uXcLaobzHYh2oeWru12n0zFgwADKysooKCho7ZKb5WIH3n1fWzjwLioqit691YUtAgMDiY+Px2aztdnP+ELLeyFX6zP2mTBvbwckTZw4kbFjx7JokTp3dFFREVFR6qolkZGRFBUVebO8Fneh5fv+5x4dHd1mPvcFCxaQlpbGtGnTPC2HtvZ7npuby969e+nfv3+7+IzPXV5o3c/YZ8K8Pfn4449ZunQpf/rTn1iwYAGbN29ucL9Op0N3oQswtwFtffkAHnroIT777DMyMzOJiori1Vdf9XZJLa6yspLJkyczffp0z/mb6rTFz/j7y9van7HPhPmlHpDUFtQtV3h4OMnJyezYsYPw8HDPqmdBQQEdO7bQVWN+IC60fN//3PPz89vE5x4REYHBYECv13Pfffexc+dOoO38njscDiZPnkxaWhopKSlA2/6Mz7e8rf0Z+0yY9+3bl5ycHE6cOIHdbicrK4ukpCRvl9XiqqqqqKio8Hz/5Zdf0qNHD5KSkli2bBkAy5YtY9iwYV6ssuVdaPnqbtc0je3btxMUFORZVfdl5/aEP//8c3r06AGo5c3KysJut3PixIkLHnj3Q6ZpGjNmzCA+Pp6MjAzP7W31M77Q8rb2Z+xT+5mvW7eO2bNn43K5GDduHJMmTfJ2SS3uxIkTPPnkk4A62mv06NFMmjSJkpISpkyZQl5eHp07d+att94iNDTUu8VeoXMPRAsPD+epp55i+PDh510+TdN46aWX2LBhAxaLhdmzZ9O3b19vL8JlOd/ybtq0iX379gHQpUsXXnrpJU+AzZs3j08//RSDwcD06dMZOnSoN8u/bFu2bGH8+PEkJCSgP3utz6lTp9KvX782+RlfaHlXrFjRqp+xT4W5EEKI8/OZNosQQogLkzAXQog2QMJcCCHaAAlzIYRoAyTMhRCiDZAwF0KINkDCXAgh2gAJcyGEaAP+P2L7tL14mVWWAAAAAElFTkSuQmCC\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"### Feature Importance\n\ntime2 = time.time()\n\nexplainerxgbc = shap.TreeExplainer(optuna_xgb)\nshap_values_XGBoost_test = explainerxgbc.shap_values(X_test)\nvals = np.abs(shap_values_XGBoost_test).mean(0)\nfeature_names = X_test.columns\nfeature_importance_xgb = pd.DataFrame(list(zip(feature_names, vals)),\n                                 columns=['col','feature_importance_vals'])\n\n#select backgroud for shap\nbackground = X_train.sample(1000)\nexplainernn = shap.KernelExplainer(optuna_nn, background.to_numpy())\nshap_values_XGBoost_test = explainernn.shap_values(X_test.sample(5))\nvals = np.abs(shap_values_XGBoost_test).mean(0)\nfeature_names = X_test.columns\nfeature_importance_nn = pd.DataFrame(list(zip(feature_names, vals)),\n                                 columns=['col','feature_importance_vals'])\nfeature_importance_xgb.sort_values('feature_importance_vals', inplace=True, ascending=False)\n#feature_importance_nn.sort_values('feature_importance_vals', inplace=True, ascending=False)\ndisplay(feature_importance_xgb[:20])\ndisplay(feature_importance_nn[:20])\ndisplay('time to run feature importances: ', time.time()-time2)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:57:17.885064Z","iopub.execute_input":"2022-09-27T22:57:17.886304Z","iopub.status.idle":"2022-09-27T23:20:01.857333Z","shell.execute_reply.started":"2022-09-27T22:57:17.886248Z","shell.execute_reply":"2022-09-27T23:20:01.856413Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"589664e7bcb74839847e7f42d8176b39"}},"metadata":{}},{"name":"stderr","text":"2022-09-27 22:57:21.993676: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 1525984000 exceeds 10% of free system memory.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"              col  feature_importance_vals\n66  cat__ind_30.0                 0.617462\n71  cat__ind_35.0                 0.391643\n6      num__mom11                 0.297900\n3         num__op                 0.291486\n58  cat__ind_22.0                 0.225409\n0     num__mom482                 0.203109\n7     num__mom122                 0.193938\n38   cat__ind_2.0                 0.169298\n79  cat__ind_43.0                 0.165351\n12       num__MAX                 0.164782\n73  cat__ind_37.0                 0.157832\n50  cat__ind_14.0                 0.131925\n14     num__vol6m                 0.131266\n16      num__size                 0.118386\n49  cat__ind_13.0                 0.100054\n74  cat__ind_38.0                 0.096747\n4         num__gp                 0.083551\n44   cat__ind_8.0                 0.082448\n68  cat__ind_32.0                 0.079308\n72  cat__ind_36.0                 0.073116","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>col</th>\n      <th>feature_importance_vals</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>66</th>\n      <td>cat__ind_30.0</td>\n      <td>0.617462</td>\n    </tr>\n    <tr>\n      <th>71</th>\n      <td>cat__ind_35.0</td>\n      <td>0.391643</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>num__mom11</td>\n      <td>0.297900</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>num__op</td>\n      <td>0.291486</td>\n    </tr>\n    <tr>\n      <th>58</th>\n      <td>cat__ind_22.0</td>\n      <td>0.225409</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>num__mom482</td>\n      <td>0.203109</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>num__mom122</td>\n      <td>0.193938</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>cat__ind_2.0</td>\n      <td>0.169298</td>\n    </tr>\n    <tr>\n      <th>79</th>\n      <td>cat__ind_43.0</td>\n      <td>0.165351</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>num__MAX</td>\n      <td>0.164782</td>\n    </tr>\n    <tr>\n      <th>73</th>\n      <td>cat__ind_37.0</td>\n      <td>0.157832</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>cat__ind_14.0</td>\n      <td>0.131925</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>num__vol6m</td>\n      <td>0.131266</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>num__size</td>\n      <td>0.118386</td>\n    </tr>\n    <tr>\n      <th>49</th>\n      <td>cat__ind_13.0</td>\n      <td>0.100054</td>\n    </tr>\n    <tr>\n      <th>74</th>\n      <td>cat__ind_38.0</td>\n      <td>0.096747</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>num__gp</td>\n      <td>0.083551</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>cat__ind_8.0</td>\n      <td>0.082448</td>\n    </tr>\n    <tr>\n      <th>68</th>\n      <td>cat__ind_32.0</td>\n      <td>0.079308</td>\n    </tr>\n    <tr>\n      <th>72</th>\n      <td>cat__ind_36.0</td>\n      <td>0.073116</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"               col                            feature_importance_vals\n0      num__mom482  [0.011832463073825746, 0.09242531214220752, 0....\n1      num__mom242  [0.013493306164459873, 0.05488467211784415, 0....\n2          num__bm  [0.004441171060644233, 0.05653844940001573, 0....\n3          num__op  [0.0, 0.04699088928965286, 0.008677883241081, ...\n4          num__gp  [0.021043406374612193, 0.2626254603460431, 0.0...\n5         num__inv  [0.025771762673275696, 0.18068655672340064, 0....\n6       num__mom11  [0.013727381091462893, 0.0492636025586608, 0.0...\n7      num__mom122  [0.026475316916828584, 0.024614796853129395, 0...\n8        num__amhd  [0.0, 1.209607692907648, 0.09317164750019735, ...\n9   num__ivol_capm  [0.01797362373768565, 0.24860689004194458, 0.0...\n10   num__ivol_ff5  [0.01213839133143102, 0.04336749659391148, 0.0...\n11    num__beta_bw  [0.011992610617754229, 0.1705775568844506, 0.0...\n12        num__MAX  [0.0, 0.7204038756125922, 0.03803748992961358,...\n13      num__vol1m  [0.01128650204904081, 0.0830833622290526, 0.00...\n14      num__vol6m  [0.0647604524960391, 0.20840619766479673, 0.00...\n15     num__vol12m  [0.25831394342483965, 0.2457307712067957, 0.00...\n16       num__size  [0.11153267734161015, 0.7326280147433207, 0.01...\n17        num__lbm  [0.05223147617526957, 0.1912399990556465, 0.05...\n18        num__lop  [0.0024837838486708175, 0.3073017658972682, 0....\n19        num__lgp  [0.008112733823157442, 0.07499494614126351, 0....","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>col</th>\n      <th>feature_importance_vals</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>num__mom482</td>\n      <td>[0.011832463073825746, 0.09242531214220752, 0....</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>num__mom242</td>\n      <td>[0.013493306164459873, 0.05488467211784415, 0....</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>num__bm</td>\n      <td>[0.004441171060644233, 0.05653844940001573, 0....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>num__op</td>\n      <td>[0.0, 0.04699088928965286, 0.008677883241081, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>num__gp</td>\n      <td>[0.021043406374612193, 0.2626254603460431, 0.0...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>num__inv</td>\n      <td>[0.025771762673275696, 0.18068655672340064, 0....</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>num__mom11</td>\n      <td>[0.013727381091462893, 0.0492636025586608, 0.0...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>num__mom122</td>\n      <td>[0.026475316916828584, 0.024614796853129395, 0...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>num__amhd</td>\n      <td>[0.0, 1.209607692907648, 0.09317164750019735, ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>num__ivol_capm</td>\n      <td>[0.01797362373768565, 0.24860689004194458, 0.0...</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>num__ivol_ff5</td>\n      <td>[0.01213839133143102, 0.04336749659391148, 0.0...</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>num__beta_bw</td>\n      <td>[0.011992610617754229, 0.1705775568844506, 0.0...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>num__MAX</td>\n      <td>[0.0, 0.7204038756125922, 0.03803748992961358,...</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>num__vol1m</td>\n      <td>[0.01128650204904081, 0.0830833622290526, 0.00...</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>num__vol6m</td>\n      <td>[0.0647604524960391, 0.20840619766479673, 0.00...</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>num__vol12m</td>\n      <td>[0.25831394342483965, 0.2457307712067957, 0.00...</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>num__size</td>\n      <td>[0.11153267734161015, 0.7326280147433207, 0.01...</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>num__lbm</td>\n      <td>[0.05223147617526957, 0.1912399990556465, 0.05...</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>num__lop</td>\n      <td>[0.0024837838486708175, 0.3073017658972682, 0....</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>num__lgp</td>\n      <td>[0.008112733823157442, 0.07499494614126351, 0....</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"feature_importance_nn.feature_importance_vals[0]\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T23:30:23.806557Z","iopub.execute_input":"2022-09-27T23:30:23.806929Z","iopub.status.idle":"2022-09-27T23:30:23.816081Z","shell.execute_reply.started":"2022-09-27T23:30:23.806900Z","shell.execute_reply":"2022-09-27T23:30:23.814842Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"array([1.18324631e-02, 9.24253121e-02, 1.16093679e-02, 8.40588579e-02,\n       1.56240708e-01, 8.71808915e-02, 2.80146355e-01, 1.52034339e-01,\n       3.84674127e+00, 2.63318337e-01, 8.41417157e-02, 2.78733442e-01,\n       4.99483138e-02, 3.45230348e-02, 2.02392049e-01, 3.69606936e-02,\n       1.55856167e+00, 3.13272706e-02, 1.28423582e-02, 1.25806189e-01,\n       1.87101193e-02, 1.54464371e+00, 3.01976316e+00, 4.35279952e-02,\n       1.18532256e-01, 8.23695233e-02, 5.76863033e-01, 9.07727155e-02,\n       4.26607779e-01, 5.69423508e-02, 1.31785110e-01, 1.24522349e-01,\n       9.84191288e-02, 1.12464454e-01, 3.92594908e-02, 1.16633601e-02,\n       4.95606509e-02, 1.78861055e-02, 5.25925074e-02, 0.00000000e+00,\n       3.48912959e-02, 3.19259524e-02, 3.30912737e-02, 7.85954460e-03,\n       4.72814141e-02, 3.20189202e-02, 3.16535868e-02, 1.01684087e-02,\n       5.20195001e-02, 6.09210402e-02, 6.58665323e-02, 4.62971598e-03,\n       4.63211611e-02, 3.58823517e-02, 4.97338440e-02, 4.84155065e-02,\n       7.79511170e-03, 8.13638076e-02, 3.11116369e+00, 1.39940441e-02,\n       4.94765651e-03, 6.46831006e-03, 0.00000000e+00, 3.39840344e-02,\n       4.87138635e-03, 2.40326180e-03, 4.09860597e-01, 1.30645186e-02,\n       2.05427011e-02, 2.10591491e-02, 5.09340194e-02, 2.40577336e-01,\n       8.30868016e-02, 8.11391003e-02, 3.87416940e-02, 4.49363991e-02,\n       4.95540271e-02, 3.12528277e-02, 4.84552790e-03, 8.81048885e-02,\n       7.18949228e-02, 2.56547800e-02, 0.00000000e+00, 5.18997001e-02,\n       7.07124577e-02, 2.29659081e-02])"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask = np.random.choice(X_train.shape[0], 1000, replace=False)\ndisplay(mask[:10])\ndisplay(X_train.iloc[mask])","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.133258Z","iopub.status.idle":"2022-09-27T22:45:01.134272Z","shell.execute_reply.started":"2022-09-27T22:45:01.133973Z","shell.execute_reply":"2022-09-27T22:45:01.134001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.sample([1,3,5,6],2)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.135925Z","iopub.status.idle":"2022-09-27T22:45:01.136591Z","shell.execute_reply.started":"2022-09-27T22:45:01.136295Z","shell.execute_reply":"2022-09-27T22:45:01.136321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[random.sample(list(X_train.index), 10)]","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.138855Z","iopub.status.idle":"2022-09-27T22:45:01.139798Z","shell.execute_reply.started":"2022-09-27T22:45:01.139512Z","shell.execute_reply":"2022-09-27T22:45:01.139540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random.sample(list(X_train.index), 10)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.141623Z","iopub.status.idle":"2022-09-27T22:45:01.142604Z","shell.execute_reply.started":"2022-09-27T22:45:01.142281Z","shell.execute_reply":"2022-09-27T22:45:01.142318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Error Analysis","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.144403Z","iopub.status.idle":"2022-09-27T22:45:01.145418Z","shell.execute_reply.started":"2022-09-27T22:45:01.145089Z","shell.execute_reply":"2022-09-27T22:45:01.145121Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.147380Z","iopub.status.idle":"2022-09-27T22:45:01.148299Z","shell.execute_reply.started":"2022-09-27T22:45:01.148002Z","shell.execute_reply":"2022-09-27T22:45:01.148030Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general point:\n# compared to NN, xgb is harder to regularize\n# in NN, you can simply shrink coefficient towards constant prediction.\n# in xgb, you can not do that. the only way to regularize is via hyperparameters.\n# in other words, by tweaking hyperpars, in NN you can approach R^2=0.0 prediction from a constant model arbitrarily close\n# in xgb, you can not do that.\n# by setting eta as low as 0.1% you can bring r2 down to 0.1%, but lowering eta further actyally increases abs(r2).\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.149960Z","iopub.status.idle":"2022-09-27T22:45:01.150816Z","shell.execute_reply.started":"2022-09-27T22:45:01.150545Z","shell.execute_reply":"2022-09-27T22:45:01.150571Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('total time for the script: ', time.time()-time0)\ndisplay(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.152459Z","iopub.status.idle":"2022-09-27T22:45:01.153355Z","shell.execute_reply.started":"2022-09-27T22:45:01.153036Z","shell.execute_reply":"2022-09-27T22:45:01.153063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" \n# def objective_nn(trial):\n    \n#     tf.keras.backend.clear_session()\n    \n#     with strategy.scope():\n#         # Generate our trial model.\n#         model = create_snnn_model(trial)\n\n#         callbacks = [\n#         tf.keras.callbacks.EarlyStopping(patience=40),\n#         TFKerasPruningCallback(trial, \"val_loss\"),\n#     ]\n\n#         # Fit the model on the training data.\n#         # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n#         history = model.fit(X_train, y_train, \n#                                 validation_data=(X_val, y_val),\n#                                 batch_size=2048, \n#                                 epochs=500, \n#                                 verbose=1, \n#                                 callbacks=callbacks)\n\n#         # Evaluate the model accuracy on the validation set.\n#         score = model.evaluate(X_val, y_val, verbose=0)\n#         return score[1]\n\n# trials = 50\n\n# study = optuna.create_study(direction=\"minimize\", \n#                             sampler=optuna.samplers.TPESampler(), \n#                             pruner=optuna.pruners.HyperbandPruner())\n# study.optimize(objective_nn, n_trials=trials)\n# pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n# complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n# temp = study.best_params\n# display(study.best_params, time.time()-time1)\n\n# optimal_hyperpars = list(temp.values())\n# display(optimal_hyperpars)\n# print(time.time()-time1, optimal_hyperpars)\n\n# optuna_nn = create_snnn_model_hyperpars(neurons_base=optimal_hyperpars[0], l2_reg_rate=optimal_hyperpars[1])\n# history = optuna_nn.fit(X_train, y_train, \n#                         validation_data=(X_val, y_val),\n#                         batch_size=2048, \n#                         epochs=1000,\n#                         verbose=1, \n#                         callbacks=[early_stopping50])\n\n# results.loc[results.min_prd==min_prd,'nn4opt_train':'nn4opt_test'] = \\\n# [r2_score(y_train, optuna_nn.predict(X_train)), \n# r2_score(y_val, optuna_nn.predict(X_val)),\n# r2_score(y_test, optuna_nn.predict(X_test))]","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.154910Z","iopub.status.idle":"2022-09-27T22:45:01.155756Z","shell.execute_reply.started":"2022-09-27T22:45:01.155489Z","shell.execute_reply":"2022-09-27T22:45:01.155515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # try optuna for NN:\n\n# def objective(trial):\n\n#     n_layers = trial.suggest_int('n_layers', 1, 3)\n#     model = tf.keras.Sequential()\n#     for i in range(n_layers):\n#         num_hidden = trial.suggest_int(f'n_units_l{i}', 4, 128, log=True)\n#         model.add(tf.keras.layers.Dense(num_hidden, activation='relu'))\n#     model.add(tf.keras.layers.Dense(1))\n#     display(model.summary())\n#     return accuracy\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=10)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.157413Z","iopub.status.idle":"2022-09-27T22:45:01.158255Z","shell.execute_reply.started":"2022-09-27T22:45:01.157967Z","shell.execute_reply":"2022-09-27T22:45:01.157995Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# on nns:\n# - try classic regularizers (l1, l2 etc)\n# - try different architecture (not snnn)\n# classic architecture:\n# He initialization, elu activation, batch norm, l2 reg, adam.\n\n# - try exotic architecture, e.g., wide'n'deep\n# \n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.159768Z","iopub.status.idle":"2022-09-27T22:45:01.160578Z","shell.execute_reply.started":"2022-09-27T22:45:01.160316Z","shell.execute_reply":"2022-09-27T22:45:01.160341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# usually self-norm seems better: it overfits less and runs faster\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-27T22:45:01.162057Z","iopub.status.idle":"2022-09-27T22:45:01.162915Z","shell.execute_reply.started":"2022-09-27T22:45:01.162619Z","shell.execute_reply":"2022-09-27T22:45:01.162662Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
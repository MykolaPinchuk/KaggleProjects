{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Define a problem, describe business context.\n2. Load data, preclean it.\n3. EDA: target, features.\n4. Descibe train/test split strategy. Show main results and discuss them.\n5. Run evth for a few periods. Show feature importance and error analysis.","metadata":{}},{"cell_type":"markdown","source":"### 1. Business problem\n\n#### Objective\n\n#### Metric\n\n#### Summary of results\n","metadata":{}},{"cell_type":"markdown","source":"### 2. Load data and preclean it","metadata":{"execution":{"iopub.status.busy":"2022-09-26T15:43:15.362648Z","iopub.execute_input":"2022-09-26T15:43:15.363102Z","iopub.status.idle":"2022-09-26T15:43:15.385837Z","shell.execute_reply.started":"2022-09-26T15:43:15.363009Z","shell.execute_reply":"2022-09-26T15:43:15.384918Z"}}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle, shap\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, KFold, PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\nfrom optuna.integration import TFKerasPruningCallback\nfrom optuna.trial import TrialState\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\noptuna.logging.set_verbosity(optuna.logging.WARNING)\npd.set_option('display.max_columns', 150)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-09-23T00:02:12.509281Z","iopub.execute_input":"2022-09-23T00:02:12.510123Z","iopub.status.idle":"2022-09-23T00:02:23.788311Z","shell.execute_reply.started":"2022-09-23T00:02:12.510041Z","shell.execute_reply":"2022-09-23T00:02:23.787197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-09-23T00:02:23.791539Z","iopub.execute_input":"2022-09-23T00:02:23.792464Z","iopub.status.idle":"2022-09-23T00:02:23.802238Z","shell.execute_reply.started":"2022-09-23T00:02:23.792427Z","shell.execute_reply":"2022-09-23T00:02:23.801277Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-09-23T00:02:23.803821Z","iopub.execute_input":"2022-09-23T00:02:23.804484Z","iopub.status.idle":"2022-09-23T00:02:23.848901Z","shell.execute_reply.started":"2022-09-23T00:02:23.804442Z","shell.execute_reply":"2022-09-23T00:02:23.847852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functions for Optuna NNs\n\ncv_nn_regularizer = 0.075\n\ndef create_snnn4_model_hyperpars(neurons_base=4, l1_reg_rate=0.1, l2_reg_rate=0.1):\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\ndef create_snnn4_model(trial):\n\n    # hyperparameter space\n    neurons_base = trial.suggest_int(\"neurons_base\", 4, 24, 2)\n    l2_reg_rate = trial.suggest_float('l2_regularizer', 0, 0.9)\n    l1_reg_rate = trial.suggest_float('l1_regularizer', 0, 0.9)\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\n\ndef create_snnn6_model_hyperpars(neurons_base=4, l1_reg_rate=0.1, l2_reg_rate=0.1):\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\ndef create_snnn6_model(trial):\n\n    # hyperparameter space\n    neurons_base = trial.suggest_int(\"neurons_base\", 4, 24, 2)\n    l2_reg_rate = trial.suggest_float('l2_regularizer', 0, 0.9)\n    l1_reg_rate = trial.suggest_float('l1_regularizer', 0, 0.9)\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate), \n                              input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l1_l2(l1=l1_reg_rate, l2=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\n\ndef objective_nn4(trial):\n    \n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_snnn4_model(trial)\n\n        callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=40),\n        TFKerasPruningCallback(trial, \"val_loss\"),\n    ]\n\n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n        history = model.fit(X_train, y_train, \n                                validation_data=(X_val, y_val),\n                                batch_size=2048, \n                                epochs=500, \n                                verbose=0, \n                                callbacks=callbacks)\n\n        # Evaluate the model accuracy on the validation set.\n        loss_train = model.evaluate(X_train, y_train, verbose=0)\n        loss_val = model.evaluate(X_val_extra, y_val_extra, verbose=0)\n        overfit = max(loss_val[1]-loss_train[1], 0)\n        return loss_val[1] + cv_nn_regularizer*overfit\n    \n    \ndef objective_nn6(trial):\n    \n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_snnn6_model(trial)\n\n        callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=40),\n        TFKerasPruningCallback(trial, \"val_loss\"),\n    ]\n\n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n        history = model.fit(X_train, y_train, \n                                validation_data=(X_val, y_val),\n                                batch_size=2048, \n                                epochs=500, \n                                verbose=0, \n                                callbacks=callbacks)\n\n        # Evaluate the model accuracy on the validation set.\n        loss_train = model.evaluate(X_train, y_train, verbose=0)\n        loss_val = model.evaluate(X_val_extra, y_val_extra, verbose=0)\n        overfit = max(loss_val[1]-loss_train[1], 0)\n        return loss_val[1] + cv_nn_regularizer*overfit\n","metadata":{"execution":{"iopub.status.busy":"2022-09-23T00:02:23.852392Z","iopub.execute_input":"2022-09-23T00:02:23.853557Z","iopub.status.idle":"2022-09-23T00:02:23.884993Z","shell.execute_reply.started":"2022-09-23T00:02:23.853521Z","shell.execute_reply":"2022-09-23T00:02:23.883772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time0 = time.time()\n\nmin_prd = 500\nwindows_width = 5*12\ncv_xgb_regularizer=0.2\noptuna_xgb_trials = 80\noptuna_nn_trials = 100\n\n\nwith open('../input/mleap-46-preprocessed/MLEAP_46_v7.pkl', 'rb') as pickled_one:\n    df = pickle.load(pickled_one)\ndf = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+10))]\ndf_cnt = df.count()\nempty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\ndf.drop(columns=empty_cols, inplace=True)\n#display(df.shape, df.head(), df.year.describe(), df.count())\n\nfeatures_miss_dummies = ['amhd', 'BAspr']\nfor col in features_miss_dummies:\n    if col in df.columns:\n        df[col+'_miss'] = df[col].isnull().astype(int)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 3. EDA","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 4. Train/test split strategy","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Main results","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5. Fit the model(s) for one window and explore results","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ntemp_cols = ['PERMNO', 'year', 'prd']\ndf.reset_index(inplace=True, drop=True)\nX = df.copy()\ny = X.pop('RET')\n\ntrain_indx = X.prd<(min_prd+windows_width-1)\nval_indx = X['prd'].isin(range(min_prd+windows_width-1, min_prd+windows_width+2))\nval_indx_extra = X['prd'].isin(range(min_prd+windows_width+5, min_prd+windows_width+8))\ntest_indx = X['prd'].isin(range(min_prd+windows_width+2, min_prd+windows_width+5))\n\nX_train = X[train_indx]\nX_val = X[val_indx]\nX_val_extra = X[val_indx_extra]\nX_test = X[test_indx]\ny_train = y[train_indx]\ny_val = y[val_indx]\ny_val_extra = y[val_indx_extra]\ny_test = y[test_indx]\n\n#display(X_train.head(3), X_train.tail(3), y_train.head(3), y_train.tail(3))\n#display(X_train.prd.describe(), X_val.prd.describe(), X_test.prd.describe())\ndisplay(X_train.shape, X_val.shape, X_test.shape)\n\nX_train.drop(columns=temp_cols, inplace=True)\nX_val.drop(columns=temp_cols, inplace=True)\nX_val_extra.drop(columns=temp_cols, inplace=True)\nX_test.drop(columns=temp_cols, inplace=True)\n\n#display(X_train.tail())\ncol_cat = ['ind']\ncol_num = [x for x in X_train.columns if x not in col_cat]\nfor col in col_num:\n    X_train[col] = X_train[col].fillna(X_train[col].median())\n    X_val[col] = X_val[col].fillna(X_train[col].median())\n    X_val_extra[col] = X_val_extra[col].fillna(X_train[col].median())\n    X_test[col] = X_test[col].fillna(X_train[col].median())\nfor col in col_cat:\n    X_train[col] = X_train[col].fillna(value=-1000)\n    X_val[col] = X_val[col].fillna(value=-1000)\n    X_val_extra[col] = X_val_extra[col].fillna(value=-1000)\n    X_test[col] = X_test[col].fillna(value=-1000)\n\n#display(X_train.tail())\nfeature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                        (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                        remainder=\"passthrough\")\n\nprint('Number of features before transformation: ', X_train.shape)\ntrain_index, val_index, val_index_extra, test_index = X_train.index, X_val.index, X_val_extra.index, X_test.index\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_val = pd.DataFrame(feature_transformer.transform(X_val), columns=feature_transformer.get_feature_names_out())\nX_val_extra = pd.DataFrame(feature_transformer.transform(X_val_extra), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ')\nprint('Number of features after transformation: ', X_train.shape, X_val.shape, X_val_extra.shape, X_test.shape)\nX_train.index = train_index\nX_val.index = val_index\nX_val_extra.index = val_index_extra\nX_test.index = test_index\n#display(X_train.tail())\n\nX = pd.concat([X_train, X_val])\ny = pd.concat([y_train, y_val])\n#display(X,y)\n\nX_ = pd.concat([X_train, X_val, X_val_extra])\ny_ = pd.concat([y_train, y_val, y_val_extra])\n#display(X,y, X_,y_)\n\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=400, max_depth=4, eta=0.02, colsample_bytree=0.4, subsample=0.6)\nxgb1.fit(X_train, y_train)\nprint('fixed XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\nprint('XGB val:', mean_absolute_error(y_val, xgb1.predict(X_val)), r2_score(y_val, xgb1.predict(X_val)))\nprint('XGB val extra:', mean_absolute_error(y_val_extra, xgb1.predict(X_val_extra)), r2_score(y_val_extra, xgb1.predict(X_val_extra)))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbf_train':'xgbf_test'] = \\\n[r2_score(y_train, xgb1.predict(X_train)), \nr2_score(y_val, xgb1.predict(X_val)),\nr2_score(y_test, xgb1.predict(X_test))]\n\ntime1 = time.time()\n\n# Create a list where train data indices are -1 and validation data indices are 0\nsplit_index = [-1 if x in X_train.index else 0 for x in X.index]\npds = PredefinedSplit(test_fold = split_index)\n\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[400, 600, 800], 'max_depth':[2,3,4,5], 'eta':[0.006, 0.012, 0.02], \n              'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbgs = GridSearchCV(estimator = xgb, cv=pds, param_grid=param_grid)\n\n# Fit with all data\nxgbgs.fit(X_, y_)\n\nprint('gs XGB', xgbgs.best_params_, xgbgs.best_score_, time.time()-time1)\nprint('XGB train:', mean_absolute_error(y_train, xgbgs.predict(X_train)), r2_score(y_train, xgbgs.predict(X_train)))\nprint('XGB validation:', mean_absolute_error(y_val, xgbgs.predict(X_val)), r2_score(y_val, xgbgs.predict(X_val)))\nprint('XGB validation extra:', mean_absolute_error(y_val_extra, xgbgs.predict(X_val_extra)), r2_score(y_val_extra, xgbgs.predict(X_val_extra)))\nprint('XGB test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_score(y_test, xgbgs.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbgs_train':'xgbgs_test'] = \\\n[r2_score(y_train, xgbgs.predict(X_train)), \nr2_score(y_val, xgbgs.predict(X_val)),\nr2_score(y_test, xgbgs.predict(X_test))]\n\ntime1 = time.time()\ndef objective_xgb(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    params = {\n    \"tree_method\": 'gpu_hist',\n    \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n    \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 1500),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n    \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.0005, 0.03),\n    \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.05, 0.95),\n    \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 0.95),\n    \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 50.0),\n    \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 500.0),\n    \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 100.0),\n    \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n    model = XGBRegressor(**params, njobs=-1)\n    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose = False)\n\n    score_train = r2_score(y_train, model.predict(X_train))\n    score_val = r2_score(y_val, model.predict(X_val))\n    score_val_extra = r2_score(y_val_extra, model.predict(X_val_extra)) \n    score_val = (score_val+score_val_extra)/2\n    overfit = np.abs(score_train-score_val)\n\n    return score_val-cv_xgb_regularizer*overfit\n\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective_xgb, n_trials=optuna_xgb_trials)\nprint('Total time for hypermarameter optimization, XGB: ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X, y)\nprint('Optuna XGB train: \\n', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), '\\nvalidation \\n',\n      mean_absolute_error(y_val, optuna_xgb.predict(X_val)), r2_score(y_val, optuna_xgb.predict(X_val)),\n      mean_absolute_error(y_val_extra, optuna_xgb.predict(X_val_extra)), r2_score(y_val_extra, optuna_xgb.predict(X_val_extra)), '\\ntest \\n',\n      mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))\n\nresults.loc[results.min_prd==min_prd,'xgbo_train':'xgbo_test'] = \\\n[r2_score(y_train, optuna_xgb.predict(X_train)), \nr2_score(y_val, optuna_xgb.predict(X_val)),\nr2_score(y_test, optuna_xgb.predict(X_test))]\n\n###########\n### NNs ###\n###########\n\nneurons_base = 8\nl2_reg_rate = 0.5\n\nmodel_snn6 = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn6.count_params())\n\nearly_stopping50 = EarlyStopping(patience=50, restore_best_weights=True)\noptimizer_adam = tf.keras.optimizers.Adam()\n\nmodel_snn6.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n\ntime1 = time.time()\nhistory = model_snn6.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=0, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nresults.loc[results.min_prd==min_prd,'nn6_train':'nn6_test'] = \\\n[r2_score(y_train, model_snn6.predict(X_train)), \nr2_score(y_val, model_snn6.predict(X_val)),\nr2_score(y_test, model_snn6.predict(X_test))]\n\n\n\nneurons_base = 8\nl2_reg_rate = 0.3\n\nmodel_snn4 = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn4.count_params())\n\ntime1 = time.time()\nmodel_snn4.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_snn4.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=0, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nresults.loc[results.min_prd==min_prd,'nn4_train':'nn4_test'] = \\\n[r2_score(y_train, model_snn4.predict(X_train)), \nr2_score(y_val, model_snn4.predict(X_val)),\nr2_score(y_test, model_snn4.predict(X_test))]\n\n\n\n# try optuna, using this kaggle notebook: https://www.kaggle.com/code/mistag/keras-model-tuning-with-optuna\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"minimize\", \n                            sampler=optuna.samplers.TPESampler(), \n                            pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective_nn4, n_trials=optuna_nn_trials)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\ntemp = study.best_params\ndisplay('Optuna NN4', study.best_params, time.time()-time1)\n\noptimal_hyperpars = list(temp.values())\ndisplay(optimal_hyperpars)\nprint('Time for hyperparameter optimization: ',time.time()-time1, optimal_hyperpars)\n\noptuna_nn = create_snnn4_model_hyperpars(neurons_base=optimal_hyperpars[0], \n                                         l2_reg_rate=optimal_hyperpars[1],\n                                        l1_reg_rate=optimal_hyperpars[2])\nhistory = optuna_nn.fit(X_train, y_train, \n                        validation_data=(X_val, y_val),\n                        batch_size=2048, \n                        epochs=1000,\n                        verbose=0, \n                        callbacks=[early_stopping50])\n\nresults.loc[results.min_prd==min_prd,'nn4opt_train':'nn4opt_test'] = \\\n[r2_score(y_train, optuna_nn.predict(X_train)), \nr2_score(y_val, optuna_nn.predict(X_val)),\nr2_score(y_test, optuna_nn.predict(X_test))]\n\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"minimize\", \n                            sampler=optuna.samplers.TPESampler(), \n                            pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective_nn6, n_trials=optuna_nn_trials)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\ntemp = study.best_params\ndisplay('Optuna NN6', study.best_params, time.time()-time1)\n\noptimal_hyperpars = list(temp.values())\ndisplay(optimal_hyperpars)\nprint('Time for hyperparameter optimization: ', time.time()-time1, optimal_hyperpars)\n\noptuna_nn = create_snnn6_model_hyperpars(neurons_base=optimal_hyperpars[0], \n                                         l2_reg_rate=optimal_hyperpars[1],\n                                        l1_reg_rate=optimal_hyperpars[2])\nhistory = optuna_nn.fit(X_train, y_train, \n                        validation_data=(X_val, y_val),\n                        batch_size=2048, \n                        epochs=1000,\n                        verbose=0, \n                        callbacks=[early_stopping50])\n\nresults.loc[results.min_prd==min_prd,'nn6opt_train':'nn6opt_test'] = \\\n[r2_score(y_train, optuna_nn.predict(X_train)), \nr2_score(y_val, optuna_nn.predict(X_val)),\nr2_score(y_test, optuna_nn.predict(X_test))]\n\n\n\ndisplay(results)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.075981Z","iopub.execute_input":"2022-09-23T02:35:36.076422Z","iopub.status.idle":"2022-09-23T02:35:36.084246Z","shell.execute_reply.started":"2022-09-23T02:35:36.076386Z","shell.execute_reply":"2022-09-23T02:35:36.083185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.best_params","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.085848Z","iopub.execute_input":"2022-09-23T02:35:36.086491Z","iopub.status.idle":"2022-09-23T02:35:36.096910Z","shell.execute_reply.started":"2022-09-23T02:35:36.086455Z","shell.execute_reply":"2022-09-23T02:35:36.095796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(study.best_params.values())[1]","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.098616Z","iopub.execute_input":"2022-09-23T02:35:36.099080Z","iopub.status.idle":"2022-09-23T02:35:36.108093Z","shell.execute_reply.started":"2022-09-23T02:35:36.099004Z","shell.execute_reply":"2022-09-23T02:35:36.106984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(results.iloc[:,1:].mean())\n# cv_regularizer = 0.5\n# optuna_trials = 80\nprint(time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.110609Z","iopub.execute_input":"2022-09-23T02:35:36.111352Z","iopub.status.idle":"2022-09-23T02:35:36.127754Z","shell.execute_reply.started":"2022-09-23T02:35:36.111317Z","shell.execute_reply":"2022-09-23T02:35:36.126816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# general point:\n# compared to NN, xgb is harder to regularize\n# in NN, you can simply shrink coefficient towards constant prediction.\n# in xgb, you can not do that. the only way to regularize is via hyperparameters.\n# in other words, by tweaking hyperpars, in NN you can approach R^2=0.0 prediction from a constant model arbitrarily close\n# in xgb, you can not do that.\n# by setting eta as low as 0.1% you can bring r2 down to 0.1%, but lowering eta further actyally increases abs(r2).\n","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.131295Z","iopub.execute_input":"2022-09-23T02:35:36.131717Z","iopub.status.idle":"2022-09-23T02:35:36.136439Z","shell.execute_reply.started":"2022-09-23T02:35:36.131676Z","shell.execute_reply":"2022-09-23T02:35:36.135211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('total time for the script: ', time.time()-time0)\ndisplay(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.138217Z","iopub.execute_input":"2022-09-23T02:35:36.138634Z","iopub.status.idle":"2022-09-23T02:35:36.167667Z","shell.execute_reply.started":"2022-09-23T02:35:36.138598Z","shell.execute_reply":"2022-09-23T02:35:36.166507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" \n# def objective_nn(trial):\n    \n#     tf.keras.backend.clear_session()\n    \n#     with strategy.scope():\n#         # Generate our trial model.\n#         model = create_snnn_model(trial)\n\n#         callbacks = [\n#         tf.keras.callbacks.EarlyStopping(patience=40),\n#         TFKerasPruningCallback(trial, \"val_loss\"),\n#     ]\n\n#         # Fit the model on the training data.\n#         # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n#         history = model.fit(X_train, y_train, \n#                                 validation_data=(X_val, y_val),\n#                                 batch_size=2048, \n#                                 epochs=500, \n#                                 verbose=1, \n#                                 callbacks=callbacks)\n\n#         # Evaluate the model accuracy on the validation set.\n#         score = model.evaluate(X_val, y_val, verbose=0)\n#         return score[1]\n\n# trials = 50\n\n# study = optuna.create_study(direction=\"minimize\", \n#                             sampler=optuna.samplers.TPESampler(), \n#                             pruner=optuna.pruners.HyperbandPruner())\n# study.optimize(objective_nn, n_trials=trials)\n# pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n# complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n# temp = study.best_params\n# display(study.best_params, time.time()-time1)\n\n# optimal_hyperpars = list(temp.values())\n# display(optimal_hyperpars)\n# print(time.time()-time1, optimal_hyperpars)\n\n# optuna_nn = create_snnn_model_hyperpars(neurons_base=optimal_hyperpars[0], l2_reg_rate=optimal_hyperpars[1])\n# history = optuna_nn.fit(X_train, y_train, \n#                         validation_data=(X_val, y_val),\n#                         batch_size=2048, \n#                         epochs=1000,\n#                         verbose=1, \n#                         callbacks=[early_stopping50])\n\n# results.loc[results.min_prd==min_prd,'nn4opt_train':'nn4opt_test'] = \\\n# [r2_score(y_train, optuna_nn.predict(X_train)), \n# r2_score(y_val, optuna_nn.predict(X_val)),\n# r2_score(y_test, optuna_nn.predict(X_test))]","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.169234Z","iopub.execute_input":"2022-09-23T02:35:36.169576Z","iopub.status.idle":"2022-09-23T02:35:36.178963Z","shell.execute_reply.started":"2022-09-23T02:35:36.169542Z","shell.execute_reply":"2022-09-23T02:35:36.177802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.to_csv('temp_models_2')","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.181507Z","iopub.execute_input":"2022-09-23T02:35:36.181764Z","iopub.status.idle":"2022-09-23T02:35:36.193666Z","shell.execute_reply.started":"2022-09-23T02:35:36.181741Z","shell.execute_reply":"2022-09-23T02:35:36.192424Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # try optuna for NN:\n\n# def objective(trial):\n\n#     n_layers = trial.suggest_int('n_layers', 1, 3)\n#     model = tf.keras.Sequential()\n#     for i in range(n_layers):\n#         num_hidden = trial.suggest_int(f'n_units_l{i}', 4, 128, log=True)\n#         model.add(tf.keras.layers.Dense(num_hidden, activation='relu'))\n#     model.add(tf.keras.layers.Dense(1))\n#     display(model.summary())\n#     return accuracy\n\n# study = optuna.create_study(direction='maximize')\n# study.optimize(objective, n_trials=10)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.196466Z","iopub.execute_input":"2022-09-23T02:35:36.196749Z","iopub.status.idle":"2022-09-23T02:35:36.204089Z","shell.execute_reply.started":"2022-09-23T02:35:36.196725Z","shell.execute_reply":"2022-09-23T02:35:36.202999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# on nns:\n# - try classic regularizers (l1, l2 etc)\n# - try different architecture (not snnn)\n# classic architecture:\n# He initialization, elu activation, batch norm, l2 reg, adam.\n\n# - try exotic architecture, e.g., wide'n'deep\n# \n","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.625436Z","iopub.status.idle":"2022-09-23T02:35:36.626010Z","shell.execute_reply.started":"2022-09-23T02:35:36.625745Z","shell.execute_reply":"2022-09-23T02:35:36.625769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# usually self-norm seems better: it overfits less and runs faster\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-23T02:35:36.628108Z","iopub.status.idle":"2022-09-23T02:35:36.628600Z","shell.execute_reply.started":"2022-09-23T02:35:36.628350Z","shell.execute_reply":"2022-09-23T02:35:36.628374Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
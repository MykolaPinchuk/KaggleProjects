{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Load libraries and data.\n2. pEDA. Look at feature distribution, fix them if they do not look right.\n3. Train-test split. Most likely couple years into test set. 2015-2018?. Impute missing values.\n4. Transform numerical features, add ohe for inds.\n5. Fit classic models: ols as a baseline, then xgb.\n6. Fir DL.\n\n\nNotes:\nideally, I want to use time-based cross-validation.\nsince I have panel data, it is not a trivial task.\nneed to find some solution online.\ne.g., https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8.\n\nfor now, will try to do siple for loop.\n","metadata":{}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\npd.set_option('display.max_columns', 110)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:25:39.871663Z","iopub.execute_input":"2022-08-25T23:25:39.872027Z","iopub.status.idle":"2022-08-25T23:25:39.882482Z","shell.execute_reply.started":"2022-08-25T23:25:39.871995Z","shell.execute_reply":"2022-08-25T23:25:39.880968Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:25:39.907003Z","iopub.execute_input":"2022-08-25T23:25:39.908584Z","iopub.status.idle":"2022-08-25T23:25:39.918482Z","shell.execute_reply.started":"2022-08-25T23:25:39.908543Z","shell.execute_reply":"2022-08-25T23:25:39.917360Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:25:39.944653Z","iopub.execute_input":"2022-08-25T23:25:39.945180Z","iopub.status.idle":"2022-08-25T23:25:39.952252Z","shell.execute_reply.started":"2022-08-25T23:25:39.945144Z","shell.execute_reply":"2022-08-25T23:25:39.950864Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"# for loop to see appx performance over the whole sample with some rolling window\n\ntime0 = time.time()\n\n#min_prd_list = range(100, 676, 25)\nmin_prd_list = [125]\nwindows_width = 3*12\ncv_regularizer=0.03\noptuna_trials = 40\n\nresults = pd.DataFrame(columns = ['min_prd', 'xgbf', 'xgbgs', 'xgbo'])\nresults.min_prd = min_prd_list\n\nfor min_prd in min_prd_list:\n    \n    \n    with open('../input/kaggle-46pkl/IMLEAP_v4.pkl', 'rb') as pickled_one:\n        df = pickle.load(pickled_one)\n    df = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+2))]\n    df_cnt = df.count()\n    empty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\n    df.drop(columns=empty_cols, inplace=True)\n    display(df.shape, df.head(), df.year.describe(), df.count())\n    \n    df = df[(df.RET>-50)&(df.RET<75)]\n    meanret = df.groupby('prd').RET.mean().to_frame().reset_index().rename(columns={'RET':'mRET'})\n    df = pd.merge(df, meanret, on='prd', how='left')\n    df.RET = df.RET-df.mRET\n    df.drop(columns='mRET', inplace=True)\n\n    features_miss_dummies = ['amhd', 'BAspr']\n    for col in features_miss_dummies:\n        if col in df.columns:\n            df[col+'_miss'] = df[col].isnull().astype(int)\n\n    temp_cols = ['PERMNO', 'year']\n    train = df[df.prd<(min_prd+windows_width)]\n    test = df[df.prd==(min_prd+windows_width)]\n    train.drop(columns=temp_cols, inplace=True)\n    test.drop(columns=temp_cols, inplace=True)\n\n    col_ignore = ['RET', 'prd']\n    col_cat = ['ind']\n    col_num = [x for x in train.columns if x not in col_ignore+col_cat]\n    for col in col_num:\n        train[col] = train[col].fillna(train[col].median())\n        test[col] = test[col].fillna(train[col].median())\n    for col in col_cat:\n        train[col] = train[col].fillna(value=-1000)\n        test[col] = test[col].fillna(value=-1000)\n\n    X_train = train.copy()\n    y_train = X_train.pop('RET')\n    X_test = test.copy()\n    y_test = X_test.pop('RET')\n\n    feature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                            (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                            remainder=\"passthrough\")\n\n    print('Number of features before transformation: ', X_train.shape)\n    X_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\n    X_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\n    print('time to do feature proprocessing: ')\n    print('Number of features after transformation: ', X_train.shape)\n    \n    X_train0 = X_train.copy()\n    y_train0 = y_train.copy()\n    \n    X_train.drop(columns=['remainder__prd'], inplace=True)\n    X_test.drop(columns=['remainder__prd'], inplace=True)\n\n    print('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n    print('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\n    xgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=300, max_depth=5, eta=0.03, colsample_bytree=0.6)\n    xgb1.fit(X_train, y_train)\n    print('XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\n\n    time1 = time.time()\n    xgb = XGBRegressor(tree_method = 'gpu_hist')\n    param_grid = {'n_estimators':[400, 700], 'max_depth':[2,3,4], 'eta':[0.006, 0.012, 0.02], 'subsample':[0.6], 'colsample_bytree':[0.6]}\n    xgbm = GridSearchCV(xgb, param_grid, cv=2, verbose=2, scoring='r2')\n    xgbm.fit(X_train, y_train)\n    print('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)\n    print('XGB train:', mean_absolute_error(y_train, xgbm.predict(X_train)), r2_score(y_train, xgbm.predict(X_train)), time.time()-time1)\n\n    time1 = time.time()\n    def objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n        params = {\n        \"tree_method\": 'gpu_hist',\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1000),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.001, 0.05),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 0.95),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 0.95),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 30.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 200.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 10.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n        temp_out = []\n\n        for i in range(cv_runs):\n\n            X = X_train\n            y = y_train\n            model = XGBRegressor(**params, njobs=-1)\n            rkf = KFold(n_splits=n_splits, shuffle=True)\n            X_values = X.values\n            y_values = y.values\n            y_pred = np.zeros_like(y_values)\n            y_pred_train = np.zeros_like(y_values)\n            for train_index, test_index in rkf.split(X_values):\n                X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n                y_A, y_B = y_values[train_index], y_values[test_index]\n                model.fit(X_A, y_A, eval_set=[(X_B, y_B)], verbose = False)\n                y_pred[test_index] = model.predict(X_B)\n                y_pred_train[train_index] = model.predict(X_A)\n\n            score_train = r2_score(y_train, y_pred_train)\n            score_test = r2_score(y_train, y_pred) \n            overfit = (score_train-score_test)\n            temp_out.append(score_test-cv_regularizer*overfit)\n\n        return (np.mean(temp_out))\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=optuna_trials)\n    print('Total time for hypermarameter optimization ', time.time()-time1)\n    hp = study.best_params\n    for key, value in hp.items():\n        print(f\"{key:>20s} : {value}\")\n    print(f\"{'best objective value':>20s} : {study.best_value}\")\n    optuna_hyperpars = study.best_params\n    optuna_hyperpars['tree_method']='gpu_hist'\n    optuna_xgb = XGBRegressor(**optuna_hyperpars)\n    optuna_xgb.fit(X_train, y_train)\n    print('Optuna XGB train:', \n          mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), time.time()-time1)\n\n    # Evaluate performance of XGB models:\n    r2_xgb1 = r2_score(y_test, xgb1.predict(X_test))\n    r2_xgbgs = r2_score(y_test, xgbm.predict(X_test))\n    r2_xgbo = r2_score(y_test, optuna_xgb.predict(X_test))\n\n    print('Min_prd: ', min_prd)\n    print('Constant guess: ', mean_absolute_error(y_test, np.ones(len(y_test))*y_test.mean()), \n          r2_score(y_test, np.ones(len(y_test))*y_test.mean()))\n    print('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_xgb1)\n    print('XGB GS test:', mean_absolute_error(y_test, xgbm.predict(X_test)), r2_xgbgs)\n    print('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_xgbo)\n\n    results.loc[results.min_prd==min_prd,'xgbf':'xgbo'] = r2_xgb1, r2_xgbgs, r2_xgbo\n    \nprint(time.time()-time0, results)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:25:39.973920Z","iopub.execute_input":"2022-08-25T23:25:39.974167Z","iopub.status.idle":"2022-08-25T23:29:26.812897Z","shell.execute_reply.started":"2022-08-25T23:25:39.974144Z","shell.execute_reply":"2022-08-25T23:29:26.812152Z"},"trusted":true},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"(40682, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"    PERMNO  prd     mom482     mom242  year      RET   ind       bm        op  \\\n49   10006  124  57.022356  27.230111  1968 -10.5198  25.0 -0.22327  0.183384   \n50   10006  125  45.576895  32.149862  1968  -6.2596  25.0 -0.22327  0.183384   \n51   10006  126  16.380849  21.566933  1968   4.3219  25.0 -0.22327  0.183384   \n52   10006  127  32.954723  53.105785  1968   9.7618  25.0 -0.22327  0.183384   \n53   10006  128  34.099293  39.415200  1968   3.9450  25.0 -0.22327  0.183384   \n\n          gp       inv    mom11     mom122      amhd  ivol_capm  ivol_ff5  \\\n49  0.269118  0.100395   3.4619  12.086336  1.536049   1.377650  1.078594   \n50  0.269118  0.100395 -10.5198  24.581595  1.493226   2.217285  1.948001   \n51  0.269118  0.100395  -6.2596  13.253947  1.469247   4.800074  4.663338   \n52  0.269118  0.100395   4.3219  11.098127  1.375395   1.477982  1.203894   \n53  0.269118  0.100395   9.7618  21.666549  1.246353   1.734429  1.621548   \n\n     beta_bw     MAX     vol1m     vol6m    vol12m      size       lbm  \\\n49  0.854703  4.9820  1.945352  2.653852  2.174071  5.867580 -0.149515   \n50  0.794842  2.7293  2.301335  2.797800  2.238881  5.751293 -0.149515   \n51  0.777908  7.2477  4.297812  3.251701  2.512787  5.691229 -0.149515   \n52  0.787439  4.4095  1.764988  3.249921  2.531631  5.737749 -0.149515   \n53  0.750551  5.4695  1.618184  3.253052  2.540064  5.824760 -0.149515   \n\n         lop       lgp      linv      llme    l1amhd      l1MAX    l3amhd  \\\n49  0.173745  0.242714  0.119169  5.740148  1.573985  21.135115  1.636271   \n50  0.173745  0.242714  0.119169  5.660875  1.536049   4.982000  1.641999   \n51  0.173745  0.242714  0.119169  5.648296  1.493226   2.729300  1.573985   \n52  0.173745  0.242714  0.119169  5.606947  1.469247   7.247700  1.536049   \n53  0.173745  0.242714  0.119169  5.549943  1.375395   4.409500  1.493226   \n\n        l3MAX    l6amhd   l6MAX   l12amhd     l12MAX  l12mom122  l12ivol_capm  \\\n49   4.208600  1.653423  2.5316  1.833293  21.135115   6.066114      1.906794   \n50   2.202200  1.585251  1.7344  1.792446   4.982000  27.909142      1.503199   \n51  21.135115  1.624355  2.1833  1.744677   2.729300  25.117139      1.051638   \n52   4.982000  1.636271  4.2086  1.727351   7.247700  26.865911      1.174851   \n53   2.729300  1.641999  2.2022  1.738743   4.409500  20.035787      1.903329   \n\n    l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  \n49     1.545510    1.069805  1.668624   1.814489  \n50     1.346875    1.007441  1.599164   1.760787  \n51     0.849621    0.989661  1.421761   1.677152  \n52     1.018713    1.014681  1.442506   1.555119  \n53     1.637691    0.996995  1.592287   1.605068  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>49</th>\n      <td>10006</td>\n      <td>124</td>\n      <td>57.022356</td>\n      <td>27.230111</td>\n      <td>1968</td>\n      <td>-10.5198</td>\n      <td>25.0</td>\n      <td>-0.22327</td>\n      <td>0.183384</td>\n      <td>0.269118</td>\n      <td>0.100395</td>\n      <td>3.4619</td>\n      <td>12.086336</td>\n      <td>1.536049</td>\n      <td>1.377650</td>\n      <td>1.078594</td>\n      <td>0.854703</td>\n      <td>4.9820</td>\n      <td>1.945352</td>\n      <td>2.653852</td>\n      <td>2.174071</td>\n      <td>5.867580</td>\n      <td>-0.149515</td>\n      <td>0.173745</td>\n      <td>0.242714</td>\n      <td>0.119169</td>\n      <td>5.740148</td>\n      <td>1.573985</td>\n      <td>21.135115</td>\n      <td>1.636271</td>\n      <td>4.208600</td>\n      <td>1.653423</td>\n      <td>2.5316</td>\n      <td>1.833293</td>\n      <td>21.135115</td>\n      <td>6.066114</td>\n      <td>1.906794</td>\n      <td>1.545510</td>\n      <td>1.069805</td>\n      <td>1.668624</td>\n      <td>1.814489</td>\n    </tr>\n    <tr>\n      <th>50</th>\n      <td>10006</td>\n      <td>125</td>\n      <td>45.576895</td>\n      <td>32.149862</td>\n      <td>1968</td>\n      <td>-6.2596</td>\n      <td>25.0</td>\n      <td>-0.22327</td>\n      <td>0.183384</td>\n      <td>0.269118</td>\n      <td>0.100395</td>\n      <td>-10.5198</td>\n      <td>24.581595</td>\n      <td>1.493226</td>\n      <td>2.217285</td>\n      <td>1.948001</td>\n      <td>0.794842</td>\n      <td>2.7293</td>\n      <td>2.301335</td>\n      <td>2.797800</td>\n      <td>2.238881</td>\n      <td>5.751293</td>\n      <td>-0.149515</td>\n      <td>0.173745</td>\n      <td>0.242714</td>\n      <td>0.119169</td>\n      <td>5.660875</td>\n      <td>1.536049</td>\n      <td>4.982000</td>\n      <td>1.641999</td>\n      <td>2.202200</td>\n      <td>1.585251</td>\n      <td>1.7344</td>\n      <td>1.792446</td>\n      <td>4.982000</td>\n      <td>27.909142</td>\n      <td>1.503199</td>\n      <td>1.346875</td>\n      <td>1.007441</td>\n      <td>1.599164</td>\n      <td>1.760787</td>\n    </tr>\n    <tr>\n      <th>51</th>\n      <td>10006</td>\n      <td>126</td>\n      <td>16.380849</td>\n      <td>21.566933</td>\n      <td>1968</td>\n      <td>4.3219</td>\n      <td>25.0</td>\n      <td>-0.22327</td>\n      <td>0.183384</td>\n      <td>0.269118</td>\n      <td>0.100395</td>\n      <td>-6.2596</td>\n      <td>13.253947</td>\n      <td>1.469247</td>\n      <td>4.800074</td>\n      <td>4.663338</td>\n      <td>0.777908</td>\n      <td>7.2477</td>\n      <td>4.297812</td>\n      <td>3.251701</td>\n      <td>2.512787</td>\n      <td>5.691229</td>\n      <td>-0.149515</td>\n      <td>0.173745</td>\n      <td>0.242714</td>\n      <td>0.119169</td>\n      <td>5.648296</td>\n      <td>1.493226</td>\n      <td>2.729300</td>\n      <td>1.573985</td>\n      <td>21.135115</td>\n      <td>1.624355</td>\n      <td>2.1833</td>\n      <td>1.744677</td>\n      <td>2.729300</td>\n      <td>25.117139</td>\n      <td>1.051638</td>\n      <td>0.849621</td>\n      <td>0.989661</td>\n      <td>1.421761</td>\n      <td>1.677152</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>10006</td>\n      <td>127</td>\n      <td>32.954723</td>\n      <td>53.105785</td>\n      <td>1968</td>\n      <td>9.7618</td>\n      <td>25.0</td>\n      <td>-0.22327</td>\n      <td>0.183384</td>\n      <td>0.269118</td>\n      <td>0.100395</td>\n      <td>4.3219</td>\n      <td>11.098127</td>\n      <td>1.375395</td>\n      <td>1.477982</td>\n      <td>1.203894</td>\n      <td>0.787439</td>\n      <td>4.4095</td>\n      <td>1.764988</td>\n      <td>3.249921</td>\n      <td>2.531631</td>\n      <td>5.737749</td>\n      <td>-0.149515</td>\n      <td>0.173745</td>\n      <td>0.242714</td>\n      <td>0.119169</td>\n      <td>5.606947</td>\n      <td>1.469247</td>\n      <td>7.247700</td>\n      <td>1.536049</td>\n      <td>4.982000</td>\n      <td>1.636271</td>\n      <td>4.2086</td>\n      <td>1.727351</td>\n      <td>7.247700</td>\n      <td>26.865911</td>\n      <td>1.174851</td>\n      <td>1.018713</td>\n      <td>1.014681</td>\n      <td>1.442506</td>\n      <td>1.555119</td>\n    </tr>\n    <tr>\n      <th>53</th>\n      <td>10006</td>\n      <td>128</td>\n      <td>34.099293</td>\n      <td>39.415200</td>\n      <td>1968</td>\n      <td>3.9450</td>\n      <td>25.0</td>\n      <td>-0.22327</td>\n      <td>0.183384</td>\n      <td>0.269118</td>\n      <td>0.100395</td>\n      <td>9.7618</td>\n      <td>21.666549</td>\n      <td>1.246353</td>\n      <td>1.734429</td>\n      <td>1.621548</td>\n      <td>0.750551</td>\n      <td>5.4695</td>\n      <td>1.618184</td>\n      <td>3.253052</td>\n      <td>2.540064</td>\n      <td>5.824760</td>\n      <td>-0.149515</td>\n      <td>0.173745</td>\n      <td>0.242714</td>\n      <td>0.119169</td>\n      <td>5.549943</td>\n      <td>1.375395</td>\n      <td>4.409500</td>\n      <td>1.493226</td>\n      <td>2.729300</td>\n      <td>1.641999</td>\n      <td>2.2022</td>\n      <td>1.738743</td>\n      <td>4.409500</td>\n      <td>20.035787</td>\n      <td>1.903329</td>\n      <td>1.637691</td>\n      <td>0.996995</td>\n      <td>1.592287</td>\n      <td>1.605068</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    40682.000000\nmean      1969.798437\nstd          0.978257\nmin       1968.000000\n25%       1969.000000\n50%       1970.000000\n75%       1971.000000\nmax       1971.000000\nName: year, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"PERMNO          40682\nprd             40682\nmom482          35826\nmom242          40286\nyear            40682\nRET             40682\nind             40682\nbm              40682\nop              40682\ngp              40682\ninv             40678\nmom11           40682\nmom122          40682\namhd            37268\nivol_capm       40681\nivol_ff5        40681\nbeta_bw         40682\nMAX             40682\nvol1m           40681\nvol6m           40682\nvol12m          40678\nsize            40682\nlbm             40682\nlop             40682\nlgp             40682\nlinv            40682\nllme            40682\nl1amhd          37313\nl1MAX           40682\nl3amhd          37398\nl3MAX           40682\nl6amhd          37547\nl6MAX           40682\nl12amhd         37988\nl12MAX          40682\nl12mom122       40497\nl12ivol_capm    40673\nl12ivol_ff5     40673\nl12beta_bw      40680\nl12vol6m        40637\nl12vol12m       40296\ndtype: int64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (38133, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (38133, 82)\nmae of a constant model 7.1541100803200415\nR2 of a constant model 0.0\nXGB train: 6.700516343240393 0.15021333381905422\nFitting 2 folds for each of 18 candidates, totalling 36 fits\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.5s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.6s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=700, subsample=0.6; total time=   1.0s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=700, subsample=0.6; total time=   0.8s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.6s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.6s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.0s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.0s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.5s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.6s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=700, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=700, subsample=0.6; total time=   0.8s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.6s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.6s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.0s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.1s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=400, subsample=0.6; total time=   0.7s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=700, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=700, subsample=0.6; total time=   0.8s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.6s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=400, subsample=0.6; total time=   0.7s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.1s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.0s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=400, subsample=0.6; total time=   0.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=700, subsample=0.6; total time=   1.5s\nXGB {'colsample_bytree': 0.6, 'eta': 0.006, 'max_depth': 4, 'n_estimators': 400, 'subsample': 0.6} 0.008323971099045424 34.10828733444214\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-08-25 23:26:17,045]\u001b[0m A new study created in memory with name: no-name-dcb1ae61-d0f9-4c94-925c-ba8c8561e892\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"XGB train: 7.036265023358253 0.043365857443366185 34.69868850708008\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-08-25 23:26:22,370]\u001b[0m Trial 0 finished with value: 0.014243362118309039 and parameters: {'n_estimators': 918, 'max_depth': 4, 'learning_rate': 0.015818278138253396, 'colsample_bytree': 0.8705046290735095, 'subsample': 0.3580922111480277, 'alpha': 20.01355931834925, 'lambda': 65.5957786917176, 'gamma': 7.128270112596257e-10, 'min_child_weight': 0.9269185763770273}. Best is trial 0 with value: 0.014243362118309039.\u001b[0m\n\u001b[32m[I 2022-08-25 23:26:25,497]\u001b[0m Trial 1 finished with value: 0.013336363709325663 and parameters: {'n_estimators': 952, 'max_depth': 2, 'learning_rate': 0.027600597650666924, 'colsample_bytree': 0.46830375618901376, 'subsample': 0.32284844460025897, 'alpha': 0.42036573214548145, 'lambda': 0.17453470429992365, 'gamma': 1.3213762680631284e-09, 'min_child_weight': 33.29476183750863}. Best is trial 0 with value: 0.014243362118309039.\u001b[0m\n\u001b[32m[I 2022-08-25 23:26:29,049]\u001b[0m Trial 2 finished with value: 0.010077829269954212 and parameters: {'n_estimators': 610, 'max_depth': 4, 'learning_rate': 0.042983679689384574, 'colsample_bytree': 0.30160467615938724, 'subsample': 0.5984988348565237, 'alpha': 1.3696282167471796, 'lambda': 188.42352664858055, 'gamma': 9.98950559890341e-06, 'min_child_weight': 4.070906945758357}. Best is trial 0 with value: 0.014243362118309039.\u001b[0m\n\u001b[32m[I 2022-08-25 23:26:36,685]\u001b[0m Trial 3 finished with value: -0.011306236954553163 and parameters: {'n_estimators': 905, 'max_depth': 5, 'learning_rate': 0.030786457133596755, 'colsample_bytree': 0.9292339458827867, 'subsample': 0.6792874818325481, 'alpha': 0.45391675308550133, 'lambda': 0.15563501627499682, 'gamma': 0.7230470985168613, 'min_child_weight': 9.270928265515938}. Best is trial 0 with value: 0.014243362118309039.\u001b[0m\n\u001b[32m[I 2022-08-25 23:26:44,001]\u001b[0m Trial 4 finished with value: 0.016575775511536765 and parameters: {'n_estimators': 849, 'max_depth': 5, 'learning_rate': 0.004124307929843576, 'colsample_bytree': 0.9451494321531112, 'subsample': 0.7044907990852362, 'alpha': 3.1922960747320204, 'lambda': 0.13411271531335317, 'gamma': 4.867352629493881e-09, 'min_child_weight': 0.12920699360350218}. Best is trial 4 with value: 0.016575775511536765.\u001b[0m\n\u001b[32m[I 2022-08-25 23:26:46,835]\u001b[0m Trial 5 finished with value: 0.009245799985410254 and parameters: {'n_estimators': 856, 'max_depth': 2, 'learning_rate': 0.03211765013551733, 'colsample_bytree': 0.43406389009322177, 'subsample': 0.42093644153794196, 'alpha': 0.43861821766226966, 'lambda': 0.26150042804005125, 'gamma': 0.049937561196161315, 'min_child_weight': 0.13808933107595495}. Best is trial 4 with value: 0.016575775511536765.\u001b[0m\n\u001b[32m[I 2022-08-25 23:26:54,086]\u001b[0m Trial 6 finished with value: -0.03359265039578541 and parameters: {'n_estimators': 939, 'max_depth': 5, 'learning_rate': 0.04850166326171775, 'colsample_bytree': 0.23466527582018684, 'subsample': 0.36425847682881995, 'alpha': 16.582796355580886, 'lambda': 18.681461919999474, 'gamma': 1.2125212186054963e-07, 'min_child_weight': 1.877261182353109}. Best is trial 4 with value: 0.016575775511536765.\u001b[0m\n\u001b[32m[I 2022-08-25 23:26:56,046]\u001b[0m Trial 7 finished with value: 0.01505431704207379 and parameters: {'n_estimators': 551, 'max_depth': 2, 'learning_rate': 0.017956055909045008, 'colsample_bytree': 0.9199508422368339, 'subsample': 0.944874463485776, 'alpha': 1.221735336290374, 'lambda': 14.16810770906486, 'gamma': 4.063568660681013, 'min_child_weight': 10.52717959723713}. Best is trial 4 with value: 0.016575775511536765.\u001b[0m\n\u001b[32m[I 2022-08-25 23:26:59,513]\u001b[0m Trial 8 finished with value: -0.0006094609778557651 and parameters: {'n_estimators': 640, 'max_depth': 4, 'learning_rate': 0.03258369039693173, 'colsample_bytree': 0.29776648177552967, 'subsample': 0.4503381347352259, 'alpha': 0.9501681446366439, 'lambda': 0.5523979067379721, 'gamma': 0.14574572568393449, 'min_child_weight': 5.084874849704111}. Best is trial 4 with value: 0.016575775511536765.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:02,027]\u001b[0m Trial 9 finished with value: 0.013981358314374194 and parameters: {'n_estimators': 640, 'max_depth': 2, 'learning_rate': 0.02523293919666279, 'colsample_bytree': 0.7918472192332465, 'subsample': 0.7401906290309637, 'alpha': 1.2824319320999586, 'lambda': 0.12684343571596904, 'gamma': 4.93272875038696e-09, 'min_child_weight': 18.892842642317834}. Best is trial 4 with value: 0.016575775511536765.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:05,313]\u001b[0m Trial 10 finished with value: 0.014794345822911469 and parameters: {'n_estimators': 782, 'max_depth': 3, 'learning_rate': 0.003209494954550675, 'colsample_bytree': 0.6807660809560384, 'subsample': 0.8435223003317585, 'alpha': 5.4489892924814445, 'lambda': 1.19788866051396, 'gamma': 0.00031872525808269623, 'min_child_weight': 0.1329439484849876}. Best is trial 4 with value: 0.016575775511536765.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:07,546]\u001b[0m Trial 11 finished with value: 0.006272753690063815 and parameters: {'n_estimators': 501, 'max_depth': 3, 'learning_rate': 0.001207416147751432, 'colsample_bytree': 0.684189515230331, 'subsample': 0.9351492270036386, 'alpha': 4.328890835888508, 'lambda': 3.746920369497363, 'gamma': 0.00035673199238262963, 'min_child_weight': 0.5195487803267109}. Best is trial 4 with value: 0.016575775511536765.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:10,813]\u001b[0m Trial 12 finished with value: 0.016146592214787245 and parameters: {'n_estimators': 757, 'max_depth': 3, 'learning_rate': 0.011746288778394547, 'colsample_bytree': 0.7060952451227674, 'subsample': 0.5614158075144711, 'alpha': 0.10036436831073106, 'lambda': 9.001370241254696, 'gamma': 9.046729079254872, 'min_child_weight': 0.39321836233842733}. Best is trial 4 with value: 0.016575775511536765.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:14,138]\u001b[0m Trial 13 finished with value: 0.016550806038284693 and parameters: {'n_estimators': 759, 'max_depth': 3, 'learning_rate': 0.011030165248170772, 'colsample_bytree': 0.65089677066857, 'subsample': 0.552984726909675, 'alpha': 0.10714370962688609, 'lambda': 2.229551229012921, 'gamma': 9.72346266050381e-07, 'min_child_weight': 0.3370925149482578}. Best is trial 4 with value: 0.016575775511536765.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:20,292]\u001b[0m Trial 14 finished with value: 0.01158738935510489 and parameters: {'n_estimators': 828, 'max_depth': 5, 'learning_rate': 0.009728379273927817, 'colsample_bytree': 0.1088107726956859, 'subsample': 0.5364101136201758, 'alpha': 0.10282516877934034, 'lambda': 1.6054133421653674, 'gamma': 7.759465217882975e-07, 'min_child_weight': 0.3062400445859434}. Best is trial 4 with value: 0.016575775511536765.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:24,512]\u001b[0m Trial 15 finished with value: 0.01878793556359398 and parameters: {'n_estimators': 709, 'max_depth': 4, 'learning_rate': 0.00644921001944682, 'colsample_bytree': 0.5649429758081871, 'subsample': 0.7413989328625584, 'alpha': 4.0751158393493165, 'lambda': 0.7498778526958146, 'gamma': 5.3697005005249645e-08, 'min_child_weight': 0.22072513489210852}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:30,174]\u001b[0m Trial 16 finished with value: 0.01844356941703497 and parameters: {'n_estimators': 697, 'max_depth': 5, 'learning_rate': 0.006027649104937037, 'colsample_bytree': 0.560310522709593, 'subsample': 0.7735248225616264, 'alpha': 3.7674322255828, 'lambda': 0.5359089890443618, 'gamma': 1.0041718310708942e-10, 'min_child_weight': 0.10533771294145314}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:34,681]\u001b[0m Trial 17 finished with value: 0.009206411762716239 and parameters: {'n_estimators': 693, 'max_depth': 4, 'learning_rate': 0.022715236434420565, 'colsample_bytree': 0.5700799839959646, 'subsample': 0.7996096821276646, 'alpha': 10.357317233319796, 'lambda': 0.5680373160567992, 'gamma': 2.875992257363169e-08, 'min_child_weight': 1.289154678087998}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:40,386]\u001b[0m Trial 18 finished with value: 0.016624189917771013 and parameters: {'n_estimators': 695, 'max_depth': 5, 'learning_rate': 0.006219751321453967, 'colsample_bytree': 0.5401436720449325, 'subsample': 0.8311309337442767, 'alpha': 7.5788193478782, 'lambda': 0.5704506344627494, 'gamma': 1.279643873057652e-10, 'min_child_weight': 0.2147332342415557}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:44,188]\u001b[0m Trial 19 finished with value: 0.012676243404608779 and parameters: {'n_estimators': 675, 'max_depth': 4, 'learning_rate': 0.018402336712055246, 'colsample_bytree': 0.4033057370767726, 'subsample': 0.7634088542655637, 'alpha': 2.6734972713343486, 'lambda': 0.9118557029775477, 'gamma': 1.7876241581270717e-10, 'min_child_weight': 0.535368645535246}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:49,118]\u001b[0m Trial 20 finished with value: -0.008796123904611451 and parameters: {'n_estimators': 574, 'max_depth': 5, 'learning_rate': 0.038697203284759965, 'colsample_bytree': 0.5909822962676157, 'subsample': 0.6461386549032284, 'alpha': 2.2899419987394283, 'lambda': 4.640114228208743, 'gamma': 9.813299618363786e-08, 'min_child_weight': 0.8953026195936775}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:27:54,989]\u001b[0m Trial 21 finished with value: 0.01508013042923299 and parameters: {'n_estimators': 708, 'max_depth': 5, 'learning_rate': 0.008115811236759899, 'colsample_bytree': 0.5165437881087566, 'subsample': 0.8494475964190706, 'alpha': 8.111218903031226, 'lambda': 0.3718139699928224, 'gamma': 1.580494291568084e-10, 'min_child_weight': 0.22926783950435137}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:01,022]\u001b[0m Trial 22 finished with value: 0.01862299455267209 and parameters: {'n_estimators': 722, 'max_depth': 5, 'learning_rate': 0.006939978627095979, 'colsample_bytree': 0.5349860075191175, 'subsample': 0.8145841618358765, 'alpha': 8.282604985327884, 'lambda': 0.7169022768668785, 'gamma': 1.170273910829939e-10, 'min_child_weight': 0.20059177190761646}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:05,493]\u001b[0m Trial 23 finished with value: 0.017979424906243834 and parameters: {'n_estimators': 801, 'max_depth': 4, 'learning_rate': 0.012451416014525908, 'colsample_bytree': 0.3801256257773503, 'subsample': 0.8937944262470044, 'alpha': 11.669141084347194, 'lambda': 0.9692269438012729, 'gamma': 1.1546367365580854e-08, 'min_child_weight': 0.10873933362917478}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:11,932]\u001b[0m Trial 24 finished with value: 0.00920764765842804 and parameters: {'n_estimators': 728, 'max_depth': 5, 'learning_rate': 0.015575595742254738, 'colsample_bytree': 0.7563578271928415, 'subsample': 0.7642075586372886, 'alpha': 28.33410275277648, 'lambda': 2.813588646240311, 'gamma': 1.0923396801860643e-09, 'min_child_weight': 0.21421264900016637}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:15,693]\u001b[0m Trial 25 finished with value: 0.01749228962209107 and parameters: {'n_estimators': 667, 'max_depth': 4, 'learning_rate': 0.005633378653599664, 'colsample_bytree': 0.4860000082559737, 'subsample': 0.7140913349605457, 'alpha': 5.081250297295812, 'lambda': 0.31092886553834315, 'gamma': 1.382897195303434e-06, 'min_child_weight': 0.7189645164907833}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:21,862]\u001b[0m Trial 26 finished with value: 0.01714831560088126 and parameters: {'n_estimators': 730, 'max_depth': 5, 'learning_rate': 0.00771843652603076, 'colsample_bytree': 0.6008735844561929, 'subsample': 0.7972302399530136, 'alpha': 3.602395400470878, 'lambda': 7.7848134261409125, 'gamma': 1.1227357477393512e-07, 'min_child_weight': 0.18990561574122228}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:25,381]\u001b[0m Trial 27 finished with value: 0.009531264633543078 and parameters: {'n_estimators': 611, 'max_depth': 4, 'learning_rate': 0.0012848854289482488, 'colsample_bytree': 0.637487046041677, 'subsample': 0.6422520198652512, 'alpha': 1.9872582133754928, 'lambda': 1.7428715265848127, 'gamma': 1.3769164937466029e-05, 'min_child_weight': 0.10554782440435025}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:31,890]\u001b[0m Trial 28 finished with value: 0.0019474893819921516 and parameters: {'n_estimators': 798, 'max_depth': 5, 'learning_rate': 0.020650028107655073, 'colsample_bytree': 0.3487474492962389, 'subsample': 0.8832682935499191, 'alpha': 6.36069926814192, 'lambda': 0.72683792051178, 'gamma': 1.2423071024093384e-09, 'min_child_weight': 1.6227444592612148}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:37,794]\u001b[0m Trial 29 finished with value: 0.01644138714161989 and parameters: {'n_estimators': 985, 'max_depth': 4, 'learning_rate': 0.014095440397325186, 'colsample_bytree': 0.835323334686217, 'subsample': 0.7877212430561785, 'alpha': 16.394209209259873, 'lambda': 43.63018348239385, 'gamma': 4.872602846562013e-10, 'min_child_weight': 3.0275621162822657}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:43,866]\u001b[0m Trial 30 finished with value: 0.010954934303313343 and parameters: {'n_estimators': 733, 'max_depth': 5, 'learning_rate': 0.01519779607904024, 'colsample_bytree': 0.521748599590841, 'subsample': 0.6877743834761904, 'alpha': 0.8222189717010192, 'lambda': 0.30998932764905635, 'gamma': 3.274987885114046e-08, 'min_child_weight': 0.5208027589609359}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:48,300]\u001b[0m Trial 31 finished with value: 0.016906088227044976 and parameters: {'n_estimators': 802, 'max_depth': 4, 'learning_rate': 0.012178402056234461, 'colsample_bytree': 0.3802212043613481, 'subsample': 0.8956154746405521, 'alpha': 13.791159571209038, 'lambda': 1.2319135058961794, 'gamma': 6.992744650557748e-09, 'min_child_weight': 0.1078496915960754}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:52,915]\u001b[0m Trial 32 finished with value: 0.0173465594584672 and parameters: {'n_estimators': 775, 'max_depth': 4, 'learning_rate': 0.00813456109376735, 'colsample_bytree': 0.4856267377322002, 'subsample': 0.8853515773055565, 'alpha': 12.084674748268338, 'lambda': 0.926174933516476, 'gamma': 5.396084294261029e-09, 'min_child_weight': 0.27422422536707025}. Best is trial 15 with value: 0.01878793556359398.\u001b[0m\n\u001b[32m[I 2022-08-25 23:28:57,770]\u001b[0m Trial 33 finished with value: 0.018893260238809585 and parameters: {'n_estimators': 872, 'max_depth': 4, 'learning_rate': 0.004981566510829739, 'colsample_bytree': 0.4474002006889836, 'subsample': 0.8351715954148976, 'alpha': 23.37645128348336, 'lambda': 0.21991422285297196, 'gamma': 5.40007694191208e-10, 'min_child_weight': 0.16056786398762687}. Best is trial 33 with value: 0.018893260238809585.\u001b[0m\n\u001b[32m[I 2022-08-25 23:29:01,722]\u001b[0m Trial 34 finished with value: 0.01612137212847331 and parameters: {'n_estimators': 911, 'max_depth': 3, 'learning_rate': 0.004074901014601317, 'colsample_bytree': 0.4382485530911527, 'subsample': 0.8166718261210345, 'alpha': 8.568175832936541, 'lambda': 0.22819359240606252, 'gamma': 8.204399301182333e-10, 'min_child_weight': 0.1830054911618686}. Best is trial 33 with value: 0.018893260238809585.\u001b[0m\n\u001b[32m[I 2022-08-25 23:29:06,724]\u001b[0m Trial 35 finished with value: 0.01861181420948766 and parameters: {'n_estimators': 871, 'max_depth': 4, 'learning_rate': 0.005722025679029831, 'colsample_bytree': 0.5617812830423055, 'subsample': 0.7292766974998116, 'alpha': 21.564192283261566, 'lambda': 0.455480380869882, 'gamma': 4.07524761656493e-10, 'min_child_weight': 0.1675358388538544}. Best is trial 33 with value: 0.018893260238809585.\u001b[0m\n\u001b[32m[I 2022-08-25 23:29:11,613]\u001b[0m Trial 36 finished with value: 0.015240913475829428 and parameters: {'n_estimators': 871, 'max_depth': 4, 'learning_rate': 0.0025910022293691133, 'colsample_bytree': 0.4496922746104964, 'subsample': 0.7135082011898972, 'alpha': 26.319599950624003, 'lambda': 0.10627457833702977, 'gamma': 6.062062647095407e-10, 'min_child_weight': 0.40020437605339054}. Best is trial 33 with value: 0.018893260238809585.\u001b[0m\n\u001b[32m[I 2022-08-25 23:29:16,453]\u001b[0m Trial 37 finished with value: 0.017002600839749974 and parameters: {'n_estimators': 874, 'max_depth': 4, 'learning_rate': 0.00958838990976629, 'colsample_bytree': 0.31267273347350494, 'subsample': 0.6063637672060087, 'alpha': 18.444972843590765, 'lambda': 0.19552332828320473, 'gamma': 0.0005502893760412938, 'min_child_weight': 0.810185995463262}. Best is trial 33 with value: 0.018893260238809585.\u001b[0m\n\u001b[32m[I 2022-08-25 23:29:20,509]\u001b[0m Trial 38 finished with value: 0.01904475392832938 and parameters: {'n_estimators': 970, 'max_depth': 3, 'learning_rate': 0.0050977294928359305, 'colsample_bytree': 0.607679988114767, 'subsample': 0.7333499650041201, 'alpha': 21.598320214878488, 'lambda': 165.2061301493154, 'gamma': 1.9262607351514998e-09, 'min_child_weight': 0.15652865250400633}. Best is trial 38 with value: 0.01904475392832938.\u001b[0m\n\u001b[32m[I 2022-08-25 23:29:24,703]\u001b[0m Trial 39 finished with value: 0.014966434034547067 and parameters: {'n_estimators': 979, 'max_depth': 3, 'learning_rate': 0.03921146012432155, 'colsample_bytree': 0.6225848260560789, 'subsample': 0.6713190124233631, 'alpha': 22.78122365311739, 'lambda': 191.00493496790895, 'gamma': 2.8964712473531805e-09, 'min_child_weight': 0.2694180093572453}. Best is trial 38 with value: 0.01904475392832938.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  187.6615331172943\n        n_estimators : 970\n           max_depth : 3\n       learning_rate : 0.0050977294928359305\n    colsample_bytree : 0.607679988114767\n           subsample : 0.7333499650041201\n               alpha : 21.598320214878488\n              lambda : 165.2061301493154\n               gamma : 1.9262607351514998e-09\n    min_child_weight : 0.15652865250400633\nbest objective value : 0.01904475392832938\nOptuna XGB train: 7.048069547071229 0.03465041881051134 189.68923211097717\nMin_prd:  125\nConstant guess:  6.801798451895731 0.0\nXGB test: 6.690736270893462 0.001886781638528956\nXGB GS test: 6.7089180299557585 0.006943571377436508\nOptuna XGB test: 6.696254549506994 0.006086495549869619\n226.8033630847931    min_prd      xgbf     xgbgs      xgbo\n0      125  0.001887  0.006944  0.006086\n","output_type":"stream"}]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:29:26.817164Z","iopub.execute_input":"2022-08-25T23:29:26.819171Z","iopub.status.idle":"2022-08-25T23:29:26.829648Z","shell.execute_reply.started":"2022-08-25T23:29:26.819135Z","shell.execute_reply":"2022-08-25T23:29:26.828448Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"   min_prd      xgbf     xgbgs      xgbo\n0      125  0.001887  0.006944  0.006086","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf</th>\n      <th>xgbgs</th>\n      <th>xgbo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>125</td>\n      <td>0.001887</td>\n      <td>0.006944</td>\n      <td>0.006086</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('Total time for a script: ', time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:29:26.831068Z","iopub.execute_input":"2022-08-25T23:29:26.831504Z","iopub.status.idle":"2022-08-25T23:29:26.840883Z","shell.execute_reply.started":"2022-08-25T23:29:26.831467Z","shell.execute_reply":"2022-08-25T23:29:26.839803Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"Total time for a script:  226.83410501480103\n","output_type":"stream"}]},{"cell_type":"code","source":"results.iloc[:,1:].mean()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:29:26.843462Z","iopub.execute_input":"2022-08-25T23:29:26.843801Z","iopub.status.idle":"2022-08-25T23:29:26.857273Z","shell.execute_reply.started":"2022-08-25T23:29:26.843766Z","shell.execute_reply":"2022-08-25T23:29:26.856342Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"xgbf     0.001887\nxgbgs    0.006944\nxgbo     0.006086\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# 3yr window, trials=20, cv_reg=0.03: 0.88%. runs 1 hr.\n# 3yr, t=40, cv_reg=0.04: 0.96%.\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:29:26.858448Z","iopub.execute_input":"2022-08-25T23:29:26.858766Z","iopub.status.idle":"2022-08-25T23:29:26.863897Z","shell.execute_reply.started":"2022-08-25T23:29:26.858736Z","shell.execute_reply":"2022-08-25T23:29:26.862771Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"results0","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:29:26.865542Z","iopub.execute_input":"2022-08-25T23:29:26.866347Z","iopub.status.idle":"2022-08-25T23:29:26.883159Z","shell.execute_reply.started":"2022-08-25T23:29:26.866267Z","shell.execute_reply":"2022-08-25T23:29:26.882047Z"},"trusted":true},"execution_count":61,"outputs":[{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"    min_prd      xgbf     xgbgs      xgbo\n0       100 -0.033926 -0.019374  -0.02375\n1       125  0.001887  0.006944  0.009033\n2       150  0.009361  0.018258  0.018259\n3       175 -0.008671 -0.002783 -0.000479\n4       200  0.012099   0.01089  0.009292\n5       225  0.024659  0.024404  0.025087\n6       250 -0.004759  0.003918  0.006247\n7       275  0.013317  0.017266  0.022251\n8       300 -0.009404 -0.000589 -0.000884\n9       325  -0.00607  0.004755  0.009447\n10      350 -0.014595  -0.00526 -0.000838\n11      375  0.001776  0.006979  0.007666\n12      400  0.006133  0.007229  0.004144\n13      425  0.009275  0.007466  0.007247\n14      450  0.012836  0.009185  0.009102\n15      475  0.016533  0.018099  0.013429\n16      500  0.049542  0.054596  0.057267\n17      525  0.026679  0.028794  0.030998\n18      550 -0.002261  0.001317 -0.000554\n19      575 -0.014905 -0.005435 -0.008113\n20      600  0.006236  0.007251  0.006108\n21      625 -0.020183 -0.013108 -0.013691\n22      650   0.04702  0.040816  0.047552\n23      675 -0.025128 -0.010631 -0.005623","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf</th>\n      <th>xgbgs</th>\n      <th>xgbo</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>100</td>\n      <td>-0.033926</td>\n      <td>-0.019374</td>\n      <td>-0.02375</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>125</td>\n      <td>0.001887</td>\n      <td>0.006944</td>\n      <td>0.009033</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>150</td>\n      <td>0.009361</td>\n      <td>0.018258</td>\n      <td>0.018259</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>175</td>\n      <td>-0.008671</td>\n      <td>-0.002783</td>\n      <td>-0.000479</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>200</td>\n      <td>0.012099</td>\n      <td>0.01089</td>\n      <td>0.009292</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>225</td>\n      <td>0.024659</td>\n      <td>0.024404</td>\n      <td>0.025087</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>250</td>\n      <td>-0.004759</td>\n      <td>0.003918</td>\n      <td>0.006247</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>275</td>\n      <td>0.013317</td>\n      <td>0.017266</td>\n      <td>0.022251</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>300</td>\n      <td>-0.009404</td>\n      <td>-0.000589</td>\n      <td>-0.000884</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>325</td>\n      <td>-0.00607</td>\n      <td>0.004755</td>\n      <td>0.009447</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>350</td>\n      <td>-0.014595</td>\n      <td>-0.00526</td>\n      <td>-0.000838</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>375</td>\n      <td>0.001776</td>\n      <td>0.006979</td>\n      <td>0.007666</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>400</td>\n      <td>0.006133</td>\n      <td>0.007229</td>\n      <td>0.004144</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>425</td>\n      <td>0.009275</td>\n      <td>0.007466</td>\n      <td>0.007247</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>450</td>\n      <td>0.012836</td>\n      <td>0.009185</td>\n      <td>0.009102</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>475</td>\n      <td>0.016533</td>\n      <td>0.018099</td>\n      <td>0.013429</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>500</td>\n      <td>0.049542</td>\n      <td>0.054596</td>\n      <td>0.057267</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>525</td>\n      <td>0.026679</td>\n      <td>0.028794</td>\n      <td>0.030998</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>550</td>\n      <td>-0.002261</td>\n      <td>0.001317</td>\n      <td>-0.000554</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>575</td>\n      <td>-0.014905</td>\n      <td>-0.005435</td>\n      <td>-0.008113</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>600</td>\n      <td>0.006236</td>\n      <td>0.007251</td>\n      <td>0.006108</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>625</td>\n      <td>-0.020183</td>\n      <td>-0.013108</td>\n      <td>-0.013691</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>650</td>\n      <td>0.04702</td>\n      <td>0.040816</td>\n      <td>0.047552</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>675</td>\n      <td>-0.025128</td>\n      <td>-0.010631</td>\n      <td>-0.005623</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train0.remainder__prd\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T23:35:36.835038Z","iopub.execute_input":"2022-08-25T23:35:36.836044Z","iopub.status.idle":"2022-08-25T23:35:36.845507Z","shell.execute_reply.started":"2022-08-25T23:35:36.836004Z","shell.execute_reply":"2022-08-25T23:35:36.844373Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"0        124.0\n1        125.0\n2        126.0\n3        127.0\n4        128.0\n         ...  \n38128    156.0\n38129    157.0\n38130    158.0\n38131    159.0\n38132    160.0\nName: remainder__prd, Length: 38133, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"### try NN  \n\nmodel_ann5_s = Sequential([\n    BatchNormalization(input_shape=(X_train.shape[0],)),\n    Dense(64, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n    Dropout(0.4),\n    BatchNormalization(),\n    Dense(64, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n    Dropout(0.4),\n    BatchNormalization(),\n    Dense(32, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n    Dropout(0.4),\n    BatchNormalization(),\n    Dense(32, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n    Dropout(0.4),\n    BatchNormalization(),\n    Dense(16, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n    Dropout(0.4),\n    BatchNormalization(),\n    Dense(16, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n    Dropout(0.4),\n    BatchNormalization(),\n    Dense(8, activation=tf.keras.layers.LeakyReLU(alpha=0.01)),\n    Dropout(0.4),\n    Dense(1)])\n\nprint(model_ann5_s.count_params())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping30 = EarlyStopping(patience=30)\ntime1 = time.time()\noptimizer_adam = tf.keras.optimizers.Adam()\nmodel_ann5_s.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_ann5_s.fit(X_train, y_train, validation_data=(s_test_X, s_test_y), \n                         batch_size=8192, epochs=50, verbose=2, callbacks=[early_stopping20])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nprint([r2_score(s_train_y, model_ann5_s.predict(s_train_X)), \n       r2_score(s_test_y, model_ann5_s.predict(s_test_X))])\nprint(time.time()-time1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
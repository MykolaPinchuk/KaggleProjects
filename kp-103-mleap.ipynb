{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Load libraries and data.\n2. pEDA. Look at feature distribution, fix them if they do not look right.\n3. Train-test split. Most likely couple years into test set. 2015-2018?. Impute missing values.\n4. Transform numerical features, add ohe for inds.\n5. Fit classic models: ols as a baseline, then xgb.\n6. Fir DL.\n\n\nNotes:\nideally, I want to use time-based cross-validation.\nsince I have panel data, it is not a trivial task.\nneed to find some solution online.\ne.g., https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8.\n","metadata":{}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, time, math, re, warnings, random, gc, dill, optuna\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\npd.set_option('display.max_columns', 110)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:38.840511Z","iopub.execute_input":"2022-08-24T22:52:38.840938Z","iopub.status.idle":"2022-08-24T22:52:38.850496Z","shell.execute_reply.started":"2022-08-24T22:52:38.840907Z","shell.execute_reply":"2022-08-24T22:52:38.849487Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:38.873219Z","iopub.execute_input":"2022-08-24T22:52:38.873509Z","iopub.status.idle":"2022-08-24T22:52:38.886266Z","shell.execute_reply.started":"2022-08-24T22:52:38.873483Z","shell.execute_reply":"2022-08-24T22:52:38.885227Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:38.891303Z","iopub.execute_input":"2022-08-24T22:52:38.892242Z","iopub.status.idle":"2022-08-24T22:52:38.900815Z","shell.execute_reply.started":"2022-08-24T22:52:38.892205Z","shell.execute_reply":"2022-08-24T22:52:38.899710Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"# 1. Import data #\n\ntime0 = time.time()\ndf = pd.read_csv('../input/cpcrsp-46/IMLEAP_v4.csv')\ndf.dropna(axis=0, subset=['bm', 'lbm', 'llme', 'lop', 'op', 'linv', 'mom122', 'beta_bw', 'ind'], inplace=True)\ndf.reset_index(inplace=True, drop=True)\ndf = df.sample(500000)\ndisplay(df.shape, df.head(), df.count())","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:38.952415Z","iopub.execute_input":"2022-08-24T22:52:38.953048Z","iopub.status.idle":"2022-08-24T22:52:53.210207Z","shell.execute_reply.started":"2022-08-24T22:52:38.953015Z","shell.execute_reply":"2022-08-24T22:52:53.209212Z"},"trusted":true},"execution_count":64,"outputs":[{"output_type":"display_data","data":{"text/plain":"(500000, 46)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         PERMNO  prd      mom482      mom242  year     RET   ind        bm  \\\n24754     10516  583  176.359758   73.441237  2006 -8.9914   2.0 -0.558674   \n871216    72486  475  -57.265976  -45.032562  1997 -5.7954  35.0  0.382251   \n1263312   91837  654         NaN         NaN  2012  7.2868   2.0  1.170077   \n1003915   79007  493   78.803834   83.898431  1999  7.0980  43.0 -0.492379   \n301830    27334  268  283.349209  133.410201  1980  3.8788  23.0 -0.278489   \n\n               op        gp       inv      mom11     mom122      amhd  \\\n24754    0.090203  0.166468 -0.039792   1.226700  51.124143 -4.463255   \n871216  -0.011705  0.512255  0.140350 -22.380465  39.656999  1.008567   \n1263312  0.020398  0.264525 -0.048470  -1.319300  13.120335  1.999913   \n1003915  0.124321  0.749175 -0.081543  20.630000   5.298861  2.637574   \n301830   0.266020  0.489221  0.325485  10.036000  49.766223  3.222145   \n\n         ivol_capm  ivol_ff5   beta_bw      MAX     vol1m     vol6m    vol12m  \\\n24754     1.893507  1.642687  1.132881   4.0361  2.026777  2.301513  2.243252   \n871216    3.865288  3.337962  1.342912   9.1820  5.153912  4.445470  4.005972   \n1263312   3.178143  2.769494  0.801295   6.1962  3.158612  6.397387  5.302053   \n1003915   3.725205  3.287315  0.841777  13.6662  3.778359  3.466458  3.594777   \n301830    3.138099  2.960963  1.148705  11.5702  3.786842  2.863860  2.601979   \n\n            BAspr       size       lbm       lop       lgp      linv  \\\n24754    0.029163  10.138971 -0.558664  0.075694  0.145437  0.127216   \n871216   0.540541   5.189703 -0.240851  0.237031  0.760029  0.110502   \n1263312  0.332226   4.781621 -0.383464 -0.026280  0.244258  0.054629   \n1003915  0.413223   5.299365 -0.146282 -0.035868  0.556935 -0.027483   \n301830        NaN   5.105031  0.230423  0.272473  0.518809  0.080581   \n\n             llme    l1amhd      l1MAX   l1BAspr    l3amhd    l3MAX   l3BAspr  \\\n24754    9.675290 -4.413513   2.957100  0.029163 -4.275726   4.9198  0.029163   \n871216   5.092572  1.025213  21.135115  0.800000  1.035256  11.2486  1.052632   \n1263312  4.792733  2.034166   3.828500  0.646204  2.096141   7.2202  0.142450   \n1003915  5.005556  2.645495   8.315300  1.485149  2.646992   3.7310  0.662252   \n301830   4.527471  3.274573   7.174400       NaN  3.303809   8.2743       NaN   \n\n           l6amhd   l6MAX   l6BAspr   l12amhd     l12MAX  l12BAspr  l12mom122  \\\n24754   -4.106334  4.6170  0.157895 -3.973023   2.957100  0.285831  26.289052   \n871216   0.844400  2.6116  2.898551  0.297797  21.135115  2.409639 -30.855380   \n1263312  2.092315  8.6379  0.655738  1.342052   3.828500  0.867679 -41.001808   \n1003915  2.650675  9.9850  2.857143  2.464920   8.315300  2.000000  64.446651   \n301830   3.126726  3.1555       NaN  3.419906   7.174400       NaN  43.534268   \n\n         l12ivol_capm  l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  \n24754        1.110956     0.829009    0.914059  1.566515   2.083331  \n871216       2.241253     1.849664    1.147520  3.002618   3.396079  \n1263312      2.674886     2.352131    0.827959  3.727579   3.493450  \n1003915      2.825106     1.892719    0.850100  3.045258   3.054876  \n301830       2.115387     1.994306    0.961529  2.137674   1.990289  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24754</th>\n      <td>10516</td>\n      <td>583</td>\n      <td>176.359758</td>\n      <td>73.441237</td>\n      <td>2006</td>\n      <td>-8.9914</td>\n      <td>2.0</td>\n      <td>-0.558674</td>\n      <td>0.090203</td>\n      <td>0.166468</td>\n      <td>-0.039792</td>\n      <td>1.226700</td>\n      <td>51.124143</td>\n      <td>-4.463255</td>\n      <td>1.893507</td>\n      <td>1.642687</td>\n      <td>1.132881</td>\n      <td>4.0361</td>\n      <td>2.026777</td>\n      <td>2.301513</td>\n      <td>2.243252</td>\n      <td>0.029163</td>\n      <td>10.138971</td>\n      <td>-0.558664</td>\n      <td>0.075694</td>\n      <td>0.145437</td>\n      <td>0.127216</td>\n      <td>9.675290</td>\n      <td>-4.413513</td>\n      <td>2.957100</td>\n      <td>0.029163</td>\n      <td>-4.275726</td>\n      <td>4.9198</td>\n      <td>0.029163</td>\n      <td>-4.106334</td>\n      <td>4.6170</td>\n      <td>0.157895</td>\n      <td>-3.973023</td>\n      <td>2.957100</td>\n      <td>0.285831</td>\n      <td>26.289052</td>\n      <td>1.110956</td>\n      <td>0.829009</td>\n      <td>0.914059</td>\n      <td>1.566515</td>\n      <td>2.083331</td>\n    </tr>\n    <tr>\n      <th>871216</th>\n      <td>72486</td>\n      <td>475</td>\n      <td>-57.265976</td>\n      <td>-45.032562</td>\n      <td>1997</td>\n      <td>-5.7954</td>\n      <td>35.0</td>\n      <td>0.382251</td>\n      <td>-0.011705</td>\n      <td>0.512255</td>\n      <td>0.140350</td>\n      <td>-22.380465</td>\n      <td>39.656999</td>\n      <td>1.008567</td>\n      <td>3.865288</td>\n      <td>3.337962</td>\n      <td>1.342912</td>\n      <td>9.1820</td>\n      <td>5.153912</td>\n      <td>4.445470</td>\n      <td>4.005972</td>\n      <td>0.540541</td>\n      <td>5.189703</td>\n      <td>-0.240851</td>\n      <td>0.237031</td>\n      <td>0.760029</td>\n      <td>0.110502</td>\n      <td>5.092572</td>\n      <td>1.025213</td>\n      <td>21.135115</td>\n      <td>0.800000</td>\n      <td>1.035256</td>\n      <td>11.2486</td>\n      <td>1.052632</td>\n      <td>0.844400</td>\n      <td>2.6116</td>\n      <td>2.898551</td>\n      <td>0.297797</td>\n      <td>21.135115</td>\n      <td>2.409639</td>\n      <td>-30.855380</td>\n      <td>2.241253</td>\n      <td>1.849664</td>\n      <td>1.147520</td>\n      <td>3.002618</td>\n      <td>3.396079</td>\n    </tr>\n    <tr>\n      <th>1263312</th>\n      <td>91837</td>\n      <td>654</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2012</td>\n      <td>7.2868</td>\n      <td>2.0</td>\n      <td>1.170077</td>\n      <td>0.020398</td>\n      <td>0.264525</td>\n      <td>-0.048470</td>\n      <td>-1.319300</td>\n      <td>13.120335</td>\n      <td>1.999913</td>\n      <td>3.178143</td>\n      <td>2.769494</td>\n      <td>0.801295</td>\n      <td>6.1962</td>\n      <td>3.158612</td>\n      <td>6.397387</td>\n      <td>5.302053</td>\n      <td>0.332226</td>\n      <td>4.781621</td>\n      <td>-0.383464</td>\n      <td>-0.026280</td>\n      <td>0.244258</td>\n      <td>0.054629</td>\n      <td>4.792733</td>\n      <td>2.034166</td>\n      <td>3.828500</td>\n      <td>0.646204</td>\n      <td>2.096141</td>\n      <td>7.2202</td>\n      <td>0.142450</td>\n      <td>2.092315</td>\n      <td>8.6379</td>\n      <td>0.655738</td>\n      <td>1.342052</td>\n      <td>3.828500</td>\n      <td>0.867679</td>\n      <td>-41.001808</td>\n      <td>2.674886</td>\n      <td>2.352131</td>\n      <td>0.827959</td>\n      <td>3.727579</td>\n      <td>3.493450</td>\n    </tr>\n    <tr>\n      <th>1003915</th>\n      <td>79007</td>\n      <td>493</td>\n      <td>78.803834</td>\n      <td>83.898431</td>\n      <td>1999</td>\n      <td>7.0980</td>\n      <td>43.0</td>\n      <td>-0.492379</td>\n      <td>0.124321</td>\n      <td>0.749175</td>\n      <td>-0.081543</td>\n      <td>20.630000</td>\n      <td>5.298861</td>\n      <td>2.637574</td>\n      <td>3.725205</td>\n      <td>3.287315</td>\n      <td>0.841777</td>\n      <td>13.6662</td>\n      <td>3.778359</td>\n      <td>3.466458</td>\n      <td>3.594777</td>\n      <td>0.413223</td>\n      <td>5.299365</td>\n      <td>-0.146282</td>\n      <td>-0.035868</td>\n      <td>0.556935</td>\n      <td>-0.027483</td>\n      <td>5.005556</td>\n      <td>2.645495</td>\n      <td>8.315300</td>\n      <td>1.485149</td>\n      <td>2.646992</td>\n      <td>3.7310</td>\n      <td>0.662252</td>\n      <td>2.650675</td>\n      <td>9.9850</td>\n      <td>2.857143</td>\n      <td>2.464920</td>\n      <td>8.315300</td>\n      <td>2.000000</td>\n      <td>64.446651</td>\n      <td>2.825106</td>\n      <td>1.892719</td>\n      <td>0.850100</td>\n      <td>3.045258</td>\n      <td>3.054876</td>\n    </tr>\n    <tr>\n      <th>301830</th>\n      <td>27334</td>\n      <td>268</td>\n      <td>283.349209</td>\n      <td>133.410201</td>\n      <td>1980</td>\n      <td>3.8788</td>\n      <td>23.0</td>\n      <td>-0.278489</td>\n      <td>0.266020</td>\n      <td>0.489221</td>\n      <td>0.325485</td>\n      <td>10.036000</td>\n      <td>49.766223</td>\n      <td>3.222145</td>\n      <td>3.138099</td>\n      <td>2.960963</td>\n      <td>1.148705</td>\n      <td>11.5702</td>\n      <td>3.786842</td>\n      <td>2.863860</td>\n      <td>2.601979</td>\n      <td>NaN</td>\n      <td>5.105031</td>\n      <td>0.230423</td>\n      <td>0.272473</td>\n      <td>0.518809</td>\n      <td>0.080581</td>\n      <td>4.527471</td>\n      <td>3.274573</td>\n      <td>7.174400</td>\n      <td>NaN</td>\n      <td>3.303809</td>\n      <td>8.2743</td>\n      <td>NaN</td>\n      <td>3.126726</td>\n      <td>3.1555</td>\n      <td>NaN</td>\n      <td>3.419906</td>\n      <td>7.174400</td>\n      <td>NaN</td>\n      <td>43.534268</td>\n      <td>2.115387</td>\n      <td>1.994306</td>\n      <td>0.961529</td>\n      <td>2.137674</td>\n      <td>1.990289</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"PERMNO          500000\nprd             500000\nmom482          425629\nmom242          490714\nyear            500000\nRET             500000\nind             500000\nbm              500000\nop              500000\ngp              500000\ninv             499547\nmom11           500000\nmom122          500000\namhd            411580\nivol_capm       499975\nivol_ff5        499975\nbeta_bw         500000\nMAX             500000\nvol1m           499917\nvol6m           499555\nvol12m          498831\nBAspr           291382\nsize            500000\nlbm             500000\nlop             500000\nlgp             500000\nlinv            500000\nllme            500000\nl1amhd          411291\nl1MAX           499983\nl1BAspr         290558\nl3amhd          410746\nl3MAX           499882\nl3BAspr         289172\nl6amhd          409922\nl6MAX           499802\nl6BAspr         286517\nl12amhd         407556\nl12MAX          499983\nl12BAspr        281649\nl12mom122       496630\nl12ivol_capm    499536\nl12ivol_ff5     499536\nl12beta_bw      499701\nl12vol6m        498639\nl12vol12m       493858\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# 2. pEDA #\n\ndf.RET.hist()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:53.212194Z","iopub.execute_input":"2022-08-24T22:52:53.212676Z","iopub.status.idle":"2022-08-24T22:52:53.435881Z","shell.execute_reply.started":"2022-08-24T22:52:53.212636Z","shell.execute_reply":"2022-08-24T22:52:53.434895Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# explore feature distibution, adjust if seems unreasonable","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:53.437276Z","iopub.execute_input":"2022-08-24T22:52:53.437634Z","iopub.status.idle":"2022-08-24T22:52:53.443440Z","shell.execute_reply.started":"2022-08-24T22:52:53.437597Z","shell.execute_reply":"2022-08-24T22:52:53.442015Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# add dummies for some missing features\n\nfeatures_miss_dummies = ['amhd', 'BAspr']\n\nfor col in features_miss_dummies:\n    df[col+'_miss'] = df[col].isnull().astype(int)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:53.447379Z","iopub.execute_input":"2022-08-24T22:52:53.447876Z","iopub.status.idle":"2022-08-24T22:52:53.496208Z","shell.execute_reply.started":"2022-08-24T22:52:53.447838Z","shell.execute_reply":"2022-08-24T22:52:53.495038Z"},"trusted":true},"execution_count":67,"outputs":[{"execution_count":67,"output_type":"execute_result","data":{"text/plain":"         PERMNO  prd      mom482      mom242  year     RET   ind        bm  \\\n24754     10516  583  176.359758   73.441237  2006 -8.9914   2.0 -0.558674   \n871216    72486  475  -57.265976  -45.032562  1997 -5.7954  35.0  0.382251   \n1263312   91837  654         NaN         NaN  2012  7.2868   2.0  1.170077   \n1003915   79007  493   78.803834   83.898431  1999  7.0980  43.0 -0.492379   \n301830    27334  268  283.349209  133.410201  1980  3.8788  23.0 -0.278489   \n\n               op        gp       inv      mom11     mom122      amhd  \\\n24754    0.090203  0.166468 -0.039792   1.226700  51.124143 -4.463255   \n871216  -0.011705  0.512255  0.140350 -22.380465  39.656999  1.008567   \n1263312  0.020398  0.264525 -0.048470  -1.319300  13.120335  1.999913   \n1003915  0.124321  0.749175 -0.081543  20.630000   5.298861  2.637574   \n301830   0.266020  0.489221  0.325485  10.036000  49.766223  3.222145   \n\n         ivol_capm  ivol_ff5   beta_bw      MAX     vol1m     vol6m    vol12m  \\\n24754     1.893507  1.642687  1.132881   4.0361  2.026777  2.301513  2.243252   \n871216    3.865288  3.337962  1.342912   9.1820  5.153912  4.445470  4.005972   \n1263312   3.178143  2.769494  0.801295   6.1962  3.158612  6.397387  5.302053   \n1003915   3.725205  3.287315  0.841777  13.6662  3.778359  3.466458  3.594777   \n301830    3.138099  2.960963  1.148705  11.5702  3.786842  2.863860  2.601979   \n\n            BAspr       size       lbm       lop       lgp      linv  \\\n24754    0.029163  10.138971 -0.558664  0.075694  0.145437  0.127216   \n871216   0.540541   5.189703 -0.240851  0.237031  0.760029  0.110502   \n1263312  0.332226   4.781621 -0.383464 -0.026280  0.244258  0.054629   \n1003915  0.413223   5.299365 -0.146282 -0.035868  0.556935 -0.027483   \n301830        NaN   5.105031  0.230423  0.272473  0.518809  0.080581   \n\n             llme    l1amhd      l1MAX   l1BAspr    l3amhd    l3MAX   l3BAspr  \\\n24754    9.675290 -4.413513   2.957100  0.029163 -4.275726   4.9198  0.029163   \n871216   5.092572  1.025213  21.135115  0.800000  1.035256  11.2486  1.052632   \n1263312  4.792733  2.034166   3.828500  0.646204  2.096141   7.2202  0.142450   \n1003915  5.005556  2.645495   8.315300  1.485149  2.646992   3.7310  0.662252   \n301830   4.527471  3.274573   7.174400       NaN  3.303809   8.2743       NaN   \n\n           l6amhd   l6MAX   l6BAspr   l12amhd     l12MAX  l12BAspr  l12mom122  \\\n24754   -4.106334  4.6170  0.157895 -3.973023   2.957100  0.285831  26.289052   \n871216   0.844400  2.6116  2.898551  0.297797  21.135115  2.409639 -30.855380   \n1263312  2.092315  8.6379  0.655738  1.342052   3.828500  0.867679 -41.001808   \n1003915  2.650675  9.9850  2.857143  2.464920   8.315300  2.000000  64.446651   \n301830   3.126726  3.1555       NaN  3.419906   7.174400       NaN  43.534268   \n\n         l12ivol_capm  l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  \\\n24754        1.110956     0.829009    0.914059  1.566515   2.083331   \n871216       2.241253     1.849664    1.147520  3.002618   3.396079   \n1263312      2.674886     2.352131    0.827959  3.727579   3.493450   \n1003915      2.825106     1.892719    0.850100  3.045258   3.054876   \n301830       2.115387     1.994306    0.961529  2.137674   1.990289   \n\n         amhd_miss  BAspr_miss  \n24754            0           0  \n871216           0           0  \n1263312          0           0  \n1003915          0           0  \n301830           0           1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>24754</th>\n      <td>10516</td>\n      <td>583</td>\n      <td>176.359758</td>\n      <td>73.441237</td>\n      <td>2006</td>\n      <td>-8.9914</td>\n      <td>2.0</td>\n      <td>-0.558674</td>\n      <td>0.090203</td>\n      <td>0.166468</td>\n      <td>-0.039792</td>\n      <td>1.226700</td>\n      <td>51.124143</td>\n      <td>-4.463255</td>\n      <td>1.893507</td>\n      <td>1.642687</td>\n      <td>1.132881</td>\n      <td>4.0361</td>\n      <td>2.026777</td>\n      <td>2.301513</td>\n      <td>2.243252</td>\n      <td>0.029163</td>\n      <td>10.138971</td>\n      <td>-0.558664</td>\n      <td>0.075694</td>\n      <td>0.145437</td>\n      <td>0.127216</td>\n      <td>9.675290</td>\n      <td>-4.413513</td>\n      <td>2.957100</td>\n      <td>0.029163</td>\n      <td>-4.275726</td>\n      <td>4.9198</td>\n      <td>0.029163</td>\n      <td>-4.106334</td>\n      <td>4.6170</td>\n      <td>0.157895</td>\n      <td>-3.973023</td>\n      <td>2.957100</td>\n      <td>0.285831</td>\n      <td>26.289052</td>\n      <td>1.110956</td>\n      <td>0.829009</td>\n      <td>0.914059</td>\n      <td>1.566515</td>\n      <td>2.083331</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>871216</th>\n      <td>72486</td>\n      <td>475</td>\n      <td>-57.265976</td>\n      <td>-45.032562</td>\n      <td>1997</td>\n      <td>-5.7954</td>\n      <td>35.0</td>\n      <td>0.382251</td>\n      <td>-0.011705</td>\n      <td>0.512255</td>\n      <td>0.140350</td>\n      <td>-22.380465</td>\n      <td>39.656999</td>\n      <td>1.008567</td>\n      <td>3.865288</td>\n      <td>3.337962</td>\n      <td>1.342912</td>\n      <td>9.1820</td>\n      <td>5.153912</td>\n      <td>4.445470</td>\n      <td>4.005972</td>\n      <td>0.540541</td>\n      <td>5.189703</td>\n      <td>-0.240851</td>\n      <td>0.237031</td>\n      <td>0.760029</td>\n      <td>0.110502</td>\n      <td>5.092572</td>\n      <td>1.025213</td>\n      <td>21.135115</td>\n      <td>0.800000</td>\n      <td>1.035256</td>\n      <td>11.2486</td>\n      <td>1.052632</td>\n      <td>0.844400</td>\n      <td>2.6116</td>\n      <td>2.898551</td>\n      <td>0.297797</td>\n      <td>21.135115</td>\n      <td>2.409639</td>\n      <td>-30.855380</td>\n      <td>2.241253</td>\n      <td>1.849664</td>\n      <td>1.147520</td>\n      <td>3.002618</td>\n      <td>3.396079</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1263312</th>\n      <td>91837</td>\n      <td>654</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2012</td>\n      <td>7.2868</td>\n      <td>2.0</td>\n      <td>1.170077</td>\n      <td>0.020398</td>\n      <td>0.264525</td>\n      <td>-0.048470</td>\n      <td>-1.319300</td>\n      <td>13.120335</td>\n      <td>1.999913</td>\n      <td>3.178143</td>\n      <td>2.769494</td>\n      <td>0.801295</td>\n      <td>6.1962</td>\n      <td>3.158612</td>\n      <td>6.397387</td>\n      <td>5.302053</td>\n      <td>0.332226</td>\n      <td>4.781621</td>\n      <td>-0.383464</td>\n      <td>-0.026280</td>\n      <td>0.244258</td>\n      <td>0.054629</td>\n      <td>4.792733</td>\n      <td>2.034166</td>\n      <td>3.828500</td>\n      <td>0.646204</td>\n      <td>2.096141</td>\n      <td>7.2202</td>\n      <td>0.142450</td>\n      <td>2.092315</td>\n      <td>8.6379</td>\n      <td>0.655738</td>\n      <td>1.342052</td>\n      <td>3.828500</td>\n      <td>0.867679</td>\n      <td>-41.001808</td>\n      <td>2.674886</td>\n      <td>2.352131</td>\n      <td>0.827959</td>\n      <td>3.727579</td>\n      <td>3.493450</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1003915</th>\n      <td>79007</td>\n      <td>493</td>\n      <td>78.803834</td>\n      <td>83.898431</td>\n      <td>1999</td>\n      <td>7.0980</td>\n      <td>43.0</td>\n      <td>-0.492379</td>\n      <td>0.124321</td>\n      <td>0.749175</td>\n      <td>-0.081543</td>\n      <td>20.630000</td>\n      <td>5.298861</td>\n      <td>2.637574</td>\n      <td>3.725205</td>\n      <td>3.287315</td>\n      <td>0.841777</td>\n      <td>13.6662</td>\n      <td>3.778359</td>\n      <td>3.466458</td>\n      <td>3.594777</td>\n      <td>0.413223</td>\n      <td>5.299365</td>\n      <td>-0.146282</td>\n      <td>-0.035868</td>\n      <td>0.556935</td>\n      <td>-0.027483</td>\n      <td>5.005556</td>\n      <td>2.645495</td>\n      <td>8.315300</td>\n      <td>1.485149</td>\n      <td>2.646992</td>\n      <td>3.7310</td>\n      <td>0.662252</td>\n      <td>2.650675</td>\n      <td>9.9850</td>\n      <td>2.857143</td>\n      <td>2.464920</td>\n      <td>8.315300</td>\n      <td>2.000000</td>\n      <td>64.446651</td>\n      <td>2.825106</td>\n      <td>1.892719</td>\n      <td>0.850100</td>\n      <td>3.045258</td>\n      <td>3.054876</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>301830</th>\n      <td>27334</td>\n      <td>268</td>\n      <td>283.349209</td>\n      <td>133.410201</td>\n      <td>1980</td>\n      <td>3.8788</td>\n      <td>23.0</td>\n      <td>-0.278489</td>\n      <td>0.266020</td>\n      <td>0.489221</td>\n      <td>0.325485</td>\n      <td>10.036000</td>\n      <td>49.766223</td>\n      <td>3.222145</td>\n      <td>3.138099</td>\n      <td>2.960963</td>\n      <td>1.148705</td>\n      <td>11.5702</td>\n      <td>3.786842</td>\n      <td>2.863860</td>\n      <td>2.601979</td>\n      <td>NaN</td>\n      <td>5.105031</td>\n      <td>0.230423</td>\n      <td>0.272473</td>\n      <td>0.518809</td>\n      <td>0.080581</td>\n      <td>4.527471</td>\n      <td>3.274573</td>\n      <td>7.174400</td>\n      <td>NaN</td>\n      <td>3.303809</td>\n      <td>8.2743</td>\n      <td>NaN</td>\n      <td>3.126726</td>\n      <td>3.1555</td>\n      <td>NaN</td>\n      <td>3.419906</td>\n      <td>7.174400</td>\n      <td>NaN</td>\n      <td>43.534268</td>\n      <td>2.115387</td>\n      <td>1.994306</td>\n      <td>0.961529</td>\n      <td>2.137674</td>\n      <td>1.990289</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 3. Train-test split #\n\ntemp_cols = ['PERMNO', 'prd', 'year']\n\ntrain = df[df.year<2016]\ntest = df[df.year>=2016]\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\ntrain.drop(columns=temp_cols, inplace=True)\ntest.drop(columns=temp_cols, inplace=True)\ndisplay(train.shape, test.shape, train.head(3), test.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:53.498128Z","iopub.execute_input":"2022-08-24T22:52:53.498485Z","iopub.status.idle":"2022-08-24T22:52:53.749876Z","shell.execute_reply.started":"2022-08-24T22:52:53.498451Z","shell.execute_reply":"2022-08-24T22:52:53.748712Z"},"trusted":true},"execution_count":68,"outputs":[{"output_type":"display_data","data":{"text/plain":"(478874, 45)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(21126, 45)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       mom482     mom242     RET   ind        bm        op        gp  \\\n0  176.359758  73.441237 -8.9914   2.0 -0.558674  0.090203  0.166468   \n1  -57.265976 -45.032562 -5.7954  35.0  0.382251 -0.011705  0.512255   \n2         NaN        NaN  7.2868   2.0  1.170077  0.020398  0.264525   \n\n        inv      mom11     mom122      amhd  ivol_capm  ivol_ff5   beta_bw  \\\n0 -0.039792   1.226700  51.124143 -4.463255   1.893507  1.642687  1.132881   \n1  0.140350 -22.380465  39.656999  1.008567   3.865288  3.337962  1.342912   \n2 -0.048470  -1.319300  13.120335  1.999913   3.178143  2.769494  0.801295   \n\n      MAX     vol1m     vol6m    vol12m     BAspr       size       lbm  \\\n0  4.0361  2.026777  2.301513  2.243252  0.029163  10.138971 -0.558664   \n1  9.1820  5.153912  4.445470  4.005972  0.540541   5.189703 -0.240851   \n2  6.1962  3.158612  6.397387  5.302053  0.332226   4.781621 -0.383464   \n\n        lop       lgp      linv      llme    l1amhd      l1MAX   l1BAspr  \\\n0  0.075694  0.145437  0.127216  9.675290 -4.413513   2.957100  0.029163   \n1  0.237031  0.760029  0.110502  5.092572  1.025213  21.135115  0.800000   \n2 -0.026280  0.244258  0.054629  4.792733  2.034166   3.828500  0.646204   \n\n     l3amhd    l3MAX   l3BAspr    l6amhd   l6MAX   l6BAspr   l12amhd  \\\n0 -4.275726   4.9198  0.029163 -4.106334  4.6170  0.157895 -3.973023   \n1  1.035256  11.2486  1.052632  0.844400  2.6116  2.898551  0.297797   \n2  2.096141   7.2202  0.142450  2.092315  8.6379  0.655738  1.342052   \n\n      l12MAX  l12BAspr  l12mom122  l12ivol_capm  l12ivol_ff5  l12beta_bw  \\\n0   2.957100  0.285831  26.289052      1.110956     0.829009    0.914059   \n1  21.135115  2.409639 -30.855380      2.241253     1.849664    1.147520   \n2   3.828500  0.867679 -41.001808      2.674886     2.352131    0.827959   \n\n   l12vol6m  l12vol12m  amhd_miss  BAspr_miss  \n0  1.566515   2.083331          0           0  \n1  3.002618   3.396079          0           0  \n2  3.727579   3.493450          0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>176.359758</td>\n      <td>73.441237</td>\n      <td>-8.9914</td>\n      <td>2.0</td>\n      <td>-0.558674</td>\n      <td>0.090203</td>\n      <td>0.166468</td>\n      <td>-0.039792</td>\n      <td>1.226700</td>\n      <td>51.124143</td>\n      <td>-4.463255</td>\n      <td>1.893507</td>\n      <td>1.642687</td>\n      <td>1.132881</td>\n      <td>4.0361</td>\n      <td>2.026777</td>\n      <td>2.301513</td>\n      <td>2.243252</td>\n      <td>0.029163</td>\n      <td>10.138971</td>\n      <td>-0.558664</td>\n      <td>0.075694</td>\n      <td>0.145437</td>\n      <td>0.127216</td>\n      <td>9.675290</td>\n      <td>-4.413513</td>\n      <td>2.957100</td>\n      <td>0.029163</td>\n      <td>-4.275726</td>\n      <td>4.9198</td>\n      <td>0.029163</td>\n      <td>-4.106334</td>\n      <td>4.6170</td>\n      <td>0.157895</td>\n      <td>-3.973023</td>\n      <td>2.957100</td>\n      <td>0.285831</td>\n      <td>26.289052</td>\n      <td>1.110956</td>\n      <td>0.829009</td>\n      <td>0.914059</td>\n      <td>1.566515</td>\n      <td>2.083331</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-57.265976</td>\n      <td>-45.032562</td>\n      <td>-5.7954</td>\n      <td>35.0</td>\n      <td>0.382251</td>\n      <td>-0.011705</td>\n      <td>0.512255</td>\n      <td>0.140350</td>\n      <td>-22.380465</td>\n      <td>39.656999</td>\n      <td>1.008567</td>\n      <td>3.865288</td>\n      <td>3.337962</td>\n      <td>1.342912</td>\n      <td>9.1820</td>\n      <td>5.153912</td>\n      <td>4.445470</td>\n      <td>4.005972</td>\n      <td>0.540541</td>\n      <td>5.189703</td>\n      <td>-0.240851</td>\n      <td>0.237031</td>\n      <td>0.760029</td>\n      <td>0.110502</td>\n      <td>5.092572</td>\n      <td>1.025213</td>\n      <td>21.135115</td>\n      <td>0.800000</td>\n      <td>1.035256</td>\n      <td>11.2486</td>\n      <td>1.052632</td>\n      <td>0.844400</td>\n      <td>2.6116</td>\n      <td>2.898551</td>\n      <td>0.297797</td>\n      <td>21.135115</td>\n      <td>2.409639</td>\n      <td>-30.855380</td>\n      <td>2.241253</td>\n      <td>1.849664</td>\n      <td>1.147520</td>\n      <td>3.002618</td>\n      <td>3.396079</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>7.2868</td>\n      <td>2.0</td>\n      <td>1.170077</td>\n      <td>0.020398</td>\n      <td>0.264525</td>\n      <td>-0.048470</td>\n      <td>-1.319300</td>\n      <td>13.120335</td>\n      <td>1.999913</td>\n      <td>3.178143</td>\n      <td>2.769494</td>\n      <td>0.801295</td>\n      <td>6.1962</td>\n      <td>3.158612</td>\n      <td>6.397387</td>\n      <td>5.302053</td>\n      <td>0.332226</td>\n      <td>4.781621</td>\n      <td>-0.383464</td>\n      <td>-0.026280</td>\n      <td>0.244258</td>\n      <td>0.054629</td>\n      <td>4.792733</td>\n      <td>2.034166</td>\n      <td>3.828500</td>\n      <td>0.646204</td>\n      <td>2.096141</td>\n      <td>7.2202</td>\n      <td>0.142450</td>\n      <td>2.092315</td>\n      <td>8.6379</td>\n      <td>0.655738</td>\n      <td>1.342052</td>\n      <td>3.828500</td>\n      <td>0.867679</td>\n      <td>-41.001808</td>\n      <td>2.674886</td>\n      <td>2.352131</td>\n      <td>0.827959</td>\n      <td>3.727579</td>\n      <td>3.493450</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"      mom482     mom242     RET   ind        bm        op        gp       inv  \\\n0  95.438135  10.733997 -3.3003   7.0 -0.455080  0.162481  0.377474  0.031809   \n1        NaN -39.913708 -3.4600  30.0  0.190221  0.170266  0.203607  0.223073   \n2  36.928655  26.462302 -4.0485  16.0 -0.578997  0.108499  0.209926  0.105991   \n\n    mom11     mom122      amhd  ivol_capm  ivol_ff5   beta_bw     MAX  \\\n0 -2.8219  13.712694  0.701320   2.263010  2.031816  1.007247  7.3186   \n1 -7.4902  28.201480 -3.570285   1.328922  1.255872  0.973313  2.1145   \n2  6.7020  23.239140 -0.439693   1.209944  0.906851  1.169143  4.4022   \n\n      vol1m     vol6m    vol12m     BAspr      size       lbm       lop  \\\n0  2.764227  2.411597  2.129968  0.292826  5.846798 -0.702919  0.184985   \n1  1.466879  2.006033  2.911217  0.039904  8.947457 -0.771174  0.137733   \n2  1.443144  1.597765  1.712648  0.053865  6.543669 -0.521442  0.112038   \n\n        lgp      linv      llme    l1amhd   l1MAX   l1BAspr    l3amhd   l3MAX  \\\n0  0.407017 -0.014497  5.723574  0.809948  7.0184  0.714967  1.009413  1.8569   \n1  0.172681  0.749959  8.649971 -3.421372  4.2825  0.038565 -3.284548  4.4537   \n2  0.227148  0.015573  6.250009 -0.425686  3.6408  0.029163 -0.317304  9.0719   \n\n    l3BAspr    l6amhd   l6MAX   l6BAspr   l12amhd  l12MAX  l12BAspr  \\\n0  0.221141  1.280392  2.9982  0.173611  1.381726  7.0184  0.323276   \n1  0.038820 -3.169157  4.5357  0.034329 -3.272097  4.2825  0.047687   \n2  0.155763 -0.224665  4.4999  0.036062 -0.206434  3.6408  0.035398   \n\n   l12mom122  l12ivol_capm  l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  \\\n0  -8.222723      1.698998     1.377168    1.372075  2.662855   2.387393   \n1 -49.765657      4.033896     3.226864    1.252497  3.314202   3.050424   \n2  -3.945672      1.689567     1.266355    1.049934  1.992002   2.418204   \n\n   amhd_miss  BAspr_miss  \n0          0           0  \n1          0           0  \n2          0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>95.438135</td>\n      <td>10.733997</td>\n      <td>-3.3003</td>\n      <td>7.0</td>\n      <td>-0.455080</td>\n      <td>0.162481</td>\n      <td>0.377474</td>\n      <td>0.031809</td>\n      <td>-2.8219</td>\n      <td>13.712694</td>\n      <td>0.701320</td>\n      <td>2.263010</td>\n      <td>2.031816</td>\n      <td>1.007247</td>\n      <td>7.3186</td>\n      <td>2.764227</td>\n      <td>2.411597</td>\n      <td>2.129968</td>\n      <td>0.292826</td>\n      <td>5.846798</td>\n      <td>-0.702919</td>\n      <td>0.184985</td>\n      <td>0.407017</td>\n      <td>-0.014497</td>\n      <td>5.723574</td>\n      <td>0.809948</td>\n      <td>7.0184</td>\n      <td>0.714967</td>\n      <td>1.009413</td>\n      <td>1.8569</td>\n      <td>0.221141</td>\n      <td>1.280392</td>\n      <td>2.9982</td>\n      <td>0.173611</td>\n      <td>1.381726</td>\n      <td>7.0184</td>\n      <td>0.323276</td>\n      <td>-8.222723</td>\n      <td>1.698998</td>\n      <td>1.377168</td>\n      <td>1.372075</td>\n      <td>2.662855</td>\n      <td>2.387393</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>NaN</td>\n      <td>-39.913708</td>\n      <td>-3.4600</td>\n      <td>30.0</td>\n      <td>0.190221</td>\n      <td>0.170266</td>\n      <td>0.203607</td>\n      <td>0.223073</td>\n      <td>-7.4902</td>\n      <td>28.201480</td>\n      <td>-3.570285</td>\n      <td>1.328922</td>\n      <td>1.255872</td>\n      <td>0.973313</td>\n      <td>2.1145</td>\n      <td>1.466879</td>\n      <td>2.006033</td>\n      <td>2.911217</td>\n      <td>0.039904</td>\n      <td>8.947457</td>\n      <td>-0.771174</td>\n      <td>0.137733</td>\n      <td>0.172681</td>\n      <td>0.749959</td>\n      <td>8.649971</td>\n      <td>-3.421372</td>\n      <td>4.2825</td>\n      <td>0.038565</td>\n      <td>-3.284548</td>\n      <td>4.4537</td>\n      <td>0.038820</td>\n      <td>-3.169157</td>\n      <td>4.5357</td>\n      <td>0.034329</td>\n      <td>-3.272097</td>\n      <td>4.2825</td>\n      <td>0.047687</td>\n      <td>-49.765657</td>\n      <td>4.033896</td>\n      <td>3.226864</td>\n      <td>1.252497</td>\n      <td>3.314202</td>\n      <td>3.050424</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>36.928655</td>\n      <td>26.462302</td>\n      <td>-4.0485</td>\n      <td>16.0</td>\n      <td>-0.578997</td>\n      <td>0.108499</td>\n      <td>0.209926</td>\n      <td>0.105991</td>\n      <td>6.7020</td>\n      <td>23.239140</td>\n      <td>-0.439693</td>\n      <td>1.209944</td>\n      <td>0.906851</td>\n      <td>1.169143</td>\n      <td>4.4022</td>\n      <td>1.443144</td>\n      <td>1.597765</td>\n      <td>1.712648</td>\n      <td>0.053865</td>\n      <td>6.543669</td>\n      <td>-0.521442</td>\n      <td>0.112038</td>\n      <td>0.227148</td>\n      <td>0.015573</td>\n      <td>6.250009</td>\n      <td>-0.425686</td>\n      <td>3.6408</td>\n      <td>0.029163</td>\n      <td>-0.317304</td>\n      <td>9.0719</td>\n      <td>0.155763</td>\n      <td>-0.224665</td>\n      <td>4.4999</td>\n      <td>0.036062</td>\n      <td>-0.206434</td>\n      <td>3.6408</td>\n      <td>0.035398</td>\n      <td>-3.945672</td>\n      <td>1.689567</td>\n      <td>1.266355</td>\n      <td>1.049934</td>\n      <td>1.992002</td>\n      <td>2.418204</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 4. Missing values #\n\ncol_ignore = ['RET']\ncol_cat = ['ind']\ncol_num = [x for x in train.columns if x not in col_ignore+col_cat]\n\nfor col in col_num:\n    train[col] = train[col].fillna(train[col].median())\n    test[col] = test[col].fillna(train[col].median())\n\nfor col in col_cat:\n    train[col] = train[col].fillna(value=-1000)\n    test[col] = test[col].fillna(value=-1000)\n    \ndisplay(train.count())","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:53.751594Z","iopub.execute_input":"2022-08-24T22:52:53.752211Z","iopub.status.idle":"2022-08-24T22:52:54.649949Z","shell.execute_reply.started":"2022-08-24T22:52:53.752171Z","shell.execute_reply":"2022-08-24T22:52:54.648855Z"},"trusted":true},"execution_count":69,"outputs":[{"output_type":"display_data","data":{"text/plain":"mom482          478874\nmom242          478874\nRET             478874\nind             478874\nbm              478874\nop              478874\ngp              478874\ninv             478874\nmom11           478874\nmom122          478874\namhd            478874\nivol_capm       478874\nivol_ff5        478874\nbeta_bw         478874\nMAX             478874\nvol1m           478874\nvol6m           478874\nvol12m          478874\nBAspr           478874\nsize            478874\nlbm             478874\nlop             478874\nlgp             478874\nlinv            478874\nllme            478874\nl1amhd          478874\nl1MAX           478874\nl1BAspr         478874\nl3amhd          478874\nl3MAX           478874\nl3BAspr         478874\nl6amhd          478874\nl6MAX           478874\nl6BAspr         478874\nl12amhd         478874\nl12MAX          478874\nl12BAspr        478874\nl12mom122       478874\nl12ivol_capm    478874\nl12ivol_ff5     478874\nl12beta_bw      478874\nl12vol6m        478874\nl12vol12m       478874\namhd_miss       478874\nBAspr_miss      478874\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# [optional] Target Encoding\n","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:54.651976Z","iopub.execute_input":"2022-08-24T22:52:54.652428Z","iopub.status.idle":"2022-08-24T22:52:54.657486Z","shell.execute_reply.started":"2022-08-24T22:52:54.652387Z","shell.execute_reply":"2022-08-24T22:52:54.656397Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"X_train = train.copy()\ny_train = X_train.pop('RET')\n\nX_test = test.copy()\ny_test = X_test.pop('RET')","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:54.659422Z","iopub.execute_input":"2022-08-24T22:52:54.660302Z","iopub.status.idle":"2022-08-24T22:52:54.735082Z","shell.execute_reply.started":"2022-08-24T22:52:54.660262Z","shell.execute_reply":"2022-08-24T22:52:54.734136Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"# 5. Feature engineering #\n\ntime1 = time.time()\n\nfeature_transformer = ColumnTransformer([\n    (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat),\n    ('num', StandardScaler(), col_num)])\n\nprint('Number of features before transformation: ', X_train.shape)\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ', time.time()-time1)\nprint('Number of features after transformation: ', X_train.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:54.736895Z","iopub.execute_input":"2022-08-24T22:52:54.737573Z","iopub.status.idle":"2022-08-24T22:52:55.891859Z","shell.execute_reply.started":"2022-08-24T22:52:54.737536Z","shell.execute_reply":"2022-08-24T22:52:55.890954Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"Number of features before transformation:  (478874, 44)\ntime to do feature proprocessing:  1.1434450149536133\nNumber of features after transformation:  (478874, 92)\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:52:55.897839Z","iopub.execute_input":"2022-08-24T22:52:55.900013Z","iopub.status.idle":"2022-08-24T22:52:56.090132Z","shell.execute_reply.started":"2022-08-24T22:52:55.899959Z","shell.execute_reply":"2022-08-24T22:52:56.088068Z"},"trusted":true},"execution_count":73,"outputs":[{"execution_count":73,"output_type":"execute_result","data":{"text/plain":"        cat__ind_1.0  cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  \\\n0                0.0           1.0           0.0           0.0           0.0   \n1                0.0           0.0           0.0           0.0           0.0   \n2                0.0           1.0           0.0           0.0           0.0   \n3                0.0           0.0           0.0           0.0           0.0   \n4                0.0           0.0           0.0           0.0           0.0   \n...              ...           ...           ...           ...           ...   \n478869           0.0           0.0           0.0           0.0           0.0   \n478870           0.0           0.0           0.0           0.0           0.0   \n478871           0.0           0.0           0.0           0.0           0.0   \n478872           0.0           0.0           0.0           0.0           0.0   \n478873           0.0           0.0           0.0           0.0           0.0   \n\n        cat__ind_6.0  cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  \\\n0                0.0           0.0           0.0           0.0            0.0   \n1                0.0           0.0           0.0           0.0            0.0   \n2                0.0           0.0           0.0           0.0            0.0   \n3                0.0           0.0           0.0           0.0            0.0   \n4                0.0           0.0           0.0           0.0            0.0   \n...              ...           ...           ...           ...            ...   \n478869           0.0           0.0           0.0           0.0            0.0   \n478870           0.0           0.0           0.0           0.0            0.0   \n478871           0.0           0.0           0.0           0.0            0.0   \n478872           0.0           0.0           0.0           0.0            0.0   \n478873           0.0           0.0           0.0           0.0            0.0   \n\n        cat__ind_11.0  cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478869            0.0            0.0            0.0            0.0   \n478870            0.0            0.0            0.0            0.0   \n478871            0.0            0.0            0.0            0.0   \n478872            0.0            0.0            0.0            0.0   \n478873            0.0            0.0            0.0            0.0   \n\n        cat__ind_15.0  cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478869            0.0            0.0            1.0            0.0   \n478870            0.0            0.0            0.0            0.0   \n478871            0.0            0.0            0.0            0.0   \n478872            0.0            0.0            0.0            0.0   \n478873            0.0            0.0            0.0            0.0   \n\n        cat__ind_19.0  cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478869            0.0            0.0            0.0            0.0   \n478870            0.0            0.0            0.0            0.0   \n478871            0.0            0.0            0.0            0.0   \n478872            0.0            0.0            0.0            0.0   \n478873            0.0            0.0            1.0            0.0   \n\n        cat__ind_23.0  cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 1.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478869            0.0            0.0            0.0            0.0   \n478870            0.0            0.0            0.0            0.0   \n478871            0.0            0.0            0.0            0.0   \n478872            0.0            0.0            0.0            0.0   \n478873            0.0            0.0            0.0            0.0   \n\n        cat__ind_27.0  cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478869            0.0            0.0            0.0            0.0   \n478870            0.0            0.0            0.0            0.0   \n478871            0.0            0.0            0.0            0.0   \n478872            0.0            0.0            0.0            0.0   \n478873            0.0            0.0            0.0            0.0   \n\n        cat__ind_31.0  cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478869            0.0            0.0            0.0            0.0   \n478870            0.0            0.0            0.0            1.0   \n478871            0.0            0.0            0.0            0.0   \n478872            0.0            0.0            0.0            0.0   \n478873            0.0            0.0            0.0            0.0   \n\n        cat__ind_35.0  cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 1.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478869            0.0            0.0            0.0            0.0   \n478870            0.0            0.0            0.0            0.0   \n478871            0.0            0.0            0.0            0.0   \n478872            0.0            0.0            0.0            0.0   \n478873            0.0            0.0            0.0            0.0   \n\n        cat__ind_39.0  cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478869            0.0            0.0            0.0            0.0   \n478870            0.0            0.0            0.0            0.0   \n478871            0.0            0.0            0.0            0.0   \n478872            0.0            0.0            1.0            0.0   \n478873            0.0            0.0            0.0            0.0   \n\n        cat__ind_43.0  cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 1.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478869            0.0            0.0            0.0            0.0   \n478870            0.0            0.0            0.0            0.0   \n478871            1.0            0.0            0.0            0.0   \n478872            0.0            0.0            0.0            0.0   \n478873            0.0            0.0            0.0            0.0   \n\n        cat__ind_47.0  cat__ind_48.0  cat__ind_49.0  num__mom482  num__mom242  \\\n0                 0.0            0.0            0.0     1.646387     0.942962   \n1                 0.0            0.0            0.0    -0.959323    -0.948070   \n2                 0.0            0.0            0.0    -0.244778    -0.185276   \n3                 0.0            0.0            0.0     0.558311     1.109876   \n4                 0.0            0.0            0.0     2.839678     1.900163   \n...               ...            ...            ...          ...          ...   \n478869            0.0            0.0            0.0    -1.219890    -1.389791   \n478870            0.0            0.0            0.0    -0.244778    -0.691906   \n478871            0.0            0.0            0.0    -0.244778     2.488506   \n478872            0.0            0.0            0.0    -0.244778    -0.448472   \n478873            0.0            0.0            0.0    -0.388625     0.409059   \n\n         num__bm   num__op   num__gp  num__inv  num__mom11  num__mom122  \\\n0      -0.152753 -0.056214 -1.106608 -0.699435    0.069402     1.070614   \n1       0.868897 -0.989710  0.462205  0.096288   -1.926357     0.797856   \n2       1.724312 -0.695638 -0.661733 -0.737769   -0.145837     0.166654   \n3      -0.080771  0.256319  1.537091 -0.883860    1.709766    -0.019388   \n4       0.151470  1.554307  0.357698  0.914065    0.814144     1.038314   \n...          ...       ...       ...       ...         ...          ...   \n478869  1.422737 -0.857728 -1.000320 -0.871737   -1.926357    -1.470519   \n478870  0.970168 -1.341430 -1.823226  0.623548   -1.915733     0.055690   \n478871 -0.734259  0.598502  0.134930  3.011419   -1.047402     1.365934   \n478872  0.686569 -1.095310 -1.110860  3.011419   -0.256492     0.499012   \n478873  0.864939  1.167856  0.453197 -0.091936   -0.071501     0.122632   \n\n        num__amhd  num__ivol_capm  num__ivol_ff5  num__beta_bw  num__MAX  \\\n0       -2.340343       -0.509160      -0.503052      0.683019 -0.559251   \n1       -0.358460        0.553213       0.526425      1.263332  0.452743   \n2        0.000604        0.182987       0.181215     -0.233147 -0.134445   \n3        0.231564        0.477738       0.495669     -0.121293  1.334607   \n4        0.443294        0.161412       0.297487      0.726741  0.922407   \n...           ...             ...            ...           ...       ...   \n478869   0.093961        1.737581       1.720748     -0.524243  0.045833   \n478870   0.727468       -0.237278      -0.251746     -0.286142 -0.715046   \n478871   0.444808       -0.156164      -0.297929      2.808758 -0.441137   \n478872   0.462727       -0.753392      -0.855424     -0.106347 -0.604660   \n478873   1.109737       -0.592271      -0.563606     -0.449188 -0.590914   \n\n        num__vol1m  num__vol6m  num__vol12m  num__BAspr  num__size  num__lbm  \\\n0        -0.542718   -0.532654    -0.606236   -0.796058   2.407773 -0.133827   \n1         1.086731    0.669647     0.416037   -0.608302   0.199965  0.208400   \n2         0.047044    1.764255     1.167687   -0.684786   0.017925  0.054831   \n3         0.369974    0.120631     0.177568   -0.655047   0.248884  0.310233   \n4         0.374395   -0.217297    -0.398196   -0.296824   0.162194  0.715877   \n...            ...         ...          ...         ...        ...       ...   \n478869    1.704962    2.653831     2.590896   -0.296824  -1.907076  0.454346   \n478870    0.459717   -0.059868    -0.286127   -0.296824  -0.348746  0.975516   \n478871   -0.120283   -0.064364     0.081669   -0.296824  -0.599161 -0.404045   \n478872   -0.866707   -0.566013    -0.299062   -0.296824  -0.861542  0.012411   \n478873   -0.686214   -0.814513    -0.714325   -0.296824  -0.777847  0.479230   \n\n        num__lop  num__lgp  num__linv  num__llme  num__l1amhd  num__l1MAX  \\\n0      -0.210169 -1.204256  -0.017280   2.261759    -2.329647   -0.774032   \n1       1.263479  1.579339  -0.088610   0.181381    -0.355467    2.816526   \n2      -1.141606 -0.756679  -0.327057   0.045265     0.010769   -0.601911   \n3      -1.229180  0.659492  -0.677485   0.141879     0.232673    0.284330   \n4       1.587206  0.486812  -0.216302  -0.075153     0.461019    0.058977   \n...          ...       ...        ...        ...          ...         ...   \n478869 -0.658416 -0.796310  -0.875171  -1.345090     0.093881    1.105567   \n478870 -1.107878 -1.629027  -0.231182  -0.352962     0.769761    1.185208   \n478871  0.637223  1.258112  -0.484015  -0.783969     0.448838   -0.172973   \n478872 -0.271376 -0.272927   2.855213  -1.130983     0.515152   -0.482845   \n478873  0.216468 -0.287884  -0.725468  -0.827685     1.113714   -0.855609   \n\n        num__l1BAspr  num__l3amhd  num__l3MAX  num__l3BAspr  num__l6amhd  \\\n0          -0.800528    -2.293970   -0.384751     -0.805106    -2.253832   \n1          -0.515894    -0.358443    0.873341     -0.425128    -0.439108   \n2          -0.572683     0.028184    0.072542     -0.763047     0.018323   \n3          -0.262900     0.228936   -0.621070     -0.570063     0.222993   \n4          -0.294858     0.468305    0.282085     -0.293025     0.397493   \n...              ...          ...         ...           ...          ...   \n478869     -0.294858     2.073226    1.944233     -0.293025     2.022305   \n478870     -0.294858     0.816125    1.094075     -0.293025     0.848348   \n478871     -0.294858     0.445518   -0.155389     -0.293025     0.480774   \n478872     -0.294858     0.544459    0.523314     -0.293025     0.747464   \n478873     -0.294858     1.174456   -0.555947     -0.293025     1.273611   \n\n        num__l6MAX  num__l6BAspr  num__l12amhd  num__l12MAX  num__l12BAspr  \\\n0        -0.445162     -0.765331     -2.249299    -0.774032      -0.733233   \n1        -0.845936      0.262360     -0.663997     2.816526       0.074373   \n2         0.358405     -0.578650     -0.276376    -0.601911      -0.511977   \n3         0.627620      0.246833      0.140426     0.284330      -0.081397   \n4        -0.737239     -0.288853      0.494911     0.058977      -0.282713   \n...            ...           ...           ...          ...            ...   \n478869    0.442941     -0.288853      1.141839     1.105567      -0.282713   \n478870   -1.032934     -0.288853      0.802996     1.185208      -0.282713   \n478871    0.303687     -0.288853      1.049198    -0.172973      -0.282713   \n478872    0.027378     -0.288853      0.099390    -0.482845      -0.282713   \n478873    0.292136     -0.288853      0.099390    -0.855609      -0.282713   \n\n        num__l12mom122  num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  \\\n0             0.466676          -0.946661         -1.015050         0.064887   \n1            -0.892594          -0.320097         -0.377383         0.706144   \n2            -1.133943          -0.079718         -0.063460        -0.171609   \n3             1.374314           0.003554         -0.350484        -0.110791   \n4             0.876880          -0.389869         -0.287016         0.195275   \n...                ...                ...               ...              ...   \n478869       -1.631924           2.802447          2.807175        -0.480253   \n478870        0.104529          -0.305816         -0.399508        -0.841417   \n478871        2.360329           1.495280          1.750332         1.753740   \n478872       -1.385912           0.373116          0.216842         0.054867   \n478873        0.765520          -0.690141         -0.800926        -0.538088   \n\n        num__l12vol6m  num__l12vol12m  num__amhd_miss  num__BAspr_miss  \n0           -0.954148       -0.702101       -0.474657        -0.878545  \n1           -0.128710        0.081178       -0.474657        -0.878545  \n2            0.287981        0.139277       -0.474657        -0.878545  \n3           -0.104201       -0.122408       -0.474657        -0.878545  \n4           -0.625859       -0.757616       -0.474657         1.138245  \n...               ...             ...             ...              ...  \n478869       2.054374        1.106485        2.106783         1.138245  \n478870      -0.796024       -0.903368       -0.474657         1.138245  \n478871       0.391594        0.292724       -0.474657         1.138245  \n478872      -0.058731        0.249519       -0.474657         1.138245  \n478873      -0.498485       -0.653228       -0.474657         1.138245  \n\n[478874 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.646387</td>\n      <td>0.942962</td>\n      <td>-0.152753</td>\n      <td>-0.056214</td>\n      <td>-1.106608</td>\n      <td>-0.699435</td>\n      <td>0.069402</td>\n      <td>1.070614</td>\n      <td>-2.340343</td>\n      <td>-0.509160</td>\n      <td>-0.503052</td>\n      <td>0.683019</td>\n      <td>-0.559251</td>\n      <td>-0.542718</td>\n      <td>-0.532654</td>\n      <td>-0.606236</td>\n      <td>-0.796058</td>\n      <td>2.407773</td>\n      <td>-0.133827</td>\n      <td>-0.210169</td>\n      <td>-1.204256</td>\n      <td>-0.017280</td>\n      <td>2.261759</td>\n      <td>-2.329647</td>\n      <td>-0.774032</td>\n      <td>-0.800528</td>\n      <td>-2.293970</td>\n      <td>-0.384751</td>\n      <td>-0.805106</td>\n      <td>-2.253832</td>\n      <td>-0.445162</td>\n      <td>-0.765331</td>\n      <td>-2.249299</td>\n      <td>-0.774032</td>\n      <td>-0.733233</td>\n      <td>0.466676</td>\n      <td>-0.946661</td>\n      <td>-1.015050</td>\n      <td>0.064887</td>\n      <td>-0.954148</td>\n      <td>-0.702101</td>\n      <td>-0.474657</td>\n      <td>-0.878545</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.959323</td>\n      <td>-0.948070</td>\n      <td>0.868897</td>\n      <td>-0.989710</td>\n      <td>0.462205</td>\n      <td>0.096288</td>\n      <td>-1.926357</td>\n      <td>0.797856</td>\n      <td>-0.358460</td>\n      <td>0.553213</td>\n      <td>0.526425</td>\n      <td>1.263332</td>\n      <td>0.452743</td>\n      <td>1.086731</td>\n      <td>0.669647</td>\n      <td>0.416037</td>\n      <td>-0.608302</td>\n      <td>0.199965</td>\n      <td>0.208400</td>\n      <td>1.263479</td>\n      <td>1.579339</td>\n      <td>-0.088610</td>\n      <td>0.181381</td>\n      <td>-0.355467</td>\n      <td>2.816526</td>\n      <td>-0.515894</td>\n      <td>-0.358443</td>\n      <td>0.873341</td>\n      <td>-0.425128</td>\n      <td>-0.439108</td>\n      <td>-0.845936</td>\n      <td>0.262360</td>\n      <td>-0.663997</td>\n      <td>2.816526</td>\n      <td>0.074373</td>\n      <td>-0.892594</td>\n      <td>-0.320097</td>\n      <td>-0.377383</td>\n      <td>0.706144</td>\n      <td>-0.128710</td>\n      <td>0.081178</td>\n      <td>-0.474657</td>\n      <td>-0.878545</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.244778</td>\n      <td>-0.185276</td>\n      <td>1.724312</td>\n      <td>-0.695638</td>\n      <td>-0.661733</td>\n      <td>-0.737769</td>\n      <td>-0.145837</td>\n      <td>0.166654</td>\n      <td>0.000604</td>\n      <td>0.182987</td>\n      <td>0.181215</td>\n      <td>-0.233147</td>\n      <td>-0.134445</td>\n      <td>0.047044</td>\n      <td>1.764255</td>\n      <td>1.167687</td>\n      <td>-0.684786</td>\n      <td>0.017925</td>\n      <td>0.054831</td>\n      <td>-1.141606</td>\n      <td>-0.756679</td>\n      <td>-0.327057</td>\n      <td>0.045265</td>\n      <td>0.010769</td>\n      <td>-0.601911</td>\n      <td>-0.572683</td>\n      <td>0.028184</td>\n      <td>0.072542</td>\n      <td>-0.763047</td>\n      <td>0.018323</td>\n      <td>0.358405</td>\n      <td>-0.578650</td>\n      <td>-0.276376</td>\n      <td>-0.601911</td>\n      <td>-0.511977</td>\n      <td>-1.133943</td>\n      <td>-0.079718</td>\n      <td>-0.063460</td>\n      <td>-0.171609</td>\n      <td>0.287981</td>\n      <td>0.139277</td>\n      <td>-0.474657</td>\n      <td>-0.878545</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.558311</td>\n      <td>1.109876</td>\n      <td>-0.080771</td>\n      <td>0.256319</td>\n      <td>1.537091</td>\n      <td>-0.883860</td>\n      <td>1.709766</td>\n      <td>-0.019388</td>\n      <td>0.231564</td>\n      <td>0.477738</td>\n      <td>0.495669</td>\n      <td>-0.121293</td>\n      <td>1.334607</td>\n      <td>0.369974</td>\n      <td>0.120631</td>\n      <td>0.177568</td>\n      <td>-0.655047</td>\n      <td>0.248884</td>\n      <td>0.310233</td>\n      <td>-1.229180</td>\n      <td>0.659492</td>\n      <td>-0.677485</td>\n      <td>0.141879</td>\n      <td>0.232673</td>\n      <td>0.284330</td>\n      <td>-0.262900</td>\n      <td>0.228936</td>\n      <td>-0.621070</td>\n      <td>-0.570063</td>\n      <td>0.222993</td>\n      <td>0.627620</td>\n      <td>0.246833</td>\n      <td>0.140426</td>\n      <td>0.284330</td>\n      <td>-0.081397</td>\n      <td>1.374314</td>\n      <td>0.003554</td>\n      <td>-0.350484</td>\n      <td>-0.110791</td>\n      <td>-0.104201</td>\n      <td>-0.122408</td>\n      <td>-0.474657</td>\n      <td>-0.878545</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2.839678</td>\n      <td>1.900163</td>\n      <td>0.151470</td>\n      <td>1.554307</td>\n      <td>0.357698</td>\n      <td>0.914065</td>\n      <td>0.814144</td>\n      <td>1.038314</td>\n      <td>0.443294</td>\n      <td>0.161412</td>\n      <td>0.297487</td>\n      <td>0.726741</td>\n      <td>0.922407</td>\n      <td>0.374395</td>\n      <td>-0.217297</td>\n      <td>-0.398196</td>\n      <td>-0.296824</td>\n      <td>0.162194</td>\n      <td>0.715877</td>\n      <td>1.587206</td>\n      <td>0.486812</td>\n      <td>-0.216302</td>\n      <td>-0.075153</td>\n      <td>0.461019</td>\n      <td>0.058977</td>\n      <td>-0.294858</td>\n      <td>0.468305</td>\n      <td>0.282085</td>\n      <td>-0.293025</td>\n      <td>0.397493</td>\n      <td>-0.737239</td>\n      <td>-0.288853</td>\n      <td>0.494911</td>\n      <td>0.058977</td>\n      <td>-0.282713</td>\n      <td>0.876880</td>\n      <td>-0.389869</td>\n      <td>-0.287016</td>\n      <td>0.195275</td>\n      <td>-0.625859</td>\n      <td>-0.757616</td>\n      <td>-0.474657</td>\n      <td>1.138245</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>478869</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.219890</td>\n      <td>-1.389791</td>\n      <td>1.422737</td>\n      <td>-0.857728</td>\n      <td>-1.000320</td>\n      <td>-0.871737</td>\n      <td>-1.926357</td>\n      <td>-1.470519</td>\n      <td>0.093961</td>\n      <td>1.737581</td>\n      <td>1.720748</td>\n      <td>-0.524243</td>\n      <td>0.045833</td>\n      <td>1.704962</td>\n      <td>2.653831</td>\n      <td>2.590896</td>\n      <td>-0.296824</td>\n      <td>-1.907076</td>\n      <td>0.454346</td>\n      <td>-0.658416</td>\n      <td>-0.796310</td>\n      <td>-0.875171</td>\n      <td>-1.345090</td>\n      <td>0.093881</td>\n      <td>1.105567</td>\n      <td>-0.294858</td>\n      <td>2.073226</td>\n      <td>1.944233</td>\n      <td>-0.293025</td>\n      <td>2.022305</td>\n      <td>0.442941</td>\n      <td>-0.288853</td>\n      <td>1.141839</td>\n      <td>1.105567</td>\n      <td>-0.282713</td>\n      <td>-1.631924</td>\n      <td>2.802447</td>\n      <td>2.807175</td>\n      <td>-0.480253</td>\n      <td>2.054374</td>\n      <td>1.106485</td>\n      <td>2.106783</td>\n      <td>1.138245</td>\n    </tr>\n    <tr>\n      <th>478870</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.244778</td>\n      <td>-0.691906</td>\n      <td>0.970168</td>\n      <td>-1.341430</td>\n      <td>-1.823226</td>\n      <td>0.623548</td>\n      <td>-1.915733</td>\n      <td>0.055690</td>\n      <td>0.727468</td>\n      <td>-0.237278</td>\n      <td>-0.251746</td>\n      <td>-0.286142</td>\n      <td>-0.715046</td>\n      <td>0.459717</td>\n      <td>-0.059868</td>\n      <td>-0.286127</td>\n      <td>-0.296824</td>\n      <td>-0.348746</td>\n      <td>0.975516</td>\n      <td>-1.107878</td>\n      <td>-1.629027</td>\n      <td>-0.231182</td>\n      <td>-0.352962</td>\n      <td>0.769761</td>\n      <td>1.185208</td>\n      <td>-0.294858</td>\n      <td>0.816125</td>\n      <td>1.094075</td>\n      <td>-0.293025</td>\n      <td>0.848348</td>\n      <td>-1.032934</td>\n      <td>-0.288853</td>\n      <td>0.802996</td>\n      <td>1.185208</td>\n      <td>-0.282713</td>\n      <td>0.104529</td>\n      <td>-0.305816</td>\n      <td>-0.399508</td>\n      <td>-0.841417</td>\n      <td>-0.796024</td>\n      <td>-0.903368</td>\n      <td>-0.474657</td>\n      <td>1.138245</td>\n    </tr>\n    <tr>\n      <th>478871</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.244778</td>\n      <td>2.488506</td>\n      <td>-0.734259</td>\n      <td>0.598502</td>\n      <td>0.134930</td>\n      <td>3.011419</td>\n      <td>-1.047402</td>\n      <td>1.365934</td>\n      <td>0.444808</td>\n      <td>-0.156164</td>\n      <td>-0.297929</td>\n      <td>2.808758</td>\n      <td>-0.441137</td>\n      <td>-0.120283</td>\n      <td>-0.064364</td>\n      <td>0.081669</td>\n      <td>-0.296824</td>\n      <td>-0.599161</td>\n      <td>-0.404045</td>\n      <td>0.637223</td>\n      <td>1.258112</td>\n      <td>-0.484015</td>\n      <td>-0.783969</td>\n      <td>0.448838</td>\n      <td>-0.172973</td>\n      <td>-0.294858</td>\n      <td>0.445518</td>\n      <td>-0.155389</td>\n      <td>-0.293025</td>\n      <td>0.480774</td>\n      <td>0.303687</td>\n      <td>-0.288853</td>\n      <td>1.049198</td>\n      <td>-0.172973</td>\n      <td>-0.282713</td>\n      <td>2.360329</td>\n      <td>1.495280</td>\n      <td>1.750332</td>\n      <td>1.753740</td>\n      <td>0.391594</td>\n      <td>0.292724</td>\n      <td>-0.474657</td>\n      <td>1.138245</td>\n    </tr>\n    <tr>\n      <th>478872</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.244778</td>\n      <td>-0.448472</td>\n      <td>0.686569</td>\n      <td>-1.095310</td>\n      <td>-1.110860</td>\n      <td>3.011419</td>\n      <td>-0.256492</td>\n      <td>0.499012</td>\n      <td>0.462727</td>\n      <td>-0.753392</td>\n      <td>-0.855424</td>\n      <td>-0.106347</td>\n      <td>-0.604660</td>\n      <td>-0.866707</td>\n      <td>-0.566013</td>\n      <td>-0.299062</td>\n      <td>-0.296824</td>\n      <td>-0.861542</td>\n      <td>0.012411</td>\n      <td>-0.271376</td>\n      <td>-0.272927</td>\n      <td>2.855213</td>\n      <td>-1.130983</td>\n      <td>0.515152</td>\n      <td>-0.482845</td>\n      <td>-0.294858</td>\n      <td>0.544459</td>\n      <td>0.523314</td>\n      <td>-0.293025</td>\n      <td>0.747464</td>\n      <td>0.027378</td>\n      <td>-0.288853</td>\n      <td>0.099390</td>\n      <td>-0.482845</td>\n      <td>-0.282713</td>\n      <td>-1.385912</td>\n      <td>0.373116</td>\n      <td>0.216842</td>\n      <td>0.054867</td>\n      <td>-0.058731</td>\n      <td>0.249519</td>\n      <td>-0.474657</td>\n      <td>1.138245</td>\n    </tr>\n    <tr>\n      <th>478873</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.388625</td>\n      <td>0.409059</td>\n      <td>0.864939</td>\n      <td>1.167856</td>\n      <td>0.453197</td>\n      <td>-0.091936</td>\n      <td>-0.071501</td>\n      <td>0.122632</td>\n      <td>1.109737</td>\n      <td>-0.592271</td>\n      <td>-0.563606</td>\n      <td>-0.449188</td>\n      <td>-0.590914</td>\n      <td>-0.686214</td>\n      <td>-0.814513</td>\n      <td>-0.714325</td>\n      <td>-0.296824</td>\n      <td>-0.777847</td>\n      <td>0.479230</td>\n      <td>0.216468</td>\n      <td>-0.287884</td>\n      <td>-0.725468</td>\n      <td>-0.827685</td>\n      <td>1.113714</td>\n      <td>-0.855609</td>\n      <td>-0.294858</td>\n      <td>1.174456</td>\n      <td>-0.555947</td>\n      <td>-0.293025</td>\n      <td>1.273611</td>\n      <td>0.292136</td>\n      <td>-0.288853</td>\n      <td>0.099390</td>\n      <td>-0.855609</td>\n      <td>-0.282713</td>\n      <td>0.765520</td>\n      <td>-0.690141</td>\n      <td>-0.800926</td>\n      <td>-0.538088</td>\n      <td>-0.498485</td>\n      <td>-0.653228</td>\n      <td>-0.474657</td>\n      <td>1.138245</td>\n    </tr>\n  </tbody>\n</table>\n<p>478874 rows  92 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Model fitting #\n\n# first, some trivial baselines:\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\ntime1 = time.time()\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=300, max_depth=6, eta=0.05, colsample_bytree=0.6)\nxgb1.fit(X_train, y_train)\nprint('XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:04:19.289615Z","iopub.execute_input":"2022-08-24T23:04:19.290142Z","iopub.status.idle":"2022-08-24T23:04:29.737362Z","shell.execute_reply.started":"2022-08-24T23:04:19.290098Z","shell.execute_reply":"2022-08-24T23:04:29.736534Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stdout","text":"mae of a constant model 10.362791243793783\nR2 of a constant model 0.0\nXGB train: 10.194930791764909 0.07814769232439722 10.420358419418335\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[300, 500], 'max_depth':[2,4,6], 'eta':[0.02, 0.04, 0.06],\n             'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbm = GridSearchCV(xgb, param_grid, cv=2, verbose=2, scoring='neg_mean_absolute_error')\nxgbm.fit(X_train, y_train)\nprint('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)\n# this runs for 40 min and finds \n# 'eta': 0.02, 'max_depth': 6, 'n_estimators': 500, 0.01095415380877135\nprint('XGB train:', mean_absolute_error(y_train, xgbm.predict(X_train)), r2_score(y_train, xgbm.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:53:06.015529Z","iopub.execute_input":"2022-08-24T22:53:06.016623Z","iopub.status.idle":"2022-08-24T22:55:32.770245Z","shell.execute_reply.started":"2022-08-24T22:53:06.016580Z","shell.execute_reply":"2022-08-24T22:55:32.769373Z"},"trusted":true},"execution_count":75,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 18 candidates, totalling 36 fits\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=300, subsample=0.6; total time=   2.2s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=300, subsample=0.6; total time=   2.0s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=300, subsample=0.6; total time=   3.1s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=500, subsample=0.6; total time=   4.3s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.4s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.4s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.7s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=500, subsample=0.6; total time=   7.0s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=300, subsample=0.6; total time=   2.0s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=300, subsample=0.6; total time=   2.2s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.6s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=500, subsample=0.6; total time=   3.5s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.8s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.9s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.3s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.6s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.6s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.7s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=300, subsample=0.6; total time=   2.0s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=300, subsample=0.6; total time=   2.0s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.9s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=500, subsample=0.6; total time=   4.1s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.6s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.4s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.6s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.7s\nXGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 6, 'n_estimators': 300, 'subsample': 0.6} 0.008747095276429306 140.87398958206177\nXGB train: 10.309917775210497 0.04393011364704058 146.74604773521423\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\n\ndef objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    cv_regularizer=0.01\n    # Usually values between 0.1 and 0.2 work fine.\n\n    params = {\n        \"tree_method\": 'gpu_hist',\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.005, 0.2),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 0.95),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 0.95),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 20.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 150.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 10.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 10)    }\n    # usually it makes sense to resrtict hyperparameter space from some solutions which Optuna will find\n    # e.g., for tmx-joined data only (downsampled tmx), optuna keeps selecting depths of 2 and 3.\n    # for my purposes (smooth left side of prc, close to 1), those solutions are no good.\n\n    temp_out = []\n\n    for i in range(cv_runs):\n\n        X = X_train\n        y = y_train\n\n        model = XGBRegressor(**params, njobs=-1)\n        rkf = KFold(n_splits=n_splits, shuffle=True)\n        X_values = X.values\n        y_values = y.values\n        y_pred = np.zeros_like(y_values)\n        y_pred_train = np.zeros_like(y_values)\n        for train_index, test_index in rkf.split(X_values):\n            X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n            y_A, y_B = y_values[train_index], y_values[test_index]\n            model.fit(X_A, y_A, eval_set=[(X_B, y_B)], verbose = False)\n            y_pred[test_index] += model.predict(X_B)\n                      \n            \n        #score_train = roc_auc_score(y_train, y_pred_train)\n        score_test = mean_absolute_error(y_train, y_pred) \n        #overfit = score_train-score_test\n        #temp_out.append(score_test-cv_regularizer*overfit)\n        temp_out.append(score_test)\n\n    return (np.mean(temp_out))\n\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=30)\n\nprint('Total time for hypermarameter optimization ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\n\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\n\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X_train, y_train)\nprint('Optuna XGB train:', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:55:32.771719Z","iopub.execute_input":"2022-08-24T22:55:32.772313Z","iopub.status.idle":"2022-08-24T23:02:01.487869Z","shell.execute_reply.started":"2022-08-24T22:55:32.772266Z","shell.execute_reply":"2022-08-24T23:02:01.486505Z"},"trusted":true},"execution_count":76,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2022-08-24 22:55:32,784]\u001b[0m A new study created in memory with name: no-name-11776cbf-0370-4b7b-924b-014672c30127\u001b[0m\n\u001b[32m[I 2022-08-24 22:56:01,909]\u001b[0m Trial 0 finished with value: 10.656709631947574 and parameters: {'n_estimators': 434, 'max_depth': 8, 'learning_rate': 0.11585359649002928, 'colsample_bytree': 0.9365182290058566, 'subsample': 0.925172831143214, 'alpha': 0.6448957939999528, 'lambda': 0.38716227485054366, 'gamma': 1.3112887819998422, 'min_child_weight': 1.1547632708971387}. Best is trial 0 with value: 10.656709631947574.\u001b[0m\n\u001b[32m[I 2022-08-24 22:56:08,629]\u001b[0m Trial 1 finished with value: 10.424179001899951 and parameters: {'n_estimators': 233, 'max_depth': 3, 'learning_rate': 0.013678529147301957, 'colsample_bytree': 0.5757720334984112, 'subsample': 0.5546188374471677, 'alpha': 18.02875663451166, 'lambda': 0.1590212506254541, 'gamma': 4.065243787297455e-08, 'min_child_weight': 9.05694318397275}. Best is trial 1 with value: 10.424179001899951.\u001b[0m\n\u001b[32m[I 2022-08-24 22:56:16,499]\u001b[0m Trial 2 finished with value: 10.408630729483427 and parameters: {'n_estimators': 172, 'max_depth': 6, 'learning_rate': 0.024950245932898983, 'colsample_bytree': 0.437824056455843, 'subsample': 0.7363236791326081, 'alpha': 0.10899500471324435, 'lambda': 107.67341336034363, 'gamma': 0.49676752411516073, 'min_child_weight': 0.6931962566743304}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:56:49,811]\u001b[0m Trial 3 finished with value: 10.506365021042 and parameters: {'n_estimators': 264, 'max_depth': 10, 'learning_rate': 0.09328064980677558, 'colsample_bytree': 0.28329811067408617, 'subsample': 0.5230543326241094, 'alpha': 4.135221098917358, 'lambda': 86.01158104751423, 'gamma': 1.0564464330967637e-08, 'min_child_weight': 3.910856573662992}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:56:59,175]\u001b[0m Trial 4 finished with value: 10.414521976747025 and parameters: {'n_estimators': 415, 'max_depth': 4, 'learning_rate': 0.04804687259510312, 'colsample_bytree': 0.8334151227782327, 'subsample': 0.6912685411143588, 'alpha': 3.527683967303829, 'lambda': 22.859293730606556, 'gamma': 3.370210351464819e-05, 'min_child_weight': 0.5766772461217604}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:57:12,416]\u001b[0m Trial 5 finished with value: 10.620515157030054 and parameters: {'n_estimators': 245, 'max_depth': 7, 'learning_rate': 0.17358093160983212, 'colsample_bytree': 0.7471383873410608, 'subsample': 0.6699891350375474, 'alpha': 1.3888479793944442, 'lambda': 11.228892115701465, 'gamma': 0.0012751217893571737, 'min_child_weight': 0.29812943929244284}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:57:19,198]\u001b[0m Trial 6 finished with value: 10.422921914094994 and parameters: {'n_estimators': 377, 'max_depth': 2, 'learning_rate': 0.08698022155702463, 'colsample_bytree': 0.10133991368476196, 'subsample': 0.5220927244082904, 'alpha': 10.335266050652749, 'lambda': 5.524870253535096, 'gamma': 0.7059608413653798, 'min_child_weight': 2.8119964214517403}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:57:26,468]\u001b[0m Trial 7 finished with value: 10.452681171523569 and parameters: {'n_estimators': 158, 'max_depth': 6, 'learning_rate': 0.13081616787854103, 'colsample_bytree': 0.23843563551781427, 'subsample': 0.9070714495475412, 'alpha': 1.1558417655147224, 'lambda': 0.43055109522009216, 'gamma': 1.7711731864356933e-05, 'min_child_weight': 5.923263068174033}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:57:49,391]\u001b[0m Trial 8 finished with value: 10.561297710742524 and parameters: {'n_estimators': 288, 'max_depth': 9, 'learning_rate': 0.08990470945811187, 'colsample_bytree': 0.3007480209358848, 'subsample': 0.6573548163347976, 'alpha': 1.4542507986308257, 'lambda': 3.343859859452912, 'gamma': 2.6009866546063533e-10, 'min_child_weight': 7.5887043042081865}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:58:16,464]\u001b[0m Trial 9 finished with value: 10.660532730859156 and parameters: {'n_estimators': 274, 'max_depth': 9, 'learning_rate': 0.09115470910776023, 'colsample_bytree': 0.7446641957416648, 'subsample': 0.5354974121887285, 'alpha': 0.5580661971538388, 'lambda': 0.7303066976734566, 'gamma': 2.9734124542209225e-05, 'min_child_weight': 2.2890046673936983}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:58:21,535]\u001b[0m Trial 10 finished with value: 10.412232469704682 and parameters: {'n_estimators': 101, 'max_depth': 5, 'learning_rate': 0.006513321128699515, 'colsample_bytree': 0.4842116906388956, 'subsample': 0.8051043392973103, 'alpha': 0.1468724074303217, 'lambda': 107.30396122366875, 'gamma': 0.01633504868448953, 'min_child_weight': 0.11698825102874352}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:58:26,788]\u001b[0m Trial 11 finished with value: 10.413374470323213 and parameters: {'n_estimators': 102, 'max_depth': 5, 'learning_rate': 0.009873325535263489, 'colsample_bytree': 0.49072312672421525, 'subsample': 0.8058704864965273, 'alpha': 0.11583890404997362, 'lambda': 138.06490423600957, 'gamma': 0.02011399101485562, 'min_child_weight': 0.16234242981729113}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:58:33,997]\u001b[0m Trial 12 finished with value: 10.410385340402156 and parameters: {'n_estimators': 158, 'max_depth': 6, 'learning_rate': 0.04482927252937828, 'colsample_bytree': 0.4889890612328309, 'subsample': 0.7854138299272677, 'alpha': 0.129873725964441, 'lambda': 40.426098078927886, 'gamma': 0.01561498199872863, 'min_child_weight': 0.10492140068811213}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:58:44,045]\u001b[0m Trial 13 finished with value: 10.418804844060357 and parameters: {'n_estimators': 183, 'max_depth': 7, 'learning_rate': 0.051515864813144706, 'colsample_bytree': 0.5934533194812341, 'subsample': 0.7787057106037595, 'alpha': 0.25306598562974886, 'lambda': 33.40283149752991, 'gamma': 6.661561484047311, 'min_child_weight': 0.45413235278151837}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:58:56,297]\u001b[0m Trial 14 finished with value: 10.417673645415343 and parameters: {'n_estimators': 348, 'max_depth': 6, 'learning_rate': 0.046729307953073, 'colsample_bytree': 0.4112686976445709, 'subsample': 0.738831044464453, 'alpha': 0.25271634530868803, 'lambda': 40.38135377311974, 'gamma': 0.034369296087724606, 'min_child_weight': 1.1889645478728046}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:59:02,391]\u001b[0m Trial 15 finished with value: 10.414687488671628 and parameters: {'n_estimators': 184, 'max_depth': 4, 'learning_rate': 0.039575051269860176, 'colsample_bytree': 0.40009800378515337, 'subsample': 0.8599555381692477, 'alpha': 0.26551528291961174, 'lambda': 2.4498836068718184, 'gamma': 0.000884184700751177, 'min_child_weight': 0.22195302331586714}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:59:25,005]\u001b[0m Trial 16 finished with value: 10.421933611236396 and parameters: {'n_estimators': 494, 'max_depth': 7, 'learning_rate': 0.031154380031201835, 'colsample_bytree': 0.6513258026101301, 'subsample': 0.6299247705565573, 'alpha': 0.10178036161534605, 'lambda': 15.379348595395257, 'gamma': 0.24432181571397232, 'min_child_weight': 0.7307578828243625}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:59:30,793]\u001b[0m Trial 17 finished with value: 10.412911352201828 and parameters: {'n_estimators': 138, 'max_depth': 5, 'learning_rate': 0.06771419856133133, 'colsample_bytree': 0.39925380618386097, 'subsample': 0.7373005943361982, 'alpha': 0.19995174939314367, 'lambda': 48.96713867517473, 'gamma': 0.0009220739187614631, 'min_child_weight': 0.1046194736706038}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:59:44,846]\u001b[0m Trial 18 finished with value: 10.543397184829955 and parameters: {'n_estimators': 213, 'max_depth': 8, 'learning_rate': 0.15593095771695004, 'colsample_bytree': 0.143140276760797, 'subsample': 0.865862443675293, 'alpha': 0.47128603601257296, 'lambda': 8.006595027585163, 'gamma': 7.405940930378717e-07, 'min_child_weight': 1.6672128563677053}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 22:59:50,155]\u001b[0m Trial 19 finished with value: 10.414822922180093 and parameters: {'n_estimators': 143, 'max_depth': 4, 'learning_rate': 0.0702454205462342, 'colsample_bytree': 0.6969704830471362, 'subsample': 0.6064925495830223, 'alpha': 0.39704532394630776, 'lambda': 62.05951927531054, 'gamma': 7.56700153144125, 'min_child_weight': 0.33526751060204557}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 23:00:01,577]\u001b[0m Trial 20 finished with value: 10.413114007917004 and parameters: {'n_estimators': 314, 'max_depth': 6, 'learning_rate': 0.0304980083367187, 'colsample_bytree': 0.5327557230925476, 'subsample': 0.7777192079518871, 'alpha': 0.8330165827651635, 'lambda': 1.796922522304118, 'gamma': 0.08751406201884795, 'min_child_weight': 0.16532051441334397}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 23:00:07,040]\u001b[0m Trial 21 finished with value: 10.41366663882644 and parameters: {'n_estimators': 103, 'max_depth': 5, 'learning_rate': 0.008323379223674767, 'colsample_bytree': 0.47819228934138713, 'subsample': 0.8298946364552398, 'alpha': 0.1580324680873509, 'lambda': 149.27558645473556, 'gamma': 0.003730698195691423, 'min_child_weight': 0.12373540732017446}. Best is trial 2 with value: 10.408630729483427.\u001b[0m\n\u001b[32m[I 2022-08-24 23:00:16,511]\u001b[0m Trial 22 finished with value: 10.407082603513471 and parameters: {'n_estimators': 198, 'max_depth': 6, 'learning_rate': 0.027669620246032926, 'colsample_bytree': 0.43148127152350096, 'subsample': 0.7036758498034279, 'alpha': 0.1398238963226233, 'lambda': 82.12577368461422, 'gamma': 0.012950446444819557, 'min_child_weight': 0.27116856873426315}. Best is trial 22 with value: 10.407082603513471.\u001b[0m\n\u001b[32m[I 2022-08-24 23:00:25,148]\u001b[0m Trial 23 finished with value: 10.423285298968162 and parameters: {'n_estimators': 208, 'max_depth': 6, 'learning_rate': 0.06738944430523959, 'colsample_bytree': 0.34247427439461786, 'subsample': 0.6993457308317794, 'alpha': 0.10366010055605526, 'lambda': 20.262355925889008, 'gamma': 8.217894698938785e-05, 'min_child_weight': 0.3003508640955139}. Best is trial 22 with value: 10.407082603513471.\u001b[0m\n\u001b[32m[I 2022-08-24 23:00:35,034]\u001b[0m Trial 24 finished with value: 10.4050568012254 and parameters: {'n_estimators': 176, 'max_depth': 7, 'learning_rate': 0.02524017396862495, 'colsample_bytree': 0.4240592889838349, 'subsample': 0.752393065433362, 'alpha': 0.34031034473211, 'lambda': 63.45292590397274, 'gamma': 0.35620860767445406, 'min_child_weight': 0.7812197839031524}. Best is trial 24 with value: 10.4050568012254.\u001b[0m\n\u001b[32m[I 2022-08-24 23:00:48,115]\u001b[0m Trial 25 finished with value: 10.409052732905625 and parameters: {'n_estimators': 188, 'max_depth': 8, 'learning_rate': 0.025016119391837174, 'colsample_bytree': 0.1847630792262554, 'subsample': 0.7297210454611579, 'alpha': 0.34901906167365215, 'lambda': 81.33526958132235, 'gamma': 1.148333055241313, 'min_child_weight': 0.7686910909665343}. Best is trial 24 with value: 10.4050568012254.\u001b[0m\n\u001b[32m[I 2022-08-24 23:01:00,304]\u001b[0m Trial 26 finished with value: 10.407325196397634 and parameters: {'n_estimators': 228, 'max_depth': 7, 'learning_rate': 0.025675467047604816, 'colsample_bytree': 0.3598209549093561, 'subsample': 0.614637128756458, 'alpha': 0.19121989903631936, 'lambda': 23.52021689005696, 'gamma': 0.19959560205408416, 'min_child_weight': 0.4203111806187192}. Best is trial 24 with value: 10.4050568012254.\u001b[0m\n\u001b[32m[I 2022-08-24 23:01:11,971]\u001b[0m Trial 27 finished with value: 10.436881280607867 and parameters: {'n_estimators': 221, 'max_depth': 7, 'learning_rate': 0.06845587764825346, 'colsample_bytree': 0.339223248120613, 'subsample': 0.6046353609187948, 'alpha': 0.205978098364466, 'lambda': 27.709768615109542, 'gamma': 0.13833486587697758, 'min_child_weight': 0.42166655909611533}. Best is trial 24 with value: 10.4050568012254.\u001b[0m\n\u001b[32m[I 2022-08-24 23:01:32,962]\u001b[0m Trial 28 finished with value: 10.56562196174506 and parameters: {'n_estimators': 324, 'max_depth': 8, 'learning_rate': 0.11197425114167996, 'colsample_bytree': 0.22888644723915635, 'subsample': 0.585772134672405, 'alpha': 0.34347847276727944, 'lambda': 9.626618699460787, 'gamma': 0.0002493994822867068, 'min_child_weight': 0.23329318922240372}. Best is trial 24 with value: 10.4050568012254.\u001b[0m\n\u001b[32m[I 2022-08-24 23:02:01,462]\u001b[0m Trial 29 finished with value: 10.59442850314261 and parameters: {'n_estimators': 261, 'max_depth': 9, 'learning_rate': 0.13210563243366624, 'colsample_bytree': 0.8984220319007994, 'subsample': 0.6354917354568673, 'alpha': 0.6985541149242325, 'lambda': 60.97699301380195, 'gamma': 2.945707820527395, 'min_child_weight': 1.0967133199515884}. Best is trial 24 with value: 10.4050568012254.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  388.68152022361755\n        n_estimators : 176\n           max_depth : 7\n       learning_rate : 0.02524017396862495\n    colsample_bytree : 0.4240592889838349\n           subsample : 0.752393065433362\n               alpha : 0.34031034473211\n              lambda : 63.45292590397274\n               gamma : 0.35620860767445406\n    min_child_weight : 0.7812197839031524\nbest objective value : 10.4050568012254\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/4031380819.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0moptuna_hyperpars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tree_method'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpu_hist'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0moptuna_xgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0moptuna_hyperpars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0moptuna_xgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m print('Optuna XGB train:', \n","\u001b[0;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"],"ename":"NameError","evalue":"name 'XGBClassifier' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# Evaluate performance of XGB models:\n\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\nprint('XGB GS test:', mean_absolute_error(y_test, xgbm.predict(X_test)), r2_score(y_test, xgbm.predict(X_test)))\nprint('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2022-08-24T23:05:23.285859Z","iopub.execute_input":"2022-08-24T23:05:23.286820Z","iopub.status.idle":"2022-08-24T23:05:24.358524Z","shell.execute_reply.started":"2022-08-24T23:05:23.286784Z","shell.execute_reply":"2022-08-24T23:05:24.357657Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"XGB test: 8.807566112297426 -0.001447285187983871\nXGB GS test: 8.787164657266725 1.2358967773629104e-06\nOptuna XGB test: 8.779803817284662 0.001301060059168302\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_size = 0.1\n# df.reset_index(inplace=True, drop=True)\n# #random.seed(2)\n# test_index = random.sample(list(df.index), int(test_size*df.shape[0]))\n# train = df.iloc[list(set(df.index)-set(test_index))]\n# test = df.iloc[test_index]\n# train.reset_index(drop=True, inplace=True)\n# test.reset_index(drop=True, inplace=True)\n# display(train.shape, test.shape, train.head(3), test.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:10:56.715852Z","iopub.status.idle":"2022-08-24T22:10:56.716304Z","shell.execute_reply.started":"2022-08-24T22:10:56.716076Z","shell.execute_reply":"2022-08-24T22:10:56.716096Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Load libraries and data.\n2. pEDA. Look at feature distribution, fix them if they do not look right.\n3. Train-test split. Most likely couple years into test set. 2015-2018?. Impute missing values.\n4. Transform numerical features, add ohe for inds.\n5. Fit classic models: ols as a baseline, then xgb.\n6. Fir DL.\n\n\nNotes:\nideally, I want to use time-based cross-validation.\nsince I have panel data, it is not a trivial task.\nneed to find some solution online.\ne.g., https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8.\n","metadata":{}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, time, math, re, warnings, random, gc, dill, optuna\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\npd.set_option('display.max_columns', 110)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:11:36.910404Z","iopub.execute_input":"2022-08-24T22:11:36.911427Z","iopub.status.idle":"2022-08-24T22:11:36.920618Z","shell.execute_reply.started":"2022-08-24T22:11:36.911380Z","shell.execute_reply":"2022-08-24T22:11:36.919542Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:11:37.104412Z","iopub.execute_input":"2022-08-24T22:11:37.105427Z","iopub.status.idle":"2022-08-24T22:11:37.115464Z","shell.execute_reply.started":"2022-08-24T22:11:37.105379Z","shell.execute_reply":"2022-08-24T22:11:37.114565Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:11:37.313989Z","iopub.execute_input":"2022-08-24T22:11:37.314667Z","iopub.status.idle":"2022-08-24T22:11:37.321686Z","shell.execute_reply.started":"2022-08-24T22:11:37.314633Z","shell.execute_reply":"2022-08-24T22:11:37.320639Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"# 1. Import data #\n\ntime0 = time.time()\ndf = pd.read_csv('../input/cpcrsp-46/IMLEAP_v4.csv')\ndf.dropna(axis=0, subset=['bm', 'lbm', 'llme', 'lop', 'op', 'linv', 'mom122', 'beta_bw', 'ind'], inplace=True)\ndf.reset_index(inplace=True, drop=True)\ndf = df.sample(500000)\ndisplay(df.shape, df.head(), df.count())","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:12:04.353866Z","iopub.execute_input":"2022-08-24T22:12:04.354560Z","iopub.status.idle":"2022-08-24T22:12:19.980252Z","shell.execute_reply.started":"2022-08-24T22:12:04.354524Z","shell.execute_reply":"2022-08-24T22:12:19.979144Z"},"trusted":true},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":"(500000, 46)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"        PERMNO  prd     mom482      mom242  year      RET   ind        bm  \\\n124460   14533  124   9.013889   37.639513  1968   1.2710   2.0 -0.703278   \n418314   36142  186 -28.658006  170.270066  1973 -21.1628  23.0 -0.826631   \n153420   16791  334        NaN   39.703780  1986  13.1605  43.0 -0.103521   \n782809   62376  649 -36.477058   51.465238  2012 -10.5996  41.0  0.859468   \n832286   66747  369 -73.852485  -45.648759  1989 -12.3147  44.0  0.767562   \n\n              op        gp       inv      mom11      mom122      amhd  \\\n124460  0.166651  0.651314  0.278148  -1.059700   12.982248  2.027949   \n418314  0.032258  0.142237  0.333585  26.592465  -61.937195  6.998093   \n153420  0.221755  0.875338  0.483727  19.440000  105.899232  4.969996   \n782809  0.054783  0.107195  0.173348  15.032400    3.430639  1.770258   \n832286  0.032514  0.146985 -0.190260  -0.630000  -10.817906  6.682770   \n\n        ivol_capm  ivol_ff5   beta_bw        MAX     vol1m     vol6m  \\\n124460   0.765013  0.647098  0.730934   1.406600  0.992017  1.706765   \n418314   7.874195  6.946893  1.293825  21.135115  8.239788  6.641868   \n153420   2.031718  1.626541  0.218155   4.520500  2.070030  3.045736   \n782809   3.177744  2.664914  1.520047   7.265700  4.104987  3.758698   \n832286   5.305862  4.908116  0.891857   7.112900  5.340215  5.048165   \n\n          vol12m    BAspr      size       lbm       lop       lgp      linv  \\\n124460  1.602684      NaN  6.334559 -0.821521  0.182498  0.704945  0.052272   \n418314  5.501024      NaN  1.007867 -1.467710 -0.164025  0.025660 -0.012422   \n153420  4.132032      NaN  3.507942 -1.471208  0.109675  0.746991  0.051571   \n782809  3.960404  0.07946  5.804482  0.736873  0.055082  0.113484  0.036444   \n832286  5.362990      NaN  2.147538  0.609390  0.038800  0.282895 -0.079604   \n\n            llme    l1amhd    l1MAX   l1BAspr    l3amhd    l3MAX   l3BAspr  \\\n124460  6.208594  2.079856   2.9249       NaN  2.159957   5.7613       NaN   \n418314  1.581038  6.890220  11.5979       NaN  6.383651  11.5145       NaN   \n153420  1.712558  5.115119   3.9164       NaN  5.777382  13.1795       NaN   \n782809  5.630611  1.710462   8.5492  0.567491  1.675587   4.6699  0.309358   \n832286  2.204697  6.682653  15.3576       NaN  6.799326   7.6633       NaN   \n\n          l6amhd    l6MAX   l6BAspr   l12amhd   l12MAX  l12BAspr   l12mom122  \\\n124460  2.123702   2.8391       NaN  2.454088   2.9249       NaN   18.081899   \n418314  5.528735   6.2290       NaN  6.850087  11.5979       NaN  105.899232   \n153420  5.821803   4.9103       NaN       NaN   3.9164       NaN  -61.937195   \n782809  1.582174  19.4215  1.290323  1.450566   8.5492  0.333969    6.280244   \n832286  6.234400   6.6447       NaN  6.024024  15.3576       NaN  -60.856614   \n\n        l12ivol_capm  l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  \n124460      1.169551     1.074638    0.852332  1.238122   1.653640  \n418314      2.205730     1.611875    2.063918  3.732448   5.808877  \n153420      2.157976     2.034485    0.410679  2.559675   2.604044  \n782809      1.401304     1.226691    1.524502  2.640329   3.148657  \n832286      7.874195     6.946893    1.180779  7.983689   6.639848  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>124460</th>\n      <td>14533</td>\n      <td>124</td>\n      <td>9.013889</td>\n      <td>37.639513</td>\n      <td>1968</td>\n      <td>1.2710</td>\n      <td>2.0</td>\n      <td>-0.703278</td>\n      <td>0.166651</td>\n      <td>0.651314</td>\n      <td>0.278148</td>\n      <td>-1.059700</td>\n      <td>12.982248</td>\n      <td>2.027949</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>0.730934</td>\n      <td>1.406600</td>\n      <td>0.992017</td>\n      <td>1.706765</td>\n      <td>1.602684</td>\n      <td>NaN</td>\n      <td>6.334559</td>\n      <td>-0.821521</td>\n      <td>0.182498</td>\n      <td>0.704945</td>\n      <td>0.052272</td>\n      <td>6.208594</td>\n      <td>2.079856</td>\n      <td>2.9249</td>\n      <td>NaN</td>\n      <td>2.159957</td>\n      <td>5.7613</td>\n      <td>NaN</td>\n      <td>2.123702</td>\n      <td>2.8391</td>\n      <td>NaN</td>\n      <td>2.454088</td>\n      <td>2.9249</td>\n      <td>NaN</td>\n      <td>18.081899</td>\n      <td>1.169551</td>\n      <td>1.074638</td>\n      <td>0.852332</td>\n      <td>1.238122</td>\n      <td>1.653640</td>\n    </tr>\n    <tr>\n      <th>418314</th>\n      <td>36142</td>\n      <td>186</td>\n      <td>-28.658006</td>\n      <td>170.270066</td>\n      <td>1973</td>\n      <td>-21.1628</td>\n      <td>23.0</td>\n      <td>-0.826631</td>\n      <td>0.032258</td>\n      <td>0.142237</td>\n      <td>0.333585</td>\n      <td>26.592465</td>\n      <td>-61.937195</td>\n      <td>6.998093</td>\n      <td>7.874195</td>\n      <td>6.946893</td>\n      <td>1.293825</td>\n      <td>21.135115</td>\n      <td>8.239788</td>\n      <td>6.641868</td>\n      <td>5.501024</td>\n      <td>NaN</td>\n      <td>1.007867</td>\n      <td>-1.467710</td>\n      <td>-0.164025</td>\n      <td>0.025660</td>\n      <td>-0.012422</td>\n      <td>1.581038</td>\n      <td>6.890220</td>\n      <td>11.5979</td>\n      <td>NaN</td>\n      <td>6.383651</td>\n      <td>11.5145</td>\n      <td>NaN</td>\n      <td>5.528735</td>\n      <td>6.2290</td>\n      <td>NaN</td>\n      <td>6.850087</td>\n      <td>11.5979</td>\n      <td>NaN</td>\n      <td>105.899232</td>\n      <td>2.205730</td>\n      <td>1.611875</td>\n      <td>2.063918</td>\n      <td>3.732448</td>\n      <td>5.808877</td>\n    </tr>\n    <tr>\n      <th>153420</th>\n      <td>16791</td>\n      <td>334</td>\n      <td>NaN</td>\n      <td>39.703780</td>\n      <td>1986</td>\n      <td>13.1605</td>\n      <td>43.0</td>\n      <td>-0.103521</td>\n      <td>0.221755</td>\n      <td>0.875338</td>\n      <td>0.483727</td>\n      <td>19.440000</td>\n      <td>105.899232</td>\n      <td>4.969996</td>\n      <td>2.031718</td>\n      <td>1.626541</td>\n      <td>0.218155</td>\n      <td>4.520500</td>\n      <td>2.070030</td>\n      <td>3.045736</td>\n      <td>4.132032</td>\n      <td>NaN</td>\n      <td>3.507942</td>\n      <td>-1.471208</td>\n      <td>0.109675</td>\n      <td>0.746991</td>\n      <td>0.051571</td>\n      <td>1.712558</td>\n      <td>5.115119</td>\n      <td>3.9164</td>\n      <td>NaN</td>\n      <td>5.777382</td>\n      <td>13.1795</td>\n      <td>NaN</td>\n      <td>5.821803</td>\n      <td>4.9103</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.9164</td>\n      <td>NaN</td>\n      <td>-61.937195</td>\n      <td>2.157976</td>\n      <td>2.034485</td>\n      <td>0.410679</td>\n      <td>2.559675</td>\n      <td>2.604044</td>\n    </tr>\n    <tr>\n      <th>782809</th>\n      <td>62376</td>\n      <td>649</td>\n      <td>-36.477058</td>\n      <td>51.465238</td>\n      <td>2012</td>\n      <td>-10.5996</td>\n      <td>41.0</td>\n      <td>0.859468</td>\n      <td>0.054783</td>\n      <td>0.107195</td>\n      <td>0.173348</td>\n      <td>15.032400</td>\n      <td>3.430639</td>\n      <td>1.770258</td>\n      <td>3.177744</td>\n      <td>2.664914</td>\n      <td>1.520047</td>\n      <td>7.265700</td>\n      <td>4.104987</td>\n      <td>3.758698</td>\n      <td>3.960404</td>\n      <td>0.07946</td>\n      <td>5.804482</td>\n      <td>0.736873</td>\n      <td>0.055082</td>\n      <td>0.113484</td>\n      <td>0.036444</td>\n      <td>5.630611</td>\n      <td>1.710462</td>\n      <td>8.5492</td>\n      <td>0.567491</td>\n      <td>1.675587</td>\n      <td>4.6699</td>\n      <td>0.309358</td>\n      <td>1.582174</td>\n      <td>19.4215</td>\n      <td>1.290323</td>\n      <td>1.450566</td>\n      <td>8.5492</td>\n      <td>0.333969</td>\n      <td>6.280244</td>\n      <td>1.401304</td>\n      <td>1.226691</td>\n      <td>1.524502</td>\n      <td>2.640329</td>\n      <td>3.148657</td>\n    </tr>\n    <tr>\n      <th>832286</th>\n      <td>66747</td>\n      <td>369</td>\n      <td>-73.852485</td>\n      <td>-45.648759</td>\n      <td>1989</td>\n      <td>-12.3147</td>\n      <td>44.0</td>\n      <td>0.767562</td>\n      <td>0.032514</td>\n      <td>0.146985</td>\n      <td>-0.190260</td>\n      <td>-0.630000</td>\n      <td>-10.817906</td>\n      <td>6.682770</td>\n      <td>5.305862</td>\n      <td>4.908116</td>\n      <td>0.891857</td>\n      <td>7.112900</td>\n      <td>5.340215</td>\n      <td>5.048165</td>\n      <td>5.362990</td>\n      <td>NaN</td>\n      <td>2.147538</td>\n      <td>0.609390</td>\n      <td>0.038800</td>\n      <td>0.282895</td>\n      <td>-0.079604</td>\n      <td>2.204697</td>\n      <td>6.682653</td>\n      <td>15.3576</td>\n      <td>NaN</td>\n      <td>6.799326</td>\n      <td>7.6633</td>\n      <td>NaN</td>\n      <td>6.234400</td>\n      <td>6.6447</td>\n      <td>NaN</td>\n      <td>6.024024</td>\n      <td>15.3576</td>\n      <td>NaN</td>\n      <td>-60.856614</td>\n      <td>7.874195</td>\n      <td>6.946893</td>\n      <td>1.180779</td>\n      <td>7.983689</td>\n      <td>6.639848</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"PERMNO          500000\nprd             500000\nmom482          425883\nmom242          490590\nyear            500000\nRET             500000\nind             500000\nbm              500000\nop              500000\ngp              500000\ninv             499556\nmom11           500000\nmom122          500000\namhd            411493\nivol_capm       499975\nivol_ff5        499975\nbeta_bw         500000\nMAX             500000\nvol1m           499927\nvol6m           499547\nvol12m          498829\nBAspr           290729\nsize            500000\nlbm             500000\nlop             500000\nlgp             500000\nlinv            500000\nllme            500000\nl1amhd          411210\nl1MAX           499983\nl1BAspr         290012\nl3amhd          410769\nl3MAX           499884\nl3BAspr         288453\nl6amhd          409847\nl6MAX           499779\nl6BAspr         285965\nl12amhd         407596\nl12MAX          499983\nl12BAspr        281156\nl12mom122       496519\nl12ivol_capm    499482\nl12ivol_ff5     499482\nl12beta_bw      499669\nl12vol6m        498565\nl12vol12m       493773\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# 2. pEDA #\n\ndf.RET.hist()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:12:26.474452Z","iopub.execute_input":"2022-08-24T22:12:26.474805Z","iopub.status.idle":"2022-08-24T22:12:26.696903Z","shell.execute_reply.started":"2022-08-24T22:12:26.474775Z","shell.execute_reply":"2022-08-24T22:12:26.695895Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# explore feature distibution, adjust if seems unreasonable","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:12:26.699613Z","iopub.execute_input":"2022-08-24T22:12:26.699942Z","iopub.status.idle":"2022-08-24T22:12:26.704456Z","shell.execute_reply.started":"2022-08-24T22:12:26.699915Z","shell.execute_reply":"2022-08-24T22:12:26.703099Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# add dummies for some missing features\n\nfeatures_miss_dummies = ['amhd', 'BAspr']\n\nfor col in features_miss_dummies:\n    df[col+'_miss'] = df[col].isnull().astype(int)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:12:26.887935Z","iopub.execute_input":"2022-08-24T22:12:26.888923Z","iopub.status.idle":"2022-08-24T22:12:26.935139Z","shell.execute_reply.started":"2022-08-24T22:12:26.888885Z","shell.execute_reply":"2022-08-24T22:12:26.934142Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"        PERMNO  prd     mom482      mom242  year      RET   ind        bm  \\\n124460   14533  124   9.013889   37.639513  1968   1.2710   2.0 -0.703278   \n418314   36142  186 -28.658006  170.270066  1973 -21.1628  23.0 -0.826631   \n153420   16791  334        NaN   39.703780  1986  13.1605  43.0 -0.103521   \n782809   62376  649 -36.477058   51.465238  2012 -10.5996  41.0  0.859468   \n832286   66747  369 -73.852485  -45.648759  1989 -12.3147  44.0  0.767562   \n\n              op        gp       inv      mom11      mom122      amhd  \\\n124460  0.166651  0.651314  0.278148  -1.059700   12.982248  2.027949   \n418314  0.032258  0.142237  0.333585  26.592465  -61.937195  6.998093   \n153420  0.221755  0.875338  0.483727  19.440000  105.899232  4.969996   \n782809  0.054783  0.107195  0.173348  15.032400    3.430639  1.770258   \n832286  0.032514  0.146985 -0.190260  -0.630000  -10.817906  6.682770   \n\n        ivol_capm  ivol_ff5   beta_bw        MAX     vol1m     vol6m  \\\n124460   0.765013  0.647098  0.730934   1.406600  0.992017  1.706765   \n418314   7.874195  6.946893  1.293825  21.135115  8.239788  6.641868   \n153420   2.031718  1.626541  0.218155   4.520500  2.070030  3.045736   \n782809   3.177744  2.664914  1.520047   7.265700  4.104987  3.758698   \n832286   5.305862  4.908116  0.891857   7.112900  5.340215  5.048165   \n\n          vol12m    BAspr      size       lbm       lop       lgp      linv  \\\n124460  1.602684      NaN  6.334559 -0.821521  0.182498  0.704945  0.052272   \n418314  5.501024      NaN  1.007867 -1.467710 -0.164025  0.025660 -0.012422   \n153420  4.132032      NaN  3.507942 -1.471208  0.109675  0.746991  0.051571   \n782809  3.960404  0.07946  5.804482  0.736873  0.055082  0.113484  0.036444   \n832286  5.362990      NaN  2.147538  0.609390  0.038800  0.282895 -0.079604   \n\n            llme    l1amhd    l1MAX   l1BAspr    l3amhd    l3MAX   l3BAspr  \\\n124460  6.208594  2.079856   2.9249       NaN  2.159957   5.7613       NaN   \n418314  1.581038  6.890220  11.5979       NaN  6.383651  11.5145       NaN   \n153420  1.712558  5.115119   3.9164       NaN  5.777382  13.1795       NaN   \n782809  5.630611  1.710462   8.5492  0.567491  1.675587   4.6699  0.309358   \n832286  2.204697  6.682653  15.3576       NaN  6.799326   7.6633       NaN   \n\n          l6amhd    l6MAX   l6BAspr   l12amhd   l12MAX  l12BAspr   l12mom122  \\\n124460  2.123702   2.8391       NaN  2.454088   2.9249       NaN   18.081899   \n418314  5.528735   6.2290       NaN  6.850087  11.5979       NaN  105.899232   \n153420  5.821803   4.9103       NaN       NaN   3.9164       NaN  -61.937195   \n782809  1.582174  19.4215  1.290323  1.450566   8.5492  0.333969    6.280244   \n832286  6.234400   6.6447       NaN  6.024024  15.3576       NaN  -60.856614   \n\n        l12ivol_capm  l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  amhd_miss  \\\n124460      1.169551     1.074638    0.852332  1.238122   1.653640          0   \n418314      2.205730     1.611875    2.063918  3.732448   5.808877          0   \n153420      2.157976     2.034485    0.410679  2.559675   2.604044          0   \n782809      1.401304     1.226691    1.524502  2.640329   3.148657          0   \n832286      7.874195     6.946893    1.180779  7.983689   6.639848          0   \n\n        BAspr_miss  \n124460           1  \n418314           1  \n153420           1  \n782809           0  \n832286           1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>124460</th>\n      <td>14533</td>\n      <td>124</td>\n      <td>9.013889</td>\n      <td>37.639513</td>\n      <td>1968</td>\n      <td>1.2710</td>\n      <td>2.0</td>\n      <td>-0.703278</td>\n      <td>0.166651</td>\n      <td>0.651314</td>\n      <td>0.278148</td>\n      <td>-1.059700</td>\n      <td>12.982248</td>\n      <td>2.027949</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>0.730934</td>\n      <td>1.406600</td>\n      <td>0.992017</td>\n      <td>1.706765</td>\n      <td>1.602684</td>\n      <td>NaN</td>\n      <td>6.334559</td>\n      <td>-0.821521</td>\n      <td>0.182498</td>\n      <td>0.704945</td>\n      <td>0.052272</td>\n      <td>6.208594</td>\n      <td>2.079856</td>\n      <td>2.9249</td>\n      <td>NaN</td>\n      <td>2.159957</td>\n      <td>5.7613</td>\n      <td>NaN</td>\n      <td>2.123702</td>\n      <td>2.8391</td>\n      <td>NaN</td>\n      <td>2.454088</td>\n      <td>2.9249</td>\n      <td>NaN</td>\n      <td>18.081899</td>\n      <td>1.169551</td>\n      <td>1.074638</td>\n      <td>0.852332</td>\n      <td>1.238122</td>\n      <td>1.653640</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>418314</th>\n      <td>36142</td>\n      <td>186</td>\n      <td>-28.658006</td>\n      <td>170.270066</td>\n      <td>1973</td>\n      <td>-21.1628</td>\n      <td>23.0</td>\n      <td>-0.826631</td>\n      <td>0.032258</td>\n      <td>0.142237</td>\n      <td>0.333585</td>\n      <td>26.592465</td>\n      <td>-61.937195</td>\n      <td>6.998093</td>\n      <td>7.874195</td>\n      <td>6.946893</td>\n      <td>1.293825</td>\n      <td>21.135115</td>\n      <td>8.239788</td>\n      <td>6.641868</td>\n      <td>5.501024</td>\n      <td>NaN</td>\n      <td>1.007867</td>\n      <td>-1.467710</td>\n      <td>-0.164025</td>\n      <td>0.025660</td>\n      <td>-0.012422</td>\n      <td>1.581038</td>\n      <td>6.890220</td>\n      <td>11.5979</td>\n      <td>NaN</td>\n      <td>6.383651</td>\n      <td>11.5145</td>\n      <td>NaN</td>\n      <td>5.528735</td>\n      <td>6.2290</td>\n      <td>NaN</td>\n      <td>6.850087</td>\n      <td>11.5979</td>\n      <td>NaN</td>\n      <td>105.899232</td>\n      <td>2.205730</td>\n      <td>1.611875</td>\n      <td>2.063918</td>\n      <td>3.732448</td>\n      <td>5.808877</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>153420</th>\n      <td>16791</td>\n      <td>334</td>\n      <td>NaN</td>\n      <td>39.703780</td>\n      <td>1986</td>\n      <td>13.1605</td>\n      <td>43.0</td>\n      <td>-0.103521</td>\n      <td>0.221755</td>\n      <td>0.875338</td>\n      <td>0.483727</td>\n      <td>19.440000</td>\n      <td>105.899232</td>\n      <td>4.969996</td>\n      <td>2.031718</td>\n      <td>1.626541</td>\n      <td>0.218155</td>\n      <td>4.520500</td>\n      <td>2.070030</td>\n      <td>3.045736</td>\n      <td>4.132032</td>\n      <td>NaN</td>\n      <td>3.507942</td>\n      <td>-1.471208</td>\n      <td>0.109675</td>\n      <td>0.746991</td>\n      <td>0.051571</td>\n      <td>1.712558</td>\n      <td>5.115119</td>\n      <td>3.9164</td>\n      <td>NaN</td>\n      <td>5.777382</td>\n      <td>13.1795</td>\n      <td>NaN</td>\n      <td>5.821803</td>\n      <td>4.9103</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.9164</td>\n      <td>NaN</td>\n      <td>-61.937195</td>\n      <td>2.157976</td>\n      <td>2.034485</td>\n      <td>0.410679</td>\n      <td>2.559675</td>\n      <td>2.604044</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>782809</th>\n      <td>62376</td>\n      <td>649</td>\n      <td>-36.477058</td>\n      <td>51.465238</td>\n      <td>2012</td>\n      <td>-10.5996</td>\n      <td>41.0</td>\n      <td>0.859468</td>\n      <td>0.054783</td>\n      <td>0.107195</td>\n      <td>0.173348</td>\n      <td>15.032400</td>\n      <td>3.430639</td>\n      <td>1.770258</td>\n      <td>3.177744</td>\n      <td>2.664914</td>\n      <td>1.520047</td>\n      <td>7.265700</td>\n      <td>4.104987</td>\n      <td>3.758698</td>\n      <td>3.960404</td>\n      <td>0.07946</td>\n      <td>5.804482</td>\n      <td>0.736873</td>\n      <td>0.055082</td>\n      <td>0.113484</td>\n      <td>0.036444</td>\n      <td>5.630611</td>\n      <td>1.710462</td>\n      <td>8.5492</td>\n      <td>0.567491</td>\n      <td>1.675587</td>\n      <td>4.6699</td>\n      <td>0.309358</td>\n      <td>1.582174</td>\n      <td>19.4215</td>\n      <td>1.290323</td>\n      <td>1.450566</td>\n      <td>8.5492</td>\n      <td>0.333969</td>\n      <td>6.280244</td>\n      <td>1.401304</td>\n      <td>1.226691</td>\n      <td>1.524502</td>\n      <td>2.640329</td>\n      <td>3.148657</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>832286</th>\n      <td>66747</td>\n      <td>369</td>\n      <td>-73.852485</td>\n      <td>-45.648759</td>\n      <td>1989</td>\n      <td>-12.3147</td>\n      <td>44.0</td>\n      <td>0.767562</td>\n      <td>0.032514</td>\n      <td>0.146985</td>\n      <td>-0.190260</td>\n      <td>-0.630000</td>\n      <td>-10.817906</td>\n      <td>6.682770</td>\n      <td>5.305862</td>\n      <td>4.908116</td>\n      <td>0.891857</td>\n      <td>7.112900</td>\n      <td>5.340215</td>\n      <td>5.048165</td>\n      <td>5.362990</td>\n      <td>NaN</td>\n      <td>2.147538</td>\n      <td>0.609390</td>\n      <td>0.038800</td>\n      <td>0.282895</td>\n      <td>-0.079604</td>\n      <td>2.204697</td>\n      <td>6.682653</td>\n      <td>15.3576</td>\n      <td>NaN</td>\n      <td>6.799326</td>\n      <td>7.6633</td>\n      <td>NaN</td>\n      <td>6.234400</td>\n      <td>6.6447</td>\n      <td>NaN</td>\n      <td>6.024024</td>\n      <td>15.3576</td>\n      <td>NaN</td>\n      <td>-60.856614</td>\n      <td>7.874195</td>\n      <td>6.946893</td>\n      <td>1.180779</td>\n      <td>7.983689</td>\n      <td>6.639848</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 3. Train-test split #\n\ntemp_cols = ['PERMNO', 'prd', 'year']\n\ntrain = df[df.year<2016]\ntest = df[df.year>=2016]\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\ntrain.drop(columns=temp_cols, inplace=True)\ntest.drop(columns=temp_cols, inplace=True)\ndisplay(train.shape, test.shape, train.head(3), test.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:12:27.098037Z","iopub.execute_input":"2022-08-24T22:12:27.098623Z","iopub.status.idle":"2022-08-24T22:12:27.337328Z","shell.execute_reply.started":"2022-08-24T22:12:27.098589Z","shell.execute_reply":"2022-08-24T22:12:27.336159Z"},"trusted":true},"execution_count":33,"outputs":[{"output_type":"display_data","data":{"text/plain":"(478954, 45)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(21046, 45)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"      mom482      mom242      RET   ind        bm        op        gp  \\\n0   9.013889   37.639513   1.2710   2.0 -0.703278  0.166651  0.651314   \n1 -28.658006  170.270066 -21.1628  23.0 -0.826631  0.032258  0.142237   \n2        NaN   39.703780  13.1605  43.0 -0.103521  0.221755  0.875338   \n\n        inv      mom11      mom122      amhd  ivol_capm  ivol_ff5   beta_bw  \\\n0  0.278148  -1.059700   12.982248  2.027949   0.765013  0.647098  0.730934   \n1  0.333585  26.592465  -61.937195  6.998093   7.874195  6.946893  1.293825   \n2  0.483727  19.440000  105.899232  4.969996   2.031718  1.626541  0.218155   \n\n         MAX     vol1m     vol6m    vol12m  BAspr      size       lbm  \\\n0   1.406600  0.992017  1.706765  1.602684    NaN  6.334559 -0.821521   \n1  21.135115  8.239788  6.641868  5.501024    NaN  1.007867 -1.467710   \n2   4.520500  2.070030  3.045736  4.132032    NaN  3.507942 -1.471208   \n\n        lop       lgp      linv      llme    l1amhd    l1MAX  l1BAspr  \\\n0  0.182498  0.704945  0.052272  6.208594  2.079856   2.9249      NaN   \n1 -0.164025  0.025660 -0.012422  1.581038  6.890220  11.5979      NaN   \n2  0.109675  0.746991  0.051571  1.712558  5.115119   3.9164      NaN   \n\n     l3amhd    l3MAX  l3BAspr    l6amhd   l6MAX  l6BAspr   l12amhd   l12MAX  \\\n0  2.159957   5.7613      NaN  2.123702  2.8391      NaN  2.454088   2.9249   \n1  6.383651  11.5145      NaN  5.528735  6.2290      NaN  6.850087  11.5979   \n2  5.777382  13.1795      NaN  5.821803  4.9103      NaN       NaN   3.9164   \n\n   l12BAspr   l12mom122  l12ivol_capm  l12ivol_ff5  l12beta_bw  l12vol6m  \\\n0       NaN   18.081899      1.169551     1.074638    0.852332  1.238122   \n1       NaN  105.899232      2.205730     1.611875    2.063918  3.732448   \n2       NaN  -61.937195      2.157976     2.034485    0.410679  2.559675   \n\n   l12vol12m  amhd_miss  BAspr_miss  \n0   1.653640          0           1  \n1   5.808877          0           1  \n2   2.604044          0           1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>9.013889</td>\n      <td>37.639513</td>\n      <td>1.2710</td>\n      <td>2.0</td>\n      <td>-0.703278</td>\n      <td>0.166651</td>\n      <td>0.651314</td>\n      <td>0.278148</td>\n      <td>-1.059700</td>\n      <td>12.982248</td>\n      <td>2.027949</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>0.730934</td>\n      <td>1.406600</td>\n      <td>0.992017</td>\n      <td>1.706765</td>\n      <td>1.602684</td>\n      <td>NaN</td>\n      <td>6.334559</td>\n      <td>-0.821521</td>\n      <td>0.182498</td>\n      <td>0.704945</td>\n      <td>0.052272</td>\n      <td>6.208594</td>\n      <td>2.079856</td>\n      <td>2.9249</td>\n      <td>NaN</td>\n      <td>2.159957</td>\n      <td>5.7613</td>\n      <td>NaN</td>\n      <td>2.123702</td>\n      <td>2.8391</td>\n      <td>NaN</td>\n      <td>2.454088</td>\n      <td>2.9249</td>\n      <td>NaN</td>\n      <td>18.081899</td>\n      <td>1.169551</td>\n      <td>1.074638</td>\n      <td>0.852332</td>\n      <td>1.238122</td>\n      <td>1.653640</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-28.658006</td>\n      <td>170.270066</td>\n      <td>-21.1628</td>\n      <td>23.0</td>\n      <td>-0.826631</td>\n      <td>0.032258</td>\n      <td>0.142237</td>\n      <td>0.333585</td>\n      <td>26.592465</td>\n      <td>-61.937195</td>\n      <td>6.998093</td>\n      <td>7.874195</td>\n      <td>6.946893</td>\n      <td>1.293825</td>\n      <td>21.135115</td>\n      <td>8.239788</td>\n      <td>6.641868</td>\n      <td>5.501024</td>\n      <td>NaN</td>\n      <td>1.007867</td>\n      <td>-1.467710</td>\n      <td>-0.164025</td>\n      <td>0.025660</td>\n      <td>-0.012422</td>\n      <td>1.581038</td>\n      <td>6.890220</td>\n      <td>11.5979</td>\n      <td>NaN</td>\n      <td>6.383651</td>\n      <td>11.5145</td>\n      <td>NaN</td>\n      <td>5.528735</td>\n      <td>6.2290</td>\n      <td>NaN</td>\n      <td>6.850087</td>\n      <td>11.5979</td>\n      <td>NaN</td>\n      <td>105.899232</td>\n      <td>2.205730</td>\n      <td>1.611875</td>\n      <td>2.063918</td>\n      <td>3.732448</td>\n      <td>5.808877</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>NaN</td>\n      <td>39.703780</td>\n      <td>13.1605</td>\n      <td>43.0</td>\n      <td>-0.103521</td>\n      <td>0.221755</td>\n      <td>0.875338</td>\n      <td>0.483727</td>\n      <td>19.440000</td>\n      <td>105.899232</td>\n      <td>4.969996</td>\n      <td>2.031718</td>\n      <td>1.626541</td>\n      <td>0.218155</td>\n      <td>4.520500</td>\n      <td>2.070030</td>\n      <td>3.045736</td>\n      <td>4.132032</td>\n      <td>NaN</td>\n      <td>3.507942</td>\n      <td>-1.471208</td>\n      <td>0.109675</td>\n      <td>0.746991</td>\n      <td>0.051571</td>\n      <td>1.712558</td>\n      <td>5.115119</td>\n      <td>3.9164</td>\n      <td>NaN</td>\n      <td>5.777382</td>\n      <td>13.1795</td>\n      <td>NaN</td>\n      <td>5.821803</td>\n      <td>4.9103</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3.9164</td>\n      <td>NaN</td>\n      <td>-61.937195</td>\n      <td>2.157976</td>\n      <td>2.034485</td>\n      <td>0.410679</td>\n      <td>2.559675</td>\n      <td>2.604044</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       mom482      mom242      RET   ind        bm        op        gp  \\\n0  145.738128  131.036151  28.3950   7.0 -2.233461  0.091727  0.370789   \n1  150.895366    8.365179  -5.5332  43.0 -1.507180  0.094363  0.741223   \n2  -13.384455   -5.724801   9.7200  33.0 -0.892905  0.082544  0.315136   \n\n        inv    mom11     mom122      amhd  ivol_capm  ivol_ff5   beta_bw  \\\n0 -0.048301  -7.3879   1.342561 -0.863785   1.039271  0.856870  1.276048   \n1  0.178561  16.8509 -32.592943  0.187449   2.329873  2.120293  1.112940   \n2  0.475418  -8.6550  -5.580777 -0.284740   1.638956  1.352308  1.112851   \n\n      MAX     vol1m     vol6m    vol12m     BAspr      size       lbm  \\\n0  2.0626  1.265321  1.823538  2.718985  0.057937  6.574528 -1.802292   \n1  5.4510  2.597804  3.556306  3.063814  0.116959  6.068495 -1.784947   \n2  4.3058  1.939359  2.421194  2.376146  0.080354  6.645138 -1.547001   \n\n        lop       lgp      linv      llme    l1amhd   l1MAX   l1BAspr  \\\n0  0.058850  0.335743 -0.169622  6.621589 -0.832456  3.7409  0.053505   \n1  0.095391  0.775690  0.757106  6.263569  0.125900  3.9757  0.196464   \n2  0.090118  0.358055  0.210109  6.788767 -0.302613  5.3228  0.037764   \n\n     l3amhd      l3MAX   l3BAspr    l6amhd   l6MAX   l6BAspr   l12amhd  \\\n0 -0.817994   5.693900  0.062305 -0.806025  7.0269  0.165563 -0.144257   \n1 -0.113624  21.135115  0.131148 -0.542371  3.6398  0.076394 -0.646052   \n2 -0.423059   4.127400  0.044228 -0.636321  6.0989  0.046275 -0.848080   \n\n   l12MAX  l12BAspr   l12mom122  l12ivol_capm  l12ivol_ff5  l12beta_bw  \\\n0  3.7409  0.169587  105.899232      2.836631     2.347055    1.042882   \n1  3.9757  0.138825   40.694634      1.725765     1.159777    0.997316   \n2  5.3228  0.107028   21.874954      1.459692     1.201918    1.163704   \n\n   l12vol6m  l12vol12m  amhd_miss  BAspr_miss  \n0  3.019141   2.934520          0           0  \n1  2.411401   2.661536          0           0  \n2  2.068168   2.438478          0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>145.738128</td>\n      <td>131.036151</td>\n      <td>28.3950</td>\n      <td>7.0</td>\n      <td>-2.233461</td>\n      <td>0.091727</td>\n      <td>0.370789</td>\n      <td>-0.048301</td>\n      <td>-7.3879</td>\n      <td>1.342561</td>\n      <td>-0.863785</td>\n      <td>1.039271</td>\n      <td>0.856870</td>\n      <td>1.276048</td>\n      <td>2.0626</td>\n      <td>1.265321</td>\n      <td>1.823538</td>\n      <td>2.718985</td>\n      <td>0.057937</td>\n      <td>6.574528</td>\n      <td>-1.802292</td>\n      <td>0.058850</td>\n      <td>0.335743</td>\n      <td>-0.169622</td>\n      <td>6.621589</td>\n      <td>-0.832456</td>\n      <td>3.7409</td>\n      <td>0.053505</td>\n      <td>-0.817994</td>\n      <td>5.693900</td>\n      <td>0.062305</td>\n      <td>-0.806025</td>\n      <td>7.0269</td>\n      <td>0.165563</td>\n      <td>-0.144257</td>\n      <td>3.7409</td>\n      <td>0.169587</td>\n      <td>105.899232</td>\n      <td>2.836631</td>\n      <td>2.347055</td>\n      <td>1.042882</td>\n      <td>3.019141</td>\n      <td>2.934520</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>150.895366</td>\n      <td>8.365179</td>\n      <td>-5.5332</td>\n      <td>43.0</td>\n      <td>-1.507180</td>\n      <td>0.094363</td>\n      <td>0.741223</td>\n      <td>0.178561</td>\n      <td>16.8509</td>\n      <td>-32.592943</td>\n      <td>0.187449</td>\n      <td>2.329873</td>\n      <td>2.120293</td>\n      <td>1.112940</td>\n      <td>5.4510</td>\n      <td>2.597804</td>\n      <td>3.556306</td>\n      <td>3.063814</td>\n      <td>0.116959</td>\n      <td>6.068495</td>\n      <td>-1.784947</td>\n      <td>0.095391</td>\n      <td>0.775690</td>\n      <td>0.757106</td>\n      <td>6.263569</td>\n      <td>0.125900</td>\n      <td>3.9757</td>\n      <td>0.196464</td>\n      <td>-0.113624</td>\n      <td>21.135115</td>\n      <td>0.131148</td>\n      <td>-0.542371</td>\n      <td>3.6398</td>\n      <td>0.076394</td>\n      <td>-0.646052</td>\n      <td>3.9757</td>\n      <td>0.138825</td>\n      <td>40.694634</td>\n      <td>1.725765</td>\n      <td>1.159777</td>\n      <td>0.997316</td>\n      <td>2.411401</td>\n      <td>2.661536</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-13.384455</td>\n      <td>-5.724801</td>\n      <td>9.7200</td>\n      <td>33.0</td>\n      <td>-0.892905</td>\n      <td>0.082544</td>\n      <td>0.315136</td>\n      <td>0.475418</td>\n      <td>-8.6550</td>\n      <td>-5.580777</td>\n      <td>-0.284740</td>\n      <td>1.638956</td>\n      <td>1.352308</td>\n      <td>1.112851</td>\n      <td>4.3058</td>\n      <td>1.939359</td>\n      <td>2.421194</td>\n      <td>2.376146</td>\n      <td>0.080354</td>\n      <td>6.645138</td>\n      <td>-1.547001</td>\n      <td>0.090118</td>\n      <td>0.358055</td>\n      <td>0.210109</td>\n      <td>6.788767</td>\n      <td>-0.302613</td>\n      <td>5.3228</td>\n      <td>0.037764</td>\n      <td>-0.423059</td>\n      <td>4.127400</td>\n      <td>0.044228</td>\n      <td>-0.636321</td>\n      <td>6.0989</td>\n      <td>0.046275</td>\n      <td>-0.848080</td>\n      <td>5.3228</td>\n      <td>0.107028</td>\n      <td>21.874954</td>\n      <td>1.459692</td>\n      <td>1.201918</td>\n      <td>1.163704</td>\n      <td>2.068168</td>\n      <td>2.438478</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 4. Missing values #\n\ncol_ignore = ['RET']\ncol_cat = ['ind']\ncol_num = [x for x in train.columns if x not in col_ignore+col_cat]\n\nfor col in col_num:\n    train[col] = train[col].fillna(train[col].median())\n    test[col] = test[col].fillna(train[col].median())\n\nfor col in col_cat:\n    train[col] = train[col].fillna(value=-1000)\n    test[col] = test[col].fillna(value=-1000)\n    \ndisplay(train.count())","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:12:27.339130Z","iopub.execute_input":"2022-08-24T22:12:27.340137Z","iopub.status.idle":"2022-08-24T22:12:28.200640Z","shell.execute_reply.started":"2022-08-24T22:12:27.340095Z","shell.execute_reply":"2022-08-24T22:12:28.199710Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"mom482          478954\nmom242          478954\nRET             478954\nind             478954\nbm              478954\nop              478954\ngp              478954\ninv             478954\nmom11           478954\nmom122          478954\namhd            478954\nivol_capm       478954\nivol_ff5        478954\nbeta_bw         478954\nMAX             478954\nvol1m           478954\nvol6m           478954\nvol12m          478954\nBAspr           478954\nsize            478954\nlbm             478954\nlop             478954\nlgp             478954\nlinv            478954\nllme            478954\nl1amhd          478954\nl1MAX           478954\nl1BAspr         478954\nl3amhd          478954\nl3MAX           478954\nl3BAspr         478954\nl6amhd          478954\nl6MAX           478954\nl6BAspr         478954\nl12amhd         478954\nl12MAX          478954\nl12BAspr        478954\nl12mom122       478954\nl12ivol_capm    478954\nl12ivol_ff5     478954\nl12beta_bw      478954\nl12vol6m        478954\nl12vol12m       478954\namhd_miss       478954\nBAspr_miss      478954\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# [optional] Target Encoding\n","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:20:22.834136Z","iopub.execute_input":"2022-08-24T22:20:22.834576Z","iopub.status.idle":"2022-08-24T22:20:22.839465Z","shell.execute_reply.started":"2022-08-24T22:20:22.834533Z","shell.execute_reply":"2022-08-24T22:20:22.838507Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"X_train = train.copy()\ny_train = X_train.pop('RET')\n\nX_test = test.copy()\ny_test = X_test.pop('RET')","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:12:28.504183Z","iopub.execute_input":"2022-08-24T22:12:28.505197Z","iopub.status.idle":"2022-08-24T22:12:28.582503Z","shell.execute_reply.started":"2022-08-24T22:12:28.505150Z","shell.execute_reply":"2022-08-24T22:12:28.581518Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# 5. Feature engineering #\n\ntime1 = time.time()\n\nfeature_transformer = ColumnTransformer([\n    (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat),\n    ('num', StandardScaler(), col_num)])\n\nprint('Number of features before transformation: ', X_train.shape)\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ', time.time()-time1)\nprint('Number of features after transformation: ', X_train.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:12:29.736824Z","iopub.execute_input":"2022-08-24T22:12:29.737481Z","iopub.status.idle":"2022-08-24T22:12:30.787172Z","shell.execute_reply.started":"2022-08-24T22:12:29.737446Z","shell.execute_reply":"2022-08-24T22:12:30.786112Z"},"trusted":true},"execution_count":37,"outputs":[{"name":"stdout","text":"Number of features before transformation:  (478954, 44)\ntime to do feature proprocessing:  1.0431735515594482\nNumber of features after transformation:  (478954, 92)\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:12:30.789170Z","iopub.execute_input":"2022-08-24T22:12:30.789548Z","iopub.status.idle":"2022-08-24T22:12:30.901420Z","shell.execute_reply.started":"2022-08-24T22:12:30.789508Z","shell.execute_reply":"2022-08-24T22:12:30.900294Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"        cat__ind_1.0  cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  \\\n0                0.0           1.0           0.0           0.0           0.0   \n1                0.0           0.0           0.0           0.0           0.0   \n2                0.0           0.0           0.0           0.0           0.0   \n3                0.0           0.0           0.0           0.0           0.0   \n4                0.0           0.0           0.0           0.0           0.0   \n...              ...           ...           ...           ...           ...   \n478949           0.0           0.0           0.0           0.0           0.0   \n478950           0.0           0.0           0.0           0.0           0.0   \n478951           0.0           1.0           0.0           0.0           0.0   \n478952           0.0           0.0           0.0           0.0           0.0   \n478953           0.0           0.0           0.0           0.0           0.0   \n\n        cat__ind_6.0  cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  \\\n0                0.0           0.0           0.0           0.0            0.0   \n1                0.0           0.0           0.0           0.0            0.0   \n2                0.0           0.0           0.0           0.0            0.0   \n3                0.0           0.0           0.0           0.0            0.0   \n4                0.0           0.0           0.0           0.0            0.0   \n...              ...           ...           ...           ...            ...   \n478949           0.0           0.0           0.0           0.0            0.0   \n478950           0.0           0.0           0.0           0.0            0.0   \n478951           0.0           0.0           0.0           0.0            0.0   \n478952           0.0           0.0           0.0           0.0            0.0   \n478953           0.0           0.0           0.0           0.0            0.0   \n\n        cat__ind_11.0  cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478949            0.0            0.0            0.0            0.0   \n478950            0.0            0.0            0.0            0.0   \n478951            0.0            0.0            0.0            0.0   \n478952            0.0            0.0            0.0            0.0   \n478953            0.0            0.0            0.0            0.0   \n\n        cat__ind_15.0  cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478949            0.0            0.0            0.0            0.0   \n478950            0.0            0.0            0.0            0.0   \n478951            0.0            0.0            0.0            0.0   \n478952            0.0            0.0            0.0            0.0   \n478953            0.0            0.0            0.0            0.0   \n\n        cat__ind_19.0  cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478949            0.0            0.0            0.0            0.0   \n478950            0.0            0.0            0.0            0.0   \n478951            0.0            0.0            0.0            0.0   \n478952            0.0            0.0            0.0            0.0   \n478953            0.0            0.0            0.0            0.0   \n\n        cat__ind_23.0  cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 1.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478949            0.0            0.0            0.0            0.0   \n478950            0.0            0.0            0.0            0.0   \n478951            0.0            0.0            0.0            0.0   \n478952            0.0            0.0            0.0            0.0   \n478953            0.0            0.0            0.0            0.0   \n\n        cat__ind_27.0  cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478949            0.0            0.0            0.0            0.0   \n478950            0.0            0.0            0.0            0.0   \n478951            0.0            0.0            0.0            0.0   \n478952            0.0            0.0            0.0            0.0   \n478953            0.0            0.0            0.0            0.0   \n\n        cat__ind_31.0  cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478949            0.0            0.0            0.0            0.0   \n478950            0.0            0.0            0.0            0.0   \n478951            0.0            0.0            0.0            0.0   \n478952            0.0            0.0            0.0            0.0   \n478953            0.0            0.0            0.0            0.0   \n\n        cat__ind_35.0  cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478949            0.0            0.0            0.0            0.0   \n478950            0.0            0.0            0.0            0.0   \n478951            0.0            0.0            0.0            0.0   \n478952            0.0            0.0            0.0            0.0   \n478953            0.0            0.0            0.0            0.0   \n\n        cat__ind_39.0  cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            1.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478949            1.0            0.0            0.0            0.0   \n478950            0.0            0.0            0.0            1.0   \n478951            0.0            0.0            0.0            0.0   \n478952            0.0            0.0            1.0            0.0   \n478953            0.0            0.0            0.0            0.0   \n\n        cat__ind_43.0  cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 1.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            1.0            0.0            0.0   \n...               ...            ...            ...            ...   \n478949            0.0            0.0            0.0            0.0   \n478950            0.0            0.0            0.0            0.0   \n478951            0.0            0.0            0.0            0.0   \n478952            0.0            0.0            0.0            0.0   \n478953            1.0            0.0            0.0            0.0   \n\n        cat__ind_47.0  cat__ind_48.0  cat__ind_49.0  num__mom482  num__mom242  \\\n0                 0.0            0.0            0.0    -0.218906     0.370929   \n1                 0.0            0.0            0.0    -0.638466     2.487226   \n2                 0.0            0.0            0.0    -0.245483     0.403867   \n3                 0.0            0.0            0.0    -0.725549     0.591537   \n4                 0.0            0.0            0.0    -1.141807    -0.958046   \n...               ...            ...            ...          ...          ...   \n478949            0.0            0.0            0.0     0.482390     0.453979   \n478950            0.0            0.0            0.0    -0.002498     0.201810   \n478951            0.0            0.0            0.0     0.239272    -0.202792   \n478952            0.0            0.0            0.0    -0.505391    -0.312457   \n478953            0.0            0.0            0.0     1.555836     0.237376   \n\n         num__bm   num__op   num__gp  num__inv  num__mom11  num__mom122  \\\n0      -0.308925  0.645274  1.091918  0.708540   -0.124805     0.161673   \n1      -0.442645 -0.582898 -1.217755  0.953810    2.210518    -1.619593   \n2       0.341242  1.148853  2.108310  1.618092    1.606467     2.370843   \n3       1.385169 -0.377046 -1.376742  0.244867    1.234230    -0.065424   \n4       1.285539 -0.580562 -1.196217 -1.363862   -0.088515    -0.404194   \n...          ...       ...       ...       ...         ...          ...   \n478949  1.800889  0.199597 -0.962025 -0.255259    0.635345     2.370843   \n478950  0.373547  0.374534  0.624135 -0.227959    0.509095    -0.201255   \n478951  0.638658 -0.162273 -0.022267 -0.462225    0.149923    -0.008209   \n478952  0.035586  0.251540  0.953190 -0.293046   -1.075540     0.304279   \n478953 -0.268304 -0.220659 -0.276874 -0.149076    0.535588     0.423677   \n\n        num__amhd  num__ivol_capm  num__ivol_ff5  num__beta_bw  num__MAX  \\\n0        0.013353       -1.118101      -1.108794     -0.429794 -1.078262   \n1        1.811069        2.715115       2.719963      1.124240  2.805525   \n2        1.077500       -0.435104      -0.513529     -1.845479 -0.465255   \n3       -0.079854        0.182824       0.117552      1.748795  0.075170   \n4        1.697016        1.330290       1.480878      0.014483  0.045089   \n...           ...             ...            ...           ...       ...   \n478949  -0.591023       -0.608247      -0.548834      1.583767  0.123539   \n478950   0.388529       -0.858651      -0.861242     -0.917528 -0.789978   \n478951   0.093625        0.822027       0.215243     -1.610718  0.471964   \n478952   0.286409       -0.399931      -0.234935      1.128249 -0.352118   \n478953  -1.746641       -0.578571      -0.494223      0.778128 -0.317904   \n\n        num__vol1m  num__vol6m  num__vol12m  num__BAspr  num__size  num__lbm  \\\n0        -1.083012   -0.866507    -0.978219   -0.296025   0.708292 -0.418304   \n1         2.696477    1.900586     1.285004   -0.296025  -1.662158 -1.115891   \n2        -0.520862   -0.115751     0.490221   -0.296025  -0.549590 -1.119668   \n3         0.540306    0.284004     0.390581   -0.777461   0.472401  1.264047   \n4         1.184439    1.007003     1.204867   -0.296025  -1.154988  1.126423   \n...            ...         ...          ...         ...        ...       ...   \n478949   -0.040185   -0.000932     0.613157   -0.781884   0.695199  0.222339   \n478950   -0.954536   -0.852136    -0.890108   -0.296025   0.561488  0.345959   \n478951    0.831548   -0.338080    -0.634888    2.194705  -0.922778  0.295831   \n478952   -0.351581    0.429452     0.248085   -0.473729   0.332903 -0.168937   \n478953   -0.619180   -1.009281    -0.826332   -0.795031   1.341422  0.061814   \n\n        num__lop  num__lgp  num__linv  num__llme  num__l1amhd  num__l1MAX  \\\n0       0.765620  1.329396  -0.336146   0.686098     0.029816   -0.779859   \n1      -2.394791 -1.748651  -0.611271  -1.410105     1.773927    0.932819   \n2       0.101445  1.519917  -0.339130  -1.350529     1.130323   -0.584065   \n3      -0.396457 -1.350695  -0.403456   0.424282    -0.104116    0.330785   \n4      -0.544952 -0.583040  -0.896973  -1.127598     1.698669    1.675256   \n...          ...       ...        ...        ...          ...         ...   \n478949 -0.032776 -1.014228  -0.548281   0.271596    -0.436176    0.254916   \n478950  0.334079  0.404258   0.051414   0.541132     0.397469   -1.002213   \n478951  0.860095  2.101497  -0.612362  -0.950750     0.094524   -0.839180   \n478952 -0.038546  0.717643  -0.758184   0.378595     0.298525   -0.958058   \n478953 -0.294419 -0.331491   0.561994   1.266458    -1.763919   -0.290286   \n\n        num__l1BAspr  num__l3amhd  num__l3MAX  num__l3BAspr  num__l6amhd  \\\n0          -0.294892     0.053703   -0.217746     -0.293140     0.032613   \n1          -0.294892     1.591001    0.927637     -0.293140     1.279635   \n2          -0.294892     1.370337    1.259116     -0.293140     1.386965   \n3          -0.600390    -0.122593   -0.435029     -0.701087    -0.165710   \n4          -0.294892     1.742294    0.160916     -0.293140     1.538070   \n...              ...          ...         ...           ...          ...   \n478949     -0.754975    -0.259260    0.333623     -0.753582    -0.124704   \n478950     -0.294892     0.345303    0.694129     -0.293140     0.308623   \n478951      0.906577     0.095470   -0.916460      0.833724     0.096592   \n478952     -0.706382     0.294974    0.524189     -0.599171     0.289759   \n478953     -0.793824    -1.786769   -0.903081     -0.802140    -1.767714   \n\n        num__l6MAX  num__l6BAspr  num__l12amhd  num__l12MAX  num__l12BAspr  \\\n0        -0.801377     -0.289551      0.138379    -0.779859      -0.283089   \n1        -0.123002     -0.289551      1.768137     0.932819      -0.283089   \n2        -0.386896     -0.289551      0.099000    -0.584065      -0.283089   \n3         2.517035     -0.341364     -0.233663     0.330785      -0.715054   \n4        -0.039814     -0.289551      1.461885     1.675256      -0.283089   \n...            ...           ...           ...          ...            ...   \n478949    0.067489     -0.781326     -0.157410     0.254916      -0.698293   \n478950    0.712584     -0.289551      0.244278    -1.002213      -0.283089   \n478951   -0.908139      0.959729      0.099000    -0.839180       0.925669   \n478952    2.266109     -0.113984      0.090837    -0.958058      -0.650760   \n478953   -0.421676     -0.814026     -1.874177    -0.290286      -0.830893   \n\n        num__l12mom122  num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  \\\n0             0.273169          -0.915345         -0.862900        -0.107744   \n1             2.361459          -0.340340         -0.526806         3.214306   \n2            -1.629679          -0.366840         -0.262422        -1.318711   \n3            -0.007473          -0.786739         -0.767776         1.735282   \n4            -1.603983           2.805255          2.810767         0.792826   \n...                ...                ...               ...              ...   \n478949       -1.629679           1.224557          1.111139         1.629587   \n478950        0.821011          -1.068243         -1.023055        -1.073116   \n478951       -0.384506          -0.903368         -0.918806        -1.793153   \n478952       -0.545360          -0.361787         -0.253859         0.935438   \n478953        0.245398          -0.986781         -0.940045         0.539242   \n\n        num__l12vol6m  num__l12vol12m  num__amhd_miss  num__BAspr_miss  \n0           -1.144295       -0.959666       -0.474919         1.135272  \n1            0.290409        1.521756       -0.474919         1.135272  \n2           -0.384155       -0.392105       -0.474919         1.135272  \n3           -0.337764       -0.066873       -0.474919        -0.880846  \n4            2.735669        2.017995       -0.474919         1.135272  \n...               ...             ...             ...              ...  \n478949       2.735669        2.440538       -0.474919        -0.880846  \n478950      -1.163582       -1.236815       -0.474919         1.135272  \n478951      -0.884180       -0.782301        2.105622        -0.880846  \n478952      -0.511760       -0.420995       -0.474919        -0.880846  \n478953      -0.979332       -1.068682       -0.474919        -0.880846  \n\n[478954 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.218906</td>\n      <td>0.370929</td>\n      <td>-0.308925</td>\n      <td>0.645274</td>\n      <td>1.091918</td>\n      <td>0.708540</td>\n      <td>-0.124805</td>\n      <td>0.161673</td>\n      <td>0.013353</td>\n      <td>-1.118101</td>\n      <td>-1.108794</td>\n      <td>-0.429794</td>\n      <td>-1.078262</td>\n      <td>-1.083012</td>\n      <td>-0.866507</td>\n      <td>-0.978219</td>\n      <td>-0.296025</td>\n      <td>0.708292</td>\n      <td>-0.418304</td>\n      <td>0.765620</td>\n      <td>1.329396</td>\n      <td>-0.336146</td>\n      <td>0.686098</td>\n      <td>0.029816</td>\n      <td>-0.779859</td>\n      <td>-0.294892</td>\n      <td>0.053703</td>\n      <td>-0.217746</td>\n      <td>-0.293140</td>\n      <td>0.032613</td>\n      <td>-0.801377</td>\n      <td>-0.289551</td>\n      <td>0.138379</td>\n      <td>-0.779859</td>\n      <td>-0.283089</td>\n      <td>0.273169</td>\n      <td>-0.915345</td>\n      <td>-0.862900</td>\n      <td>-0.107744</td>\n      <td>-1.144295</td>\n      <td>-0.959666</td>\n      <td>-0.474919</td>\n      <td>1.135272</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.638466</td>\n      <td>2.487226</td>\n      <td>-0.442645</td>\n      <td>-0.582898</td>\n      <td>-1.217755</td>\n      <td>0.953810</td>\n      <td>2.210518</td>\n      <td>-1.619593</td>\n      <td>1.811069</td>\n      <td>2.715115</td>\n      <td>2.719963</td>\n      <td>1.124240</td>\n      <td>2.805525</td>\n      <td>2.696477</td>\n      <td>1.900586</td>\n      <td>1.285004</td>\n      <td>-0.296025</td>\n      <td>-1.662158</td>\n      <td>-1.115891</td>\n      <td>-2.394791</td>\n      <td>-1.748651</td>\n      <td>-0.611271</td>\n      <td>-1.410105</td>\n      <td>1.773927</td>\n      <td>0.932819</td>\n      <td>-0.294892</td>\n      <td>1.591001</td>\n      <td>0.927637</td>\n      <td>-0.293140</td>\n      <td>1.279635</td>\n      <td>-0.123002</td>\n      <td>-0.289551</td>\n      <td>1.768137</td>\n      <td>0.932819</td>\n      <td>-0.283089</td>\n      <td>2.361459</td>\n      <td>-0.340340</td>\n      <td>-0.526806</td>\n      <td>3.214306</td>\n      <td>0.290409</td>\n      <td>1.521756</td>\n      <td>-0.474919</td>\n      <td>1.135272</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.245483</td>\n      <td>0.403867</td>\n      <td>0.341242</td>\n      <td>1.148853</td>\n      <td>2.108310</td>\n      <td>1.618092</td>\n      <td>1.606467</td>\n      <td>2.370843</td>\n      <td>1.077500</td>\n      <td>-0.435104</td>\n      <td>-0.513529</td>\n      <td>-1.845479</td>\n      <td>-0.465255</td>\n      <td>-0.520862</td>\n      <td>-0.115751</td>\n      <td>0.490221</td>\n      <td>-0.296025</td>\n      <td>-0.549590</td>\n      <td>-1.119668</td>\n      <td>0.101445</td>\n      <td>1.519917</td>\n      <td>-0.339130</td>\n      <td>-1.350529</td>\n      <td>1.130323</td>\n      <td>-0.584065</td>\n      <td>-0.294892</td>\n      <td>1.370337</td>\n      <td>1.259116</td>\n      <td>-0.293140</td>\n      <td>1.386965</td>\n      <td>-0.386896</td>\n      <td>-0.289551</td>\n      <td>0.099000</td>\n      <td>-0.584065</td>\n      <td>-0.283089</td>\n      <td>-1.629679</td>\n      <td>-0.366840</td>\n      <td>-0.262422</td>\n      <td>-1.318711</td>\n      <td>-0.384155</td>\n      <td>-0.392105</td>\n      <td>-0.474919</td>\n      <td>1.135272</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.725549</td>\n      <td>0.591537</td>\n      <td>1.385169</td>\n      <td>-0.377046</td>\n      <td>-1.376742</td>\n      <td>0.244867</td>\n      <td>1.234230</td>\n      <td>-0.065424</td>\n      <td>-0.079854</td>\n      <td>0.182824</td>\n      <td>0.117552</td>\n      <td>1.748795</td>\n      <td>0.075170</td>\n      <td>0.540306</td>\n      <td>0.284004</td>\n      <td>0.390581</td>\n      <td>-0.777461</td>\n      <td>0.472401</td>\n      <td>1.264047</td>\n      <td>-0.396457</td>\n      <td>-1.350695</td>\n      <td>-0.403456</td>\n      <td>0.424282</td>\n      <td>-0.104116</td>\n      <td>0.330785</td>\n      <td>-0.600390</td>\n      <td>-0.122593</td>\n      <td>-0.435029</td>\n      <td>-0.701087</td>\n      <td>-0.165710</td>\n      <td>2.517035</td>\n      <td>-0.341364</td>\n      <td>-0.233663</td>\n      <td>0.330785</td>\n      <td>-0.715054</td>\n      <td>-0.007473</td>\n      <td>-0.786739</td>\n      <td>-0.767776</td>\n      <td>1.735282</td>\n      <td>-0.337764</td>\n      <td>-0.066873</td>\n      <td>-0.474919</td>\n      <td>-0.880846</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-1.141807</td>\n      <td>-0.958046</td>\n      <td>1.285539</td>\n      <td>-0.580562</td>\n      <td>-1.196217</td>\n      <td>-1.363862</td>\n      <td>-0.088515</td>\n      <td>-0.404194</td>\n      <td>1.697016</td>\n      <td>1.330290</td>\n      <td>1.480878</td>\n      <td>0.014483</td>\n      <td>0.045089</td>\n      <td>1.184439</td>\n      <td>1.007003</td>\n      <td>1.204867</td>\n      <td>-0.296025</td>\n      <td>-1.154988</td>\n      <td>1.126423</td>\n      <td>-0.544952</td>\n      <td>-0.583040</td>\n      <td>-0.896973</td>\n      <td>-1.127598</td>\n      <td>1.698669</td>\n      <td>1.675256</td>\n      <td>-0.294892</td>\n      <td>1.742294</td>\n      <td>0.160916</td>\n      <td>-0.293140</td>\n      <td>1.538070</td>\n      <td>-0.039814</td>\n      <td>-0.289551</td>\n      <td>1.461885</td>\n      <td>1.675256</td>\n      <td>-0.283089</td>\n      <td>-1.603983</td>\n      <td>2.805255</td>\n      <td>2.810767</td>\n      <td>0.792826</td>\n      <td>2.735669</td>\n      <td>2.017995</td>\n      <td>-0.474919</td>\n      <td>1.135272</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>478949</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.482390</td>\n      <td>0.453979</td>\n      <td>1.800889</td>\n      <td>0.199597</td>\n      <td>-0.962025</td>\n      <td>-0.255259</td>\n      <td>0.635345</td>\n      <td>2.370843</td>\n      <td>-0.591023</td>\n      <td>-0.608247</td>\n      <td>-0.548834</td>\n      <td>1.583767</td>\n      <td>0.123539</td>\n      <td>-0.040185</td>\n      <td>-0.000932</td>\n      <td>0.613157</td>\n      <td>-0.781884</td>\n      <td>0.695199</td>\n      <td>0.222339</td>\n      <td>-0.032776</td>\n      <td>-1.014228</td>\n      <td>-0.548281</td>\n      <td>0.271596</td>\n      <td>-0.436176</td>\n      <td>0.254916</td>\n      <td>-0.754975</td>\n      <td>-0.259260</td>\n      <td>0.333623</td>\n      <td>-0.753582</td>\n      <td>-0.124704</td>\n      <td>0.067489</td>\n      <td>-0.781326</td>\n      <td>-0.157410</td>\n      <td>0.254916</td>\n      <td>-0.698293</td>\n      <td>-1.629679</td>\n      <td>1.224557</td>\n      <td>1.111139</td>\n      <td>1.629587</td>\n      <td>2.735669</td>\n      <td>2.440538</td>\n      <td>-0.474919</td>\n      <td>-0.880846</td>\n    </tr>\n    <tr>\n      <th>478950</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.002498</td>\n      <td>0.201810</td>\n      <td>0.373547</td>\n      <td>0.374534</td>\n      <td>0.624135</td>\n      <td>-0.227959</td>\n      <td>0.509095</td>\n      <td>-0.201255</td>\n      <td>0.388529</td>\n      <td>-0.858651</td>\n      <td>-0.861242</td>\n      <td>-0.917528</td>\n      <td>-0.789978</td>\n      <td>-0.954536</td>\n      <td>-0.852136</td>\n      <td>-0.890108</td>\n      <td>-0.296025</td>\n      <td>0.561488</td>\n      <td>0.345959</td>\n      <td>0.334079</td>\n      <td>0.404258</td>\n      <td>0.051414</td>\n      <td>0.541132</td>\n      <td>0.397469</td>\n      <td>-1.002213</td>\n      <td>-0.294892</td>\n      <td>0.345303</td>\n      <td>0.694129</td>\n      <td>-0.293140</td>\n      <td>0.308623</td>\n      <td>0.712584</td>\n      <td>-0.289551</td>\n      <td>0.244278</td>\n      <td>-1.002213</td>\n      <td>-0.283089</td>\n      <td>0.821011</td>\n      <td>-1.068243</td>\n      <td>-1.023055</td>\n      <td>-1.073116</td>\n      <td>-1.163582</td>\n      <td>-1.236815</td>\n      <td>-0.474919</td>\n      <td>1.135272</td>\n    </tr>\n    <tr>\n      <th>478951</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.239272</td>\n      <td>-0.202792</td>\n      <td>0.638658</td>\n      <td>-0.162273</td>\n      <td>-0.022267</td>\n      <td>-0.462225</td>\n      <td>0.149923</td>\n      <td>-0.008209</td>\n      <td>0.093625</td>\n      <td>0.822027</td>\n      <td>0.215243</td>\n      <td>-1.610718</td>\n      <td>0.471964</td>\n      <td>0.831548</td>\n      <td>-0.338080</td>\n      <td>-0.634888</td>\n      <td>2.194705</td>\n      <td>-0.922778</td>\n      <td>0.295831</td>\n      <td>0.860095</td>\n      <td>2.101497</td>\n      <td>-0.612362</td>\n      <td>-0.950750</td>\n      <td>0.094524</td>\n      <td>-0.839180</td>\n      <td>0.906577</td>\n      <td>0.095470</td>\n      <td>-0.916460</td>\n      <td>0.833724</td>\n      <td>0.096592</td>\n      <td>-0.908139</td>\n      <td>0.959729</td>\n      <td>0.099000</td>\n      <td>-0.839180</td>\n      <td>0.925669</td>\n      <td>-0.384506</td>\n      <td>-0.903368</td>\n      <td>-0.918806</td>\n      <td>-1.793153</td>\n      <td>-0.884180</td>\n      <td>-0.782301</td>\n      <td>2.105622</td>\n      <td>-0.880846</td>\n    </tr>\n    <tr>\n      <th>478952</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>-0.505391</td>\n      <td>-0.312457</td>\n      <td>0.035586</td>\n      <td>0.251540</td>\n      <td>0.953190</td>\n      <td>-0.293046</td>\n      <td>-1.075540</td>\n      <td>0.304279</td>\n      <td>0.286409</td>\n      <td>-0.399931</td>\n      <td>-0.234935</td>\n      <td>1.128249</td>\n      <td>-0.352118</td>\n      <td>-0.351581</td>\n      <td>0.429452</td>\n      <td>0.248085</td>\n      <td>-0.473729</td>\n      <td>0.332903</td>\n      <td>-0.168937</td>\n      <td>-0.038546</td>\n      <td>0.717643</td>\n      <td>-0.758184</td>\n      <td>0.378595</td>\n      <td>0.298525</td>\n      <td>-0.958058</td>\n      <td>-0.706382</td>\n      <td>0.294974</td>\n      <td>0.524189</td>\n      <td>-0.599171</td>\n      <td>0.289759</td>\n      <td>2.266109</td>\n      <td>-0.113984</td>\n      <td>0.090837</td>\n      <td>-0.958058</td>\n      <td>-0.650760</td>\n      <td>-0.545360</td>\n      <td>-0.361787</td>\n      <td>-0.253859</td>\n      <td>0.935438</td>\n      <td>-0.511760</td>\n      <td>-0.420995</td>\n      <td>-0.474919</td>\n      <td>-0.880846</td>\n    </tr>\n    <tr>\n      <th>478953</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.555836</td>\n      <td>0.237376</td>\n      <td>-0.268304</td>\n      <td>-0.220659</td>\n      <td>-0.276874</td>\n      <td>-0.149076</td>\n      <td>0.535588</td>\n      <td>0.423677</td>\n      <td>-1.746641</td>\n      <td>-0.578571</td>\n      <td>-0.494223</td>\n      <td>0.778128</td>\n      <td>-0.317904</td>\n      <td>-0.619180</td>\n      <td>-1.009281</td>\n      <td>-0.826332</td>\n      <td>-0.795031</td>\n      <td>1.341422</td>\n      <td>0.061814</td>\n      <td>-0.294419</td>\n      <td>-0.331491</td>\n      <td>0.561994</td>\n      <td>1.266458</td>\n      <td>-1.763919</td>\n      <td>-0.290286</td>\n      <td>-0.793824</td>\n      <td>-1.786769</td>\n      <td>-0.903081</td>\n      <td>-0.802140</td>\n      <td>-1.767714</td>\n      <td>-0.421676</td>\n      <td>-0.814026</td>\n      <td>-1.874177</td>\n      <td>-0.290286</td>\n      <td>-0.830893</td>\n      <td>0.245398</td>\n      <td>-0.986781</td>\n      <td>-0.940045</td>\n      <td>0.539242</td>\n      <td>-0.979332</td>\n      <td>-1.068682</td>\n      <td>-0.474919</td>\n      <td>-0.880846</td>\n    </tr>\n  </tbody>\n</table>\n<p>478954 rows  92 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 6. Model fitting #\n\n# first, some trivial baselines:\nprint('mse of a constant model', mean_squared_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\ntime1 = time.time()\nxgb = XGBRegressor(tree_method = 'gpu_hist', n_estimators=300, max_depth=6, eta=0.05, colsample_bytree=0.6)\nxgb.fit(X_train, y_train)\nprint('XGB train:', mean_squared_error(y_train, xgb.predict(X_train)), r2_score(y_train, xgb.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:20:29.755049Z","iopub.execute_input":"2022-08-24T22:20:29.755428Z","iopub.status.idle":"2022-08-24T22:20:39.674194Z","shell.execute_reply.started":"2022-08-24T22:20:29.755399Z","shell.execute_reply":"2022-08-24T22:20:39.673025Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"mse of a constant model 246.36492244330125\nR2 of a constant model 0.0\nXGB train: 229.0785362046512 0.0806672292894085 9.895293235778809\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[300, 500], 'max_depth':[2,4,6], 'eta':[0.02, 0.04, 0.06],\n             'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbm = GridSearchCV(xgb, param_grid, cv=2, verbose=2)\nxgbm.fit(X_train, y_train)\nprint('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)\n# this runs for 40 min and finds \n# 'eta': 0.02, 'max_depth': 6, 'n_estimators': 500, 0.01095415380877135\nprint('XGB train:', mean_squared_error(y_train, xgbm.predict(X_train)), r2_score(y_train, xgbm.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:25:22.536617Z","iopub.execute_input":"2022-08-24T22:25:22.537294Z","iopub.status.idle":"2022-08-24T22:27:55.449954Z","shell.execute_reply.started":"2022-08-24T22:25:22.537258Z","shell.execute_reply":"2022-08-24T22:27:55.449030Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 18 candidates, totalling 36 fits\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=300, subsample=0.6; total time=   1.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=300, subsample=0.6; total time=   2.0s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.6s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=300, subsample=0.6; total time=   3.0s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.8s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.4s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.4s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.5s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=300, subsample=0.6; total time=   2.0s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=300, subsample=0.6; total time=   2.4s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.9s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=2, n_estimators=500, subsample=0.6; total time=   3.4s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=300, subsample=0.6; total time=   3.6s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=300, subsample=0.6; total time=   3.0s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.8s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=4, n_estimators=500, subsample=0.6; total time=   4.1s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.3s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.2s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.6s\n[CV] END colsample_bytree=0.6, eta=0.04, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.8s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=300, subsample=0.6; total time=   1.9s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=300, subsample=0.6; total time=   1.9s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.6s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=2, n_estimators=500, subsample=0.6; total time=   2.6s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=300, subsample=0.6; total time=   2.6s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=500, subsample=0.6; total time=   3.8s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=4, n_estimators=500, subsample=0.6; total time=   4.1s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.7s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=300, subsample=0.6; total time=   4.3s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.7s\n[CV] END colsample_bytree=0.6, eta=0.06, max_depth=6, n_estimators=500, subsample=0.6; total time=   6.9s\nXGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 6, 'n_estimators': 500, 'subsample': 0.6} 0.00924904154358297 142.94360518455505\nXGB train: 233.13571832380555 0.06438503829641429 152.90478205680847\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\n\ndef objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    cv_regularizer=0.01\n    # Usually values between 0.1 and 0.2 work fine.\n\n    params = {\n        \"tree_method\": 'gpu_hist',\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 500),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.005, 0.2),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 0.95),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 0.95),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 10.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 150.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 10.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 10)    }\n    # usually it makes sense to resrtict hyperparameter space from some solutions which Optuna will find\n    # e.g., for tmx-joined data only (downsampled tmx), optuna keeps selecting depths of 2 and 3.\n    # for my purposes (smooth left side of prc, close to 1), those solutions are no good.\n\n    temp_out = []\n\n    for i in range(cv_runs):\n\n        X = X_train\n        y = y_train\n\n        model = XGBRegressor(**params, njobs=-1)\n        rkf = KFold(n_splits=n_splits, shuffle=True)\n        X_values = X.values\n        y_values = y.values\n        y_pred = np.zeros_like(y_values)\n        y_pred_train = np.zeros_like(y_values)\n        for train_index, test_index in rkf.split(X_values):\n            X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n            y_A, y_B = y_values[train_index], y_values[test_index]\n            model.fit(X_A, y_A, eval_set=[(X_B, y_B)], verbose = False)\n            y_pred[test_index] += model.predict(X_B)\n                      \n            \n        #score_train = roc_auc_score(y_train, y_pred_train)\n        score_test = mean_squared_error(y_train, y_pred) \n        #overfit = score_train-score_test\n        #temp_out.append(score_test-cv_regularizer*overfit)\n        temp_out.append(score_test)\n\n    return (np.mean(temp_out))\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=30)\n\nprint('Total time for hypermarameter optimization ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\n\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_hyperpars = starting_hyperparameters\n\noptuna_xgb = XGBClassifier(**optuna_hyperpars, seed=8)\noptuna_xgb.fit(X_train, y_train)\nprint('Optuna XGB train:', \n      mean_squared_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:28:56.737269Z","iopub.execute_input":"2022-08-24T22:28:56.737762Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2022-08-24 22:28:56,748]\u001b[0m A new study created in memory with name: no-name-285cbf6d-0f8d-4eea-8d10-5f722559fd9c\u001b[0m\n\u001b[32m[I 2022-08-24 22:29:04,121]\u001b[0m Trial 0 finished with value: 247.11419125260838 and parameters: {'n_estimators': 376, 'max_depth': 3, 'learning_rate': 0.05467772377871922, 'colsample_bytree': 0.1254964116210025, 'subsample': 0.7940295304746346, 'alpha': 4.3493039375588145, 'lambda': 104.21758728378975, 'gamma': 2.772044762209101e-10, 'min_child_weight': 1.541228073605977}. Best is trial 0 with value: 247.11419125260838.\u001b[0m\n\u001b[32m[I 2022-08-24 22:29:12,127]\u001b[0m Trial 1 finished with value: 249.9759969349031 and parameters: {'n_estimators': 205, 'max_depth': 6, 'learning_rate': 0.145078487872431, 'colsample_bytree': 0.1627912461620199, 'subsample': 0.9235419407693417, 'alpha': 1.882724582431167, 'lambda': 0.13531543105955623, 'gamma': 2.5421644364130322e-08, 'min_child_weight': 0.13120192584847673}. Best is trial 1 with value: 249.9759969349031.\u001b[0m\n\u001b[32m[I 2022-08-24 22:29:20,178]\u001b[0m Trial 2 finished with value: 247.15199593616364 and parameters: {'n_estimators': 492, 'max_depth': 2, 'learning_rate': 0.05635441924938534, 'colsample_bytree': 0.7598598882386954, 'subsample': 0.6771693129262778, 'alpha': 0.8915985787237726, 'lambda': 52.3315363356773, 'gamma': 0.27891414280145915, 'min_child_weight': 4.71168980420225}. Best is trial 1 with value: 249.9759969349031.\u001b[0m\n\u001b[32m[I 2022-08-24 22:29:25,121]\u001b[0m Trial 3 finished with value: 247.33675166755162 and parameters: {'n_estimators': 160, 'max_depth': 2, 'learning_rate': 0.11271676672756105, 'colsample_bytree': 0.6840346659717227, 'subsample': 0.8978558401048258, 'alpha': 0.18465624916137885, 'lambda': 1.0359061420086013, 'gamma': 1.6736025762224251, 'min_child_weight': 1.3856493098804465}. Best is trial 1 with value: 249.9759969349031.\u001b[0m\n\u001b[32m[I 2022-08-24 22:29:29,657]\u001b[0m Trial 4 finished with value: 247.5367914776804 and parameters: {'n_estimators': 142, 'max_depth': 2, 'learning_rate': 0.14159115898974828, 'colsample_bytree': 0.13065066614860069, 'subsample': 0.5903342085154917, 'alpha': 2.5399847984587067, 'lambda': 75.81262908073508, 'gamma': 7.517143842970966e-09, 'min_child_weight': 0.17339600914923212}. Best is trial 1 with value: 249.9759969349031.\u001b[0m\n\u001b[32m[I 2022-08-24 22:29:38,042]\u001b[0m Trial 5 finished with value: 247.10019415151518 and parameters: {'n_estimators': 295, 'max_depth': 5, 'learning_rate': 0.020679242021354103, 'colsample_bytree': 0.19838902761646737, 'subsample': 0.7353599172442263, 'alpha': 1.3006003811530586, 'lambda': 0.4869659401899956, 'gamma': 0.012084887637974488, 'min_child_weight': 3.9674895305568554}. Best is trial 1 with value: 249.9759969349031.\u001b[0m\n\u001b[32m[I 2022-08-24 22:30:37,425]\u001b[0m Trial 6 finished with value: 250.50118080001445 and parameters: {'n_estimators': 474, 'max_depth': 10, 'learning_rate': 0.08150755624798965, 'colsample_bytree': 0.1454543132731975, 'subsample': 0.9193627204244206, 'alpha': 4.5064987824498814, 'lambda': 52.70531081120267, 'gamma': 6.391848601170306e-10, 'min_child_weight': 1.4974987046689845}. Best is trial 6 with value: 250.50118080001445.\u001b[0m\n\u001b[32m[I 2022-08-24 22:30:47,170]\u001b[0m Trial 7 finished with value: 250.9034196783284 and parameters: {'n_estimators': 262, 'max_depth': 6, 'learning_rate': 0.1649481627439341, 'colsample_bytree': 0.4248927891421421, 'subsample': 0.8850842725453899, 'alpha': 0.6140414047855883, 'lambda': 11.264403529110622, 'gamma': 5.6546128381956626e-05, 'min_child_weight': 5.81426450860857}. Best is trial 7 with value: 250.9034196783284.\u001b[0m\n\u001b[32m[I 2022-08-24 22:30:57,499]\u001b[0m Trial 8 finished with value: 251.68611520624827 and parameters: {'n_estimators': 193, 'max_depth': 7, 'learning_rate': 0.1512324355582703, 'colsample_bytree': 0.5957532604144113, 'subsample': 0.9263178196933757, 'alpha': 8.038647112706535, 'lambda': 3.6102129464337285, 'gamma': 0.02810422225804383, 'min_child_weight': 0.13146695954091622}. Best is trial 8 with value: 251.68611520624827.\u001b[0m\n\u001b[32m[I 2022-08-24 22:31:06,317]\u001b[0m Trial 9 finished with value: 249.05576285744235 and parameters: {'n_estimators': 402, 'max_depth': 4, 'learning_rate': 0.14643507331241162, 'colsample_bytree': 0.872657793245661, 'subsample': 0.7205363423137455, 'alpha': 0.376557048116327, 'lambda': 6.182237687945745, 'gamma': 0.0002495953401891799, 'min_child_weight': 3.9408379458356904}. Best is trial 8 with value: 251.68611520624827.\u001b[0m\n\u001b[32m[I 2022-08-24 22:31:18,304]\u001b[0m Trial 10 finished with value: 266.41858053788707 and parameters: {'n_estimators': 102, 'max_depth': 9, 'learning_rate': 0.1955369157079838, 'colsample_bytree': 0.4894449806075203, 'subsample': 0.5062868937827522, 'alpha': 9.029159858535458, 'lambda': 1.14949806156654, 'gamma': 8.882880528144945e-07, 'min_child_weight': 0.3581767305462555}. Best is trial 10 with value: 266.41858053788707.\u001b[0m\n\u001b[32m[I 2022-08-24 22:31:30,433]\u001b[0m Trial 11 finished with value: 265.46534968844895 and parameters: {'n_estimators': 106, 'max_depth': 9, 'learning_rate': 0.19904148384202067, 'colsample_bytree': 0.49495678549318994, 'subsample': 0.5178654875512043, 'alpha': 9.859108875676363, 'lambda': 1.579304784859465, 'gamma': 3.6573158943495684e-07, 'min_child_weight': 0.36973451291907306}. Best is trial 10 with value: 266.41858053788707.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate performance of XGB models:\n\nprint('XGB test:', mean_squared_error(y_test, xgb.predict(X_test)), r2_score(y_test, xgb.predict(X_test)))\nprint('XGB GS test:', mean_squared_error(y_test, xgbm.predict(X_test)), r2_score(y_test, xgbm.predict(X_test)))\nprint('Optuna XGB test:', mean_squared_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_size = 0.1\n# df.reset_index(inplace=True, drop=True)\n# #random.seed(2)\n# test_index = random.sample(list(df.index), int(test_size*df.shape[0]))\n# train = df.iloc[list(set(df.index)-set(test_index))]\n# test = df.iloc[test_index]\n# train.reset_index(drop=True, inplace=True)\n# test.reset_index(drop=True, inplace=True)\n# display(train.shape, test.shape, train.head(3), test.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-08-24T22:10:56.715852Z","iopub.status.idle":"2022-08-24T22:10:56.716304Z","shell.execute_reply.started":"2022-08-24T22:10:56.716076Z","shell.execute_reply":"2022-08-24T22:10:56.716096Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
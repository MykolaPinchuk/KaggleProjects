{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Load libraries and data.\n2. pEDA. Look at feature distribution, fix them if they do not look right.\n3. Train-test split. Most likely couple years into test set. 2015-2018?. Impute missing values.\n4. Transform numerical features, add ohe for inds.\n5. Fit classic models: ols as a baseline, then xgb.\n6. Fir DL.\n\n","metadata":{}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle, shap\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, KFold, PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\npd.set_option('display.max_columns', 110)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:34:36.360821Z","iopub.execute_input":"2022-09-06T18:34:36.361873Z","iopub.status.idle":"2022-09-06T18:34:36.371900Z","shell.execute_reply.started":"2022-09-06T18:34:36.361824Z","shell.execute_reply":"2022-09-06T18:34:36.370665Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:34:36.373621Z","iopub.execute_input":"2022-09-06T18:34:36.374199Z","iopub.status.idle":"2022-09-06T18:34:36.386841Z","shell.execute_reply.started":"2022-09-06T18:34:36.374162Z","shell.execute_reply":"2022-09-06T18:34:36.385825Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:34:36.390218Z","iopub.execute_input":"2022-09-06T18:34:36.390510Z","iopub.status.idle":"2022-09-06T18:34:36.402387Z","shell.execute_reply.started":"2022-09-06T18:34:36.390485Z","shell.execute_reply":"2022-09-06T18:34:36.401321Z"},"trusted":true},"execution_count":51,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"#min_prd_list = range(100, 676, 25)\nmin_prd_list = [150, 250, 350, 450, 550, 650]\n#min_prd = min_prd_list[0]\nwindows_width = 3*12\ncv_regularizer=0.2\noptuna_trials = 10\ntime0 = time.time()\n\nresults = pd.DataFrame(columns = ['min_prd', 'xgbf_train', 'xgbf_val', 'xgbf_test', \n                                  'xgbgs_train', 'xgbgs_val', 'xgbgs_test', \n                                  'xgbo_train', 'xgbo_val', 'xgbo_test'])\nresults.min_prd = min_prd_list\n\nfor min_prd in min_prd_list:\n\n    with open('../input/mleap-46-preprocessed/MLEAP_46_v6.pkl', 'rb') as pickled_one:\n        df = pickle.load(pickled_one)\n    df = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+3))]\n    df_cnt = df.count()\n    empty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\n    df.drop(columns=empty_cols, inplace=True)\n    #display(df.shape, df.head(), df.year.describe(), df.count())\n\n    features_miss_dummies = ['amhd', 'BAspr']\n    for col in features_miss_dummies:\n        if col in df.columns:\n            df[col+'_miss'] = df[col].isnull().astype(int)\n\n    temp_cols = ['PERMNO', 'year', 'prd']\n    df.reset_index(inplace=True, drop=True)\n    X = df.copy()\n    y = X.pop('RET')\n\n    train_indx = X.prd<(min_prd+windows_width-1)\n    val_indx = X.prd==(min_prd+windows_width-1)\n    val_indx_extra = X.prd==(min_prd+windows_width+1)\n    test_indx = X.prd==(min_prd+windows_width)\n\n    X_train = X[train_indx]\n    X_val = X[val_indx]\n    X_val_extra = X[val_indx_extra]\n    X_test = X[test_indx]\n    y_train = y[train_indx]\n    y_val = y[val_indx]\n    y_val_extra = y[val_indx_extra]\n    y_test = y[test_indx]\n\n    #display(X_train.head(3), X_train.tail(3), y_train.head(3), y_train.tail(3))\n    display(X_train.shape, X_val.shape, X_test.shape, X_train.prd.describe(), X_val.prd.describe(), X_test.prd.describe())\n\n    X_train.drop(columns=temp_cols, inplace=True)\n    X_val.drop(columns=temp_cols, inplace=True)\n    X_val_extra.drop(columns=temp_cols, inplace=True)\n    X_test.drop(columns=temp_cols, inplace=True)\n\n    #display(X_train.tail())\n    col_cat = ['ind']\n    col_num = [x for x in X_train.columns if x not in col_cat]\n    for col in col_num:\n        X_train[col] = X_train[col].fillna(X_train[col].median())\n        X_val[col] = X_val[col].fillna(X_train[col].median())\n        X_val_extra[col] = X_val_extra[col].fillna(X_train[col].median())\n        X_test[col] = X_test[col].fillna(X_train[col].median())\n    for col in col_cat:\n        X_train[col] = X_train[col].fillna(value=-1000)\n        X_val[col] = X_val[col].fillna(value=-1000)\n        X_val_extra[col] = X_val_extra[col].fillna(value=-1000)\n        X_test[col] = X_test[col].fillna(value=-1000)\n\n    #display(X_train.tail())\n    feature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                            (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                            remainder=\"passthrough\")\n\n    print('Number of features before transformation: ', X_train.shape)\n    train_index, val_index, val_index_extra, test_index = X_train.index, X_val.index, X_val_extra.index, X_test.index\n    X_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\n    X_val = pd.DataFrame(feature_transformer.transform(X_val), columns=feature_transformer.get_feature_names_out())\n    X_val_extra = pd.DataFrame(feature_transformer.transform(X_val_extra), columns=feature_transformer.get_feature_names_out())\n    X_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\n    print('time to do feature proprocessing: ')\n    print('Number of features after transformation: ', X_train.shape, X_val.shape, X_val_extra.shape, X_test.shape)\n    X_train.index = train_index\n    X_val.index = val_index\n    X_val_extra.index = val_index_extra\n    X_test.index = test_index\n    #display(X_train.tail())\n\n    X = pd.concat([X_train, X_val])\n    y = pd.concat([y_train, y_val])\n    #display(X,y)\n\n    X_ = pd.concat([X_train, X_val, X_val_extra])\n    y_ = pd.concat([y_train, y_val, y_val_extra])\n    #display(X,y, X_,y_)\n\n    print('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n    print('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\n    xgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=400, max_depth=4, eta=0.02, colsample_bytree=0.4, subsample=0.6)\n    xgb1.fit(X_train, y_train)\n    print('fixed XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\n    print('XGB val:', mean_absolute_error(y_val, xgb1.predict(X_val)), r2_score(y_val, xgb1.predict(X_val)))\n    print('XGB val extra:', mean_absolute_error(y_val_extra, xgb1.predict(X_val_extra)), r2_score(y_val_extra, xgb1.predict(X_val_extra)))\n    print('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbf_train':'xgbf_test'] = \\\n    [r2_score(y_train, xgb1.predict(X_train)), \n    r2_score(y_val, xgb1.predict(X_val)),\n    r2_score(y_test, xgb1.predict(X_test))]\n\n    time1 = time.time()\n\n    # Create a list where train data indices are -1 and validation data indices are 0\n    split_index = [-1 if x in X_train.index else 0 for x in X.index]\n    pds = PredefinedSplit(test_fold = split_index)\n\n    xgb = XGBRegressor(tree_method = 'gpu_hist')\n    param_grid = {'n_estimators':[400, 600, 800], 'max_depth':[2,3,4,5], 'eta':[0.006, 0.012, 0.02], \n                  'subsample':[0.6], 'colsample_bytree':[0.6]}\n    xgbgs = GridSearchCV(estimator = xgb, cv=pds, param_grid=param_grid)\n\n    # Fit with all data\n    xgbgs.fit(X_, y_)\n\n    print('gs XGB', xgbgs.best_params_, xgbgs.best_score_, time.time()-time1)\n    print('XGB train:', mean_absolute_error(y_train, xgbgs.predict(X_train)), r2_score(y_train, xgbgs.predict(X_train)))\n    print('XGB validation:', mean_absolute_error(y_val, xgbgs.predict(X_val)), r2_score(y_val, xgbgs.predict(X_val)))\n    print('XGB validation extra:', mean_absolute_error(y_val_extra, xgbgs.predict(X_val_extra)), r2_score(y_val_extra, xgbgs.predict(X_val_extra)))\n    print('XGB test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_score(y_test, xgbgs.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbgs_train':'xgbgs_test'] = \\\n    [r2_score(y_train, xgbgs.predict(X_train)), \n    r2_score(y_val, xgbgs.predict(X_val)),\n    r2_score(y_test, xgbgs.predict(X_test))]\n\n    time1 = time.time()\n    def objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n        params = {\n        \"tree_method\": 'gpu_hist',\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 1500),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.0005, 0.03),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.05, 0.95),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 0.95),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 50.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 500.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 100.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 100)    }\n\n        model = XGBRegressor(**params, njobs=-1)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose = False)\n\n        score_train = r2_score(y_train, model.predict(X_train))\n        score_val = r2_score(y_val, model.predict(X_val))\n        score_val_extra = r2_score(y_val_extra, model.predict(X_val_extra)) \n        score_val = (score_val+score_val_extra)/2\n        overfit = np.abs(score_train-score_val)\n\n        return score_val-cv_regularizer*overfit\n\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=optuna_trials)\n    print('Total time for hypermarameter optimization ', time.time()-time1)\n    hp = study.best_params\n    for key, value in hp.items():\n        print(f\"{key:>20s} : {value}\")\n    print(f\"{'best objective value':>20s} : {study.best_value}\")\n    optuna_hyperpars = study.best_params\n    optuna_hyperpars['tree_method']='gpu_hist'\n    optuna_xgb = XGBRegressor(**optuna_hyperpars)\n    optuna_xgb.fit(X, y)\n    print('Optuna XGB train: \\n', \n          mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), '\\nvalidation \\n',\n          mean_absolute_error(y_val, optuna_xgb.predict(X_val)), r2_score(y_val, optuna_xgb.predict(X_val)),\n          mean_absolute_error(y_val_extra, optuna_xgb.predict(X_val_extra)), r2_score(y_val_extra, optuna_xgb.predict(X_val_extra)), '\\ntest \\n',\n          mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbo_train':'xgbo_test'] = \\\n    [r2_score(y_train, optuna_xgb.predict(X_train)), \n    r2_score(y_val, optuna_xgb.predict(X_val)),\n    r2_score(y_test, optuna_xgb.predict(X_test))]\n\n    display(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:34:36.459090Z","iopub.execute_input":"2022-09-06T18:34:36.459360Z","iopub.status.idle":"2022-09-06T18:43:26.568614Z","shell.execute_reply.started":"2022-09-06T18:34:36.459334Z","shell.execute_reply":"2022-09-06T18:43:26.567782Z"},"trusted":true},"execution_count":52,"outputs":[{"output_type":"display_data","data":{"text/plain":"(46500, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(1485, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(1485, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    46500.000000\nmean       167.403097\nstd         10.350122\nmin        149.000000\n25%        159.000000\n50%        168.000000\n75%        176.000000\nmax        184.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    1485.0\nmean      185.0\nstd         0.0\nmin       185.0\n25%       185.0\n50%       185.0\n75%       185.0\nmax       185.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    1485.0\nmean      186.0\nstd         0.0\nmin       186.0\n25%       186.0\n50%       186.0\n75%       186.0\nmax       186.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (46500, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (46500, 84) (1485, 84) (1474, 84) (1485, 84)\nmae of a constant model 7.813703742906884\nR2 of a constant model 0.0\nfixed XGB train: 7.256378832680428 0.07728994610231632\nXGB val: 9.347012538938365 0.0010293202131163026\nXGB val extra: 9.08759853046273 0.034361489997165595\nXGB test: 9.20902787973362 0.024901688053023063\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.006, 'max_depth': 3, 'n_estimators': 800, 'subsample': 0.6} 0.00891335264587767 49.27702236175537\nXGB train: 7.387385993934151 0.037630376633517315\nXGB validation: 9.21058173710746 0.030909114475185517\nXGB validation extra: 8.990940878671502 0.06624695387097657\nXGB test: 9.173434447489123 0.03268227400609813\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 18:35:32,951]\u001b[0m A new study created in memory with name: no-name-6215f4e2-f3b7-4be6-b57d-a67ab80d2b54\u001b[0m\n\u001b[32m[I 2022-09-06 18:35:33,491]\u001b[0m Trial 0 finished with value: 0.01006686595984243 and parameters: {'n_estimators': 988, 'max_depth': 2, 'learning_rate': 0.021561840089205517, 'colsample_bytree': 0.8897186655965688, 'subsample': 0.6256626885792115, 'alpha': 38.50804931486862, 'lambda': 16.03571943228863, 'gamma': 6.749467816161943e-07, 'min_child_weight': 1.28569937815238}. Best is trial 0 with value: 0.01006686595984243.\u001b[0m\n\u001b[32m[I 2022-09-06 18:35:34,673]\u001b[0m Trial 1 finished with value: 0.012454044740311244 and parameters: {'n_estimators': 798, 'max_depth': 2, 'learning_rate': 0.009094895277489894, 'colsample_bytree': 0.10669146008829394, 'subsample': 0.7857177858312014, 'alpha': 0.10404082788828217, 'lambda': 39.7216170296757, 'gamma': 4.603886053917563e-09, 'min_child_weight': 8.337702998587575}. Best is trial 1 with value: 0.012454044740311244.\u001b[0m\n\u001b[32m[I 2022-09-06 18:35:35,464]\u001b[0m Trial 2 finished with value: 0.012881938903927881 and parameters: {'n_estimators': 823, 'max_depth': 3, 'learning_rate': 0.023429490217165942, 'colsample_bytree': 0.265849941632552, 'subsample': 0.46791380675979255, 'alpha': 1.1031348817255437, 'lambda': 301.34592716602685, 'gamma': 1.078177843629226e-06, 'min_child_weight': 5.556947926036546}. Best is trial 2 with value: 0.012881938903927881.\u001b[0m\n\u001b[32m[I 2022-09-06 18:35:36,699]\u001b[0m Trial 3 finished with value: 0.010205284279694516 and parameters: {'n_estimators': 771, 'max_depth': 2, 'learning_rate': 0.0034448894607812283, 'colsample_bytree': 0.14768288187677703, 'subsample': 0.103979429915255, 'alpha': 6.715926878816639, 'lambda': 0.2284067777055398, 'gamma': 0.14309903530435064, 'min_child_weight': 12.615232793687426}. Best is trial 2 with value: 0.012881938903927881.\u001b[0m\n\u001b[32m[I 2022-09-06 18:35:38,668]\u001b[0m Trial 4 finished with value: 0.006212468303249974 and parameters: {'n_estimators': 705, 'max_depth': 6, 'learning_rate': 0.003196708395716133, 'colsample_bytree': 0.13674177095547096, 'subsample': 0.18502754108468333, 'alpha': 14.84658841032, 'lambda': 8.879338372289704, 'gamma': 9.024718418889227e-05, 'min_child_weight': 15.821795117975812}. Best is trial 2 with value: 0.012881938903927881.\u001b[0m\n\u001b[32m[I 2022-09-06 18:35:40,750]\u001b[0m Trial 5 finished with value: 0.011455396818175357 and parameters: {'n_estimators': 713, 'max_depth': 4, 'learning_rate': 0.005312025466284403, 'colsample_bytree': 0.8003102570533425, 'subsample': 0.647133386651136, 'alpha': 1.0957380724907233, 'lambda': 3.4565002709578456, 'gamma': 6.25079637910774e-05, 'min_child_weight': 30.39086202096265}. Best is trial 2 with value: 0.012881938903927881.\u001b[0m\n\u001b[32m[I 2022-09-06 18:35:41,456]\u001b[0m Trial 6 finished with value: 0.013847684093109191 and parameters: {'n_estimators': 828, 'max_depth': 3, 'learning_rate': 0.027182370288777066, 'colsample_bytree': 0.4352791976102105, 'subsample': 0.28908928150103885, 'alpha': 22.437323635007836, 'lambda': 33.36064774048631, 'gamma': 8.937351216694563e-09, 'min_child_weight': 0.5738790386616692}. Best is trial 6 with value: 0.013847684093109191.\u001b[0m\n\u001b[32m[I 2022-09-06 18:35:42,378]\u001b[0m Trial 7 finished with value: 0.007238047370310019 and parameters: {'n_estimators': 742, 'max_depth': 6, 'learning_rate': 0.014930263442825975, 'colsample_bytree': 0.06387244025844666, 'subsample': 0.6945643005205914, 'alpha': 0.100262862831115, 'lambda': 0.13014654361203473, 'gamma': 1.0909020342796904e-07, 'min_child_weight': 1.3601524504456741}. Best is trial 6 with value: 0.013847684093109191.\u001b[0m\n\u001b[32m[I 2022-09-06 18:35:43,048]\u001b[0m Trial 8 finished with value: 0.012629978596768666 and parameters: {'n_estimators': 796, 'max_depth': 2, 'learning_rate': 0.028436580361844634, 'colsample_bytree': 0.9434534419256719, 'subsample': 0.7358323287755278, 'alpha': 0.11785289278975951, 'lambda': 2.666539865394718, 'gamma': 2.7544718387479996, 'min_child_weight': 2.6844772807879926}. Best is trial 6 with value: 0.013847684093109191.\u001b[0m\n\u001b[32m[I 2022-09-06 18:35:44,144]\u001b[0m Trial 9 finished with value: 0.012887145727347771 and parameters: {'n_estimators': 963, 'max_depth': 2, 'learning_rate': 0.013352077864652393, 'colsample_bytree': 0.5223842239184531, 'subsample': 0.48927562335285246, 'alpha': 5.041461603144399, 'lambda': 92.5103948211524, 'gamma': 0.0020964182574801705, 'min_child_weight': 0.677200565729954}. Best is trial 6 with value: 0.013847684093109191.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  11.195975542068481\n        n_estimators : 828\n           max_depth : 3\n       learning_rate : 0.027182370288777066\n    colsample_bytree : 0.4352791976102105\n           subsample : 0.28908928150103885\n               alpha : 22.437323635007836\n              lambda : 33.36064774048631\n               gamma : 8.937351216694563e-09\n    min_child_weight : 0.5738790386616692\nbest objective value : 0.013847684093109191\nOptuna XGB train: \n 7.252646673801833 0.07449587202844354 \nvalidation \n 9.042556078768335 0.06752332659493365 9.113891171501365 0.02658884596140043 \ntest \n 9.202117210219814 0.018465352145230707\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150    0.07729  0.001029  0.024902     0.03763  0.030909   0.032682   \n1      250        NaN       NaN       NaN         NaN       NaN        NaN   \n2      350        NaN       NaN       NaN         NaN       NaN        NaN   \n3      450        NaN       NaN       NaN         NaN       NaN        NaN   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.074496  0.067523  0.018465  \n1        NaN       NaN       NaN  \n2        NaN       NaN       NaN  \n3        NaN       NaN       NaN  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.07729</td>\n      <td>0.001029</td>\n      <td>0.024902</td>\n      <td>0.03763</td>\n      <td>0.030909</td>\n      <td>0.032682</td>\n      <td>0.074496</td>\n      <td>0.067523</td>\n      <td>0.018465</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(79162, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2145, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2137, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    79162.000000\nmean       266.337409\nstd         10.396123\nmin        249.000000\n25%        257.000000\n50%        266.000000\n75%        275.000000\nmax        284.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2145.0\nmean      285.0\nstd         0.0\nmin       285.0\n25%       285.0\n50%       285.0\n75%       285.0\nmax       285.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2137.0\nmean      286.0\nstd         0.0\nmin       286.0\n25%       286.0\n50%       286.0\n75%       286.0\nmax       286.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (79162, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (79162, 85) (2145, 85) (2123, 85) (2137, 85)\nmae of a constant model 8.353804972264347\nR2 of a constant model 0.0\nfixed XGB train: 8.067013881780515 0.07312293387862001\nXGB val: 7.358684552817834 0.004711616198505686\nXGB val extra: 8.416244834285159 -0.008118187484788653\nXGB test: 7.4821132582139995 -0.04537456258193595\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.012, 'max_depth': 5, 'n_estimators': 800, 'subsample': 0.6} 0.010502350402396887 58.62754559516907\nXGB train: 7.944840773533399 0.10835807614667792\nXGB validation: 7.0679282785943185 0.09794943516405952\nXGB validation extra: 8.159837855676226 0.07015683057235556\nXGB test: 7.4075493289188605 -0.01988332806426185\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 18:36:52,183]\u001b[0m A new study created in memory with name: no-name-8b8f3c5c-60ce-4170-b037-eb521ab878b4\u001b[0m\n\u001b[32m[I 2022-09-06 18:36:52,558]\u001b[0m Trial 0 finished with value: -0.0019621001485304657 and parameters: {'n_estimators': 732, 'max_depth': 2, 'learning_rate': 0.028293190153945803, 'colsample_bytree': 0.5086805637572053, 'subsample': 0.2460708435194757, 'alpha': 0.3168458982804165, 'lambda': 0.14009875005071343, 'gamma': 7.588656552672495e-05, 'min_child_weight': 73.8683090399674}. Best is trial 0 with value: -0.0019621001485304657.\u001b[0m\n\u001b[32m[I 2022-09-06 18:36:52,930]\u001b[0m Trial 1 finished with value: -0.0015446740379178702 and parameters: {'n_estimators': 877, 'max_depth': 2, 'learning_rate': 0.009910194691477366, 'colsample_bytree': 0.5749645072010726, 'subsample': 0.6148345456481085, 'alpha': 49.476484578807465, 'lambda': 1.7377822905440228, 'gamma': 3.1719636755837354e-05, 'min_child_weight': 46.372014818499125}. Best is trial 1 with value: -0.0015446740379178702.\u001b[0m\n\u001b[32m[I 2022-09-06 18:36:53,335]\u001b[0m Trial 2 finished with value: -0.0016676420807020475 and parameters: {'n_estimators': 798, 'max_depth': 3, 'learning_rate': 0.018821615328223573, 'colsample_bytree': 0.7524751468362431, 'subsample': 0.5423124792810122, 'alpha': 0.4282812308488683, 'lambda': 5.516391600326742, 'gamma': 1.559576047237927e-10, 'min_child_weight': 6.602190911750615}. Best is trial 1 with value: -0.0015446740379178702.\u001b[0m\n\u001b[32m[I 2022-09-06 18:36:53,750]\u001b[0m Trial 3 finished with value: -0.002034327220269816 and parameters: {'n_estimators': 951, 'max_depth': 4, 'learning_rate': 0.02946502108284701, 'colsample_bytree': 0.36577671437219383, 'subsample': 0.5528489809684367, 'alpha': 0.5326196202196946, 'lambda': 82.62428503429196, 'gamma': 1.5535799699427293e-09, 'min_child_weight': 3.8268032464982205}. Best is trial 1 with value: -0.0015446740379178702.\u001b[0m\n\u001b[32m[I 2022-09-06 18:36:54,116]\u001b[0m Trial 4 finished with value: -0.0017090491119488237 and parameters: {'n_estimators': 881, 'max_depth': 2, 'learning_rate': 0.020390782278462347, 'colsample_bytree': 0.50464252255673, 'subsample': 0.36551545714585254, 'alpha': 6.199567433855803, 'lambda': 40.38540883909508, 'gamma': 0.00022654503655631652, 'min_child_weight': 0.8870905912810342}. Best is trial 1 with value: -0.0015446740379178702.\u001b[0m\n\u001b[32m[I 2022-09-06 18:36:54,537]\u001b[0m Trial 5 finished with value: -0.0027961594492779797 and parameters: {'n_estimators': 854, 'max_depth': 3, 'learning_rate': 0.019563390081655755, 'colsample_bytree': 0.5578893592800681, 'subsample': 0.1589468190008782, 'alpha': 0.8614271589422576, 'lambda': 0.9793590099855736, 'gamma': 11.565908057017927, 'min_child_weight': 10.709185685318637}. Best is trial 1 with value: -0.0015446740379178702.\u001b[0m\n\u001b[32m[I 2022-09-06 18:36:55,269]\u001b[0m Trial 6 finished with value: -0.0015549161972905124 and parameters: {'n_estimators': 682, 'max_depth': 6, 'learning_rate': 0.00388633647573017, 'colsample_bytree': 0.29272094962065665, 'subsample': 0.10035516875818522, 'alpha': 0.5171689003065778, 'lambda': 7.0494964745069195, 'gamma': 0.00034092353210639267, 'min_child_weight': 51.03743117864805}. Best is trial 1 with value: -0.0015446740379178702.\u001b[0m\n\u001b[32m[I 2022-09-06 18:36:56,243]\u001b[0m Trial 7 finished with value: -0.0055247114422789295 and parameters: {'n_estimators': 561, 'max_depth': 5, 'learning_rate': 0.008729391268135387, 'colsample_bytree': 0.3432123704586462, 'subsample': 0.8319264244104333, 'alpha': 0.7051624734312749, 'lambda': 2.516557544563772, 'gamma': 2.7889064585526266e-09, 'min_child_weight': 0.35031771135563966}. Best is trial 1 with value: -0.0015446740379178702.\u001b[0m\n\u001b[32m[I 2022-09-06 18:36:58,034]\u001b[0m Trial 8 finished with value: -0.01830849000884549 and parameters: {'n_estimators': 690, 'max_depth': 5, 'learning_rate': 0.027627923161870262, 'colsample_bytree': 0.7517518918508297, 'subsample': 0.5507635162475665, 'alpha': 0.18111282672788795, 'lambda': 1.1181587815936354, 'gamma': 0.00013724364536953895, 'min_child_weight': 1.1332089659481923}. Best is trial 1 with value: -0.0015446740379178702.\u001b[0m\n\u001b[32m[I 2022-09-06 18:36:58,459]\u001b[0m Trial 9 finished with value: -0.0013574295757464274 and parameters: {'n_estimators': 921, 'max_depth': 4, 'learning_rate': 0.02247981696470075, 'colsample_bytree': 0.6250797890569504, 'subsample': 0.75907814526054, 'alpha': 12.459675625043435, 'lambda': 0.1893145949903012, 'gamma': 0.0015152020500387375, 'min_child_weight': 73.91839214672494}. Best is trial 9 with value: -0.0013574295757464274.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  6.277012348175049\n        n_estimators : 921\n           max_depth : 4\n       learning_rate : 0.02247981696470075\n    colsample_bytree : 0.6250797890569504\n           subsample : 0.75907814526054\n               alpha : 12.459675625043435\n              lambda : 0.1893145949903012\n               gamma : 0.0015152020500387375\n    min_child_weight : 73.91839214672494\nbest objective value : -0.0013574295757464274\nOptuna XGB train: \n 7.957734870500398 0.09916116130818564 \nvalidation \n 7.122513664048829 0.07798090879852093 8.438627727794348 -0.0074686614379917415 \ntest \n 7.497542067668344 -0.04083041818491773\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150    0.07729  0.001029  0.024902     0.03763  0.030909   0.032682   \n1      250   0.073123  0.004712 -0.045375    0.108358  0.097949  -0.019883   \n2      350        NaN       NaN       NaN         NaN       NaN        NaN   \n3      450        NaN       NaN       NaN         NaN       NaN        NaN   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.074496  0.067523  0.018465  \n1   0.099161  0.077981  -0.04083  \n2        NaN       NaN       NaN  \n3        NaN       NaN       NaN  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.07729</td>\n      <td>0.001029</td>\n      <td>0.024902</td>\n      <td>0.03763</td>\n      <td>0.030909</td>\n      <td>0.032682</td>\n      <td>0.074496</td>\n      <td>0.067523</td>\n      <td>0.018465</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.073123</td>\n      <td>0.004712</td>\n      <td>-0.045375</td>\n      <td>0.108358</td>\n      <td>0.097949</td>\n      <td>-0.019883</td>\n      <td>0.099161</td>\n      <td>0.077981</td>\n      <td>-0.04083</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(86525, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2346, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2318, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    86525.000000\nmean       366.510604\nstd         10.388374\nmin        349.000000\n25%        357.000000\n50%        366.000000\n75%        376.000000\nmax        384.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2346.0\nmean      385.0\nstd         0.0\nmin       385.0\n25%       385.0\n50%       385.0\n75%       385.0\nmax       385.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2318.0\nmean      386.0\nstd         0.0\nmin       386.0\n25%       386.0\n50%       386.0\n75%       386.0\nmax       386.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (86525, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (86525, 86) (2346, 86) (2610, 86) (2318, 86)\nmae of a constant model 9.253398697737389\nR2 of a constant model 0.0\nfixed XGB train: 8.65947560566903 0.0833912118519563\nXGB val: 10.5257146862587 0.05107780157395547\nXGB val extra: 9.548262729976102 0.019625930955531112\nXGB test: 8.956099591523875 0.020909127377985448\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.012, 'max_depth': 5, 'n_estimators': 800, 'subsample': 0.6} 0.05258054464976114 61.48468255996704\nXGB train: 8.526913196396853 0.11795065714812925\nXGB validation: 10.20689445812439 0.11576717828449556\nXGB validation extra: 9.202852085746425 0.10932013752434888\nXGB test: 8.948867755996243 0.02025337345381617\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 18:38:13,205]\u001b[0m A new study created in memory with name: no-name-ea4badde-7d5d-4f84-9a25-8aee8e70c94c\u001b[0m\n\u001b[32m[I 2022-09-06 18:38:16,318]\u001b[0m Trial 0 finished with value: 0.022319677355437762 and parameters: {'n_estimators': 916, 'max_depth': 3, 'learning_rate': 0.0048792979317155965, 'colsample_bytree': 0.8531594756179868, 'subsample': 0.1661366564495132, 'alpha': 9.565537949264632, 'lambda': 123.61049683788235, 'gamma': 7.564190836624741e-06, 'min_child_weight': 4.805555111382459}. Best is trial 0 with value: 0.022319677355437762.\u001b[0m\n\u001b[32m[I 2022-09-06 18:38:18,583]\u001b[0m Trial 1 finished with value: 0.028425613130856342 and parameters: {'n_estimators': 766, 'max_depth': 4, 'learning_rate': 0.026309600011423864, 'colsample_bytree': 0.6028589922191593, 'subsample': 0.7254001598077506, 'alpha': 5.8845633088809315, 'lambda': 100.95678807710145, 'gamma': 5.301886064750247e-08, 'min_child_weight': 0.2739247711355722}. Best is trial 1 with value: 0.028425613130856342.\u001b[0m\n\u001b[32m[I 2022-09-06 18:38:22,404]\u001b[0m Trial 2 finished with value: 0.026283847547075134 and parameters: {'n_estimators': 676, 'max_depth': 5, 'learning_rate': 0.006188977676400731, 'colsample_bytree': 0.28875785906759865, 'subsample': 0.6646490171094528, 'alpha': 3.543942404605202, 'lambda': 253.2227898004321, 'gamma': 0.0003795980147753342, 'min_child_weight': 0.828238078310866}. Best is trial 1 with value: 0.028425613130856342.\u001b[0m\n\u001b[32m[I 2022-09-06 18:38:26,240]\u001b[0m Trial 3 finished with value: 0.02122849614219511 and parameters: {'n_estimators': 884, 'max_depth': 4, 'learning_rate': 0.004078521653531608, 'colsample_bytree': 0.5173785414213319, 'subsample': 0.7145631266534498, 'alpha': 1.0172454550963483, 'lambda': 1.9123121481863539, 'gamma': 5.816790963332407e-06, 'min_child_weight': 69.54277432917479}. Best is trial 1 with value: 0.028425613130856342.\u001b[0m\n\u001b[32m[I 2022-09-06 18:38:28,170]\u001b[0m Trial 4 finished with value: 0.029942060005702896 and parameters: {'n_estimators': 874, 'max_depth': 3, 'learning_rate': 0.017173488448552078, 'colsample_bytree': 0.1321973410185549, 'subsample': 0.3615871840407212, 'alpha': 0.44397591961640487, 'lambda': 1.5811416641827494, 'gamma': 2.467396354240828e-05, 'min_child_weight': 0.3488000620308633}. Best is trial 4 with value: 0.029942060005702896.\u001b[0m\n\u001b[32m[I 2022-09-06 18:38:31,339]\u001b[0m Trial 5 finished with value: 0.027387624575097823 and parameters: {'n_estimators': 588, 'max_depth': 5, 'learning_rate': 0.013012148494989658, 'colsample_bytree': 0.20497750777432638, 'subsample': 0.79431944603763, 'alpha': 29.03136291710802, 'lambda': 0.20142517232249368, 'gamma': 4.239670061402234e-05, 'min_child_weight': 0.9032033078083969}. Best is trial 4 with value: 0.029942060005702896.\u001b[0m\n\u001b[32m[I 2022-09-06 18:38:34,781]\u001b[0m Trial 6 finished with value: 0.014395904873331311 and parameters: {'n_estimators': 945, 'max_depth': 3, 'learning_rate': 0.0017776638230714854, 'colsample_bytree': 0.5938371332750231, 'subsample': 0.3150277306604592, 'alpha': 1.314178406695075, 'lambda': 389.46386306308574, 'gamma': 3.960789296282064e-05, 'min_child_weight': 6.3966772222932295}. Best is trial 4 with value: 0.029942060005702896.\u001b[0m\n\u001b[32m[I 2022-09-06 18:38:37,504]\u001b[0m Trial 7 finished with value: 0.015012109551245767 and parameters: {'n_estimators': 854, 'max_depth': 6, 'learning_rate': 0.023142089251737753, 'colsample_bytree': 0.6415105746412757, 'subsample': 0.7860150040782091, 'alpha': 0.42512263116071425, 'lambda': 0.16213298272744583, 'gamma': 1.0178457302699802, 'min_child_weight': 0.39142142706905475}. Best is trial 4 with value: 0.029942060005702896.\u001b[0m\n\u001b[32m[I 2022-09-06 18:38:42,248]\u001b[0m Trial 8 finished with value: 0.024543881575889867 and parameters: {'n_estimators': 765, 'max_depth': 5, 'learning_rate': 0.008333760113722517, 'colsample_bytree': 0.9454066234836858, 'subsample': 0.12226250389649565, 'alpha': 1.9902329154678835, 'lambda': 1.0692611242733454, 'gamma': 0.0007053803686758434, 'min_child_weight': 6.398872935949038}. Best is trial 4 with value: 0.029942060005702896.\u001b[0m\n\u001b[32m[I 2022-09-06 18:38:44,903]\u001b[0m Trial 9 finished with value: 0.029587400266979858 and parameters: {'n_estimators': 941, 'max_depth': 2, 'learning_rate': 0.02358777584532894, 'colsample_bytree': 0.4853041145022417, 'subsample': 0.8675217332271129, 'alpha': 11.714388943882142, 'lambda': 153.71930890704246, 'gamma': 0.02270765633783937, 'min_child_weight': 1.0744970165125494}. Best is trial 4 with value: 0.029942060005702896.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  31.701236724853516\n        n_estimators : 874\n           max_depth : 3\n       learning_rate : 0.017173488448552078\n    colsample_bytree : 0.1321973410185549\n           subsample : 0.3615871840407212\n               alpha : 0.44397591961640487\n              lambda : 1.5811416641827494\n               gamma : 2.467396354240828e-05\n    min_child_weight : 0.3488000620308633\nbest objective value : 0.029942060005702896\nOptuna XGB train: \n 8.690008280973299 0.07297795234127757 \nvalidation \n 10.380520063431495 0.07460905703298337 9.525602257937626 0.022041708926874404 \ntest \n 8.938925243941275 0.02433339638364984\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150    0.07729  0.001029  0.024902     0.03763  0.030909   0.032682   \n1      250   0.073123  0.004712 -0.045375    0.108358  0.097949  -0.019883   \n2      350   0.083391  0.051078  0.020909    0.117951  0.115767   0.020253   \n3      450        NaN       NaN       NaN         NaN       NaN        NaN   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.074496  0.067523  0.018465  \n1   0.099161  0.077981  -0.04083  \n2   0.072978  0.074609  0.024333  \n3        NaN       NaN       NaN  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.07729</td>\n      <td>0.001029</td>\n      <td>0.024902</td>\n      <td>0.03763</td>\n      <td>0.030909</td>\n      <td>0.032682</td>\n      <td>0.074496</td>\n      <td>0.067523</td>\n      <td>0.018465</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.073123</td>\n      <td>0.004712</td>\n      <td>-0.045375</td>\n      <td>0.108358</td>\n      <td>0.097949</td>\n      <td>-0.019883</td>\n      <td>0.099161</td>\n      <td>0.077981</td>\n      <td>-0.04083</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.083391</td>\n      <td>0.051078</td>\n      <td>0.020909</td>\n      <td>0.117951</td>\n      <td>0.115767</td>\n      <td>0.020253</td>\n      <td>0.072978</td>\n      <td>0.074609</td>\n      <td>0.024333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(99700, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2833, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2787, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    99700.000000\nmean       466.677944\nstd         10.329035\nmin        449.000000\n25%        458.000000\n50%        467.000000\n75%        476.000000\nmax        484.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2833.0\nmean      485.0\nstd         0.0\nmin       485.0\n25%       485.0\n50%       485.0\n75%       485.0\nmax       485.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2787.0\nmean      486.0\nstd         0.0\nmin       486.0\n25%       486.0\n50%       486.0\n75%       486.0\nmax       486.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (99700, 44)\ntime to do feature proprocessing: \nNumber of features after transformation:  (99700, 92) (2833, 92) (2727, 92) (2787, 92)\nmae of a constant model 10.365488651654134\nR2 of a constant model 0.0\nfixed XGB train: 9.749960424621484 0.04724754520163588\nXGB val: 14.146123615719672 0.008147098387071616\nXGB val extra: 12.779976997541198 0.007014293307955666\nXGB test: 13.521120225110478 0.01349017377695183\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.012, 'max_depth': 2, 'n_estimators': 800, 'subsample': 0.6} 0.013922885819506847 66.22337365150452\nXGB train: 9.846213048207966 0.02481834857320364\nXGB validation: 13.990381583433855 0.03135395477796832\nXGB validation extra: 12.739694568957077 0.017938380203997295\nXGB test: 13.508572850767635 0.019879771199703855\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 18:40:00,921]\u001b[0m A new study created in memory with name: no-name-c130eccb-123c-4e82-9e18-796bac7488d4\u001b[0m\n\u001b[32m[I 2022-09-06 18:40:02,833]\u001b[0m Trial 0 finished with value: 0.008209541688725185 and parameters: {'n_estimators': 535, 'max_depth': 2, 'learning_rate': 0.010804301753353342, 'colsample_bytree': 0.8207832679838404, 'subsample': 0.7774185787639283, 'alpha': 1.1314660034521875, 'lambda': 1.7462028150267617, 'gamma': 4.203224111589481e-08, 'min_child_weight': 0.2813090271307589}. Best is trial 0 with value: 0.008209541688725185.\u001b[0m\n\u001b[32m[I 2022-09-06 18:40:04,162]\u001b[0m Trial 1 finished with value: 0.0032482028765890673 and parameters: {'n_estimators': 529, 'max_depth': 4, 'learning_rate': 0.028474211702037486, 'colsample_bytree': 0.4966931490224098, 'subsample': 0.6986894992980225, 'alpha': 47.29852755417907, 'lambda': 2.3533729148451825, 'gamma': 0.00021724378186160247, 'min_child_weight': 0.2913043598373075}. Best is trial 0 with value: 0.008209541688725185.\u001b[0m\n\u001b[32m[I 2022-09-06 18:40:12,382]\u001b[0m Trial 2 finished with value: 0.0012455606372520742 and parameters: {'n_estimators': 905, 'max_depth': 6, 'learning_rate': 0.0007984624838480842, 'colsample_bytree': 0.577508711060792, 'subsample': 0.5210751660915633, 'alpha': 0.10013038655261686, 'lambda': 0.7044573523002193, 'gamma': 9.635049687815312e-05, 'min_child_weight': 10.051511186387412}. Best is trial 0 with value: 0.008209541688725185.\u001b[0m\n\u001b[32m[I 2022-09-06 18:40:13,988]\u001b[0m Trial 3 finished with value: 0.0066728826828680084 and parameters: {'n_estimators': 570, 'max_depth': 2, 'learning_rate': 0.026895760272555563, 'colsample_bytree': 0.9022931941194292, 'subsample': 0.3999693354340611, 'alpha': 1.8782157782581477, 'lambda': 157.44405291550265, 'gamma': 0.04945470449842119, 'min_child_weight': 0.2745400551224645}. Best is trial 0 with value: 0.008209541688725185.\u001b[0m\n\u001b[32m[I 2022-09-06 18:40:17,019]\u001b[0m Trial 4 finished with value: 0.006772107856926968 and parameters: {'n_estimators': 869, 'max_depth': 2, 'learning_rate': 0.006664288553287283, 'colsample_bytree': 0.309533361338077, 'subsample': 0.795929510860322, 'alpha': 3.26156737657465, 'lambda': 1.5210888013695334, 'gamma': 2.614585910249482e-10, 'min_child_weight': 1.1503829678698858}. Best is trial 0 with value: 0.008209541688725185.\u001b[0m\n\u001b[32m[I 2022-09-06 18:40:18,230]\u001b[0m Trial 5 finished with value: 0.00019658446833050727 and parameters: {'n_estimators': 636, 'max_depth': 3, 'learning_rate': 0.0052971655667837705, 'colsample_bytree': 0.07642821087715661, 'subsample': 0.2763899638199234, 'alpha': 0.15953776165672026, 'lambda': 340.9670445672354, 'gamma': 3.3708353054850644, 'min_child_weight': 0.23669498340042305}. Best is trial 0 with value: 0.008209541688725185.\u001b[0m\n\u001b[32m[I 2022-09-06 18:40:19,259]\u001b[0m Trial 6 finished with value: 0.004057783051160757 and parameters: {'n_estimators': 507, 'max_depth': 3, 'learning_rate': 0.022835118251604374, 'colsample_bytree': 0.5203224369022189, 'subsample': 0.2331971007735613, 'alpha': 0.350848095865895, 'lambda': 1.9183428749032119, 'gamma': 0.5192847395083581, 'min_child_weight': 0.3428742940689516}. Best is trial 0 with value: 0.008209541688725185.\u001b[0m\n\u001b[32m[I 2022-09-06 18:40:20,481]\u001b[0m Trial 7 finished with value: 0.006679280738418658 and parameters: {'n_estimators': 627, 'max_depth': 3, 'learning_rate': 0.020945801328628395, 'colsample_bytree': 0.9441126242925622, 'subsample': 0.3259337851334533, 'alpha': 0.5015039251953302, 'lambda': 0.2497228019866018, 'gamma': 1.469366575827664e-10, 'min_child_weight': 0.12389169362253026}. Best is trial 0 with value: 0.008209541688725185.\u001b[0m\n\u001b[32m[I 2022-09-06 18:40:22,210]\u001b[0m Trial 8 finished with value: 0.0033714554334035585 and parameters: {'n_estimators': 518, 'max_depth': 2, 'learning_rate': 0.005831753797458147, 'colsample_bytree': 0.3674805687359962, 'subsample': 0.7055327175452871, 'alpha': 0.7755091424975408, 'lambda': 286.26225249857157, 'gamma': 0.010725731943296554, 'min_child_weight': 0.1776899477474363}. Best is trial 0 with value: 0.008209541688725185.\u001b[0m\n\u001b[32m[I 2022-09-06 18:40:23,791]\u001b[0m Trial 9 finished with value: -0.0010167514306877127 and parameters: {'n_estimators': 869, 'max_depth': 6, 'learning_rate': 0.018094912185081108, 'colsample_bytree': 0.15698923686318803, 'subsample': 0.8892416200796633, 'alpha': 1.3262210111657717, 'lambda': 0.6631216007811925, 'gamma': 5.680932456978624e-05, 'min_child_weight': 42.37785922391708}. Best is trial 0 with value: 0.008209541688725185.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  22.878270149230957\n        n_estimators : 535\n           max_depth : 2\n       learning_rate : 0.010804301753353342\n    colsample_bytree : 0.8207832679838404\n           subsample : 0.7774185787639283\n               alpha : 1.1314660034521875\n              lambda : 1.7462028150267617\n               gamma : 4.203224111589481e-08\n    min_child_weight : 0.2813090271307589\nbest objective value : 0.008209541688725185\nOptuna XGB train: \n 9.87714888962485 0.019711801375132953 \nvalidation \n 14.027024514710792 0.024982086384013558 12.817724683943627 0.008088964278446964 \ntest \n 13.547414756495376 0.01631374908222727\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150    0.07729  0.001029  0.024902     0.03763  0.030909   0.032682   \n1      250   0.073123  0.004712 -0.045375    0.108358  0.097949  -0.019883   \n2      350   0.083391  0.051078  0.020909    0.117951  0.115767   0.020253   \n3      450   0.047248  0.008147   0.01349    0.024818  0.031354    0.01988   \n4      550        NaN       NaN       NaN         NaN       NaN        NaN   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.074496  0.067523  0.018465  \n1   0.099161  0.077981  -0.04083  \n2   0.072978  0.074609  0.024333  \n3   0.019712  0.024982  0.016314  \n4        NaN       NaN       NaN  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.07729</td>\n      <td>0.001029</td>\n      <td>0.024902</td>\n      <td>0.03763</td>\n      <td>0.030909</td>\n      <td>0.032682</td>\n      <td>0.074496</td>\n      <td>0.067523</td>\n      <td>0.018465</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.073123</td>\n      <td>0.004712</td>\n      <td>-0.045375</td>\n      <td>0.108358</td>\n      <td>0.097949</td>\n      <td>-0.019883</td>\n      <td>0.099161</td>\n      <td>0.077981</td>\n      <td>-0.04083</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.083391</td>\n      <td>0.051078</td>\n      <td>0.020909</td>\n      <td>0.117951</td>\n      <td>0.115767</td>\n      <td>0.020253</td>\n      <td>0.072978</td>\n      <td>0.074609</td>\n      <td>0.024333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>0.047248</td>\n      <td>0.008147</td>\n      <td>0.01349</td>\n      <td>0.024818</td>\n      <td>0.031354</td>\n      <td>0.01988</td>\n      <td>0.019712</td>\n      <td>0.024982</td>\n      <td>0.016314</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(78603, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2065, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2059, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    78603.000000\nmean       566.290854\nstd         10.352088\nmin        549.000000\n25%        557.000000\n50%        566.000000\n75%        575.000000\nmax        584.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2065.0\nmean      585.0\nstd         0.0\nmin       585.0\n25%       585.0\n50%       585.0\n75%       585.0\nmax       585.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    2059.0\nmean      586.0\nstd         0.0\nmin       586.0\n25%       586.0\n50%       586.0\n75%       586.0\nmax       586.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (78603, 44)\ntime to do feature proprocessing: \nNumber of features after transformation:  (78603, 92) (2065, 92) (2042, 92) (2059, 92)\nmae of a constant model 8.519680054852152\nR2 of a constant model 0.0\nfixed XGB train: 8.235850445923683 0.08136952860250068\nXGB val: 7.620718204123159 -0.0009789597275189355\nXGB val extra: 6.971460226646326 0.053303221814123614\nXGB test: 6.872035975098583 0.012146857125521437\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.012, 'max_depth': 5, 'n_estimators': 800, 'subsample': 0.6} 0.001215814778881219 61.908923864364624\nXGB train: 8.09109171612814 0.12202018331808517\nXGB validation: 7.429084159902022 0.06418024495539765\nXGB validation extra: 6.824911961570563 0.09835299194890978\nXGB test: 6.834245450795436 0.019529132627083667\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 18:41:34,998]\u001b[0m A new study created in memory with name: no-name-37f4e0e6-be31-4960-9bfd-a38255af6a9b\u001b[0m\n\u001b[32m[I 2022-09-06 18:41:35,523]\u001b[0m Trial 0 finished with value: 0.0021713252831286447 and parameters: {'n_estimators': 728, 'max_depth': 2, 'learning_rate': 0.005453381027536638, 'colsample_bytree': 0.9288402000009144, 'subsample': 0.6854262871759049, 'alpha': 0.8759531533169649, 'lambda': 1.1795101545988227, 'gamma': 1.3161111989802334e-05, 'min_child_weight': 13.39068141352379}. Best is trial 0 with value: 0.0021713252831286447.\u001b[0m\n\u001b[32m[I 2022-09-06 18:41:37,965]\u001b[0m Trial 1 finished with value: 0.0027337211789780326 and parameters: {'n_estimators': 572, 'max_depth': 4, 'learning_rate': 0.0005163520072931833, 'colsample_bytree': 0.1593088474247944, 'subsample': 0.8200995031255063, 'alpha': 2.598024753924979, 'lambda': 14.732550510976463, 'gamma': 3.120522273103181e-10, 'min_child_weight': 0.15148321613336965}. Best is trial 1 with value: 0.0027337211789780326.\u001b[0m\n\u001b[32m[I 2022-09-06 18:41:39,438]\u001b[0m Trial 2 finished with value: 0.01935821463226479 and parameters: {'n_estimators': 839, 'max_depth': 6, 'learning_rate': 0.027284022461124463, 'colsample_bytree': 0.2053861416968346, 'subsample': 0.8736248825207231, 'alpha': 0.14955578488859264, 'lambda': 313.3342053523726, 'gamma': 0.04632027159752942, 'min_child_weight': 1.3913609820580313}. Best is trial 2 with value: 0.01935821463226479.\u001b[0m\n\u001b[32m[I 2022-09-06 18:41:40,208]\u001b[0m Trial 3 finished with value: 0.01384925573775726 and parameters: {'n_estimators': 788, 'max_depth': 3, 'learning_rate': 0.018509147113099523, 'colsample_bytree': 0.1068006972907905, 'subsample': 0.8587304809110503, 'alpha': 0.3427460852365979, 'lambda': 360.0605095316202, 'gamma': 0.08777715010391815, 'min_child_weight': 2.53043640094667}. Best is trial 2 with value: 0.01935821463226479.\u001b[0m\n\u001b[32m[I 2022-09-06 18:41:41,759]\u001b[0m Trial 4 finished with value: 0.009643186161280104 and parameters: {'n_estimators': 582, 'max_depth': 5, 'learning_rate': 0.00364330400829752, 'colsample_bytree': 0.8356520610027813, 'subsample': 0.9255686120701146, 'alpha': 4.2032157967072, 'lambda': 15.217234740253318, 'gamma': 0.05780674172852182, 'min_child_weight': 25.006499805650098}. Best is trial 2 with value: 0.01935821463226479.\u001b[0m\n\u001b[32m[I 2022-09-06 18:41:42,222]\u001b[0m Trial 5 finished with value: 0.002065174911998158 and parameters: {'n_estimators': 738, 'max_depth': 2, 'learning_rate': 0.011361702708353403, 'colsample_bytree': 0.8342417046608286, 'subsample': 0.7085247657867572, 'alpha': 6.537948076926408, 'lambda': 0.7746291154465352, 'gamma': 0.00016084144908636136, 'min_child_weight': 0.13462947689014024}. Best is trial 2 with value: 0.01935821463226479.\u001b[0m\n\u001b[32m[I 2022-09-06 18:41:43,639]\u001b[0m Trial 6 finished with value: 0.012740610160545085 and parameters: {'n_estimators': 602, 'max_depth': 5, 'learning_rate': 0.005341643704932368, 'colsample_bytree': 0.38612019013235976, 'subsample': 0.8925097152726311, 'alpha': 21.20957142423251, 'lambda': 2.8030787716238588, 'gamma': 2.4565909992533285e-09, 'min_child_weight': 0.24924568401898253}. Best is trial 2 with value: 0.01935821463226479.\u001b[0m\n\u001b[32m[I 2022-09-06 18:41:45,171]\u001b[0m Trial 7 finished with value: 0.021235354131022576 and parameters: {'n_estimators': 936, 'max_depth': 5, 'learning_rate': 0.020437491073816205, 'colsample_bytree': 0.12900945588782559, 'subsample': 0.7219342728485209, 'alpha': 0.3256599641897066, 'lambda': 0.23462529934000642, 'gamma': 2.676716102561287e-07, 'min_child_weight': 62.31943730456402}. Best is trial 7 with value: 0.021235354131022576.\u001b[0m\n\u001b[32m[I 2022-09-06 18:41:46,090]\u001b[0m Trial 8 finished with value: 0.019230211031067236 and parameters: {'n_estimators': 970, 'max_depth': 4, 'learning_rate': 0.020273823496419537, 'colsample_bytree': 0.29043356555560773, 'subsample': 0.26703131368882993, 'alpha': 1.2777446817431792, 'lambda': 6.459954603409304, 'gamma': 0.011382875649985346, 'min_child_weight': 0.11343596450587214}. Best is trial 7 with value: 0.021235354131022576.\u001b[0m\n\u001b[32m[I 2022-09-06 18:41:46,631]\u001b[0m Trial 9 finished with value: 0.0036795501700318755 and parameters: {'n_estimators': 719, 'max_depth': 2, 'learning_rate': 0.006161250196101805, 'colsample_bytree': 0.743451249237245, 'subsample': 0.6093447555301437, 'alpha': 3.399489339440366, 'lambda': 434.83035621661725, 'gamma': 0.004668776849450183, 'min_child_weight': 2.4614596294761233}. Best is trial 7 with value: 0.021235354131022576.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  11.636184930801392\n        n_estimators : 936\n           max_depth : 5\n       learning_rate : 0.020437491073816205\n    colsample_bytree : 0.12900945588782559\n           subsample : 0.7219342728485209\n               alpha : 0.3256599641897066\n              lambda : 0.23462529934000642\n               gamma : 2.676716102561287e-07\n    min_child_weight : 62.31943730456402\nbest objective value : 0.021235354131022576\nOptuna XGB train: \n 8.09812289775442 0.11366962648389045 \nvalidation \n 7.429769331323628 0.0579144393736416 6.950972016373878 0.05516819060061462 \ntest \n 6.832690954034456 0.022552296540677585\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150    0.07729  0.001029  0.024902     0.03763  0.030909   0.032682   \n1      250   0.073123  0.004712 -0.045375    0.108358  0.097949  -0.019883   \n2      350   0.083391  0.051078  0.020909    0.117951  0.115767   0.020253   \n3      450   0.047248  0.008147   0.01349    0.024818  0.031354    0.01988   \n4      550    0.08137 -0.000979  0.012147     0.12202   0.06418   0.019529   \n5      650        NaN       NaN       NaN         NaN       NaN        NaN   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.074496  0.067523  0.018465  \n1   0.099161  0.077981  -0.04083  \n2   0.072978  0.074609  0.024333  \n3   0.019712  0.024982  0.016314  \n4    0.11367  0.057914  0.022552  \n5        NaN       NaN       NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.07729</td>\n      <td>0.001029</td>\n      <td>0.024902</td>\n      <td>0.03763</td>\n      <td>0.030909</td>\n      <td>0.032682</td>\n      <td>0.074496</td>\n      <td>0.067523</td>\n      <td>0.018465</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.073123</td>\n      <td>0.004712</td>\n      <td>-0.045375</td>\n      <td>0.108358</td>\n      <td>0.097949</td>\n      <td>-0.019883</td>\n      <td>0.099161</td>\n      <td>0.077981</td>\n      <td>-0.04083</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.083391</td>\n      <td>0.051078</td>\n      <td>0.020909</td>\n      <td>0.117951</td>\n      <td>0.115767</td>\n      <td>0.020253</td>\n      <td>0.072978</td>\n      <td>0.074609</td>\n      <td>0.024333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>0.047248</td>\n      <td>0.008147</td>\n      <td>0.01349</td>\n      <td>0.024818</td>\n      <td>0.031354</td>\n      <td>0.01988</td>\n      <td>0.019712</td>\n      <td>0.024982</td>\n      <td>0.016314</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>0.08137</td>\n      <td>-0.000979</td>\n      <td>0.012147</td>\n      <td>0.12202</td>\n      <td>0.06418</td>\n      <td>0.019529</td>\n      <td>0.11367</td>\n      <td>0.057914</td>\n      <td>0.022552</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(61151, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(1594, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(1582, 47)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    61151.000000\nmean       666.117741\nstd         10.398283\nmin        649.000000\n25%        657.000000\n50%        666.000000\n75%        675.000000\nmax        684.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    1594.0\nmean      685.0\nstd         0.0\nmin       685.0\n25%       685.0\n50%       685.0\n75%       685.0\nmax       685.0\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    1582.0\nmean      686.0\nstd         0.0\nmin       686.0\n25%       686.0\n50%       686.0\n75%       686.0\nmax       686.0\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (61151, 44)\ntime to do feature proprocessing: \nNumber of features after transformation:  (61151, 92) (1594, 92) (1585, 92) (1582, 92)\nmae of a constant model 7.686066596194761\nR2 of a constant model 0.0\nfixed XGB train: 7.3383723904844524 0.0847433819825738\nXGB val: 7.518511068980925 0.11203499614529056\nXGB val extra: 8.95612346403713 0.09447757498001275\nXGB test: 6.599484231129147 0.10894386400350753\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 5, 'n_estimators': 800, 'subsample': 0.6} 0.11512374714990747 56.65503525733948\nXGB train: 7.017268518638493 0.19285857139213314\nXGB validation: 6.802916932588982 0.3020200153568052\nXGB validation extra: 8.160043481423203 0.2845653572213611\nXGB test: 6.486287854613717 0.15250278603393164\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-06 18:42:56,768]\u001b[0m A new study created in memory with name: no-name-bacd7eb1-1c09-45ee-8cd6-9d4be1e98c30\u001b[0m\n\u001b[32m[I 2022-09-06 18:42:59,567]\u001b[0m Trial 0 finished with value: 0.09003994439464362 and parameters: {'n_estimators': 726, 'max_depth': 5, 'learning_rate': 0.026219125560923845, 'colsample_bytree': 0.8486994151303494, 'subsample': 0.6618131284002827, 'alpha': 0.15125936180376742, 'lambda': 2.166947038594244, 'gamma': 4.336640258613554e-05, 'min_child_weight': 4.256597149687839}. Best is trial 0 with value: 0.09003994439464362.\u001b[0m\n\u001b[32m[I 2022-09-06 18:43:01,133]\u001b[0m Trial 1 finished with value: 0.032753227525378924 and parameters: {'n_estimators': 611, 'max_depth': 2, 'learning_rate': 0.0020109958013238916, 'colsample_bytree': 0.14341506502491702, 'subsample': 0.9018286757813467, 'alpha': 1.1074323853515733, 'lambda': 3.080441416418462, 'gamma': 0.008275225614341363, 'min_child_weight': 0.4668589676940913}. Best is trial 0 with value: 0.09003994439464362.\u001b[0m\n\u001b[32m[I 2022-09-06 18:43:02,487]\u001b[0m Trial 2 finished with value: 0.09161019799325003 and parameters: {'n_estimators': 907, 'max_depth': 3, 'learning_rate': 0.028095009386151486, 'colsample_bytree': 0.8142109744912384, 'subsample': 0.23106549371599872, 'alpha': 9.553870019158486, 'lambda': 16.39444102914078, 'gamma': 0.006837845791677634, 'min_child_weight': 1.1416130109702698}. Best is trial 2 with value: 0.09161019799325003.\u001b[0m\n\u001b[32m[I 2022-09-06 18:43:04,843]\u001b[0m Trial 3 finished with value: 0.10208094460414881 and parameters: {'n_estimators': 646, 'max_depth': 4, 'learning_rate': 0.02402519731987866, 'colsample_bytree': 0.14239556622499527, 'subsample': 0.8594172835092595, 'alpha': 0.11933079604375436, 'lambda': 19.407429852302467, 'gamma': 2.453454710836551e-10, 'min_child_weight': 5.715819916281655}. Best is trial 3 with value: 0.10208094460414881.\u001b[0m\n\u001b[32m[I 2022-09-06 18:43:07,292]\u001b[0m Trial 4 finished with value: 0.08529568687174513 and parameters: {'n_estimators': 812, 'max_depth': 4, 'learning_rate': 0.008525392495848988, 'colsample_bytree': 0.4386802221023223, 'subsample': 0.3120236784548517, 'alpha': 35.507983786510664, 'lambda': 11.581572778812875, 'gamma': 3.681257139600608, 'min_child_weight': 53.417934542879735}. Best is trial 3 with value: 0.10208094460414881.\u001b[0m\n\u001b[32m[I 2022-09-06 18:43:10,471]\u001b[0m Trial 5 finished with value: 0.0952934283618249 and parameters: {'n_estimators': 572, 'max_depth': 5, 'learning_rate': 0.016676530014074795, 'colsample_bytree': 0.8293153694141652, 'subsample': 0.6845948691844473, 'alpha': 42.77469349604295, 'lambda': 36.89939261473745, 'gamma': 1.848951736167729e-07, 'min_child_weight': 11.040686102533849}. Best is trial 3 with value: 0.10208094460414881.\u001b[0m\n\u001b[32m[I 2022-09-06 18:43:12,679]\u001b[0m Trial 6 finished with value: 0.09645445163382867 and parameters: {'n_estimators': 828, 'max_depth': 5, 'learning_rate': 0.028586946813624947, 'colsample_bytree': 0.4370612014852743, 'subsample': 0.7364090331179568, 'alpha': 0.36508288376648734, 'lambda': 11.832156373068456, 'gamma': 5.563685228236524e-07, 'min_child_weight': 0.7599112485150551}. Best is trial 3 with value: 0.10208094460414881.\u001b[0m\n\u001b[32m[I 2022-09-06 18:43:15,836]\u001b[0m Trial 7 finished with value: 0.100822019879652 and parameters: {'n_estimators': 655, 'max_depth': 5, 'learning_rate': 0.014266496114154502, 'colsample_bytree': 0.9183055919625067, 'subsample': 0.8745691823433922, 'alpha': 0.3611977302109426, 'lambda': 0.931208231758181, 'gamma': 6.294770569310748, 'min_child_weight': 38.28562605376323}. Best is trial 3 with value: 0.10208094460414881.\u001b[0m\n\u001b[32m[I 2022-09-06 18:43:19,680]\u001b[0m Trial 8 finished with value: 0.09195873572996224 and parameters: {'n_estimators': 922, 'max_depth': 5, 'learning_rate': 0.016345190823279596, 'colsample_bytree': 0.06678469936338795, 'subsample': 0.38957603181104883, 'alpha': 11.175142253663477, 'lambda': 16.259856415037607, 'gamma': 6.32693045821944e-08, 'min_child_weight': 88.42514632134139}. Best is trial 3 with value: 0.10208094460414881.\u001b[0m\n\u001b[32m[I 2022-09-06 18:43:23,565]\u001b[0m Trial 9 finished with value: 0.09742217198248053 and parameters: {'n_estimators': 971, 'max_depth': 4, 'learning_rate': 0.0083918417947822, 'colsample_bytree': 0.49705006423486175, 'subsample': 0.6940876198428818, 'alpha': 5.316164711381873, 'lambda': 5.549652691222016, 'gamma': 0.4362595226166941, 'min_child_weight': 2.671374119718423}. Best is trial 3 with value: 0.10208094460414881.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  26.798556804656982\n        n_estimators : 646\n           max_depth : 4\n       learning_rate : 0.02402519731987866\n    colsample_bytree : 0.14239556622499527\n           subsample : 0.8594172835092595\n               alpha : 0.11933079604375436\n              lambda : 19.407429852302467\n               gamma : 2.453454710836551e-10\n    min_child_weight : 5.715819916281655\nbest objective value : 0.10208094460414881\nOptuna XGB train: \n 7.307720797146404 0.08847642352123175 \nvalidation \n 7.245302002437245 0.18352615918184123 8.88263842887947 0.11320556348051425 \ntest \n 6.527023043584636 0.13817613937273687\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      150    0.07729  0.001029  0.024902     0.03763  0.030909   0.032682   \n1      250   0.073123  0.004712 -0.045375    0.108358  0.097949  -0.019883   \n2      350   0.083391  0.051078  0.020909    0.117951  0.115767   0.020253   \n3      450   0.047248  0.008147   0.01349    0.024818  0.031354    0.01988   \n4      550    0.08137 -0.000979  0.012147     0.12202   0.06418   0.019529   \n5      650   0.084743  0.112035  0.108944    0.192859   0.30202   0.152503   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.074496  0.067523  0.018465  \n1   0.099161  0.077981  -0.04083  \n2   0.072978  0.074609  0.024333  \n3   0.019712  0.024982  0.016314  \n4    0.11367  0.057914  0.022552  \n5   0.088476  0.183526  0.138176  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>150</td>\n      <td>0.07729</td>\n      <td>0.001029</td>\n      <td>0.024902</td>\n      <td>0.03763</td>\n      <td>0.030909</td>\n      <td>0.032682</td>\n      <td>0.074496</td>\n      <td>0.067523</td>\n      <td>0.018465</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>250</td>\n      <td>0.073123</td>\n      <td>0.004712</td>\n      <td>-0.045375</td>\n      <td>0.108358</td>\n      <td>0.097949</td>\n      <td>-0.019883</td>\n      <td>0.099161</td>\n      <td>0.077981</td>\n      <td>-0.04083</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>350</td>\n      <td>0.083391</td>\n      <td>0.051078</td>\n      <td>0.020909</td>\n      <td>0.117951</td>\n      <td>0.115767</td>\n      <td>0.020253</td>\n      <td>0.072978</td>\n      <td>0.074609</td>\n      <td>0.024333</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>450</td>\n      <td>0.047248</td>\n      <td>0.008147</td>\n      <td>0.01349</td>\n      <td>0.024818</td>\n      <td>0.031354</td>\n      <td>0.01988</td>\n      <td>0.019712</td>\n      <td>0.024982</td>\n      <td>0.016314</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>550</td>\n      <td>0.08137</td>\n      <td>-0.000979</td>\n      <td>0.012147</td>\n      <td>0.12202</td>\n      <td>0.06418</td>\n      <td>0.019529</td>\n      <td>0.11367</td>\n      <td>0.057914</td>\n      <td>0.022552</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>650</td>\n      <td>0.084743</td>\n      <td>0.112035</td>\n      <td>0.108944</td>\n      <td>0.192859</td>\n      <td>0.30202</td>\n      <td>0.152503</td>\n      <td>0.088476</td>\n      <td>0.183526</td>\n      <td>0.138176</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"results.iloc[:,1:].mean()\n# cv_regularizer = 0.5\n# optuna_trials = 80\nprint(time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:43:26.572789Z","iopub.execute_input":"2022-09-06T18:43:26.573394Z","iopub.status.idle":"2022-09-06T18:43:26.582704Z","shell.execute_reply.started":"2022-09-06T18:43:26.573353Z","shell.execute_reply":"2022-09-06T18:43:26.581658Z"},"trusted":true},"execution_count":53,"outputs":[{"name":"stdout","text":"530.081832408905\n","output_type":"stream"}]},{"cell_type":"code","source":"# general point:\n# compared to NN, xgb is harder to regularize\n# in NN, you can simply shrink coefficient towards constant prediction.\n# in xgb, you can not do that. the only way to regularize is via hyperparameters.\n# in other words, by tweaking hyperpars, in NN you can approach R^2=0.0 prediction from a constant model arbitrarily close\n# in xgb, you can not do that.\n# by setting eta as low as 0.1% you can bring r2 down to 0.1%, but lowering eta further actyally increases abs(r2).\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:43:26.584466Z","iopub.execute_input":"2022-09-06T18:43:26.585183Z","iopub.status.idle":"2022-09-06T18:43:26.592073Z","shell.execute_reply.started":"2022-09-06T18:43:26.585148Z","shell.execute_reply":"2022-09-06T18:43:26.590770Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"optuna_xgb","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:43:26.595147Z","iopub.execute_input":"2022-09-06T18:43:26.596182Z","iopub.status.idle":"2022-09-06T18:43:26.610021Z","shell.execute_reply.started":"2022-09-06T18:43:26.596148Z","shell.execute_reply":"2022-09-06T18:43:26.609027Z"},"trusted":true},"execution_count":55,"outputs":[{"execution_count":55,"output_type":"execute_result","data":{"text/plain":"XGBRegressor(alpha=0.11933079604375436, base_score=0.5, booster='gbtree',\n             callbacks=None, colsample_bylevel=1, colsample_bynode=1,\n             colsample_bytree=0.14239556622499527, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None,\n             gamma=2.453454710836551e-10, gpu_id=0, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             lambda=19.407429852302467, learning_rate=0.02402519731987866,\n             max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=4,\n             max_leaves=0, min_child_weight=5.715819916281655, missing=nan,\n             monotone_constraints='()', n_estimators=646, n_jobs=0,\n             num_parallel_tree=1, predictor='auto', random_state=0, ...)"},"metadata":{}}]},{"cell_type":"code","source":"explainerxgbc = shap.TreeExplainer(optuna_xgb)\nshap_values_XGBoost_test = explainerxgbc.shap_values(X_test)\n\nvals = np.abs(shap_values_XGBoost_test).mean(0)\nfeature_names = X_test.columns\nfeature_importance = pd.DataFrame(list(zip(feature_names, vals)),\n                                 columns=['col_name','feature_importance_vals'])\nfeature_importance.sort_values(by=['feature_importance_vals'],\n                              ascending=False, inplace=True)\n\nshap.summary_plot(shap_values_XGBoost_test, X_test, \n                  plot_type=\"bar\", plot_size=(6,6), max_display=20)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T18:43:26.613357Z","iopub.execute_input":"2022-09-06T18:43:26.615729Z","iopub.status.idle":"2022-09-06T18:43:27.388626Z","shell.execute_reply.started":"2022-09-06T18:43:26.615703Z","shell.execute_reply":"2022-09-06T18:43:27.387674Z"},"trusted":true},"execution_count":56,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate performance of XGB models:\nr2_xgb1 = r2_score(y_test, xgb1.predict(X_test))\nr2_xgbgs = r2_score(y_test, xgbgs.predict(X_test))\nr2_xgbo = r2_score(y_test, optuna_xgb.predict(X_test))\n\nprint('Min_prd: ', min_prd)\nprint('Constant guess: ', mean_absolute_error(y_test, np.ones(len(y_test))*y_test.mean()), \n      r2_score(y_test, np.ones(len(y_test))*y_test.mean()))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_xgb1)\nprint('XGB GS test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_xgbgs)\nprint('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_xgbo)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:04:33.007641Z","iopub.execute_input":"2022-09-06T17:04:33.008142Z","iopub.status.idle":"2022-09-06T17:04:33.123104Z","shell.execute_reply.started":"2022-09-06T17:04:33.008105Z","shell.execute_reply":"2022-09-06T17:04:33.122412Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Min_prd:  650\nConstant guess:  6.466409312983865 0.0\nXGB test: 6.390771035406137 0.020476758928961947\nXGB GS test: 6.344917696653318 0.045586733442827376\nOptuna XGB test: 6.336654870859579 0.048473194170501666\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"col_ignore = ['RET', 'prd']\ncol_cat = ['ind']\ncol_num = [x for x in train.columns if x not in col_ignore+col_cat]\nfor col in col_num:\n    train[col] = train[col].fillna(train[col].median())\n    test[col] = test[col].fillna(train[col].median())\nfor col in col_cat:\n    train[col] = train[col].fillna(value=-1000)\n    test[col] = test[col].fillna(value=-1000)\n\nX_train = train.copy()\ny_train = X_train.pop('RET')\nX_test = test.copy()\ny_test = X_test.pop('RET')\ny_train.reset_index(inplace=True, drop=True)\n\nfeature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                        (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                        remainder=\"passthrough\")\n\nprint('Number of features before transformation: ', X_train.shape)\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ')\nprint('Number of features after transformation: ', X_train.shape)\n\nX_train0 = X_train.copy()\ny_train0 = y_train.copy()\n\nX_train.drop(columns=['remainder__prd'], inplace=True)\nX_test.drop(columns=['remainder__prd'], inplace=True)\n\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=300, max_depth=5, eta=0.03, colsample_bytree=0.6)\nxgb1.fit(X_train, y_train)\nprint('XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\n\ntime1 = time.time()\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[400, 700], 'max_depth':[2,3,4], 'eta':[0.006, 0.012, 0.02], 'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbm = GridSearchCV(xgb, param_grid, cv=2, verbose=2, scoring='r2')\nxgbm.fit(X_train, y_train)\nprint('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)\nprint('XGB train:', mean_absolute_error(y_train, xgbm.predict(X_train)), r2_score(y_train, xgbm.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:04:33.127421Z","iopub.execute_input":"2022-09-06T17:04:33.127687Z","iopub.status.idle":"2022-09-06T17:04:33.144284Z","shell.execute_reply.started":"2022-09-06T17:04:33.127662Z","shell.execute_reply":"2022-09-06T17:04:33.142939Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_17/2143843005.py\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    \u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"],"ename":"SyntaxError","evalue":"invalid character in identifier (2143843005.py, line 10)","output_type":"error"}]},{"cell_type":"code","source":"time1 = time.time()\ndef objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    params = {\n    \"tree_method\": 'gpu_hist',\n    \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n    \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1000),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n    \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.001, 0.05),\n    \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 0.95),\n    \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 0.95),\n    \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 30.0),\n    \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 200.0),\n    \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 10.0),\n    \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n    temp_out = []\n\n    for i in range(cv_runs):\n\n        X = X_train\n        y = y_train\n        model = XGBRegressor(**params, njobs=-1)\n        rkf = KFold(n_splits=n_splits, shuffle=True)\n        X_values = X.values\n        y_values = y.values\n        y_pred = np.zeros_like(y_values)\n        y_pred_train = np.zeros_like(y_values)\n        for train_index, test_index in rkf.split(X_values):\n            X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n            y_A, y_B = y_values[train_index], y_values[test_index]\n            model.fit(X_A, y_A, eval_set=[(X_B, y_B)], verbose = False)\n            y_pred[test_index] = model.predict(X_B)\n            y_pred_train[train_index] = model.predict(X_A)\n\n        score_train = r2_score(y_train, y_pred_train)\n        score_test = r2_score(y_train, y_pred) \n        overfit = (score_train-score_test)\n        temp_out.append(score_test-cv_regularizer*overfit)\n\n    return (np.mean(temp_out))\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=optuna_trials)\nprint('Total time for hypermarameter optimization ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X_train, y_train)\nprint('Optuna XGB train:', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), time.time()-time1)\n\n# Evaluate performance of XGB models:\nr2_xgb1 = r2_score(y_test, xgb1.predict(X_test))\nr2_xgbgs = r2_score(y_test, xgbm.predict(X_test))\nr2_xgbo = r2_score(y_test, optuna_xgb.predict(X_test))\n\nprint('Min_prd: ', min_prd)\nprint('Constant guess: ', mean_absolute_error(y_test, np.ones(len(y_test))*y_test.mean()), \n      r2_score(y_test, np.ones(len(y_test))*y_test.mean()))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_xgb1)\nprint('XGB GS test:', mean_absolute_error(y_test, xgbm.predict(X_test)), r2_xgbgs)\nprint('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_xgbo)\n\nresults.loc[results.min_prd==min_prd,'xgbf':'xgbo'] = r2_xgb1, r2_xgbgs, r2_xgbo","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:04:33.146675Z","iopub.status.idle":"2022-09-06T17:04:33.147223Z","shell.execute_reply.started":"2022-09-06T17:04:33.147045Z","shell.execute_reply":"2022-09-06T17:04:33.147064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for loop to see appx performance over the whole sample with some rolling window\n\ntime0 = time.time()\n\n#min_prd_list = range(100, 676, 25)\nmin_prd_list = [125]\nwindows_width = 40*12\ncv_regularizer=0.05\noptuna_trials = 2\n\nmin_prd = 100\n    \nwith open('../input/kaggle-46pkl/IMLEAP_v4.pkl', 'rb') as pickled_one:\n    df = pickle.load(pickled_one)\ndf = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+2))]\ndf_cnt = df.count()\nempty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\ndf.drop(columns=empty_cols, inplace=True)\ndisplay(df.shape, df.head(), df.year.describe(), df.count())\n\ndf = df[(df.RET>-50)&(df.RET<75)]\nmeanret = df.groupby('prd').RET.mean().to_frame().reset_index().rename(columns={'RET':'mRET'})\ndf = pd.merge(df, meanret, on='prd', how='left')\ndf.RET = df.RET-df.mRET\ndf.drop(columns='mRET', inplace=True)\n\nfeatures_miss_dummies = ['amhd', 'BAspr']\nfor col in features_miss_dummies:\n    if col in df.columns:\n        df[col+'_miss'] = df[col].isnull().astype(int)\n\ndf.reset_index(inplace=True, drop=True)\ntemp_cols = ['PERMNO', 'year']\ntrain = df[df.prd<(min_prd+windows_width)]\ntest = df[df.prd==(min_prd+windows_width)]\ntrain.drop(columns=temp_cols, inplace=True)\ntest.drop(columns=temp_cols, inplace=True)\n\ncol_ignore = ['RET', 'prd']\ncol_cat = ['ind']\ncol_num = [x for x in train.columns if x not in col_ignore+col_cat]\nfor col in col_num:\n    train[col] = train[col].fillna(train[col].median())\n    test[col] = test[col].fillna(train[col].median())\nfor col in col_cat:\n    train[col] = train[col].fillna(value=-1000)\n    test[col] = test[col].fillna(value=-1000)\n\nX_train = train.copy()\ny_train = X_train.pop('RET')\nX_test = test.copy()\ny_test = X_test.pop('RET')\ny_train.reset_index(inplace=True, drop=True)\n\nfeature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                        (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                        remainder=\"passthrough\")\n\nprint('Number of features before transformation: ', X_train.shape)\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ')\nprint('Number of features after transformation: ', X_train.shape)\n\nX_train0 = X_train.copy()\ny_train0 = y_train.copy()\n\nX_train.drop(columns=['remainder__prd'], inplace=True)\nX_test.drop(columns=['remainder__prd'], inplace=True)\n\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=300, max_depth=5, eta=0.03, colsample_bytree=0.6)\nxgb1.fit(X_train, y_train)\nprint('XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\n\ntime1 = time.time()\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[400, 700], 'max_depth':[2,3,4], 'eta':[0.006, 0.012, 0.02], 'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbm = GridSearchCV(xgb, param_grid, cv=2, verbose=2, scoring='r2')\nxgbm.fit(X_train, y_train)\nprint('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)\nprint('XGB train:', mean_absolute_error(y_train, xgbm.predict(X_train)), r2_score(y_train, xgbm.predict(X_train)), time.time()-time1)\n\ntime1 = time.time()\ndef objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    params = {\n    \"tree_method\": 'gpu_hist',\n    \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n    \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1000),\n    \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n    \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.001, 0.03),\n    \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 0.95),\n    \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 0.95),\n    \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 30.0),\n    \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 200.0),\n    \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 10.0),\n    \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n    temp_out = []\n\n    for i in range(cv_runs):\n\n        X = X_train\n        y = y_train\n        model = XGBRegressor(**params, njobs=-1)\n        rkf = KFold(n_splits=n_splits, shuffle=True)\n        X_values = X.values\n        y_values = y.values\n        y_pred = np.zeros_like(y_values)\n        y_pred_train = np.zeros_like(y_values)\n        for train_index, test_index in rkf.split(X_values):\n            X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n            y_A, y_B = y_values[train_index], y_values[test_index]\n            model.fit(X_A, y_A, eval_set=[(X_B, y_B)], verbose = False)\n            y_pred[test_index] = model.predict(X_B)\n            y_pred_train[train_index] = model.predict(X_A)\n\n        score_train = r2_score(y_train, y_pred_train)\n        score_test = r2_score(y_train, y_pred) \n        overfit = (score_train-score_test)\n        temp_out.append(score_test-cv_regularizer*overfit)\n\n    return (np.mean(temp_out))\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=optuna_trials)\nprint('Total time for hypermarameter optimization ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X_train, y_train)\nprint('Optuna XGB train:', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), time.time()-time1)\n\n# Evaluate performance of XGB models:\nr2_xgb1 = r2_score(y_test, xgb1.predict(X_test))\nr2_xgbgs = r2_score(y_test, xgbm.predict(X_test))\nr2_xgbo = r2_score(y_test, optuna_xgb.predict(X_test))\n\nprint('Min_prd: ', min_prd)\nprint('Constant guess: ', mean_absolute_error(y_test, np.ones(len(y_test))*y_test.mean()), \n      r2_score(y_test, np.ones(len(y_test))*y_test.mean()))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_xgb1)\nprint('XGB GS test:', mean_absolute_error(y_test, xgbm.predict(X_test)), r2_xgbgs)\nprint('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_xgbo)\n\nresults.loc[results.min_prd==min_prd,'xgbf':'xgbo'] = r2_xgb1, r2_xgbgs, r2_xgbo\n    \nprint(time.time()-time0, results)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:04:33.150823Z","iopub.status.idle":"2022-09-06T17:04:33.151220Z","shell.execute_reply.started":"2022-09-06T17:04:33.151032Z","shell.execute_reply":"2022-09-06T17:04:33.151048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.skew()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T02:04:00.475156Z","iopub.execute_input":"2022-09-06T02:04:00.475842Z","iopub.status.idle":"2022-09-06T02:04:01.129536Z","shell.execute_reply.started":"2022-09-06T02:04:00.475807Z","shell.execute_reply":"2022-09-06T02:04:01.128404Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"PERMNO          0.087933\nprd            -0.198148\nmom482          1.193992\nmom242          0.890617\nyear           -0.198149\nRET             0.792396\nind            -0.131010\nbm             -0.590142\nop             -0.827249\ngp              0.446368\ninv             1.261316\nmom11           0.275975\nmom122          0.651943\namhd           -0.376405\nivol_capm       1.226450\nivol_ff5        1.236885\nbeta_bw         0.457922\nMAX             1.345936\nvol1m           1.211599\nvol6m           1.144974\nvol12m          1.088969\nsize            0.379107\nlbm            -0.595647\nlop            -0.846113\nlgp             0.433002\nlinv            1.218153\nllme            0.419597\nl1amhd         -0.378887\nl1MAX           1.346688\nl3amhd         -0.383315\nl3MAX           1.348446\nl6amhd         -0.388782\nl6MAX           1.347230\nl12amhd        -0.399100\nl12MAX          1.346688\nl12mom122       0.660565\nl12ivol_capm    1.225928\nl12ivol_ff5     1.237615\nl12beta_bw      0.494384\nl12vol6m        1.136219\nl12vol12m       1.079234\namhd_miss       1.377965\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-09-06T03:22:40.089070Z","iopub.execute_input":"2022-09-06T03:22:40.089617Z","iopub.status.idle":"2022-09-06T03:22:40.167922Z","shell.execute_reply.started":"2022-09-06T03:22:40.089572Z","shell.execute_reply":"2022-09-06T03:22:40.166956Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"       PERMNO  prd      mom482      mom242  year        RET   ind        bm  \\\n0       10025  649   30.604529   47.614289  2012   6.600503  15.0 -0.953163   \n1       10025  650   97.106725   44.386493  2012  22.413854  15.0 -0.953163   \n2       10025  651  138.786753   50.502876  2012   9.260382  15.0 -0.899741   \n3       10025  652  162.913720   87.364374  2012   4.114279  15.0 -0.899741   \n4       10025  653  174.780357  113.102274  2012  16.416518  15.0 -0.899741   \n...       ...  ...         ...         ...   ...        ...   ...       ...   \n48154   93423  672         NaN   85.363516  2014   2.891556   7.0 -1.226522   \n48155   93423  673         NaN   95.330763  2014   1.794075   7.0 -1.226522   \n48156   93423  674         NaN   73.564394  2014   1.531531   7.0 -1.226522   \n48157   93423  675         NaN   65.099184  2014  -5.751390   7.0 -1.909081   \n48158   93423  676         NaN   52.233824  2014  -7.425820   7.0 -1.909081   \n\n             op        gp       inv    mom11     mom122      amhd  ivol_capm  \\\n0      0.060639  0.373348 -0.025756   0.1724  14.759414  1.259688   1.204700   \n1      0.060639  0.373348 -0.025756  -0.9851  13.571797  1.141539   0.765013   \n2      0.072943  0.362002  0.184931  26.1222  18.270241  1.103795   1.517394   \n3      0.072943  0.362002  0.184931   7.8760  60.787559  1.047262   0.937883   \n4      0.072943  0.362002  0.184931   7.1633  72.766685  0.949094   1.120278   \n...         ...       ...       ...      ...        ...       ...        ...   \n48154  0.100139  0.189565  0.154149  -1.5931  18.300911 -3.226472   0.779887   \n48155  0.100139  0.189565  0.154149  -0.0249  15.793176 -3.219131   0.952419   \n48156  0.100139  0.189565  0.154149   1.9681  11.776170 -3.241532   0.914882   \n48157  0.130961  0.232296 -0.146767   5.1656  20.864465 -3.238086   0.837567   \n48158  0.130961  0.232296 -0.146767 -10.1763  21.476203 -3.247683   1.268193   \n\n       ivol_ff5   beta_bw     MAX     vol1m     vol6m    vol12m     BAspr  \\\n0      0.923609  1.058419  2.5839  1.533648  2.224711  2.413616  0.404975   \n1      0.647098  1.026432  1.4066  0.866870  1.761031  2.337123  0.579206   \n2      1.511676  1.007201  5.7938  1.525676  1.485825  2.334760  0.136519   \n3      0.768468  0.991882  3.1364  0.989203  1.371582  2.299344  0.578902   \n4      0.963537  1.038772  4.3231  1.269428  1.271617  2.116573  0.335306   \n...         ...       ...     ...       ...       ...       ...       ...   \n48154  0.759569  0.902643  1.4066  0.908718  1.377351  1.451315  0.029163   \n48155  0.914672  0.949362  2.1480  1.275081  1.248010  1.359812  0.029163   \n48156  0.855551  0.932978  2.3892  0.997929  1.240860  1.341014  0.029163   \n48157  0.751698  0.990565  2.4722  0.881300  1.221605  1.319646  0.047048   \n48158  1.108560  1.007570  2.7474  1.434456  1.280950  1.308248  0.029163   \n\n           size       lbm       lop       lgp      linv      llme    l1amhd  \\\n0      5.260005 -1.100725  0.176879  0.498498 -0.078728  5.226473  1.278464   \n1      5.250206 -1.100725  0.176879  0.498498 -0.078728  5.238599  1.259688   \n2      5.482288 -0.953163  0.060639  0.373348 -0.025756  5.188325  1.141539   \n3      5.558824 -0.953163  0.060639  0.373348 -0.025756  5.001724  1.103795   \n4      5.628101 -0.953163  0.060639  0.373348 -0.025756  5.005778  1.047262   \n...         ...       ...       ...       ...       ...       ...       ...   \n48154  8.244170 -0.835300  0.096950  0.203164 -0.031126  8.179936 -3.222051   \n48155  8.247775 -0.835300  0.096950  0.203164 -0.031126  8.155703 -3.226472   \n48156  8.255716 -0.835300  0.096950  0.203164 -0.031126  8.178869 -3.219131   \n48157  8.306082 -1.226522  0.100139  0.189565  0.154149  8.120186 -3.241532   \n48158  8.200147 -1.226522  0.100139  0.189565  0.154149  8.166935 -3.238086   \n\n        l1MAX   l1BAspr    l3amhd   l3MAX   l3BAspr    l6amhd   l6MAX  \\\n0      2.9882  0.371854  1.383731  4.1328  0.513906  1.510489  8.7149   \n1      2.5839  0.404975  1.369098  3.7563  0.490055  1.482684  9.2927   \n2      1.4066  0.579206  1.278464  2.9882  0.371854  1.460316  8.6057   \n3      5.7938  0.136519  1.259688  2.5839  0.404975  1.383731  4.1328   \n4      3.1364  0.578902  1.141539  1.4066  0.579206  1.369098  3.7563   \n...       ...       ...       ...     ...       ...       ...     ...   \n48154  4.6275  0.029163 -3.315129  1.8033  0.081566 -3.386130  2.4529   \n48155  1.4066  0.029163 -3.255144  3.6344  0.029163 -3.355495  6.5831   \n48156  2.1480  0.029163 -3.222051  4.6275  0.029163 -3.388181  2.9005   \n48157  2.3892  0.029163 -3.226472  1.4066  0.029163 -3.315129  1.8033   \n48158  2.4722  0.047048 -3.219131  2.1480  0.029163 -3.255144  3.6344   \n\n        l6BAspr   l12amhd  l12MAX  l12BAspr  l12mom122  l12ivol_capm  \\\n0      0.558214  1.431610  2.9882  0.397878   7.446355      1.532176   \n1      0.468883  1.488235  2.5839  0.579216  20.525370      1.116961   \n2      0.567577  1.494722  1.4066  0.307377  28.446093      1.852721   \n3      0.513906  1.504704  5.7938  0.367918   0.922674      1.208446   \n4      0.490055  1.523562  3.1364  0.681302  14.425587      2.072178   \n...         ...       ...     ...       ...        ...           ...   \n48154  0.029481 -3.092530  4.6275  0.029163  50.350497      0.765013   \n48155  0.029163 -3.181713  1.4066  0.029163  59.229231      2.117291   \n48156  0.029163 -3.257544  2.1480  0.029163  65.920814      0.979222   \n48157  0.081566 -3.317072  2.3892  0.029163  44.723091      1.057337   \n48158  0.029163 -3.357783  2.4722  0.029163  28.349347      1.602867   \n\n       l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  amhd_miss  BAspr_miss  \n0         1.429862    1.297452  2.318278   2.736604          0           0  \n1         0.998634    1.245832  2.151214   2.495713          0           0  \n2         1.680914    1.262682  2.178625   2.343239          0           0  \n3         0.908065    1.307383  2.056547   2.325010          0           0  \n4         1.968630    1.081815  2.335976   2.367897          0           0  \n...            ...         ...       ...        ...        ...         ...  \n48154     0.647098    0.792883  1.553267   1.526357          0           0  \n48155     1.899401    0.771122  1.459107   1.609127          0           0  \n48156     0.920754    0.772685  1.454548   1.602632          0           0  \n48157     0.845541    0.695503  1.379176   1.518435          0           0  \n48158     1.509123    0.694050  1.499910   1.509470          0           0  \n\n[48159 rows x 48 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10025</td>\n      <td>649</td>\n      <td>30.604529</td>\n      <td>47.614289</td>\n      <td>2012</td>\n      <td>6.600503</td>\n      <td>15.0</td>\n      <td>-0.953163</td>\n      <td>0.060639</td>\n      <td>0.373348</td>\n      <td>-0.025756</td>\n      <td>0.1724</td>\n      <td>14.759414</td>\n      <td>1.259688</td>\n      <td>1.204700</td>\n      <td>0.923609</td>\n      <td>1.058419</td>\n      <td>2.5839</td>\n      <td>1.533648</td>\n      <td>2.224711</td>\n      <td>2.413616</td>\n      <td>0.404975</td>\n      <td>5.260005</td>\n      <td>-1.100725</td>\n      <td>0.176879</td>\n      <td>0.498498</td>\n      <td>-0.078728</td>\n      <td>5.226473</td>\n      <td>1.278464</td>\n      <td>2.9882</td>\n      <td>0.371854</td>\n      <td>1.383731</td>\n      <td>4.1328</td>\n      <td>0.513906</td>\n      <td>1.510489</td>\n      <td>8.7149</td>\n      <td>0.558214</td>\n      <td>1.431610</td>\n      <td>2.9882</td>\n      <td>0.397878</td>\n      <td>7.446355</td>\n      <td>1.532176</td>\n      <td>1.429862</td>\n      <td>1.297452</td>\n      <td>2.318278</td>\n      <td>2.736604</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10025</td>\n      <td>650</td>\n      <td>97.106725</td>\n      <td>44.386493</td>\n      <td>2012</td>\n      <td>22.413854</td>\n      <td>15.0</td>\n      <td>-0.953163</td>\n      <td>0.060639</td>\n      <td>0.373348</td>\n      <td>-0.025756</td>\n      <td>-0.9851</td>\n      <td>13.571797</td>\n      <td>1.141539</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>1.026432</td>\n      <td>1.4066</td>\n      <td>0.866870</td>\n      <td>1.761031</td>\n      <td>2.337123</td>\n      <td>0.579206</td>\n      <td>5.250206</td>\n      <td>-1.100725</td>\n      <td>0.176879</td>\n      <td>0.498498</td>\n      <td>-0.078728</td>\n      <td>5.238599</td>\n      <td>1.259688</td>\n      <td>2.5839</td>\n      <td>0.404975</td>\n      <td>1.369098</td>\n      <td>3.7563</td>\n      <td>0.490055</td>\n      <td>1.482684</td>\n      <td>9.2927</td>\n      <td>0.468883</td>\n      <td>1.488235</td>\n      <td>2.5839</td>\n      <td>0.579216</td>\n      <td>20.525370</td>\n      <td>1.116961</td>\n      <td>0.998634</td>\n      <td>1.245832</td>\n      <td>2.151214</td>\n      <td>2.495713</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10025</td>\n      <td>651</td>\n      <td>138.786753</td>\n      <td>50.502876</td>\n      <td>2012</td>\n      <td>9.260382</td>\n      <td>15.0</td>\n      <td>-0.899741</td>\n      <td>0.072943</td>\n      <td>0.362002</td>\n      <td>0.184931</td>\n      <td>26.1222</td>\n      <td>18.270241</td>\n      <td>1.103795</td>\n      <td>1.517394</td>\n      <td>1.511676</td>\n      <td>1.007201</td>\n      <td>5.7938</td>\n      <td>1.525676</td>\n      <td>1.485825</td>\n      <td>2.334760</td>\n      <td>0.136519</td>\n      <td>5.482288</td>\n      <td>-0.953163</td>\n      <td>0.060639</td>\n      <td>0.373348</td>\n      <td>-0.025756</td>\n      <td>5.188325</td>\n      <td>1.141539</td>\n      <td>1.4066</td>\n      <td>0.579206</td>\n      <td>1.278464</td>\n      <td>2.9882</td>\n      <td>0.371854</td>\n      <td>1.460316</td>\n      <td>8.6057</td>\n      <td>0.567577</td>\n      <td>1.494722</td>\n      <td>1.4066</td>\n      <td>0.307377</td>\n      <td>28.446093</td>\n      <td>1.852721</td>\n      <td>1.680914</td>\n      <td>1.262682</td>\n      <td>2.178625</td>\n      <td>2.343239</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10025</td>\n      <td>652</td>\n      <td>162.913720</td>\n      <td>87.364374</td>\n      <td>2012</td>\n      <td>4.114279</td>\n      <td>15.0</td>\n      <td>-0.899741</td>\n      <td>0.072943</td>\n      <td>0.362002</td>\n      <td>0.184931</td>\n      <td>7.8760</td>\n      <td>60.787559</td>\n      <td>1.047262</td>\n      <td>0.937883</td>\n      <td>0.768468</td>\n      <td>0.991882</td>\n      <td>3.1364</td>\n      <td>0.989203</td>\n      <td>1.371582</td>\n      <td>2.299344</td>\n      <td>0.578902</td>\n      <td>5.558824</td>\n      <td>-0.953163</td>\n      <td>0.060639</td>\n      <td>0.373348</td>\n      <td>-0.025756</td>\n      <td>5.001724</td>\n      <td>1.103795</td>\n      <td>5.7938</td>\n      <td>0.136519</td>\n      <td>1.259688</td>\n      <td>2.5839</td>\n      <td>0.404975</td>\n      <td>1.383731</td>\n      <td>4.1328</td>\n      <td>0.513906</td>\n      <td>1.504704</td>\n      <td>5.7938</td>\n      <td>0.367918</td>\n      <td>0.922674</td>\n      <td>1.208446</td>\n      <td>0.908065</td>\n      <td>1.307383</td>\n      <td>2.056547</td>\n      <td>2.325010</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10025</td>\n      <td>653</td>\n      <td>174.780357</td>\n      <td>113.102274</td>\n      <td>2012</td>\n      <td>16.416518</td>\n      <td>15.0</td>\n      <td>-0.899741</td>\n      <td>0.072943</td>\n      <td>0.362002</td>\n      <td>0.184931</td>\n      <td>7.1633</td>\n      <td>72.766685</td>\n      <td>0.949094</td>\n      <td>1.120278</td>\n      <td>0.963537</td>\n      <td>1.038772</td>\n      <td>4.3231</td>\n      <td>1.269428</td>\n      <td>1.271617</td>\n      <td>2.116573</td>\n      <td>0.335306</td>\n      <td>5.628101</td>\n      <td>-0.953163</td>\n      <td>0.060639</td>\n      <td>0.373348</td>\n      <td>-0.025756</td>\n      <td>5.005778</td>\n      <td>1.047262</td>\n      <td>3.1364</td>\n      <td>0.578902</td>\n      <td>1.141539</td>\n      <td>1.4066</td>\n      <td>0.579206</td>\n      <td>1.369098</td>\n      <td>3.7563</td>\n      <td>0.490055</td>\n      <td>1.523562</td>\n      <td>3.1364</td>\n      <td>0.681302</td>\n      <td>14.425587</td>\n      <td>2.072178</td>\n      <td>1.968630</td>\n      <td>1.081815</td>\n      <td>2.335976</td>\n      <td>2.367897</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>48154</th>\n      <td>93423</td>\n      <td>672</td>\n      <td>NaN</td>\n      <td>85.363516</td>\n      <td>2014</td>\n      <td>2.891556</td>\n      <td>7.0</td>\n      <td>-1.226522</td>\n      <td>0.100139</td>\n      <td>0.189565</td>\n      <td>0.154149</td>\n      <td>-1.5931</td>\n      <td>18.300911</td>\n      <td>-3.226472</td>\n      <td>0.779887</td>\n      <td>0.759569</td>\n      <td>0.902643</td>\n      <td>1.4066</td>\n      <td>0.908718</td>\n      <td>1.377351</td>\n      <td>1.451315</td>\n      <td>0.029163</td>\n      <td>8.244170</td>\n      <td>-0.835300</td>\n      <td>0.096950</td>\n      <td>0.203164</td>\n      <td>-0.031126</td>\n      <td>8.179936</td>\n      <td>-3.222051</td>\n      <td>4.6275</td>\n      <td>0.029163</td>\n      <td>-3.315129</td>\n      <td>1.8033</td>\n      <td>0.081566</td>\n      <td>-3.386130</td>\n      <td>2.4529</td>\n      <td>0.029481</td>\n      <td>-3.092530</td>\n      <td>4.6275</td>\n      <td>0.029163</td>\n      <td>50.350497</td>\n      <td>0.765013</td>\n      <td>0.647098</td>\n      <td>0.792883</td>\n      <td>1.553267</td>\n      <td>1.526357</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48155</th>\n      <td>93423</td>\n      <td>673</td>\n      <td>NaN</td>\n      <td>95.330763</td>\n      <td>2014</td>\n      <td>1.794075</td>\n      <td>7.0</td>\n      <td>-1.226522</td>\n      <td>0.100139</td>\n      <td>0.189565</td>\n      <td>0.154149</td>\n      <td>-0.0249</td>\n      <td>15.793176</td>\n      <td>-3.219131</td>\n      <td>0.952419</td>\n      <td>0.914672</td>\n      <td>0.949362</td>\n      <td>2.1480</td>\n      <td>1.275081</td>\n      <td>1.248010</td>\n      <td>1.359812</td>\n      <td>0.029163</td>\n      <td>8.247775</td>\n      <td>-0.835300</td>\n      <td>0.096950</td>\n      <td>0.203164</td>\n      <td>-0.031126</td>\n      <td>8.155703</td>\n      <td>-3.226472</td>\n      <td>1.4066</td>\n      <td>0.029163</td>\n      <td>-3.255144</td>\n      <td>3.6344</td>\n      <td>0.029163</td>\n      <td>-3.355495</td>\n      <td>6.5831</td>\n      <td>0.029163</td>\n      <td>-3.181713</td>\n      <td>1.4066</td>\n      <td>0.029163</td>\n      <td>59.229231</td>\n      <td>2.117291</td>\n      <td>1.899401</td>\n      <td>0.771122</td>\n      <td>1.459107</td>\n      <td>1.609127</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48156</th>\n      <td>93423</td>\n      <td>674</td>\n      <td>NaN</td>\n      <td>73.564394</td>\n      <td>2014</td>\n      <td>1.531531</td>\n      <td>7.0</td>\n      <td>-1.226522</td>\n      <td>0.100139</td>\n      <td>0.189565</td>\n      <td>0.154149</td>\n      <td>1.9681</td>\n      <td>11.776170</td>\n      <td>-3.241532</td>\n      <td>0.914882</td>\n      <td>0.855551</td>\n      <td>0.932978</td>\n      <td>2.3892</td>\n      <td>0.997929</td>\n      <td>1.240860</td>\n      <td>1.341014</td>\n      <td>0.029163</td>\n      <td>8.255716</td>\n      <td>-0.835300</td>\n      <td>0.096950</td>\n      <td>0.203164</td>\n      <td>-0.031126</td>\n      <td>8.178869</td>\n      <td>-3.219131</td>\n      <td>2.1480</td>\n      <td>0.029163</td>\n      <td>-3.222051</td>\n      <td>4.6275</td>\n      <td>0.029163</td>\n      <td>-3.388181</td>\n      <td>2.9005</td>\n      <td>0.029163</td>\n      <td>-3.257544</td>\n      <td>2.1480</td>\n      <td>0.029163</td>\n      <td>65.920814</td>\n      <td>0.979222</td>\n      <td>0.920754</td>\n      <td>0.772685</td>\n      <td>1.454548</td>\n      <td>1.602632</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48157</th>\n      <td>93423</td>\n      <td>675</td>\n      <td>NaN</td>\n      <td>65.099184</td>\n      <td>2014</td>\n      <td>-5.751390</td>\n      <td>7.0</td>\n      <td>-1.909081</td>\n      <td>0.130961</td>\n      <td>0.232296</td>\n      <td>-0.146767</td>\n      <td>5.1656</td>\n      <td>20.864465</td>\n      <td>-3.238086</td>\n      <td>0.837567</td>\n      <td>0.751698</td>\n      <td>0.990565</td>\n      <td>2.4722</td>\n      <td>0.881300</td>\n      <td>1.221605</td>\n      <td>1.319646</td>\n      <td>0.047048</td>\n      <td>8.306082</td>\n      <td>-1.226522</td>\n      <td>0.100139</td>\n      <td>0.189565</td>\n      <td>0.154149</td>\n      <td>8.120186</td>\n      <td>-3.241532</td>\n      <td>2.3892</td>\n      <td>0.029163</td>\n      <td>-3.226472</td>\n      <td>1.4066</td>\n      <td>0.029163</td>\n      <td>-3.315129</td>\n      <td>1.8033</td>\n      <td>0.081566</td>\n      <td>-3.317072</td>\n      <td>2.3892</td>\n      <td>0.029163</td>\n      <td>44.723091</td>\n      <td>1.057337</td>\n      <td>0.845541</td>\n      <td>0.695503</td>\n      <td>1.379176</td>\n      <td>1.518435</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>48158</th>\n      <td>93423</td>\n      <td>676</td>\n      <td>NaN</td>\n      <td>52.233824</td>\n      <td>2014</td>\n      <td>-7.425820</td>\n      <td>7.0</td>\n      <td>-1.909081</td>\n      <td>0.130961</td>\n      <td>0.232296</td>\n      <td>-0.146767</td>\n      <td>-10.1763</td>\n      <td>21.476203</td>\n      <td>-3.247683</td>\n      <td>1.268193</td>\n      <td>1.108560</td>\n      <td>1.007570</td>\n      <td>2.7474</td>\n      <td>1.434456</td>\n      <td>1.280950</td>\n      <td>1.308248</td>\n      <td>0.029163</td>\n      <td>8.200147</td>\n      <td>-1.226522</td>\n      <td>0.100139</td>\n      <td>0.189565</td>\n      <td>0.154149</td>\n      <td>8.166935</td>\n      <td>-3.238086</td>\n      <td>2.4722</td>\n      <td>0.047048</td>\n      <td>-3.219131</td>\n      <td>2.1480</td>\n      <td>0.029163</td>\n      <td>-3.255144</td>\n      <td>3.6344</td>\n      <td>0.029163</td>\n      <td>-3.357783</td>\n      <td>2.4722</td>\n      <td>0.029163</td>\n      <td>28.349347</td>\n      <td>1.602867</td>\n      <td>1.509123</td>\n      <td>0.694050</td>\n      <td>1.499910</td>\n      <td>1.509470</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>48159 rows  48 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.128921Z","iopub.status.idle":"2022-09-06T00:36:53.129664Z","shell.execute_reply.started":"2022-09-06T00:36:53.129409Z","shell.execute_reply":"2022-09-06T00:36:53.129433Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total time for a script: ', time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.131169Z","iopub.status.idle":"2022-09-06T00:36:53.131976Z","shell.execute_reply.started":"2022-09-06T00:36:53.131718Z","shell.execute_reply":"2022-09-06T00:36:53.131742Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results.iloc[:,1:].mean()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.133368Z","iopub.status.idle":"2022-09-06T00:36:53.134132Z","shell.execute_reply.started":"2022-09-06T00:36:53.133849Z","shell.execute_reply":"2022-09-06T00:36:53.133875Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 3yr window, trials=20, cv_reg=0.03: 0.88%. runs 1 hr.\n# 3yr, t=40, cv_reg=0.04: 0.96%.\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.135461Z","iopub.status.idle":"2022-09-06T00:36:53.136216Z","shell.execute_reply.started":"2022-09-06T00:36:53.135933Z","shell.execute_reply":"2022-09-06T00:36:53.135958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(X_train, X_val, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:45:09.378763Z","iopub.execute_input":"2022-09-06T00:45:09.379158Z","iopub.status.idle":"2022-09-06T00:45:09.707560Z","shell.execute_reply.started":"2022-09-06T00:45:09.379127Z","shell.execute_reply":"2022-09-06T00:45:09.706650Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n0         2.440252     0.030032 -1.497142  1.080837  0.724789  0.010338   \n1         1.943299    -0.222745 -1.497142  1.080837  0.724789  0.010338   \n2         2.107274    -0.246470 -1.497142  1.080837  0.724789  0.010338   \n3         1.685620    -0.374008 -1.497142  1.080837  0.724789  0.010338   \n4         0.690976    -0.960773 -1.116726  0.846425  0.659979 -0.425881   \n...            ...          ...       ...       ...       ...       ...   \n76883    -0.566013    -0.667712  0.154976 -0.449011  0.225954 -0.367814   \n76884    -0.611810    -0.723772  0.154976 -0.449011  0.225954 -0.367814   \n76885    -0.605239    -0.659530  0.154976 -0.449011  0.225954 -0.367814   \n76886    -0.636869     0.194059  0.154976 -0.449011  0.225954 -0.367814   \n76887    -0.612396     1.516430  0.154976 -0.449011  0.225954 -0.367814   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n0       -0.001164    -0.769063   0.061164       -0.521796      -0.613313   \n1       -0.101503    -0.679058   0.111973       -0.476189      -0.779485   \n2       -0.589553    -0.665144   0.154523       -0.408752      -0.524974   \n3       -0.618918    -0.822612   0.211327       -0.498429      -0.506168   \n4       -1.749070    -0.921894   0.244523        0.492161       0.500216   \n...           ...          ...        ...             ...            ...   \n76883    0.612983     0.285701   1.814529        0.666135       0.386534   \n76884   -0.496413    -0.259853   1.815901       -0.143222      -0.002847   \n76885   -0.174600    -0.146213   1.836054       -0.038214       0.183720   \n76886   -0.157636    -0.246570   1.827610       -0.591596      -0.602846   \n76887   -0.217149    -0.008820   1.811085       -0.820075      -0.854234   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n0          0.588274 -0.300375   -0.346009   -0.753732    -0.858156   \n1          0.796765  0.336568    0.151111   -0.611420    -0.768969   \n2          0.808655 -0.458075   -0.365669   -0.517451    -0.719170   \n3          0.804064 -0.440318   -0.506501   -0.577175    -0.709441   \n4          0.802548 -0.766582    0.249959   -0.365264    -0.586161   \n...             ...       ...         ...         ...          ...   \n76883     -1.700196  0.520511    0.372631   -0.104971    -0.283154   \n76884     -1.605932 -0.449028   -0.208101   -0.059078    -0.341420   \n76885     -1.478466 -0.567504   -0.317410   -0.098314    -0.326505   \n76886     -1.377626 -0.812586   -0.835152   -0.279218    -0.371848   \n76887     -1.399491 -1.068270   -1.062484   -0.517437    -0.444693   \n\n       num__BAspr  num__size  num__lbm  num__lop  num__lgp  num__linv  \\\n0       -0.246214  -0.287970 -2.285884  0.420012  0.545504  -1.453573   \n1       -0.067952  -0.289231 -2.285884  0.420012  0.545504  -1.453573   \n2       -0.391055  -0.327156 -2.285884  0.420012  0.545504  -1.453573   \n3        0.015571  -0.361888 -2.285884  0.420012  0.545504  -1.453573   \n4       -0.355706  -0.554908 -1.350873  1.033453  0.734261  -0.196466   \n...           ...        ...       ...       ...       ...        ...   \n76883    0.578188  -1.096146 -0.204261 -0.003571  0.471093  -0.542937   \n76884    1.810455  -1.123628 -0.204261 -0.003571  0.471093  -0.542937   \n76885    4.784910  -1.130472 -0.204261 -0.003571  0.471093  -0.542937   \n76886    0.264961  -1.136254 -0.204261 -0.003571  0.471093  -0.542937   \n76887   -0.264628  -1.145775 -0.204261 -0.003571  0.471093  -0.542937   \n\n       num__llme  num__l1amhd  num__l1MAX  num__l1BAspr  num__l3amhd  \\\n0      -0.079910     0.006677   -0.437054     -0.263244    -0.017419   \n1      -0.103939     0.066074   -0.314577     -0.248255    -0.011362   \n2      -0.115354     0.117049    0.323205     -0.070163     0.018450   \n3      -0.092495     0.159737   -0.472485     -0.392958     0.078257   \n4      -0.107086     0.216727   -0.454704      0.013279     0.129583   \n...          ...          ...         ...           ...          ...   \n76883  -1.281176     1.768226   -0.382531      4.403380     1.779250   \n76884  -1.115603     1.825157    0.507389      0.575359     1.787660   \n76885  -1.172635     1.826533   -0.463426      1.806448     1.792151   \n76886  -1.154434     1.846752   -0.582058      4.778059     1.849475   \n76887  -1.218224     1.838280   -0.827462      0.262431     1.850861   \n\n       num__l3MAX  num__l3BAspr  num__l6amhd  num__l6MAX  num__l6BAspr  \\\n0       -0.263946     -0.307586    -0.080405    0.615517     -0.196028   \n1       -1.055385     -0.403408    -0.066179   -0.733851     -0.412131   \n2       -0.444897     -0.264606    -0.061915   -0.784494     -0.392271   \n3       -0.321323     -0.249523     0.002885   -0.268268     -0.303054   \n4        0.322169     -0.070310     0.009010   -1.065697     -0.400682   \n...           ...           ...          ...         ...           ...   \n76883    0.919108      0.244168     1.925952   -0.366884      2.330492   \n76884    1.129089     -0.080817     1.849914   -0.779174      0.290555   \n76885   -0.389885      4.431401     1.829081   -0.505598      0.306861   \n76886    0.508003      0.579277     1.819658    0.923738      0.259099   \n76887   -0.471504      1.818117     1.828163    1.135308     -0.072012   \n\n       num__l12amhd  num__l12MAX  num__l12BAspr  num__l12mom122  \\\n0         -0.055520    -0.437054      -0.300325        1.723461   \n1         -0.085070    -0.314577      -0.258891        0.909845   \n2         -0.064350     0.323205      -0.377451        0.554145   \n3         -0.075501    -0.472485      -0.344694        0.702872   \n4         -0.079914    -0.454704      -0.286153        0.816731   \n...             ...          ...            ...             ...   \n76883      1.981934    -0.382531       2.973730       -1.310315   \n76884      1.990553     0.507389       0.907904       -1.211626   \n76885      1.990080    -0.463426       1.463785       -0.708503   \n76886      1.995408    -0.582058       1.706591       -0.915727   \n76887      2.011370    -0.827462       0.550428        0.462225   \n\n       num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n0              -0.612851         -0.587156        -0.248782      -0.897198   \n1              -0.555438         -0.378270        -0.329453      -0.907155   \n2              -0.862790         -0.815012        -0.193126      -0.992504   \n3              -0.546726         -0.572587         0.058540      -0.900193   \n4              -0.796444         -0.772408         0.374460      -0.883001   \n...                  ...               ...              ...            ...   \n76883           0.503558          0.788956        -1.184905       0.752818   \n76884           0.495542          0.588720        -1.014403       0.106444   \n76885          -0.311360         -0.419508        -0.842612       0.006417   \n76886           0.107819          0.304902        -0.620406      -0.061611   \n76887           0.356451          0.487998        -0.286047      -0.059314   \n\n       num__l12vol12m  num__amhd_miss  num__BAspr_miss  cat__ind_1.0  \\\n0           -0.999512       -0.178017        -0.078007           0.0   \n1           -0.987610       -0.178017        -0.078007           0.0   \n2           -0.974490       -0.178017        -0.078007           0.0   \n3           -0.950343       -0.178017        -0.078007           0.0   \n4           -0.930604       -0.178017        -0.078007           0.0   \n...               ...             ...              ...           ...   \n76883        1.244635       -0.178017        -0.078007           0.0   \n76884        1.229502       -0.178017        -0.078007           0.0   \n76885        1.229408       -0.178017        -0.078007           0.0   \n76886        1.250421       -0.178017        -0.078007           0.0   \n76887        1.007059       -0.178017        -0.078007           0.0   \n\n       cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  \\\n0               0.0           0.0           0.0           0.0           0.0   \n1               0.0           0.0           0.0           0.0           0.0   \n2               0.0           0.0           0.0           0.0           0.0   \n3               0.0           0.0           0.0           0.0           0.0   \n4               0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n76883           0.0           0.0           0.0           0.0           0.0   \n76884           0.0           0.0           0.0           0.0           0.0   \n76885           0.0           0.0           0.0           0.0           0.0   \n76886           0.0           0.0           0.0           0.0           0.0   \n76887           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n0               0.0           0.0           0.0            0.0            0.0   \n1               0.0           0.0           0.0            0.0            0.0   \n2               0.0           0.0           0.0            0.0            0.0   \n3               0.0           0.0           0.0            0.0            0.0   \n4               0.0           0.0           0.0            0.0            0.0   \n...             ...           ...           ...            ...            ...   \n76883           0.0           0.0           0.0            0.0            0.0   \n76884           0.0           0.0           0.0            0.0            0.0   \n76885           0.0           0.0           0.0            0.0            0.0   \n76886           0.0           0.0           0.0            0.0            0.0   \n76887           0.0           0.0           0.0            0.0            0.0   \n\n       cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n0                0.0            0.0            0.0            1.0   \n1                0.0            0.0            0.0            1.0   \n2                0.0            0.0            0.0            1.0   \n3                0.0            0.0            0.0            1.0   \n4                0.0            0.0            0.0            1.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            1.0            0.0   \n76884            0.0            0.0            1.0            0.0   \n76885            0.0            0.0            1.0            0.0   \n76886            0.0            0.0            1.0            0.0   \n76887            0.0            0.0            1.0            0.0   \n\n       cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  cat__ind_47.0  \\\n0                0.0            0.0            0.0            0.0   \n1                0.0            0.0            0.0            0.0   \n2                0.0            0.0            0.0            0.0   \n3                0.0            0.0            0.0            0.0   \n4                0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76883            0.0            0.0            0.0            0.0   \n76884            0.0            0.0            0.0            0.0   \n76885            0.0            0.0            0.0            0.0   \n76886            0.0            0.0            0.0            0.0   \n76887            0.0            0.0            0.0            0.0   \n\n       cat__ind_48.0  cat__ind_49.0  \n0                0.0            0.0  \n1                0.0            0.0  \n2                0.0            0.0  \n3                0.0            0.0  \n4                0.0            0.0  \n...              ...            ...  \n76883            0.0            0.0  \n76884            0.0            0.0  \n76885            0.0            0.0  \n76886            0.0            0.0  \n76887            0.0            0.0  \n\n[69441 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2.440252</td>\n      <td>0.030032</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.001164</td>\n      <td>-0.769063</td>\n      <td>0.061164</td>\n      <td>-0.521796</td>\n      <td>-0.613313</td>\n      <td>0.588274</td>\n      <td>-0.300375</td>\n      <td>-0.346009</td>\n      <td>-0.753732</td>\n      <td>-0.858156</td>\n      <td>-0.246214</td>\n      <td>-0.287970</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.079910</td>\n      <td>0.006677</td>\n      <td>-0.437054</td>\n      <td>-0.263244</td>\n      <td>-0.017419</td>\n      <td>-0.263946</td>\n      <td>-0.307586</td>\n      <td>-0.080405</td>\n      <td>0.615517</td>\n      <td>-0.196028</td>\n      <td>-0.055520</td>\n      <td>-0.437054</td>\n      <td>-0.300325</td>\n      <td>1.723461</td>\n      <td>-0.612851</td>\n      <td>-0.587156</td>\n      <td>-0.248782</td>\n      <td>-0.897198</td>\n      <td>-0.999512</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1.943299</td>\n      <td>-0.222745</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.101503</td>\n      <td>-0.679058</td>\n      <td>0.111973</td>\n      <td>-0.476189</td>\n      <td>-0.779485</td>\n      <td>0.796765</td>\n      <td>0.336568</td>\n      <td>0.151111</td>\n      <td>-0.611420</td>\n      <td>-0.768969</td>\n      <td>-0.067952</td>\n      <td>-0.289231</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.103939</td>\n      <td>0.066074</td>\n      <td>-0.314577</td>\n      <td>-0.248255</td>\n      <td>-0.011362</td>\n      <td>-1.055385</td>\n      <td>-0.403408</td>\n      <td>-0.066179</td>\n      <td>-0.733851</td>\n      <td>-0.412131</td>\n      <td>-0.085070</td>\n      <td>-0.314577</td>\n      <td>-0.258891</td>\n      <td>0.909845</td>\n      <td>-0.555438</td>\n      <td>-0.378270</td>\n      <td>-0.329453</td>\n      <td>-0.907155</td>\n      <td>-0.987610</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2.107274</td>\n      <td>-0.246470</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.589553</td>\n      <td>-0.665144</td>\n      <td>0.154523</td>\n      <td>-0.408752</td>\n      <td>-0.524974</td>\n      <td>0.808655</td>\n      <td>-0.458075</td>\n      <td>-0.365669</td>\n      <td>-0.517451</td>\n      <td>-0.719170</td>\n      <td>-0.391055</td>\n      <td>-0.327156</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.115354</td>\n      <td>0.117049</td>\n      <td>0.323205</td>\n      <td>-0.070163</td>\n      <td>0.018450</td>\n      <td>-0.444897</td>\n      <td>-0.264606</td>\n      <td>-0.061915</td>\n      <td>-0.784494</td>\n      <td>-0.392271</td>\n      <td>-0.064350</td>\n      <td>0.323205</td>\n      <td>-0.377451</td>\n      <td>0.554145</td>\n      <td>-0.862790</td>\n      <td>-0.815012</td>\n      <td>-0.193126</td>\n      <td>-0.992504</td>\n      <td>-0.974490</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1.685620</td>\n      <td>-0.374008</td>\n      <td>-1.497142</td>\n      <td>1.080837</td>\n      <td>0.724789</td>\n      <td>0.010338</td>\n      <td>-0.618918</td>\n      <td>-0.822612</td>\n      <td>0.211327</td>\n      <td>-0.498429</td>\n      <td>-0.506168</td>\n      <td>0.804064</td>\n      <td>-0.440318</td>\n      <td>-0.506501</td>\n      <td>-0.577175</td>\n      <td>-0.709441</td>\n      <td>0.015571</td>\n      <td>-0.361888</td>\n      <td>-2.285884</td>\n      <td>0.420012</td>\n      <td>0.545504</td>\n      <td>-1.453573</td>\n      <td>-0.092495</td>\n      <td>0.159737</td>\n      <td>-0.472485</td>\n      <td>-0.392958</td>\n      <td>0.078257</td>\n      <td>-0.321323</td>\n      <td>-0.249523</td>\n      <td>0.002885</td>\n      <td>-0.268268</td>\n      <td>-0.303054</td>\n      <td>-0.075501</td>\n      <td>-0.472485</td>\n      <td>-0.344694</td>\n      <td>0.702872</td>\n      <td>-0.546726</td>\n      <td>-0.572587</td>\n      <td>0.058540</td>\n      <td>-0.900193</td>\n      <td>-0.950343</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.690976</td>\n      <td>-0.960773</td>\n      <td>-1.116726</td>\n      <td>0.846425</td>\n      <td>0.659979</td>\n      <td>-0.425881</td>\n      <td>-1.749070</td>\n      <td>-0.921894</td>\n      <td>0.244523</td>\n      <td>0.492161</td>\n      <td>0.500216</td>\n      <td>0.802548</td>\n      <td>-0.766582</td>\n      <td>0.249959</td>\n      <td>-0.365264</td>\n      <td>-0.586161</td>\n      <td>-0.355706</td>\n      <td>-0.554908</td>\n      <td>-1.350873</td>\n      <td>1.033453</td>\n      <td>0.734261</td>\n      <td>-0.196466</td>\n      <td>-0.107086</td>\n      <td>0.216727</td>\n      <td>-0.454704</td>\n      <td>0.013279</td>\n      <td>0.129583</td>\n      <td>0.322169</td>\n      <td>-0.070310</td>\n      <td>0.009010</td>\n      <td>-1.065697</td>\n      <td>-0.400682</td>\n      <td>-0.079914</td>\n      <td>-0.454704</td>\n      <td>-0.286153</td>\n      <td>0.816731</td>\n      <td>-0.796444</td>\n      <td>-0.772408</td>\n      <td>0.374460</td>\n      <td>-0.883001</td>\n      <td>-0.930604</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76883</th>\n      <td>-0.566013</td>\n      <td>-0.667712</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>0.612983</td>\n      <td>0.285701</td>\n      <td>1.814529</td>\n      <td>0.666135</td>\n      <td>0.386534</td>\n      <td>-1.700196</td>\n      <td>0.520511</td>\n      <td>0.372631</td>\n      <td>-0.104971</td>\n      <td>-0.283154</td>\n      <td>0.578188</td>\n      <td>-1.096146</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.281176</td>\n      <td>1.768226</td>\n      <td>-0.382531</td>\n      <td>4.403380</td>\n      <td>1.779250</td>\n      <td>0.919108</td>\n      <td>0.244168</td>\n      <td>1.925952</td>\n      <td>-0.366884</td>\n      <td>2.330492</td>\n      <td>1.981934</td>\n      <td>-0.382531</td>\n      <td>2.973730</td>\n      <td>-1.310315</td>\n      <td>0.503558</td>\n      <td>0.788956</td>\n      <td>-1.184905</td>\n      <td>0.752818</td>\n      <td>1.244635</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76884</th>\n      <td>-0.611810</td>\n      <td>-0.723772</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.496413</td>\n      <td>-0.259853</td>\n      <td>1.815901</td>\n      <td>-0.143222</td>\n      <td>-0.002847</td>\n      <td>-1.605932</td>\n      <td>-0.449028</td>\n      <td>-0.208101</td>\n      <td>-0.059078</td>\n      <td>-0.341420</td>\n      <td>1.810455</td>\n      <td>-1.123628</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.115603</td>\n      <td>1.825157</td>\n      <td>0.507389</td>\n      <td>0.575359</td>\n      <td>1.787660</td>\n      <td>1.129089</td>\n      <td>-0.080817</td>\n      <td>1.849914</td>\n      <td>-0.779174</td>\n      <td>0.290555</td>\n      <td>1.990553</td>\n      <td>0.507389</td>\n      <td>0.907904</td>\n      <td>-1.211626</td>\n      <td>0.495542</td>\n      <td>0.588720</td>\n      <td>-1.014403</td>\n      <td>0.106444</td>\n      <td>1.229502</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76885</th>\n      <td>-0.605239</td>\n      <td>-0.659530</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.174600</td>\n      <td>-0.146213</td>\n      <td>1.836054</td>\n      <td>-0.038214</td>\n      <td>0.183720</td>\n      <td>-1.478466</td>\n      <td>-0.567504</td>\n      <td>-0.317410</td>\n      <td>-0.098314</td>\n      <td>-0.326505</td>\n      <td>4.784910</td>\n      <td>-1.130472</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.172635</td>\n      <td>1.826533</td>\n      <td>-0.463426</td>\n      <td>1.806448</td>\n      <td>1.792151</td>\n      <td>-0.389885</td>\n      <td>4.431401</td>\n      <td>1.829081</td>\n      <td>-0.505598</td>\n      <td>0.306861</td>\n      <td>1.990080</td>\n      <td>-0.463426</td>\n      <td>1.463785</td>\n      <td>-0.708503</td>\n      <td>-0.311360</td>\n      <td>-0.419508</td>\n      <td>-0.842612</td>\n      <td>0.006417</td>\n      <td>1.229408</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76886</th>\n      <td>-0.636869</td>\n      <td>0.194059</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.157636</td>\n      <td>-0.246570</td>\n      <td>1.827610</td>\n      <td>-0.591596</td>\n      <td>-0.602846</td>\n      <td>-1.377626</td>\n      <td>-0.812586</td>\n      <td>-0.835152</td>\n      <td>-0.279218</td>\n      <td>-0.371848</td>\n      <td>0.264961</td>\n      <td>-1.136254</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.154434</td>\n      <td>1.846752</td>\n      <td>-0.582058</td>\n      <td>4.778059</td>\n      <td>1.849475</td>\n      <td>0.508003</td>\n      <td>0.579277</td>\n      <td>1.819658</td>\n      <td>0.923738</td>\n      <td>0.259099</td>\n      <td>1.995408</td>\n      <td>-0.582058</td>\n      <td>1.706591</td>\n      <td>-0.915727</td>\n      <td>0.107819</td>\n      <td>0.304902</td>\n      <td>-0.620406</td>\n      <td>-0.061611</td>\n      <td>1.250421</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76887</th>\n      <td>-0.612396</td>\n      <td>1.516430</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.217149</td>\n      <td>-0.008820</td>\n      <td>1.811085</td>\n      <td>-0.820075</td>\n      <td>-0.854234</td>\n      <td>-1.399491</td>\n      <td>-1.068270</td>\n      <td>-1.062484</td>\n      <td>-0.517437</td>\n      <td>-0.444693</td>\n      <td>-0.264628</td>\n      <td>-1.145775</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.218224</td>\n      <td>1.838280</td>\n      <td>-0.827462</td>\n      <td>0.262431</td>\n      <td>1.850861</td>\n      <td>-0.471504</td>\n      <td>1.818117</td>\n      <td>1.828163</td>\n      <td>1.135308</td>\n      <td>-0.072012</td>\n      <td>2.011370</td>\n      <td>-0.827462</td>\n      <td>0.550428</td>\n      <td>0.462225</td>\n      <td>0.356451</td>\n      <td>0.487998</td>\n      <td>-0.286047</td>\n      <td>-0.059314</td>\n      <td>1.007059</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>69441 rows  92 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n36       -0.447636     1.503621 -0.616017  0.807145  0.566597 -0.676510   \n76        1.082342     2.932444  0.409891 -0.407999  0.054079 -0.291606   \n116      -0.038815     1.490517 -0.764228  1.661493  1.180223 -0.148263   \n183       0.725718     2.932444  1.004495 -1.707367 -0.945570 -0.468686   \n223       1.156386     1.442577 -1.101432  1.080693  0.088267 -0.321219   \n...            ...          ...       ...       ...       ...       ...   \n76808    -0.208849     1.877030 -0.097743  0.250530 -0.844100  1.878685   \n76820    -0.208849     0.180506 -0.479475  0.136375 -0.134496 -0.308538   \n76832    -0.208849    -0.030012 -0.391261  0.852059 -0.361299  1.687970   \n76848    -0.208849     2.667662 -1.806515 -1.257885  1.031139 -0.502160   \n76888    -0.681134     0.851401  0.154976 -0.449011  0.225954 -0.367814   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n36       0.041370    -0.444748   0.286301       -0.897920      -0.885205   \n76      -0.494304     2.201976   1.496388       -0.425977      -0.353926   \n116      0.073340     0.476343   0.792124       -1.047382      -0.969301   \n183      1.596658     2.201976   1.897724        1.685851       0.595604   \n223      0.139841     0.631494  -2.086450       -1.158357      -1.062442   \n...           ...          ...        ...             ...            ...   \n76808   -0.188176     1.104220  -0.374849       -0.970493      -0.937521   \n76820   -1.749070    -0.208232  -0.673038        2.482681       2.569855   \n76832    0.340067    -0.707754  -0.263601       -0.838144      -0.726073   \n76848    0.796485     2.201976  -0.277872       -0.478127      -0.643053   \n76888   -0.197249    -0.371359   1.771866       -0.237781      -0.182437   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n36         0.669968 -0.996079   -0.899538   -0.823745    -0.714925   \n76        -0.845733 -0.672869   -0.675338   -0.443324    -0.122958   \n116       -1.318185 -1.001774   -1.186375   -1.471881    -1.437878   \n183       -0.920109  2.095057    1.318192    2.145037     2.084527   \n223       -0.191328 -0.798725   -0.987976   -1.260964    -1.413439   \n...             ...       ...         ...         ...          ...   \n76808      0.150749 -0.914616   -1.141697   -0.557570    -0.777044   \n76820     -0.799717 -0.861063    1.967085   -0.098038    -0.578841   \n76832      0.518787 -1.026050   -0.958595   -0.389652    -0.598841   \n76848      0.846803 -0.448241   -0.343692   -0.448423    -0.564333   \n76888     -1.507184 -0.333492   -0.526458   -0.562515    -0.479081   \n\n       num__BAspr  num__size  num__lbm  num__lop  num__lgp  num__linv  \\\n36      -0.357029  -0.373732 -0.394250 -0.569229 -0.440817   0.242727   \n76       0.004582  -1.042218  0.421057  0.116165  0.326541  -1.125382   \n116     -0.154287  -0.845688 -0.283119  1.622306  1.928796  -1.026000   \n183     -0.322355  -1.567221  1.564946 -2.080395 -1.770265  -0.728135   \n223     -0.466201   2.825176 -0.664197  0.890849 -0.008299   0.969864   \n...           ...        ...       ...       ...       ...        ...   \n76808   -0.434216   0.249006  0.140510  0.324503 -0.716776   1.592674   \n76820   -0.449947   0.357319 -0.000264 -0.144985 -0.379309   1.387099   \n76832   -0.409310  -0.068315  0.117335  1.076196 -0.028161   1.083701   \n76848   -0.464551   0.750689 -0.923249 -1.272632  0.706670  -0.675277   \n76888    0.067450  -1.151990 -0.204261 -0.003571  0.471093  -0.542937   \n\n       num__llme  num__l1amhd  num__l1MAX  num__l1BAspr  num__l3amhd  \\\n36     -0.275573     0.267270   -0.269506     -0.318411     0.205912   \n76     -1.573113     1.527156   -0.594924      0.712950     1.761770   \n116    -1.010973     0.805248   -0.393897     -0.194811     0.822153   \n183    -2.319771     1.908623    0.307918      0.395609     2.123751   \n223     2.717759    -2.082807   -1.103496     -0.468662    -2.064685   \n...          ...          ...         ...           ...          ...   \n76808   0.001962    -0.378625   -0.626435     -0.437696    -0.401772   \n76820   0.508948    -0.659527   -0.753338     -0.460048    -0.628558   \n76832   0.048485    -0.272724   -0.908995     -0.445693    -0.278341   \n76848   0.285313    -0.255770   -0.190112     -0.466064    -0.178502   \n76888  -1.137309     1.821702   -1.083483     -0.266651     1.871219   \n\n       num__l3MAX  num__l3BAspr  num__l6amhd  num__l6MAX  num__l6BAspr  \\\n36      -0.104132     -0.331265     0.072720   -0.946680     -0.412905   \n76      -0.679498      0.038660     1.822095    0.159410      0.331043   \n116     -1.143382     -0.218875     0.889428   -0.854871     -0.437584   \n183      2.460112      0.569178     2.168014    2.476404      4.909389   \n223     -1.156894     -0.468742    -2.049128   -1.029393     -0.463778   \n...           ...           ...          ...         ...           ...   \n76808   -0.636862     -0.263074    -0.374597   -0.687537     -0.456005   \n76820    0.006270     -0.462151    -0.576820   -0.657888     -0.354463   \n76832   -0.623313     -0.452715    -0.304512   -0.314697     -0.448004   \n76848    0.670976     -0.467300    -0.090091    0.268836     -0.460818   \n76888   -0.591198      4.808439     1.832704   -0.395160      4.525245   \n\n       num__l12amhd  num__l12MAX  num__l12BAspr  num__l12mom122  \\\n36         0.484859    -0.269506      -0.423203        2.672025   \n76         2.221642    -0.594924       0.995690        1.607994   \n116        1.100826    -0.393897      -0.315926        1.487194   \n183       -0.140420     0.307918       4.010015        2.157728   \n223       -1.998789    -1.103496      -0.456639        1.337993   \n...             ...          ...            ...             ...   \n76808     -0.054501    -0.626435      -0.255555        1.189582   \n76820     -0.463467    -0.753338      -0.432884        1.033342   \n76832     -0.192267    -0.908995      -0.424018        2.672025   \n76848      0.050297    -0.190112      -0.402299        1.856043   \n76888      2.050753    -1.083483       2.233048        2.518535   \n\n       num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n36             -0.595621         -0.602738         0.655205      -0.705329   \n76             -0.186524         -0.124067        -2.448014       0.095269   \n116            -1.219011         -1.187009        -0.936426      -1.256309   \n183             1.577745          1.776789         3.967493       2.218228   \n223            -1.117885         -1.013970        -0.382679      -1.246534   \n...                  ...               ...              ...            ...   \n76808          -0.540612         -0.551645         0.953966      -0.311244   \n76820          -0.835113         -0.701370        -0.737806      -0.829634   \n76832          -0.765681         -0.780979         0.172734      -0.440386   \n76848           0.576474          0.139332         0.663355      -0.276019   \n76888           0.155802          0.049098        -0.276814      -0.146158   \n\n       num__l12vol12m  num__amhd_miss  num__BAspr_miss  cat__ind_1.0  \\\n36          -0.016263       -0.178017        -0.078007           0.0   \n76           0.861902       -0.178017        -0.078007           0.0   \n116         -0.357296       -0.178017        -0.078007           0.0   \n183          2.230892       -0.178017        -0.078007           0.0   \n223         -1.062468       -0.178017        -0.078007           0.0   \n...               ...             ...              ...           ...   \n76808        0.013890       -0.178017        -0.078007           0.0   \n76820       -0.770382       -0.178017        -0.078007           0.0   \n76832       -0.020364       -0.178017        -0.078007           0.0   \n76848        0.117571       -0.178017        -0.078007           0.0   \n76888        0.813927       -0.178017        -0.078007           0.0   \n\n       cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  \\\n36              0.0           0.0           0.0           0.0           0.0   \n76              0.0           0.0           0.0           0.0           0.0   \n116             1.0           0.0           0.0           0.0           0.0   \n183             0.0           0.0           0.0           0.0           0.0   \n223             0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n76808           0.0           0.0           0.0           0.0           0.0   \n76820           0.0           0.0           0.0           0.0           0.0   \n76832           0.0           0.0           0.0           0.0           0.0   \n76848           0.0           0.0           0.0           0.0           0.0   \n76888           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n36              0.0           0.0           0.0            0.0            0.0   \n76              0.0           0.0           0.0            0.0            0.0   \n116             0.0           0.0           0.0            0.0            0.0   \n183             0.0           0.0           0.0            0.0            0.0   \n223             0.0           0.0           0.0            0.0            0.0   \n...             ...           ...           ...            ...            ...   \n76808           0.0           0.0           0.0            0.0            0.0   \n76820           0.0           0.0           0.0            0.0            0.0   \n76832           0.0           0.0           0.0            0.0            0.0   \n76848           0.0           0.0           0.0            0.0            0.0   \n76888           0.0           0.0           0.0            0.0            0.0   \n\n       cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n36               0.0            0.0            0.0            1.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            1.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            1.0            0.0   \n\n       cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            1.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            1.0            0.0   \n76820            0.0            0.0            1.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              1.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            1.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            1.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  cat__ind_47.0  \\\n36               0.0            0.0            0.0            0.0   \n76               0.0            0.0            0.0            0.0   \n116              0.0            0.0            0.0            0.0   \n183              0.0            0.0            0.0            0.0   \n223              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76808            0.0            0.0            0.0            0.0   \n76820            0.0            0.0            0.0            0.0   \n76832            0.0            0.0            0.0            0.0   \n76848            0.0            0.0            0.0            0.0   \n76888            0.0            0.0            0.0            0.0   \n\n       cat__ind_48.0  cat__ind_49.0  \n36               0.0            0.0  \n76               0.0            0.0  \n116              0.0            0.0  \n183              0.0            0.0  \n223              0.0            0.0  \n...              ...            ...  \n76808            0.0            0.0  \n76820            0.0            0.0  \n76832            0.0            0.0  \n76848            0.0            0.0  \n76888            0.0            0.0  \n\n[1881 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36</th>\n      <td>-0.447636</td>\n      <td>1.503621</td>\n      <td>-0.616017</td>\n      <td>0.807145</td>\n      <td>0.566597</td>\n      <td>-0.676510</td>\n      <td>0.041370</td>\n      <td>-0.444748</td>\n      <td>0.286301</td>\n      <td>-0.897920</td>\n      <td>-0.885205</td>\n      <td>0.669968</td>\n      <td>-0.996079</td>\n      <td>-0.899538</td>\n      <td>-0.823745</td>\n      <td>-0.714925</td>\n      <td>-0.357029</td>\n      <td>-0.373732</td>\n      <td>-0.394250</td>\n      <td>-0.569229</td>\n      <td>-0.440817</td>\n      <td>0.242727</td>\n      <td>-0.275573</td>\n      <td>0.267270</td>\n      <td>-0.269506</td>\n      <td>-0.318411</td>\n      <td>0.205912</td>\n      <td>-0.104132</td>\n      <td>-0.331265</td>\n      <td>0.072720</td>\n      <td>-0.946680</td>\n      <td>-0.412905</td>\n      <td>0.484859</td>\n      <td>-0.269506</td>\n      <td>-0.423203</td>\n      <td>2.672025</td>\n      <td>-0.595621</td>\n      <td>-0.602738</td>\n      <td>0.655205</td>\n      <td>-0.705329</td>\n      <td>-0.016263</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76</th>\n      <td>1.082342</td>\n      <td>2.932444</td>\n      <td>0.409891</td>\n      <td>-0.407999</td>\n      <td>0.054079</td>\n      <td>-0.291606</td>\n      <td>-0.494304</td>\n      <td>2.201976</td>\n      <td>1.496388</td>\n      <td>-0.425977</td>\n      <td>-0.353926</td>\n      <td>-0.845733</td>\n      <td>-0.672869</td>\n      <td>-0.675338</td>\n      <td>-0.443324</td>\n      <td>-0.122958</td>\n      <td>0.004582</td>\n      <td>-1.042218</td>\n      <td>0.421057</td>\n      <td>0.116165</td>\n      <td>0.326541</td>\n      <td>-1.125382</td>\n      <td>-1.573113</td>\n      <td>1.527156</td>\n      <td>-0.594924</td>\n      <td>0.712950</td>\n      <td>1.761770</td>\n      <td>-0.679498</td>\n      <td>0.038660</td>\n      <td>1.822095</td>\n      <td>0.159410</td>\n      <td>0.331043</td>\n      <td>2.221642</td>\n      <td>-0.594924</td>\n      <td>0.995690</td>\n      <td>1.607994</td>\n      <td>-0.186524</td>\n      <td>-0.124067</td>\n      <td>-2.448014</td>\n      <td>0.095269</td>\n      <td>0.861902</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>116</th>\n      <td>-0.038815</td>\n      <td>1.490517</td>\n      <td>-0.764228</td>\n      <td>1.661493</td>\n      <td>1.180223</td>\n      <td>-0.148263</td>\n      <td>0.073340</td>\n      <td>0.476343</td>\n      <td>0.792124</td>\n      <td>-1.047382</td>\n      <td>-0.969301</td>\n      <td>-1.318185</td>\n      <td>-1.001774</td>\n      <td>-1.186375</td>\n      <td>-1.471881</td>\n      <td>-1.437878</td>\n      <td>-0.154287</td>\n      <td>-0.845688</td>\n      <td>-0.283119</td>\n      <td>1.622306</td>\n      <td>1.928796</td>\n      <td>-1.026000</td>\n      <td>-1.010973</td>\n      <td>0.805248</td>\n      <td>-0.393897</td>\n      <td>-0.194811</td>\n      <td>0.822153</td>\n      <td>-1.143382</td>\n      <td>-0.218875</td>\n      <td>0.889428</td>\n      <td>-0.854871</td>\n      <td>-0.437584</td>\n      <td>1.100826</td>\n      <td>-0.393897</td>\n      <td>-0.315926</td>\n      <td>1.487194</td>\n      <td>-1.219011</td>\n      <td>-1.187009</td>\n      <td>-0.936426</td>\n      <td>-1.256309</td>\n      <td>-0.357296</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>183</th>\n      <td>0.725718</td>\n      <td>2.932444</td>\n      <td>1.004495</td>\n      <td>-1.707367</td>\n      <td>-0.945570</td>\n      <td>-0.468686</td>\n      <td>1.596658</td>\n      <td>2.201976</td>\n      <td>1.897724</td>\n      <td>1.685851</td>\n      <td>0.595604</td>\n      <td>-0.920109</td>\n      <td>2.095057</td>\n      <td>1.318192</td>\n      <td>2.145037</td>\n      <td>2.084527</td>\n      <td>-0.322355</td>\n      <td>-1.567221</td>\n      <td>1.564946</td>\n      <td>-2.080395</td>\n      <td>-1.770265</td>\n      <td>-0.728135</td>\n      <td>-2.319771</td>\n      <td>1.908623</td>\n      <td>0.307918</td>\n      <td>0.395609</td>\n      <td>2.123751</td>\n      <td>2.460112</td>\n      <td>0.569178</td>\n      <td>2.168014</td>\n      <td>2.476404</td>\n      <td>4.909389</td>\n      <td>-0.140420</td>\n      <td>0.307918</td>\n      <td>4.010015</td>\n      <td>2.157728</td>\n      <td>1.577745</td>\n      <td>1.776789</td>\n      <td>3.967493</td>\n      <td>2.218228</td>\n      <td>2.230892</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>223</th>\n      <td>1.156386</td>\n      <td>1.442577</td>\n      <td>-1.101432</td>\n      <td>1.080693</td>\n      <td>0.088267</td>\n      <td>-0.321219</td>\n      <td>0.139841</td>\n      <td>0.631494</td>\n      <td>-2.086450</td>\n      <td>-1.158357</td>\n      <td>-1.062442</td>\n      <td>-0.191328</td>\n      <td>-0.798725</td>\n      <td>-0.987976</td>\n      <td>-1.260964</td>\n      <td>-1.413439</td>\n      <td>-0.466201</td>\n      <td>2.825176</td>\n      <td>-0.664197</td>\n      <td>0.890849</td>\n      <td>-0.008299</td>\n      <td>0.969864</td>\n      <td>2.717759</td>\n      <td>-2.082807</td>\n      <td>-1.103496</td>\n      <td>-0.468662</td>\n      <td>-2.064685</td>\n      <td>-1.156894</td>\n      <td>-0.468742</td>\n      <td>-2.049128</td>\n      <td>-1.029393</td>\n      <td>-0.463778</td>\n      <td>-1.998789</td>\n      <td>-1.103496</td>\n      <td>-0.456639</td>\n      <td>1.337993</td>\n      <td>-1.117885</td>\n      <td>-1.013970</td>\n      <td>-0.382679</td>\n      <td>-1.246534</td>\n      <td>-1.062468</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76808</th>\n      <td>-0.208849</td>\n      <td>1.877030</td>\n      <td>-0.097743</td>\n      <td>0.250530</td>\n      <td>-0.844100</td>\n      <td>1.878685</td>\n      <td>-0.188176</td>\n      <td>1.104220</td>\n      <td>-0.374849</td>\n      <td>-0.970493</td>\n      <td>-0.937521</td>\n      <td>0.150749</td>\n      <td>-0.914616</td>\n      <td>-1.141697</td>\n      <td>-0.557570</td>\n      <td>-0.777044</td>\n      <td>-0.434216</td>\n      <td>0.249006</td>\n      <td>0.140510</td>\n      <td>0.324503</td>\n      <td>-0.716776</td>\n      <td>1.592674</td>\n      <td>0.001962</td>\n      <td>-0.378625</td>\n      <td>-0.626435</td>\n      <td>-0.437696</td>\n      <td>-0.401772</td>\n      <td>-0.636862</td>\n      <td>-0.263074</td>\n      <td>-0.374597</td>\n      <td>-0.687537</td>\n      <td>-0.456005</td>\n      <td>-0.054501</td>\n      <td>-0.626435</td>\n      <td>-0.255555</td>\n      <td>1.189582</td>\n      <td>-0.540612</td>\n      <td>-0.551645</td>\n      <td>0.953966</td>\n      <td>-0.311244</td>\n      <td>0.013890</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76820</th>\n      <td>-0.208849</td>\n      <td>0.180506</td>\n      <td>-0.479475</td>\n      <td>0.136375</td>\n      <td>-0.134496</td>\n      <td>-0.308538</td>\n      <td>-1.749070</td>\n      <td>-0.208232</td>\n      <td>-0.673038</td>\n      <td>2.482681</td>\n      <td>2.569855</td>\n      <td>-0.799717</td>\n      <td>-0.861063</td>\n      <td>1.967085</td>\n      <td>-0.098038</td>\n      <td>-0.578841</td>\n      <td>-0.449947</td>\n      <td>0.357319</td>\n      <td>-0.000264</td>\n      <td>-0.144985</td>\n      <td>-0.379309</td>\n      <td>1.387099</td>\n      <td>0.508948</td>\n      <td>-0.659527</td>\n      <td>-0.753338</td>\n      <td>-0.460048</td>\n      <td>-0.628558</td>\n      <td>0.006270</td>\n      <td>-0.462151</td>\n      <td>-0.576820</td>\n      <td>-0.657888</td>\n      <td>-0.354463</td>\n      <td>-0.463467</td>\n      <td>-0.753338</td>\n      <td>-0.432884</td>\n      <td>1.033342</td>\n      <td>-0.835113</td>\n      <td>-0.701370</td>\n      <td>-0.737806</td>\n      <td>-0.829634</td>\n      <td>-0.770382</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76832</th>\n      <td>-0.208849</td>\n      <td>-0.030012</td>\n      <td>-0.391261</td>\n      <td>0.852059</td>\n      <td>-0.361299</td>\n      <td>1.687970</td>\n      <td>0.340067</td>\n      <td>-0.707754</td>\n      <td>-0.263601</td>\n      <td>-0.838144</td>\n      <td>-0.726073</td>\n      <td>0.518787</td>\n      <td>-1.026050</td>\n      <td>-0.958595</td>\n      <td>-0.389652</td>\n      <td>-0.598841</td>\n      <td>-0.409310</td>\n      <td>-0.068315</td>\n      <td>0.117335</td>\n      <td>1.076196</td>\n      <td>-0.028161</td>\n      <td>1.083701</td>\n      <td>0.048485</td>\n      <td>-0.272724</td>\n      <td>-0.908995</td>\n      <td>-0.445693</td>\n      <td>-0.278341</td>\n      <td>-0.623313</td>\n      <td>-0.452715</td>\n      <td>-0.304512</td>\n      <td>-0.314697</td>\n      <td>-0.448004</td>\n      <td>-0.192267</td>\n      <td>-0.908995</td>\n      <td>-0.424018</td>\n      <td>2.672025</td>\n      <td>-0.765681</td>\n      <td>-0.780979</td>\n      <td>0.172734</td>\n      <td>-0.440386</td>\n      <td>-0.020364</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76848</th>\n      <td>-0.208849</td>\n      <td>2.667662</td>\n      <td>-1.806515</td>\n      <td>-1.257885</td>\n      <td>1.031139</td>\n      <td>-0.502160</td>\n      <td>0.796485</td>\n      <td>2.201976</td>\n      <td>-0.277872</td>\n      <td>-0.478127</td>\n      <td>-0.643053</td>\n      <td>0.846803</td>\n      <td>-0.448241</td>\n      <td>-0.343692</td>\n      <td>-0.448423</td>\n      <td>-0.564333</td>\n      <td>-0.464551</td>\n      <td>0.750689</td>\n      <td>-0.923249</td>\n      <td>-1.272632</td>\n      <td>0.706670</td>\n      <td>-0.675277</td>\n      <td>0.285313</td>\n      <td>-0.255770</td>\n      <td>-0.190112</td>\n      <td>-0.466064</td>\n      <td>-0.178502</td>\n      <td>0.670976</td>\n      <td>-0.467300</td>\n      <td>-0.090091</td>\n      <td>0.268836</td>\n      <td>-0.460818</td>\n      <td>0.050297</td>\n      <td>-0.190112</td>\n      <td>-0.402299</td>\n      <td>1.856043</td>\n      <td>0.576474</td>\n      <td>0.139332</td>\n      <td>0.663355</td>\n      <td>-0.276019</td>\n      <td>0.117571</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76888</th>\n      <td>-0.681134</td>\n      <td>0.851401</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.197249</td>\n      <td>-0.371359</td>\n      <td>1.771866</td>\n      <td>-0.237781</td>\n      <td>-0.182437</td>\n      <td>-1.507184</td>\n      <td>-0.333492</td>\n      <td>-0.526458</td>\n      <td>-0.562515</td>\n      <td>-0.479081</td>\n      <td>0.067450</td>\n      <td>-1.151990</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.137309</td>\n      <td>1.821702</td>\n      <td>-1.083483</td>\n      <td>-0.266651</td>\n      <td>1.871219</td>\n      <td>-0.591198</td>\n      <td>4.808439</td>\n      <td>1.832704</td>\n      <td>-0.395160</td>\n      <td>4.525245</td>\n      <td>2.050753</td>\n      <td>-1.083483</td>\n      <td>2.233048</td>\n      <td>2.518535</td>\n      <td>0.155802</td>\n      <td>0.049098</td>\n      <td>-0.276814</td>\n      <td>-0.146158</td>\n      <td>0.813927</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1881 rows  92 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"       num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n37       -0.370711     1.045747 -0.616017  0.807145  0.566597 -0.676510   \n77        1.562862     2.932444  0.409891 -0.407999  0.054079 -0.291606   \n117      -0.017971     1.548951 -0.764228  1.661493  1.180223 -0.148263   \n184       0.995702     2.932444  1.004495 -1.707367 -0.945570 -0.468686   \n224       1.147943     1.282408 -1.101432  1.080693  0.088267 -0.321219   \n...            ...          ...       ...       ...       ...       ...   \n76809    -0.208849     1.361380 -0.097743  0.250530 -0.844100  1.878685   \n76821    -0.208849    -0.112372 -0.479475  0.136375 -0.134496 -0.308538   \n76833    -0.208849    -0.300522 -0.391261  0.852059 -0.361299  1.687970   \n76849    -0.208849     1.938339 -1.806515 -1.257885  1.031139 -0.502160   \n76889    -0.690620     0.644627  0.154976 -0.449011  0.225954 -0.367814   \n\n       num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n37       0.375087     0.151844   0.324364       -0.926429      -0.835087   \n77       1.619434     1.765912   1.497371       -0.321803      -0.310475   \n117      0.001117     0.335290   0.784002       -1.173535      -1.074915   \n184     -0.118384     2.201976   1.897724        1.946351       2.146835   \n224      0.057190     0.589992  -2.076221       -1.190013      -1.156021   \n...           ...          ...        ...             ...            ...   \n76809   -1.457988     0.954821  -0.377764       -0.554896      -0.484308   \n76821    0.503895    -0.692448  -0.679762       -0.529885      -0.387674   \n76833   -1.046258    -0.672714  -0.253326       -0.697715      -0.685183   \n76849   -0.298017     2.201976  -0.300283       -0.712674      -0.693444   \n76889   -0.064676    -0.604371   1.771012       -0.826214      -0.853646   \n\n       num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n37         0.819344 -0.767050   -0.415347   -0.837416    -0.742606   \n77        -0.819265 -0.310528   -0.604683   -0.665054    -0.198205   \n117       -1.418253 -1.138869   -1.358698   -1.498212    -1.529875   \n184       -0.986489  1.522755    1.681061    2.134220     2.084527   \n224       -0.079273 -0.976936   -0.943820   -1.358742    -1.372436   \n...             ...       ...         ...         ...          ...   \n76809      0.210192 -0.860239   -0.538398   -0.528166    -0.760821   \n76821     -0.777245 -0.624373   -0.747129   -0.121647    -0.547000   \n76833      0.464065 -1.147242   -0.860877   -0.522436    -0.591345   \n76849      0.865974 -0.488813   -0.815114   -0.553735    -0.551772   \n76889     -1.525174 -0.806854   -1.030321   -0.850105    -0.520813   \n\n       num__BAspr  num__size  num__lbm  num__lop  num__lgp  num__linv  \\\n37      -0.273727  -0.347390 -0.394250 -0.569229 -0.440817   0.242727   \n77       0.019889  -0.948273  0.421057  0.116165  0.326541  -1.125382   \n117     -0.196580  -0.841639 -0.283119  1.622306  1.928796  -1.026000   \n184      2.409377  -1.570553  1.564946 -2.080395 -1.770265  -0.728135   \n224     -0.454743   2.832604 -0.664197  0.890849 -0.008299   0.969864   \n...           ...        ...       ...       ...       ...        ...   \n76809   -0.397030   0.215362  0.140510  0.324503 -0.716776   1.592674   \n76821   -0.452278   0.391971 -0.000264 -0.144985 -0.379309   1.387099   \n76833   -0.440970  -0.131511  0.117335  1.076196 -0.028161   1.083701   \n76849   -0.451074   0.746059 -0.923249 -1.272632  0.706670  -0.675277   \n76889    1.154986  -1.151990 -0.204261 -0.003571  0.471093  -0.542937   \n\n       num__llme  num__l1amhd  num__l1MAX  num__l1BAspr  num__l3amhd  \\\n37     -0.416237     0.291945   -1.011197     -0.358964     0.243817   \n77     -1.415715     1.505979   -0.687561      0.002301     1.688294   \n117    -0.975310     0.799418   -1.016899     -0.156416     0.818801   \n184    -2.259244     1.908623    2.084007     -0.324324     1.933517   \n224     2.738320    -2.088544   -0.813583     -0.468032    -2.077690   \n...          ...          ...         ...           ...          ...   \n76809   0.017774    -0.371362   -0.929627     -0.436078    -0.384307   \n76821   0.496130    -0.670522   -0.876003     -0.451794    -0.642843   \n76833   0.063224    -0.259750   -1.041207     -0.411195    -0.267628   \n76849   0.369738    -0.274068   -0.462638     -0.466384    -0.209352   \n76889  -1.076504     1.782355   -0.347738      0.065109     1.862689   \n\n       num__l3MAX  num__l3BAspr  num__l6amhd  num__l6MAX  num__l6BAspr  \\\n37      -0.839387     -0.324342     0.095959   -0.731315     -0.357240   \n77      -0.429360      0.854002     1.819425    0.480177      0.086252   \n117     -0.878843     -0.312308     0.904621   -1.013453     -0.255808   \n184      1.854294      0.172582     2.168014    2.476404      4.909389   \n224     -0.794177     -0.457275    -2.050656    0.040278     -0.467010   \n...           ...           ...          ...         ...           ...   \n76809    0.216762     -0.442030    -0.394194   -0.249067     -0.408983   \n76821   -0.738654     -0.463502    -0.585459   -0.954593     -0.462559   \n76833   -0.419104     -0.448052    -0.286616    0.417637     -0.447920   \n76849   -0.783334     -0.467451    -0.108760    0.803386     -0.464383   \n76889   -0.838800      0.264378     1.890670    0.509522      0.600523   \n\n       num__l12amhd  num__l12MAX  num__l12BAspr  num__l12mom122  \\\n37         0.260356    -1.011197      -0.440167        2.672025   \n77         2.144114    -0.687561       2.133388        2.306243   \n117        1.068031    -1.016899      -0.067369        1.237983   \n184       -0.140420     2.084007       3.674272        0.206004   \n224       -2.036770    -0.813583      -0.457157        1.051903   \n...             ...          ...            ...             ...   \n76809     -0.169345    -0.929627      -0.440086        1.073954   \n76821     -0.505047    -0.876003      -0.409439        1.389443   \n76833     -0.268217    -1.041207      -0.449478        0.965275   \n76849      0.019749    -0.462638      -0.443034        0.389688   \n76889      2.019325    -0.347738       2.482332        1.956218   \n\n       num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n37             -0.081076         -0.058999         0.166738      -0.602469   \n77              1.035512          1.048353        -2.348474       0.271762   \n117            -0.498111         -0.379121        -1.343626      -1.164610   \n184             2.463395          2.423646        -0.487464       2.218228   \n224            -1.203442         -1.138263        -0.489305      -1.280598   \n...                  ...               ...              ...            ...   \n76809          -0.449619         -0.438896         1.020797      -0.401676   \n76821          -1.141501         -1.187009        -0.780925      -0.876849   \n76833          -0.818476         -0.690079         0.571615      -0.709454   \n76849          -0.734271         -0.657172         0.494761      -0.346837   \n76889          -0.317879         -0.206623        -1.289419      -0.263619   \n\n       num__l12vol12m  num__amhd_miss  num__BAspr_miss  cat__ind_1.0  \\\n37          -0.391493       -0.178017        -0.078007           0.0   \n77           0.884246       -0.178017        -0.078007           0.0   \n117         -0.710352       -0.178017        -0.078007           0.0   \n184          2.230892       -0.178017        -0.078007           0.0   \n224         -1.270395       -0.178017        -0.078007           0.0   \n...               ...             ...              ...           ...   \n76809       -0.174816       -0.178017        -0.078007           0.0   \n76821       -0.868594       -0.178017        -0.078007           0.0   \n76833       -0.189070       -0.178017        -0.078007           0.0   \n76849       -0.123037       -0.178017        -0.078007           0.0   \n76889        0.330423       -0.178017        -0.078007           0.0   \n\n       cat__ind_2.0  cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  \\\n37              0.0           0.0           0.0           0.0           0.0   \n77              0.0           0.0           0.0           0.0           0.0   \n117             1.0           0.0           0.0           0.0           0.0   \n184             0.0           0.0           0.0           0.0           0.0   \n224             0.0           0.0           0.0           0.0           0.0   \n...             ...           ...           ...           ...           ...   \n76809           0.0           0.0           0.0           0.0           0.0   \n76821           0.0           0.0           0.0           0.0           0.0   \n76833           0.0           0.0           0.0           0.0           0.0   \n76849           0.0           0.0           0.0           0.0           0.0   \n76889           0.0           0.0           0.0           0.0           0.0   \n\n       cat__ind_7.0  cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n37              0.0           0.0           0.0            0.0            0.0   \n77              0.0           0.0           0.0            0.0            0.0   \n117             0.0           0.0           0.0            0.0            0.0   \n184             0.0           0.0           0.0            0.0            0.0   \n224             0.0           0.0           0.0            0.0            0.0   \n...             ...           ...           ...            ...            ...   \n76809           0.0           0.0           0.0            0.0            0.0   \n76821           0.0           0.0           0.0            0.0            0.0   \n76833           0.0           0.0           0.0            0.0            0.0   \n76849           0.0           0.0           0.0            0.0            0.0   \n76889           0.0           0.0           0.0            0.0            0.0   \n\n       cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n37               0.0            0.0            0.0            1.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            1.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            1.0            0.0   \n\n       cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            1.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            1.0            0.0   \n76821            0.0            0.0            1.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              1.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            1.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            1.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_44.0  cat__ind_45.0  cat__ind_46.0  cat__ind_47.0  \\\n37               0.0            0.0            0.0            0.0   \n77               0.0            0.0            0.0            0.0   \n117              0.0            0.0            0.0            0.0   \n184              0.0            0.0            0.0            0.0   \n224              0.0            0.0            0.0            0.0   \n...              ...            ...            ...            ...   \n76809            0.0            0.0            0.0            0.0   \n76821            0.0            0.0            0.0            0.0   \n76833            0.0            0.0            0.0            0.0   \n76849            0.0            0.0            0.0            0.0   \n76889            0.0            0.0            0.0            0.0   \n\n       cat__ind_48.0  cat__ind_49.0  \n37               0.0            0.0  \n77               0.0            0.0  \n117              0.0            0.0  \n184              0.0            0.0  \n224              0.0            0.0  \n...              ...            ...  \n76809            0.0            0.0  \n76821            0.0            0.0  \n76833            0.0            0.0  \n76849            0.0            0.0  \n76889            0.0            0.0  \n\n[1873 rows x 92 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__BAspr</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l1BAspr</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l3BAspr</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l6BAspr</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12BAspr</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>num__BAspr_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_46.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>37</th>\n      <td>-0.370711</td>\n      <td>1.045747</td>\n      <td>-0.616017</td>\n      <td>0.807145</td>\n      <td>0.566597</td>\n      <td>-0.676510</td>\n      <td>0.375087</td>\n      <td>0.151844</td>\n      <td>0.324364</td>\n      <td>-0.926429</td>\n      <td>-0.835087</td>\n      <td>0.819344</td>\n      <td>-0.767050</td>\n      <td>-0.415347</td>\n      <td>-0.837416</td>\n      <td>-0.742606</td>\n      <td>-0.273727</td>\n      <td>-0.347390</td>\n      <td>-0.394250</td>\n      <td>-0.569229</td>\n      <td>-0.440817</td>\n      <td>0.242727</td>\n      <td>-0.416237</td>\n      <td>0.291945</td>\n      <td>-1.011197</td>\n      <td>-0.358964</td>\n      <td>0.243817</td>\n      <td>-0.839387</td>\n      <td>-0.324342</td>\n      <td>0.095959</td>\n      <td>-0.731315</td>\n      <td>-0.357240</td>\n      <td>0.260356</td>\n      <td>-1.011197</td>\n      <td>-0.440167</td>\n      <td>2.672025</td>\n      <td>-0.081076</td>\n      <td>-0.058999</td>\n      <td>0.166738</td>\n      <td>-0.602469</td>\n      <td>-0.391493</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>77</th>\n      <td>1.562862</td>\n      <td>2.932444</td>\n      <td>0.409891</td>\n      <td>-0.407999</td>\n      <td>0.054079</td>\n      <td>-0.291606</td>\n      <td>1.619434</td>\n      <td>1.765912</td>\n      <td>1.497371</td>\n      <td>-0.321803</td>\n      <td>-0.310475</td>\n      <td>-0.819265</td>\n      <td>-0.310528</td>\n      <td>-0.604683</td>\n      <td>-0.665054</td>\n      <td>-0.198205</td>\n      <td>0.019889</td>\n      <td>-0.948273</td>\n      <td>0.421057</td>\n      <td>0.116165</td>\n      <td>0.326541</td>\n      <td>-1.125382</td>\n      <td>-1.415715</td>\n      <td>1.505979</td>\n      <td>-0.687561</td>\n      <td>0.002301</td>\n      <td>1.688294</td>\n      <td>-0.429360</td>\n      <td>0.854002</td>\n      <td>1.819425</td>\n      <td>0.480177</td>\n      <td>0.086252</td>\n      <td>2.144114</td>\n      <td>-0.687561</td>\n      <td>2.133388</td>\n      <td>2.306243</td>\n      <td>1.035512</td>\n      <td>1.048353</td>\n      <td>-2.348474</td>\n      <td>0.271762</td>\n      <td>0.884246</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>117</th>\n      <td>-0.017971</td>\n      <td>1.548951</td>\n      <td>-0.764228</td>\n      <td>1.661493</td>\n      <td>1.180223</td>\n      <td>-0.148263</td>\n      <td>0.001117</td>\n      <td>0.335290</td>\n      <td>0.784002</td>\n      <td>-1.173535</td>\n      <td>-1.074915</td>\n      <td>-1.418253</td>\n      <td>-1.138869</td>\n      <td>-1.358698</td>\n      <td>-1.498212</td>\n      <td>-1.529875</td>\n      <td>-0.196580</td>\n      <td>-0.841639</td>\n      <td>-0.283119</td>\n      <td>1.622306</td>\n      <td>1.928796</td>\n      <td>-1.026000</td>\n      <td>-0.975310</td>\n      <td>0.799418</td>\n      <td>-1.016899</td>\n      <td>-0.156416</td>\n      <td>0.818801</td>\n      <td>-0.878843</td>\n      <td>-0.312308</td>\n      <td>0.904621</td>\n      <td>-1.013453</td>\n      <td>-0.255808</td>\n      <td>1.068031</td>\n      <td>-1.016899</td>\n      <td>-0.067369</td>\n      <td>1.237983</td>\n      <td>-0.498111</td>\n      <td>-0.379121</td>\n      <td>-1.343626</td>\n      <td>-1.164610</td>\n      <td>-0.710352</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>184</th>\n      <td>0.995702</td>\n      <td>2.932444</td>\n      <td>1.004495</td>\n      <td>-1.707367</td>\n      <td>-0.945570</td>\n      <td>-0.468686</td>\n      <td>-0.118384</td>\n      <td>2.201976</td>\n      <td>1.897724</td>\n      <td>1.946351</td>\n      <td>2.146835</td>\n      <td>-0.986489</td>\n      <td>1.522755</td>\n      <td>1.681061</td>\n      <td>2.134220</td>\n      <td>2.084527</td>\n      <td>2.409377</td>\n      <td>-1.570553</td>\n      <td>1.564946</td>\n      <td>-2.080395</td>\n      <td>-1.770265</td>\n      <td>-0.728135</td>\n      <td>-2.259244</td>\n      <td>1.908623</td>\n      <td>2.084007</td>\n      <td>-0.324324</td>\n      <td>1.933517</td>\n      <td>1.854294</td>\n      <td>0.172582</td>\n      <td>2.168014</td>\n      <td>2.476404</td>\n      <td>4.909389</td>\n      <td>-0.140420</td>\n      <td>2.084007</td>\n      <td>3.674272</td>\n      <td>0.206004</td>\n      <td>2.463395</td>\n      <td>2.423646</td>\n      <td>-0.487464</td>\n      <td>2.218228</td>\n      <td>2.230892</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>224</th>\n      <td>1.147943</td>\n      <td>1.282408</td>\n      <td>-1.101432</td>\n      <td>1.080693</td>\n      <td>0.088267</td>\n      <td>-0.321219</td>\n      <td>0.057190</td>\n      <td>0.589992</td>\n      <td>-2.076221</td>\n      <td>-1.190013</td>\n      <td>-1.156021</td>\n      <td>-0.079273</td>\n      <td>-0.976936</td>\n      <td>-0.943820</td>\n      <td>-1.358742</td>\n      <td>-1.372436</td>\n      <td>-0.454743</td>\n      <td>2.832604</td>\n      <td>-0.664197</td>\n      <td>0.890849</td>\n      <td>-0.008299</td>\n      <td>0.969864</td>\n      <td>2.738320</td>\n      <td>-2.088544</td>\n      <td>-0.813583</td>\n      <td>-0.468032</td>\n      <td>-2.077690</td>\n      <td>-0.794177</td>\n      <td>-0.457275</td>\n      <td>-2.050656</td>\n      <td>0.040278</td>\n      <td>-0.467010</td>\n      <td>-2.036770</td>\n      <td>-0.813583</td>\n      <td>-0.457157</td>\n      <td>1.051903</td>\n      <td>-1.203442</td>\n      <td>-1.138263</td>\n      <td>-0.489305</td>\n      <td>-1.280598</td>\n      <td>-1.270395</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>76809</th>\n      <td>-0.208849</td>\n      <td>1.361380</td>\n      <td>-0.097743</td>\n      <td>0.250530</td>\n      <td>-0.844100</td>\n      <td>1.878685</td>\n      <td>-1.457988</td>\n      <td>0.954821</td>\n      <td>-0.377764</td>\n      <td>-0.554896</td>\n      <td>-0.484308</td>\n      <td>0.210192</td>\n      <td>-0.860239</td>\n      <td>-0.538398</td>\n      <td>-0.528166</td>\n      <td>-0.760821</td>\n      <td>-0.397030</td>\n      <td>0.215362</td>\n      <td>0.140510</td>\n      <td>0.324503</td>\n      <td>-0.716776</td>\n      <td>1.592674</td>\n      <td>0.017774</td>\n      <td>-0.371362</td>\n      <td>-0.929627</td>\n      <td>-0.436078</td>\n      <td>-0.384307</td>\n      <td>0.216762</td>\n      <td>-0.442030</td>\n      <td>-0.394194</td>\n      <td>-0.249067</td>\n      <td>-0.408983</td>\n      <td>-0.169345</td>\n      <td>-0.929627</td>\n      <td>-0.440086</td>\n      <td>1.073954</td>\n      <td>-0.449619</td>\n      <td>-0.438896</td>\n      <td>1.020797</td>\n      <td>-0.401676</td>\n      <td>-0.174816</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76821</th>\n      <td>-0.208849</td>\n      <td>-0.112372</td>\n      <td>-0.479475</td>\n      <td>0.136375</td>\n      <td>-0.134496</td>\n      <td>-0.308538</td>\n      <td>0.503895</td>\n      <td>-0.692448</td>\n      <td>-0.679762</td>\n      <td>-0.529885</td>\n      <td>-0.387674</td>\n      <td>-0.777245</td>\n      <td>-0.624373</td>\n      <td>-0.747129</td>\n      <td>-0.121647</td>\n      <td>-0.547000</td>\n      <td>-0.452278</td>\n      <td>0.391971</td>\n      <td>-0.000264</td>\n      <td>-0.144985</td>\n      <td>-0.379309</td>\n      <td>1.387099</td>\n      <td>0.496130</td>\n      <td>-0.670522</td>\n      <td>-0.876003</td>\n      <td>-0.451794</td>\n      <td>-0.642843</td>\n      <td>-0.738654</td>\n      <td>-0.463502</td>\n      <td>-0.585459</td>\n      <td>-0.954593</td>\n      <td>-0.462559</td>\n      <td>-0.505047</td>\n      <td>-0.876003</td>\n      <td>-0.409439</td>\n      <td>1.389443</td>\n      <td>-1.141501</td>\n      <td>-1.187009</td>\n      <td>-0.780925</td>\n      <td>-0.876849</td>\n      <td>-0.868594</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76833</th>\n      <td>-0.208849</td>\n      <td>-0.300522</td>\n      <td>-0.391261</td>\n      <td>0.852059</td>\n      <td>-0.361299</td>\n      <td>1.687970</td>\n      <td>-1.046258</td>\n      <td>-0.672714</td>\n      <td>-0.253326</td>\n      <td>-0.697715</td>\n      <td>-0.685183</td>\n      <td>0.464065</td>\n      <td>-1.147242</td>\n      <td>-0.860877</td>\n      <td>-0.522436</td>\n      <td>-0.591345</td>\n      <td>-0.440970</td>\n      <td>-0.131511</td>\n      <td>0.117335</td>\n      <td>1.076196</td>\n      <td>-0.028161</td>\n      <td>1.083701</td>\n      <td>0.063224</td>\n      <td>-0.259750</td>\n      <td>-1.041207</td>\n      <td>-0.411195</td>\n      <td>-0.267628</td>\n      <td>-0.419104</td>\n      <td>-0.448052</td>\n      <td>-0.286616</td>\n      <td>0.417637</td>\n      <td>-0.447920</td>\n      <td>-0.268217</td>\n      <td>-1.041207</td>\n      <td>-0.449478</td>\n      <td>0.965275</td>\n      <td>-0.818476</td>\n      <td>-0.690079</td>\n      <td>0.571615</td>\n      <td>-0.709454</td>\n      <td>-0.189070</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76849</th>\n      <td>-0.208849</td>\n      <td>1.938339</td>\n      <td>-1.806515</td>\n      <td>-1.257885</td>\n      <td>1.031139</td>\n      <td>-0.502160</td>\n      <td>-0.298017</td>\n      <td>2.201976</td>\n      <td>-0.300283</td>\n      <td>-0.712674</td>\n      <td>-0.693444</td>\n      <td>0.865974</td>\n      <td>-0.488813</td>\n      <td>-0.815114</td>\n      <td>-0.553735</td>\n      <td>-0.551772</td>\n      <td>-0.451074</td>\n      <td>0.746059</td>\n      <td>-0.923249</td>\n      <td>-1.272632</td>\n      <td>0.706670</td>\n      <td>-0.675277</td>\n      <td>0.369738</td>\n      <td>-0.274068</td>\n      <td>-0.462638</td>\n      <td>-0.466384</td>\n      <td>-0.209352</td>\n      <td>-0.783334</td>\n      <td>-0.467451</td>\n      <td>-0.108760</td>\n      <td>0.803386</td>\n      <td>-0.464383</td>\n      <td>0.019749</td>\n      <td>-0.462638</td>\n      <td>-0.443034</td>\n      <td>0.389688</td>\n      <td>-0.734271</td>\n      <td>-0.657172</td>\n      <td>0.494761</td>\n      <td>-0.346837</td>\n      <td>-0.123037</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>76889</th>\n      <td>-0.690620</td>\n      <td>0.644627</td>\n      <td>0.154976</td>\n      <td>-0.449011</td>\n      <td>0.225954</td>\n      <td>-0.367814</td>\n      <td>-0.064676</td>\n      <td>-0.604371</td>\n      <td>1.771012</td>\n      <td>-0.826214</td>\n      <td>-0.853646</td>\n      <td>-1.525174</td>\n      <td>-0.806854</td>\n      <td>-1.030321</td>\n      <td>-0.850105</td>\n      <td>-0.520813</td>\n      <td>1.154986</td>\n      <td>-1.151990</td>\n      <td>-0.204261</td>\n      <td>-0.003571</td>\n      <td>0.471093</td>\n      <td>-0.542937</td>\n      <td>-1.076504</td>\n      <td>1.782355</td>\n      <td>-0.347738</td>\n      <td>0.065109</td>\n      <td>1.862689</td>\n      <td>-0.838800</td>\n      <td>0.264378</td>\n      <td>1.890670</td>\n      <td>0.509522</td>\n      <td>0.600523</td>\n      <td>2.019325</td>\n      <td>-0.347738</td>\n      <td>2.482332</td>\n      <td>1.956218</td>\n      <td>-0.317879</td>\n      <td>-0.206623</td>\n      <td>-1.289419</td>\n      <td>-0.263619</td>\n      <td>0.330423</td>\n      <td>-0.178017</td>\n      <td>-0.078007</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>1873 rows  92 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results0","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.137582Z","iopub.status.idle":"2022-09-06T00:36:53.138490Z","shell.execute_reply.started":"2022-09-06T00:36:53.138181Z","shell.execute_reply":"2022-09-06T00:36:53.138210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create validation set \nval_indx = X_train0.remainder__prd == min_prd+windows_width-1\nX_val = X_train0[val_indx]\nX_val.drop(columns='remainder__prd', inplace=True)\ny_val = y_train0[val_indx]\ndisplay(X_val, y_val)\ntrain_indx = X_train0.remainder__prd < min_prd+windows_width-1\nX_train = X_train0[train_indx]\nX_train.drop(columns='remainder__prd', inplace=True)\ny_train = y_train0[train_indx]\ndisplay(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:43:37.704986Z","iopub.execute_input":"2022-09-06T00:43:37.705414Z","iopub.status.idle":"2022-09-06T00:43:37.736895Z","shell.execute_reply.started":"2022-09-06T00:43:37.705381Z","shell.execute_reply":"2022-09-06T00:43:37.735112Z"},"trusted":true},"execution_count":10,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_16/751690025.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# create validation set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mval_indx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremainder__prd\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mmin_prd\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mwindows_width\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mX_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_indx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'remainder__prd'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_train0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_indx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'X_train0' is not defined"],"ename":"NameError","evalue":"name 'X_train0' is not defined","output_type":"error"}]},{"cell_type":"code","source":"neurons_base = 16\ndropout_rate = 0.05\n# n_b=8 was ok with small overfit.\n# n_b=32 starts clearly overfitting. \n# 128 fits clearly slower than 64 and becomes somewhat unstable. regularization could make it work, but i see no reason to go wider.\n# 64 seems to have nice balance of flexibility and runtime, but its variance may be too large. dropout makes variance vene worse.\n# 6 hidden layers is probably most this architecture can hold\n\n# in this framework the optimal model seems to have width of 16 or 32, somehow regularized. try l1/l2?\n# w32 can take at most 0.03 dropout.\n# w16 looks good w/o dropout.\n\n# more general point:\n# main drawback of dropout is in incresing variance\n# for textbook problems with high s/n ratio (e.g., mnist) this may be ok.\n# for application like this with very low s/n ratio dropout may be a bad idea.\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*32, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train.shape[1:]),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*16, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    Dense(1)])\n\nprint(model_snn.count_params())","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:45:30.311529Z","iopub.execute_input":"2022-09-06T00:45:30.312230Z","iopub.status.idle":"2022-09-06T00:45:30.385460Z","shell.execute_reply.started":"2022-09-06T00:45:30.312193Z","shell.execute_reply":"2022-09-06T00:45:30.384376Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"222721\n","output_type":"stream"}]},{"cell_type":"code","source":"neurons_base = 16\ndropout_rate = 0.05\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*27, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train.shape[1:]),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*9, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*3, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    Dense(1)])\n\nprint(model_snn.count_params())\n\n# similar problem as before: model seems ok in terms of flexibility and variance, but adding dropout breaks it before i can fix overfitting.\n# the solution is to either use smaller models or to use laternative regularizers (which do not increase variance.)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T01:56:18.293248Z","iopub.execute_input":"2022-09-06T01:56:18.293809Z","iopub.status.idle":"2022-09-06T01:56:23.218663Z","shell.execute_reply.started":"2022-09-06T01:56:18.293759Z","shell.execute_reply":"2022-09-06T01:56:23.217686Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"2022-09-06 01:56:18.346069: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:18.351748: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:18.352749: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:18.354627: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-09-06 01:56:18.354980: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:18.356110: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:18.357102: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:22.788783: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:22.789725: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from S","output_type":"stream"},{"name":"stdout","text":"110289\n","output_type":"stream"},{"name":"stderr","text":"ysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:22.790430: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-06 01:56:22.791027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14879 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping50 = EarlyStopping(patience=50, restore_best_weights=True)\ntime1 = time.time()\noptimizer_adam = tf.keras.optimizers.Adam()\nmodel_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_snn.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=2, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nprint([r2_score(y_train, model_snn.predict(X_train)), \n       r2_score(y_val, model_snn.predict(X_val)),\n       r2_score(y_test, model_snn.predict(X_test))])\nprint(time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T01:56:23.220689Z","iopub.execute_input":"2022-09-06T01:56:23.221370Z","iopub.status.idle":"2022-09-06T01:56:35.380012Z","shell.execute_reply.started":"2022-09-06T01:56:23.221331Z","shell.execute_reply":"2022-09-06T01:56:35.379062Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"2022-09-06 01:56:23.446034: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1000\n34/34 - 2s - loss: 203.5653 - mean_squared_error: 203.5653 - val_loss: 134.1529 - val_mean_squared_error: 134.1529\nEpoch 2/1000\n34/34 - 0s - loss: 202.2396 - mean_squared_error: 202.2396 - val_loss: 133.9866 - val_mean_squared_error: 133.9866\nEpoch 3/1000\n34/34 - 0s - loss: 201.9889 - mean_squared_error: 201.9889 - val_loss: 133.8872 - val_mean_squared_error: 133.8872\nEpoch 4/1000\n34/34 - 0s - loss: 201.7041 - mean_squared_error: 201.7041 - val_loss: 133.9403 - val_mean_squared_error: 133.9403\nEpoch 5/1000\n34/34 - 0s - loss: 201.2448 - mean_squared_error: 201.2448 - val_loss: 133.7487 - val_mean_squared_error: 133.7487\nEpoch 6/1000\n34/34 - 0s - loss: 200.6950 - mean_squared_error: 200.6950 - val_loss: 134.1508 - val_mean_squared_error: 134.1508\nEpoch 7/1000\n34/34 - 0s - loss: 200.2834 - mean_squared_error: 200.2834 - val_loss: 134.3009 - val_mean_squared_error: 134.3009\nEpoch 8/1000\n34/34 - 0s - loss: 200.2230 - mean_squared_error: 200.2230 - val_loss: 134.6417 - val_mean_squared_error: 134.6417\nEpoch 9/1000\n34/34 - 0s - loss: 199.7150 - mean_squared_error: 199.7150 - val_loss: 135.4830 - val_mean_squared_error: 135.4830\nEpoch 10/1000\n34/34 - 0s - loss: 199.6890 - mean_squared_error: 199.6890 - val_loss: 134.2659 - val_mean_squared_error: 134.2659\nEpoch 11/1000\n34/34 - 0s - loss: 199.4605 - mean_squared_error: 199.4605 - val_loss: 134.4543 - val_mean_squared_error: 134.4543\nEpoch 12/1000\n34/34 - 0s - loss: 198.7751 - mean_squared_error: 198.7751 - val_loss: 136.4794 - val_mean_squared_error: 136.4794\nEpoch 13/1000\n34/34 - 0s - loss: 198.8705 - mean_squared_error: 198.8705 - val_loss: 135.3572 - val_mean_squared_error: 135.3572\nEpoch 14/1000\n34/34 - 0s - loss: 198.3818 - mean_squared_error: 198.3818 - val_loss: 135.1687 - val_mean_squared_error: 135.1687\nEpoch 15/1000\n34/34 - 0s - loss: 197.8335 - mean_squared_error: 197.8335 - val_loss: 136.0797 - val_mean_squared_error: 136.0797\nEpoch 16/1000\n34/34 - 0s - loss: 197.5053 - mean_squared_error: 197.5053 - val_loss: 135.6482 - val_mean_squared_error: 135.6482\nEpoch 17/1000\n34/34 - 0s - loss: 196.9620 - mean_squared_error: 196.9620 - val_loss: 137.2255 - val_mean_squared_error: 137.2255\nEpoch 18/1000\n34/34 - 0s - loss: 196.6492 - mean_squared_error: 196.6492 - val_loss: 137.1179 - val_mean_squared_error: 137.1179\nEpoch 19/1000\n34/34 - 0s - loss: 196.6281 - mean_squared_error: 196.6281 - val_loss: 136.7719 - val_mean_squared_error: 136.7719\nEpoch 20/1000\n34/34 - 0s - loss: 196.1639 - mean_squared_error: 196.1639 - val_loss: 136.6230 - val_mean_squared_error: 136.6230\nEpoch 21/1000\n34/34 - 0s - loss: 195.6301 - mean_squared_error: 195.6301 - val_loss: 137.6051 - val_mean_squared_error: 137.6051\nEpoch 22/1000\n34/34 - 0s - loss: 195.3080 - mean_squared_error: 195.3080 - val_loss: 137.0344 - val_mean_squared_error: 137.0344\nEpoch 23/1000\n34/34 - 0s - loss: 194.7576 - mean_squared_error: 194.7576 - val_loss: 137.2127 - val_mean_squared_error: 137.2127\nEpoch 24/1000\n34/34 - 0s - loss: 194.3277 - mean_squared_error: 194.3277 - val_loss: 138.1346 - val_mean_squared_error: 138.1346\nEpoch 25/1000\n34/34 - 0s - loss: 194.0926 - mean_squared_error: 194.0926 - val_loss: 139.3853 - val_mean_squared_error: 139.3853\nEpoch 26/1000\n34/34 - 0s - loss: 193.5556 - mean_squared_error: 193.5556 - val_loss: 136.8358 - val_mean_squared_error: 136.8358\nEpoch 27/1000\n34/34 - 0s - loss: 193.3162 - mean_squared_error: 193.3162 - val_loss: 141.0429 - val_mean_squared_error: 141.0429\nEpoch 28/1000\n34/34 - 0s - loss: 192.9642 - mean_squared_error: 192.9642 - val_loss: 139.2817 - val_mean_squared_error: 139.2817\nEpoch 29/1000\n34/34 - 0s - loss: 192.5963 - mean_squared_error: 192.5963 - val_loss: 139.8153 - val_mean_squared_error: 139.8153\nEpoch 30/1000\n34/34 - 0s - loss: 191.9129 - mean_squared_error: 191.9129 - val_loss: 141.0102 - val_mean_squared_error: 141.0102\nEpoch 31/1000\n34/34 - 0s - loss: 192.1046 - mean_squared_error: 192.1046 - val_loss: 138.9361 - val_mean_squared_error: 138.9361\nEpoch 32/1000\n34/34 - 0s - loss: 190.7665 - mean_squared_error: 190.7665 - val_loss: 140.3271 - val_mean_squared_error: 140.3271\nEpoch 33/1000\n34/34 - 0s - loss: 190.8974 - mean_squared_error: 190.8974 - val_loss: 140.6469 - val_mean_squared_error: 140.6469\nEpoch 34/1000\n34/34 - 0s - loss: 190.4319 - mean_squared_error: 190.4319 - val_loss: 140.6880 - val_mean_squared_error: 140.6880\nEpoch 35/1000\n34/34 - 0s - loss: 190.2489 - mean_squared_error: 190.2489 - val_loss: 140.8828 - val_mean_squared_error: 140.8828\nEpoch 36/1000\n34/34 - 0s - loss: 189.7810 - mean_squared_error: 189.7810 - val_loss: 139.9942 - val_mean_squared_error: 139.9942\nEpoch 37/1000\n34/34 - 0s - loss: 189.4464 - mean_squared_error: 189.4464 - val_loss: 142.8586 - val_mean_squared_error: 142.8586\nEpoch 38/1000\n34/34 - 0s - loss: 188.9634 - mean_squared_error: 188.9634 - val_loss: 141.4897 - val_mean_squared_error: 141.4897\nEpoch 39/1000\n34/34 - 0s - loss: 188.0409 - mean_squared_error: 188.0409 - val_loss: 140.2587 - val_mean_squared_error: 140.2587\nEpoch 40/1000\n34/34 - 0s - loss: 187.8784 - mean_squared_error: 187.8784 - val_loss: 141.7381 - val_mean_squared_error: 141.7381\nEpoch 41/1000\n34/34 - 0s - loss: 187.8020 - mean_squared_error: 187.8020 - val_loss: 142.5590 - val_mean_squared_error: 142.5590\nEpoch 42/1000\n34/34 - 0s - loss: 186.9467 - mean_squared_error: 186.9467 - val_loss: 141.5645 - val_mean_squared_error: 141.5645\nEpoch 43/1000\n34/34 - 0s - loss: 186.8272 - mean_squared_error: 186.8272 - val_loss: 139.7713 - val_mean_squared_error: 139.7713\nEpoch 44/1000\n34/34 - 0s - loss: 185.7294 - mean_squared_error: 185.7294 - val_loss: 141.3277 - val_mean_squared_error: 141.3277\nEpoch 45/1000\n34/34 - 0s - loss: 185.4902 - mean_squared_error: 185.4902 - val_loss: 141.0635 - val_mean_squared_error: 141.0635\nEpoch 46/1000\n34/34 - 0s - loss: 185.1499 - mean_squared_error: 185.1499 - val_loss: 139.8159 - val_mean_squared_error: 139.8159\nEpoch 47/1000\n34/34 - 0s - loss: 184.3077 - mean_squared_error: 184.3077 - val_loss: 141.8927 - val_mean_squared_error: 141.8927\nEpoch 48/1000\n34/34 - 0s - loss: 184.3616 - mean_squared_error: 184.3616 - val_loss: 141.0018 - val_mean_squared_error: 141.0018\nEpoch 49/1000\n34/34 - 0s - loss: 184.0969 - mean_squared_error: 184.0969 - val_loss: 140.8256 - val_mean_squared_error: 140.8256\nEpoch 50/1000\n34/34 - 0s - loss: 183.6438 - mean_squared_error: 183.6438 - val_loss: 144.9551 - val_mean_squared_error: 144.9551\nEpoch 51/1000\n34/34 - 0s - loss: 182.8962 - mean_squared_error: 182.8962 - val_loss: 140.7123 - val_mean_squared_error: 140.7123\nEpoch 52/1000\n34/34 - 0s - loss: 182.7739 - mean_squared_error: 182.7739 - val_loss: 144.1363 - val_mean_squared_error: 144.1363\nEpoch 53/1000\n34/34 - 0s - loss: 182.4254 - mean_squared_error: 182.4254 - val_loss: 141.3552 - val_mean_squared_error: 141.3552\nEpoch 54/1000\n34/34 - 0s - loss: 182.3056 - mean_squared_error: 182.3056 - val_loss: 142.5166 - val_mean_squared_error: 142.5166\nEpoch 55/1000\n34/34 - 0s - loss: 180.9704 - mean_squared_error: 180.9704 - val_loss: 142.7140 - val_mean_squared_error: 142.7140\nMinimum Validation Loss: 133.7487\n[0.015923566593638583, 0.010245079730373696, -0.003292317941543388]\n12.024835586547852\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"X_train.skew()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T01:58:14.615470Z","iopub.execute_input":"2022-09-06T01:58:14.616043Z","iopub.status.idle":"2022-09-06T01:58:14.787794Z","shell.execute_reply.started":"2022-09-06T01:58:14.615998Z","shell.execute_reply":"2022-09-06T01:58:14.786906Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"num__mom482       2.014820\nnum__mom242       1.343044\nnum__bm           0.122894\nnum__op          -0.848916\nnum__gp           0.607772\n                   ...    \ncat__ind_45.0    19.090080\ncat__ind_46.0    79.436081\ncat__ind_47.0    12.898892\ncat__ind_48.0    20.318332\ncat__ind_49.0    13.162803\nLength: 92, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"X_val","metadata":{"execution":{"iopub.status.busy":"2022-09-06T00:36:53.150674Z","iopub.status.idle":"2022-09-06T00:36:53.151429Z","shell.execute_reply.started":"2022-09-06T00:36:53.151166Z","shell.execute_reply":"2022-09-06T00:36:53.151191Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# on nns:\n# - try classic regularizers\n# - try differene architecture (not snnn)\n","metadata":{},"execution_count":null,"outputs":[]}]}
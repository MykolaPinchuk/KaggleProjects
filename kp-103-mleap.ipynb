{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Load libraries and data.\n2. pEDA. Look at feature distribution, fix them if they do not look right.\n3. Train-test split. Most likely couple years into test set. 2015-2018?. Impute missing values.\n4. Transform numerical features, add ohe for inds.\n5. Fit classic models: ols as a baseline, then xgb.\n6. Fir DL.\n\n\nNotes:\nideally, I want to use time-based cross-validation.\nsince I have panel data, it is not a trivial task.\nneed to find some solution online.\ne.g., https://towardsdatascience.com/time-based-cross-validation-d259b13d42b8.\n\nfor now, will try to do siple for loop.\n","metadata":{}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, KFold\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\npd.set_option('display.max_columns', 110)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:28.832696Z","iopub.execute_input":"2022-08-25T15:36:28.833869Z","iopub.status.idle":"2022-08-25T15:36:28.843744Z","shell.execute_reply.started":"2022-08-25T15:36:28.833809Z","shell.execute_reply":"2022-08-25T15:36:28.842663Z"},"trusted":true},"execution_count":205,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:28.846169Z","iopub.execute_input":"2022-08-25T15:36:28.847291Z","iopub.status.idle":"2022-08-25T15:36:28.859283Z","shell.execute_reply.started":"2022-08-25T15:36:28.847253Z","shell.execute_reply":"2022-08-25T15:36:28.858212Z"},"trusted":true},"execution_count":206,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:28.862317Z","iopub.execute_input":"2022-08-25T15:36:28.863270Z","iopub.status.idle":"2022-08-25T15:36:28.873301Z","shell.execute_reply.started":"2022-08-25T15:36:28.863222Z","shell.execute_reply":"2022-08-25T15:36:28.872139Z"},"trusted":true},"execution_count":207,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"# 1. Import data #\n\nmin_prd = 480\n\ntime0 = time.time()\n#df = pd.read_csv('../input/cpcrsp-46/IMLEAP_v4.csv')\nwith open('../input/kaggle-46pkl/IMLEAP_v4.pkl', 'rb') as pickled_one:\n    df = pickle.load(pickled_one)\ndf = df[df.prd.isin(range(min_prd-1, min_prd+62))]\ndisplay(df.shape, df.head(), df.count())","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:28.878112Z","iopub.execute_input":"2022-08-25T15:36:28.878380Z","iopub.status.idle":"2022-08-25T15:36:29.679309Z","shell.execute_reply.started":"2022-08-25T15:36:28.878356Z","shell.execute_reply":"2022-08-25T15:36:29.678120Z"},"trusted":true},"execution_count":208,"outputs":[{"output_type":"display_data","data":{"text/plain":"(158913, 46)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"     PERMNO  prd     mom482     mom242  year     RET   ind        bm  \\\n612   10016  479   8.947149 -14.382829  1998  8.4335  21.0 -0.791018   \n613   10016  480  18.465014  -4.988726  1998 -2.6823  21.0 -0.791018   \n614   10016  481   1.909526 -17.673870  1998  8.3558  21.0 -0.791018   \n615   10016  482  19.908305 -12.287263  1998 -1.2575  21.0 -0.791018   \n616   10016  483  17.362376   7.691972  1998 -2.1094  21.0 -0.922990   \n\n           op        gp       inv   mom11     mom122      amhd  ivol_capm  \\\n612  0.072323  0.295900  0.014439 -4.1636   5.347944  1.886058   2.794417   \n613  0.072323  0.295900  0.014439  8.4335   0.347849  1.878089   1.084225   \n614  0.072323  0.295900  0.014439 -2.6823  11.591432  1.780930   1.940155   \n615  0.072323  0.295900  0.014439  8.3558   4.702749  1.782176   1.875627   \n616  0.018059  0.235379 -0.079193 -1.2575   2.447512  1.740401   1.285586   \n\n     ivol_ff5   beta_bw     MAX     vol1m     vol6m    vol12m     BAspr  \\\n612  2.233004  0.604865  7.5059  2.676935  2.064740  1.921379  0.490196   \n613  1.055252  0.607529  2.2861  1.107580  2.051557  1.913618  0.892857   \n614  1.624940  0.603401  3.9448  2.034676  2.014330  1.933689  1.415094   \n615  1.851700  0.634076  6.4615  1.937531  2.059049  1.940786  0.854701   \n616  1.204386  0.662170  2.7647  1.511975  2.075485  1.820699  1.702128   \n\n         size       lbm       lop       lgp      linv      llme    l1amhd  \\\n612  5.165042 -1.049362 -0.000786  0.269403 -0.230583  5.101937  1.880899   \n613  5.255047 -1.049362 -0.000786  0.269403 -0.230583  5.110254  1.886058   \n614  5.232267 -1.049362 -0.000786  0.269403 -0.230583  5.089420  1.878089   \n615  5.316201 -1.049362 -0.000786  0.269403 -0.230583  5.130662  1.780930   \n616  5.310584 -0.791018  0.072323  0.295900  0.014439  5.237193  1.782176   \n\n      l1MAX   l1BAspr    l3amhd   l3MAX   l3BAspr    l6amhd   l6MAX   l6BAspr  \\\n612  9.3849  2.884615  1.626017  4.1457  0.904977  1.772101  1.7982  1.785714   \n613  7.5059  0.490196  1.877705  1.8828  0.943396  1.665406  3.0901  0.416667   \n614  2.2861  0.892857  1.880899  9.3849  2.884615  1.685590  2.6136  1.886792   \n615  3.9448  1.415094  1.886058  7.5059  0.490196  1.626017  4.1457  0.904977   \n616  6.4615  0.854701  1.878089  2.2861  0.892857  1.877705  1.8828  0.943396   \n\n      l12amhd  l12MAX  l12BAspr  l12mom122  l12ivol_capm  l12ivol_ff5  \\\n612  1.772232  9.3849  2.030457  -4.657184      1.772763     1.469041   \n613  1.744671  7.5059  2.040816 -15.288829      1.301430     1.257374   \n614  1.866881  2.2861  3.125000 -12.753601      1.602968     1.380534   \n615  1.835511  3.9448  2.020202 -22.631577      1.901159     1.705052   \n616  1.863802  6.4615  1.801802 -22.104592      2.746955     2.574926   \n\n     l12beta_bw  l12vol6m  l12vol12m  \n612    0.544673  2.239938   2.213453  \n613    0.528119  2.257473   2.177653  \n614    0.580225  2.077777   2.189271  \n615    0.527894  1.778749   2.145822  \n616    0.561722  1.944769   2.203243  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>612</th>\n      <td>10016</td>\n      <td>479</td>\n      <td>8.947149</td>\n      <td>-14.382829</td>\n      <td>1998</td>\n      <td>8.4335</td>\n      <td>21.0</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.295900</td>\n      <td>0.014439</td>\n      <td>-4.1636</td>\n      <td>5.347944</td>\n      <td>1.886058</td>\n      <td>2.794417</td>\n      <td>2.233004</td>\n      <td>0.604865</td>\n      <td>7.5059</td>\n      <td>2.676935</td>\n      <td>2.064740</td>\n      <td>1.921379</td>\n      <td>0.490196</td>\n      <td>5.165042</td>\n      <td>-1.049362</td>\n      <td>-0.000786</td>\n      <td>0.269403</td>\n      <td>-0.230583</td>\n      <td>5.101937</td>\n      <td>1.880899</td>\n      <td>9.3849</td>\n      <td>2.884615</td>\n      <td>1.626017</td>\n      <td>4.1457</td>\n      <td>0.904977</td>\n      <td>1.772101</td>\n      <td>1.7982</td>\n      <td>1.785714</td>\n      <td>1.772232</td>\n      <td>9.3849</td>\n      <td>2.030457</td>\n      <td>-4.657184</td>\n      <td>1.772763</td>\n      <td>1.469041</td>\n      <td>0.544673</td>\n      <td>2.239938</td>\n      <td>2.213453</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>10016</td>\n      <td>480</td>\n      <td>18.465014</td>\n      <td>-4.988726</td>\n      <td>1998</td>\n      <td>-2.6823</td>\n      <td>21.0</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.295900</td>\n      <td>0.014439</td>\n      <td>8.4335</td>\n      <td>0.347849</td>\n      <td>1.878089</td>\n      <td>1.084225</td>\n      <td>1.055252</td>\n      <td>0.607529</td>\n      <td>2.2861</td>\n      <td>1.107580</td>\n      <td>2.051557</td>\n      <td>1.913618</td>\n      <td>0.892857</td>\n      <td>5.255047</td>\n      <td>-1.049362</td>\n      <td>-0.000786</td>\n      <td>0.269403</td>\n      <td>-0.230583</td>\n      <td>5.110254</td>\n      <td>1.886058</td>\n      <td>7.5059</td>\n      <td>0.490196</td>\n      <td>1.877705</td>\n      <td>1.8828</td>\n      <td>0.943396</td>\n      <td>1.665406</td>\n      <td>3.0901</td>\n      <td>0.416667</td>\n      <td>1.744671</td>\n      <td>7.5059</td>\n      <td>2.040816</td>\n      <td>-15.288829</td>\n      <td>1.301430</td>\n      <td>1.257374</td>\n      <td>0.528119</td>\n      <td>2.257473</td>\n      <td>2.177653</td>\n    </tr>\n    <tr>\n      <th>614</th>\n      <td>10016</td>\n      <td>481</td>\n      <td>1.909526</td>\n      <td>-17.673870</td>\n      <td>1998</td>\n      <td>8.3558</td>\n      <td>21.0</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.295900</td>\n      <td>0.014439</td>\n      <td>-2.6823</td>\n      <td>11.591432</td>\n      <td>1.780930</td>\n      <td>1.940155</td>\n      <td>1.624940</td>\n      <td>0.603401</td>\n      <td>3.9448</td>\n      <td>2.034676</td>\n      <td>2.014330</td>\n      <td>1.933689</td>\n      <td>1.415094</td>\n      <td>5.232267</td>\n      <td>-1.049362</td>\n      <td>-0.000786</td>\n      <td>0.269403</td>\n      <td>-0.230583</td>\n      <td>5.089420</td>\n      <td>1.878089</td>\n      <td>2.2861</td>\n      <td>0.892857</td>\n      <td>1.880899</td>\n      <td>9.3849</td>\n      <td>2.884615</td>\n      <td>1.685590</td>\n      <td>2.6136</td>\n      <td>1.886792</td>\n      <td>1.866881</td>\n      <td>2.2861</td>\n      <td>3.125000</td>\n      <td>-12.753601</td>\n      <td>1.602968</td>\n      <td>1.380534</td>\n      <td>0.580225</td>\n      <td>2.077777</td>\n      <td>2.189271</td>\n    </tr>\n    <tr>\n      <th>615</th>\n      <td>10016</td>\n      <td>482</td>\n      <td>19.908305</td>\n      <td>-12.287263</td>\n      <td>1998</td>\n      <td>-1.2575</td>\n      <td>21.0</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.295900</td>\n      <td>0.014439</td>\n      <td>8.3558</td>\n      <td>4.702749</td>\n      <td>1.782176</td>\n      <td>1.875627</td>\n      <td>1.851700</td>\n      <td>0.634076</td>\n      <td>6.4615</td>\n      <td>1.937531</td>\n      <td>2.059049</td>\n      <td>1.940786</td>\n      <td>0.854701</td>\n      <td>5.316201</td>\n      <td>-1.049362</td>\n      <td>-0.000786</td>\n      <td>0.269403</td>\n      <td>-0.230583</td>\n      <td>5.130662</td>\n      <td>1.780930</td>\n      <td>3.9448</td>\n      <td>1.415094</td>\n      <td>1.886058</td>\n      <td>7.5059</td>\n      <td>0.490196</td>\n      <td>1.626017</td>\n      <td>4.1457</td>\n      <td>0.904977</td>\n      <td>1.835511</td>\n      <td>3.9448</td>\n      <td>2.020202</td>\n      <td>-22.631577</td>\n      <td>1.901159</td>\n      <td>1.705052</td>\n      <td>0.527894</td>\n      <td>1.778749</td>\n      <td>2.145822</td>\n    </tr>\n    <tr>\n      <th>616</th>\n      <td>10016</td>\n      <td>483</td>\n      <td>17.362376</td>\n      <td>7.691972</td>\n      <td>1998</td>\n      <td>-2.1094</td>\n      <td>21.0</td>\n      <td>-0.922990</td>\n      <td>0.018059</td>\n      <td>0.235379</td>\n      <td>-0.079193</td>\n      <td>-1.2575</td>\n      <td>2.447512</td>\n      <td>1.740401</td>\n      <td>1.285586</td>\n      <td>1.204386</td>\n      <td>0.662170</td>\n      <td>2.7647</td>\n      <td>1.511975</td>\n      <td>2.075485</td>\n      <td>1.820699</td>\n      <td>1.702128</td>\n      <td>5.310584</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.295900</td>\n      <td>0.014439</td>\n      <td>5.237193</td>\n      <td>1.782176</td>\n      <td>6.4615</td>\n      <td>0.854701</td>\n      <td>1.878089</td>\n      <td>2.2861</td>\n      <td>0.892857</td>\n      <td>1.877705</td>\n      <td>1.8828</td>\n      <td>0.943396</td>\n      <td>1.863802</td>\n      <td>6.4615</td>\n      <td>1.801802</td>\n      <td>-22.104592</td>\n      <td>2.746955</td>\n      <td>2.574926</td>\n      <td>0.561722</td>\n      <td>1.944769</td>\n      <td>2.203243</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"PERMNO          158913\nprd             158913\nmom482          131830\nmom242          155910\nyear            158913\nRET             158913\nind             158913\nbm              158913\nop              158913\ngp              158913\ninv             158793\nmom11           158913\nmom122          158913\namhd            141609\nivol_capm       158908\nivol_ff5        158908\nbeta_bw         158913\nMAX             158913\nvol1m           158907\nvol6m           158790\nvol12m          158624\nBAspr           148528\nsize            158913\nlbm             158913\nlop             158913\nlgp             158913\nlinv            158913\nllme            158913\nl1amhd          141747\nl1MAX           158906\nl1BAspr         148550\nl3amhd          141981\nl3MAX           158872\nl3BAspr         148514\nl6amhd          142260\nl6MAX           158838\nl6BAspr         148581\nl12amhd         142587\nl12MAX          158906\nl12BAspr        148611\nl12mom122       158564\nl12ivol_capm    158779\nl12ivol_ff5     158779\nl12beta_bw      158818\nl12vol6m        158591\nl12vol12m       156844\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# 2. pEDA #\n\nprint(df.shape)\ndf = df[(df.RET>-50)&(df.RET<75)]\nprint(df.shape)\ndf.RET.hist()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:29.681138Z","iopub.execute_input":"2022-08-25T15:36:29.682267Z","iopub.status.idle":"2022-08-25T15:36:29.914602Z","shell.execute_reply.started":"2022-08-25T15:36:29.682229Z","shell.execute_reply":"2022-08-25T15:36:29.913583Z"},"trusted":true},"execution_count":209,"outputs":[{"name":"stdout","text":"(158913, 46)\n(156358, 46)\n","output_type":"stream"},{"execution_count":209,"output_type":"execute_result","data":{"text/plain":"<AxesSubplot:>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# explore feature distibution, adjust if seems unreasonable","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:29.916613Z","iopub.execute_input":"2022-08-25T15:36:29.917576Z","iopub.status.idle":"2022-08-25T15:36:29.922008Z","shell.execute_reply.started":"2022-08-25T15:36:29.917537Z","shell.execute_reply":"2022-08-25T15:36:29.921014Z"},"trusted":true},"execution_count":210,"outputs":[]},{"cell_type":"code","source":"# add dummies for some missing features\n\nfeatures_miss_dummies = ['amhd', 'BAspr']\n\nfor col in features_miss_dummies:\n    df[col+'_miss'] = df[col].isnull().astype(int)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:29.924818Z","iopub.execute_input":"2022-08-25T15:36:29.925825Z","iopub.status.idle":"2022-08-25T15:36:29.997691Z","shell.execute_reply.started":"2022-08-25T15:36:29.925789Z","shell.execute_reply":"2022-08-25T15:36:29.994910Z"},"trusted":true},"execution_count":211,"outputs":[{"execution_count":211,"output_type":"execute_result","data":{"text/plain":"     PERMNO  prd     mom482     mom242  year     RET   ind        bm  \\\n612   10016  479   8.947149 -14.382829  1998  8.4335  21.0 -0.791018   \n613   10016  480  18.465014  -4.988726  1998 -2.6823  21.0 -0.791018   \n614   10016  481   1.909526 -17.673870  1998  8.3558  21.0 -0.791018   \n615   10016  482  19.908305 -12.287263  1998 -1.2575  21.0 -0.791018   \n616   10016  483  17.362376   7.691972  1998 -2.1094  21.0 -0.922990   \n\n           op        gp       inv   mom11     mom122      amhd  ivol_capm  \\\n612  0.072323  0.295900  0.014439 -4.1636   5.347944  1.886058   2.794417   \n613  0.072323  0.295900  0.014439  8.4335   0.347849  1.878089   1.084225   \n614  0.072323  0.295900  0.014439 -2.6823  11.591432  1.780930   1.940155   \n615  0.072323  0.295900  0.014439  8.3558   4.702749  1.782176   1.875627   \n616  0.018059  0.235379 -0.079193 -1.2575   2.447512  1.740401   1.285586   \n\n     ivol_ff5   beta_bw     MAX     vol1m     vol6m    vol12m     BAspr  \\\n612  2.233004  0.604865  7.5059  2.676935  2.064740  1.921379  0.490196   \n613  1.055252  0.607529  2.2861  1.107580  2.051557  1.913618  0.892857   \n614  1.624940  0.603401  3.9448  2.034676  2.014330  1.933689  1.415094   \n615  1.851700  0.634076  6.4615  1.937531  2.059049  1.940786  0.854701   \n616  1.204386  0.662170  2.7647  1.511975  2.075485  1.820699  1.702128   \n\n         size       lbm       lop       lgp      linv      llme    l1amhd  \\\n612  5.165042 -1.049362 -0.000786  0.269403 -0.230583  5.101937  1.880899   \n613  5.255047 -1.049362 -0.000786  0.269403 -0.230583  5.110254  1.886058   \n614  5.232267 -1.049362 -0.000786  0.269403 -0.230583  5.089420  1.878089   \n615  5.316201 -1.049362 -0.000786  0.269403 -0.230583  5.130662  1.780930   \n616  5.310584 -0.791018  0.072323  0.295900  0.014439  5.237193  1.782176   \n\n      l1MAX   l1BAspr    l3amhd   l3MAX   l3BAspr    l6amhd   l6MAX   l6BAspr  \\\n612  9.3849  2.884615  1.626017  4.1457  0.904977  1.772101  1.7982  1.785714   \n613  7.5059  0.490196  1.877705  1.8828  0.943396  1.665406  3.0901  0.416667   \n614  2.2861  0.892857  1.880899  9.3849  2.884615  1.685590  2.6136  1.886792   \n615  3.9448  1.415094  1.886058  7.5059  0.490196  1.626017  4.1457  0.904977   \n616  6.4615  0.854701  1.878089  2.2861  0.892857  1.877705  1.8828  0.943396   \n\n      l12amhd  l12MAX  l12BAspr  l12mom122  l12ivol_capm  l12ivol_ff5  \\\n612  1.772232  9.3849  2.030457  -4.657184      1.772763     1.469041   \n613  1.744671  7.5059  2.040816 -15.288829      1.301430     1.257374   \n614  1.866881  2.2861  3.125000 -12.753601      1.602968     1.380534   \n615  1.835511  3.9448  2.020202 -22.631577      1.901159     1.705052   \n616  1.863802  6.4615  1.801802 -22.104592      2.746955     2.574926   \n\n     l12beta_bw  l12vol6m  l12vol12m  amhd_miss  BAspr_miss  \n612    0.544673  2.239938   2.213453          0           0  \n613    0.528119  2.257473   2.177653          0           0  \n614    0.580225  2.077777   2.189271          0           0  \n615    0.527894  1.778749   2.145822          0           0  \n616    0.561722  1.944769   2.203243          0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PERMNO</th>\n      <th>prd</th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>year</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>612</th>\n      <td>10016</td>\n      <td>479</td>\n      <td>8.947149</td>\n      <td>-14.382829</td>\n      <td>1998</td>\n      <td>8.4335</td>\n      <td>21.0</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.295900</td>\n      <td>0.014439</td>\n      <td>-4.1636</td>\n      <td>5.347944</td>\n      <td>1.886058</td>\n      <td>2.794417</td>\n      <td>2.233004</td>\n      <td>0.604865</td>\n      <td>7.5059</td>\n      <td>2.676935</td>\n      <td>2.064740</td>\n      <td>1.921379</td>\n      <td>0.490196</td>\n      <td>5.165042</td>\n      <td>-1.049362</td>\n      <td>-0.000786</td>\n      <td>0.269403</td>\n      <td>-0.230583</td>\n      <td>5.101937</td>\n      <td>1.880899</td>\n      <td>9.3849</td>\n      <td>2.884615</td>\n      <td>1.626017</td>\n      <td>4.1457</td>\n      <td>0.904977</td>\n      <td>1.772101</td>\n      <td>1.7982</td>\n      <td>1.785714</td>\n      <td>1.772232</td>\n      <td>9.3849</td>\n      <td>2.030457</td>\n      <td>-4.657184</td>\n      <td>1.772763</td>\n      <td>1.469041</td>\n      <td>0.544673</td>\n      <td>2.239938</td>\n      <td>2.213453</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>10016</td>\n      <td>480</td>\n      <td>18.465014</td>\n      <td>-4.988726</td>\n      <td>1998</td>\n      <td>-2.6823</td>\n      <td>21.0</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.295900</td>\n      <td>0.014439</td>\n      <td>8.4335</td>\n      <td>0.347849</td>\n      <td>1.878089</td>\n      <td>1.084225</td>\n      <td>1.055252</td>\n      <td>0.607529</td>\n      <td>2.2861</td>\n      <td>1.107580</td>\n      <td>2.051557</td>\n      <td>1.913618</td>\n      <td>0.892857</td>\n      <td>5.255047</td>\n      <td>-1.049362</td>\n      <td>-0.000786</td>\n      <td>0.269403</td>\n      <td>-0.230583</td>\n      <td>5.110254</td>\n      <td>1.886058</td>\n      <td>7.5059</td>\n      <td>0.490196</td>\n      <td>1.877705</td>\n      <td>1.8828</td>\n      <td>0.943396</td>\n      <td>1.665406</td>\n      <td>3.0901</td>\n      <td>0.416667</td>\n      <td>1.744671</td>\n      <td>7.5059</td>\n      <td>2.040816</td>\n      <td>-15.288829</td>\n      <td>1.301430</td>\n      <td>1.257374</td>\n      <td>0.528119</td>\n      <td>2.257473</td>\n      <td>2.177653</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>614</th>\n      <td>10016</td>\n      <td>481</td>\n      <td>1.909526</td>\n      <td>-17.673870</td>\n      <td>1998</td>\n      <td>8.3558</td>\n      <td>21.0</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.295900</td>\n      <td>0.014439</td>\n      <td>-2.6823</td>\n      <td>11.591432</td>\n      <td>1.780930</td>\n      <td>1.940155</td>\n      <td>1.624940</td>\n      <td>0.603401</td>\n      <td>3.9448</td>\n      <td>2.034676</td>\n      <td>2.014330</td>\n      <td>1.933689</td>\n      <td>1.415094</td>\n      <td>5.232267</td>\n      <td>-1.049362</td>\n      <td>-0.000786</td>\n      <td>0.269403</td>\n      <td>-0.230583</td>\n      <td>5.089420</td>\n      <td>1.878089</td>\n      <td>2.2861</td>\n      <td>0.892857</td>\n      <td>1.880899</td>\n      <td>9.3849</td>\n      <td>2.884615</td>\n      <td>1.685590</td>\n      <td>2.6136</td>\n      <td>1.886792</td>\n      <td>1.866881</td>\n      <td>2.2861</td>\n      <td>3.125000</td>\n      <td>-12.753601</td>\n      <td>1.602968</td>\n      <td>1.380534</td>\n      <td>0.580225</td>\n      <td>2.077777</td>\n      <td>2.189271</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>615</th>\n      <td>10016</td>\n      <td>482</td>\n      <td>19.908305</td>\n      <td>-12.287263</td>\n      <td>1998</td>\n      <td>-1.2575</td>\n      <td>21.0</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.295900</td>\n      <td>0.014439</td>\n      <td>8.3558</td>\n      <td>4.702749</td>\n      <td>1.782176</td>\n      <td>1.875627</td>\n      <td>1.851700</td>\n      <td>0.634076</td>\n      <td>6.4615</td>\n      <td>1.937531</td>\n      <td>2.059049</td>\n      <td>1.940786</td>\n      <td>0.854701</td>\n      <td>5.316201</td>\n      <td>-1.049362</td>\n      <td>-0.000786</td>\n      <td>0.269403</td>\n      <td>-0.230583</td>\n      <td>5.130662</td>\n      <td>1.780930</td>\n      <td>3.9448</td>\n      <td>1.415094</td>\n      <td>1.886058</td>\n      <td>7.5059</td>\n      <td>0.490196</td>\n      <td>1.626017</td>\n      <td>4.1457</td>\n      <td>0.904977</td>\n      <td>1.835511</td>\n      <td>3.9448</td>\n      <td>2.020202</td>\n      <td>-22.631577</td>\n      <td>1.901159</td>\n      <td>1.705052</td>\n      <td>0.527894</td>\n      <td>1.778749</td>\n      <td>2.145822</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>616</th>\n      <td>10016</td>\n      <td>483</td>\n      <td>17.362376</td>\n      <td>7.691972</td>\n      <td>1998</td>\n      <td>-2.1094</td>\n      <td>21.0</td>\n      <td>-0.922990</td>\n      <td>0.018059</td>\n      <td>0.235379</td>\n      <td>-0.079193</td>\n      <td>-1.2575</td>\n      <td>2.447512</td>\n      <td>1.740401</td>\n      <td>1.285586</td>\n      <td>1.204386</td>\n      <td>0.662170</td>\n      <td>2.7647</td>\n      <td>1.511975</td>\n      <td>2.075485</td>\n      <td>1.820699</td>\n      <td>1.702128</td>\n      <td>5.310584</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.295900</td>\n      <td>0.014439</td>\n      <td>5.237193</td>\n      <td>1.782176</td>\n      <td>6.4615</td>\n      <td>0.854701</td>\n      <td>1.878089</td>\n      <td>2.2861</td>\n      <td>0.892857</td>\n      <td>1.877705</td>\n      <td>1.8828</td>\n      <td>0.943396</td>\n      <td>1.863802</td>\n      <td>6.4615</td>\n      <td>1.801802</td>\n      <td>-22.104592</td>\n      <td>2.746955</td>\n      <td>2.574926</td>\n      <td>0.561722</td>\n      <td>1.944769</td>\n      <td>2.203243</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 3. Train-test split #\n\ntemp_cols = ['PERMNO', 'prd', 'year']\n\ntrain = df[df.prd<(min_prd+60)]\ntest = df[df.prd==(min_prd+60)]\ndisplay(train.shape, test.shape, train.head(3), test.head(3))\ntrain.drop(columns=temp_cols, inplace=True)\ntest.drop(columns=temp_cols, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:29.999701Z","iopub.execute_input":"2022-08-25T15:36:30.000391Z","iopub.status.idle":"2022-08-25T15:36:30.167236Z","shell.execute_reply.started":"2022-08-25T15:36:30.000344Z","shell.execute_reply":"2022-08-25T15:36:30.166355Z"},"trusted":true},"execution_count":212,"outputs":[{"output_type":"display_data","data":{"text/plain":"(152082, 45)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(2160, 45)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"        mom482     mom242     RET   ind        bm        op      gp       inv  \\\n612   8.947149 -14.382829  8.4335  21.0 -0.791018  0.072323  0.2959  0.014439   \n613  18.465014  -4.988726 -2.6823  21.0 -0.791018  0.072323  0.2959  0.014439   \n614   1.909526 -17.673870  8.3558  21.0 -0.791018  0.072323  0.2959  0.014439   \n\n      mom11     mom122      amhd  ivol_capm  ivol_ff5   beta_bw     MAX  \\\n612 -4.1636   5.347944  1.886058   2.794417  2.233004  0.604865  7.5059   \n613  8.4335   0.347849  1.878089   1.084225  1.055252  0.607529  2.2861   \n614 -2.6823  11.591432  1.780930   1.940155  1.624940  0.603401  3.9448   \n\n        vol1m     vol6m    vol12m     BAspr      size       lbm       lop  \\\n612  2.676935  2.064740  1.921379  0.490196  5.165042 -1.049362 -0.000786   \n613  1.107580  2.051557  1.913618  0.892857  5.255047 -1.049362 -0.000786   \n614  2.034676  2.014330  1.933689  1.415094  5.232267 -1.049362 -0.000786   \n\n          lgp      linv      llme    l1amhd   l1MAX   l1BAspr    l3amhd  \\\n612  0.269403 -0.230583  5.101937  1.880899  9.3849  2.884615  1.626017   \n613  0.269403 -0.230583  5.110254  1.886058  7.5059  0.490196  1.877705   \n614  0.269403 -0.230583  5.089420  1.878089  2.2861  0.892857  1.880899   \n\n      l3MAX   l3BAspr    l6amhd   l6MAX   l6BAspr   l12amhd  l12MAX  l12BAspr  \\\n612  4.1457  0.904977  1.772101  1.7982  1.785714  1.772232  9.3849  2.030457   \n613  1.8828  0.943396  1.665406  3.0901  0.416667  1.744671  7.5059  2.040816   \n614  9.3849  2.884615  1.685590  2.6136  1.886792  1.866881  2.2861  3.125000   \n\n     l12mom122  l12ivol_capm  l12ivol_ff5  l12beta_bw  l12vol6m  l12vol12m  \\\n612  -4.657184      1.772763     1.469041    0.544673  2.239938   2.213453   \n613 -15.288829      1.301430     1.257374    0.528119  2.257473   2.177653   \n614 -12.753601      1.602968     1.380534    0.580225  2.077777   2.189271   \n\n     amhd_miss  BAspr_miss  \n612          0           0  \n613          0           0  \n614          0           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>612</th>\n      <td>8.947149</td>\n      <td>-14.382829</td>\n      <td>8.4335</td>\n      <td>21.0</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.2959</td>\n      <td>0.014439</td>\n      <td>-4.1636</td>\n      <td>5.347944</td>\n      <td>1.886058</td>\n      <td>2.794417</td>\n      <td>2.233004</td>\n      <td>0.604865</td>\n      <td>7.5059</td>\n      <td>2.676935</td>\n      <td>2.064740</td>\n      <td>1.921379</td>\n      <td>0.490196</td>\n      <td>5.165042</td>\n      <td>-1.049362</td>\n      <td>-0.000786</td>\n      <td>0.269403</td>\n      <td>-0.230583</td>\n      <td>5.101937</td>\n      <td>1.880899</td>\n      <td>9.3849</td>\n      <td>2.884615</td>\n      <td>1.626017</td>\n      <td>4.1457</td>\n      <td>0.904977</td>\n      <td>1.772101</td>\n      <td>1.7982</td>\n      <td>1.785714</td>\n      <td>1.772232</td>\n      <td>9.3849</td>\n      <td>2.030457</td>\n      <td>-4.657184</td>\n      <td>1.772763</td>\n      <td>1.469041</td>\n      <td>0.544673</td>\n      <td>2.239938</td>\n      <td>2.213453</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>613</th>\n      <td>18.465014</td>\n      <td>-4.988726</td>\n      <td>-2.6823</td>\n      <td>21.0</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.2959</td>\n      <td>0.014439</td>\n      <td>8.4335</td>\n      <td>0.347849</td>\n      <td>1.878089</td>\n      <td>1.084225</td>\n      <td>1.055252</td>\n      <td>0.607529</td>\n      <td>2.2861</td>\n      <td>1.107580</td>\n      <td>2.051557</td>\n      <td>1.913618</td>\n      <td>0.892857</td>\n      <td>5.255047</td>\n      <td>-1.049362</td>\n      <td>-0.000786</td>\n      <td>0.269403</td>\n      <td>-0.230583</td>\n      <td>5.110254</td>\n      <td>1.886058</td>\n      <td>7.5059</td>\n      <td>0.490196</td>\n      <td>1.877705</td>\n      <td>1.8828</td>\n      <td>0.943396</td>\n      <td>1.665406</td>\n      <td>3.0901</td>\n      <td>0.416667</td>\n      <td>1.744671</td>\n      <td>7.5059</td>\n      <td>2.040816</td>\n      <td>-15.288829</td>\n      <td>1.301430</td>\n      <td>1.257374</td>\n      <td>0.528119</td>\n      <td>2.257473</td>\n      <td>2.177653</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>614</th>\n      <td>1.909526</td>\n      <td>-17.673870</td>\n      <td>8.3558</td>\n      <td>21.0</td>\n      <td>-0.791018</td>\n      <td>0.072323</td>\n      <td>0.2959</td>\n      <td>0.014439</td>\n      <td>-2.6823</td>\n      <td>11.591432</td>\n      <td>1.780930</td>\n      <td>1.940155</td>\n      <td>1.624940</td>\n      <td>0.603401</td>\n      <td>3.9448</td>\n      <td>2.034676</td>\n      <td>2.014330</td>\n      <td>1.933689</td>\n      <td>1.415094</td>\n      <td>5.232267</td>\n      <td>-1.049362</td>\n      <td>-0.000786</td>\n      <td>0.269403</td>\n      <td>-0.230583</td>\n      <td>5.089420</td>\n      <td>1.878089</td>\n      <td>2.2861</td>\n      <td>0.892857</td>\n      <td>1.880899</td>\n      <td>9.3849</td>\n      <td>2.884615</td>\n      <td>1.685590</td>\n      <td>2.6136</td>\n      <td>1.886792</td>\n      <td>1.866881</td>\n      <td>2.2861</td>\n      <td>3.125000</td>\n      <td>-12.753601</td>\n      <td>1.602968</td>\n      <td>1.380534</td>\n      <td>0.580225</td>\n      <td>2.077777</td>\n      <td>2.189271</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"         mom482     mom242      RET   ind        bm        op        gp  \\\n1097 -75.881509 -72.706514 -18.1347  15.0 -1.137114  0.059508  0.350297   \n1427  17.793448  60.697573   4.6164   2.0 -0.308836  0.211760  0.875338   \n1627 -68.192896 -72.706514   3.3783  43.0 -1.318496 -0.002719  0.440526   \n\n           inv    mom11     mom122      amhd  ivol_capm  ivol_ff5   beta_bw  \\\n1097 -0.071339  18.7187 -61.937195  2.500611   7.350406  5.120025  0.674696   \n1427  0.020187   4.7047 -23.721961  0.804802   1.295174  1.171381  0.752184   \n1627 -0.150078  -0.1000 -46.058962       NaN   4.702748  3.919165  0.611406   \n\n          MAX     vol1m     vol6m    vol12m      BAspr      size       lbm  \\\n1097  17.9611  7.466631  6.845902  5.011527   0.229095  4.224549 -1.862209   \n1427   3.5123  1.552548  3.183676  3.153236   0.400147  5.579947  0.000848   \n1627  13.0791  4.751450  4.851858  6.097074  12.500000  1.731850 -2.123096   \n\n           lop       lgp      linv      llme    l1amhd    l1MAX    l1BAspr  \\\n1097  0.045365  0.330293 -0.084081  5.600352  2.314870  17.6536   0.140449   \n1427  0.195438  0.867714  0.029759  5.779157  0.750326   5.9521   0.523013   \n1627  0.045255  0.489898  0.076053  2.334026       NaN   7.3121  11.009174   \n\n        l3amhd    l3MAX   l3BAspr    l6amhd    l6MAX   l6BAspr   l12amhd  \\\n1097  2.257347  10.2894  0.307692  2.412977   2.9073  0.480256  2.460650   \n1427  0.847930   8.2261  0.369108  2.153512   2.6281  0.325000  2.500545   \n1627       NaN   4.7120  1.834862       NaN  17.5369  7.407407       NaN   \n\n       l12MAX  l12BAspr   l12mom122  l12ivol_capm  l12ivol_ff5  l12beta_bw  \\\n1097  17.6536  0.466881  -41.804197      3.850307     3.525294    0.592745   \n1427   5.9521  0.029163  105.899232      2.768857     2.536051    0.640884   \n1627   7.3121  5.000000  -59.271061      3.662766     3.025033    0.676663   \n\n      l12vol6m  l12vol12m  amhd_miss  BAspr_miss  \n1097  2.462821   2.750145          0           0  \n1427  3.461543   3.035536          0           0  \n1627  7.364649   7.438592          1           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>mom482</th>\n      <th>mom242</th>\n      <th>RET</th>\n      <th>ind</th>\n      <th>bm</th>\n      <th>op</th>\n      <th>gp</th>\n      <th>inv</th>\n      <th>mom11</th>\n      <th>mom122</th>\n      <th>amhd</th>\n      <th>ivol_capm</th>\n      <th>ivol_ff5</th>\n      <th>beta_bw</th>\n      <th>MAX</th>\n      <th>vol1m</th>\n      <th>vol6m</th>\n      <th>vol12m</th>\n      <th>BAspr</th>\n      <th>size</th>\n      <th>lbm</th>\n      <th>lop</th>\n      <th>lgp</th>\n      <th>linv</th>\n      <th>llme</th>\n      <th>l1amhd</th>\n      <th>l1MAX</th>\n      <th>l1BAspr</th>\n      <th>l3amhd</th>\n      <th>l3MAX</th>\n      <th>l3BAspr</th>\n      <th>l6amhd</th>\n      <th>l6MAX</th>\n      <th>l6BAspr</th>\n      <th>l12amhd</th>\n      <th>l12MAX</th>\n      <th>l12BAspr</th>\n      <th>l12mom122</th>\n      <th>l12ivol_capm</th>\n      <th>l12ivol_ff5</th>\n      <th>l12beta_bw</th>\n      <th>l12vol6m</th>\n      <th>l12vol12m</th>\n      <th>amhd_miss</th>\n      <th>BAspr_miss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1097</th>\n      <td>-75.881509</td>\n      <td>-72.706514</td>\n      <td>-18.1347</td>\n      <td>15.0</td>\n      <td>-1.137114</td>\n      <td>0.059508</td>\n      <td>0.350297</td>\n      <td>-0.071339</td>\n      <td>18.7187</td>\n      <td>-61.937195</td>\n      <td>2.500611</td>\n      <td>7.350406</td>\n      <td>5.120025</td>\n      <td>0.674696</td>\n      <td>17.9611</td>\n      <td>7.466631</td>\n      <td>6.845902</td>\n      <td>5.011527</td>\n      <td>0.229095</td>\n      <td>4.224549</td>\n      <td>-1.862209</td>\n      <td>0.045365</td>\n      <td>0.330293</td>\n      <td>-0.084081</td>\n      <td>5.600352</td>\n      <td>2.314870</td>\n      <td>17.6536</td>\n      <td>0.140449</td>\n      <td>2.257347</td>\n      <td>10.2894</td>\n      <td>0.307692</td>\n      <td>2.412977</td>\n      <td>2.9073</td>\n      <td>0.480256</td>\n      <td>2.460650</td>\n      <td>17.6536</td>\n      <td>0.466881</td>\n      <td>-41.804197</td>\n      <td>3.850307</td>\n      <td>3.525294</td>\n      <td>0.592745</td>\n      <td>2.462821</td>\n      <td>2.750145</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1427</th>\n      <td>17.793448</td>\n      <td>60.697573</td>\n      <td>4.6164</td>\n      <td>2.0</td>\n      <td>-0.308836</td>\n      <td>0.211760</td>\n      <td>0.875338</td>\n      <td>0.020187</td>\n      <td>4.7047</td>\n      <td>-23.721961</td>\n      <td>0.804802</td>\n      <td>1.295174</td>\n      <td>1.171381</td>\n      <td>0.752184</td>\n      <td>3.5123</td>\n      <td>1.552548</td>\n      <td>3.183676</td>\n      <td>3.153236</td>\n      <td>0.400147</td>\n      <td>5.579947</td>\n      <td>0.000848</td>\n      <td>0.195438</td>\n      <td>0.867714</td>\n      <td>0.029759</td>\n      <td>5.779157</td>\n      <td>0.750326</td>\n      <td>5.9521</td>\n      <td>0.523013</td>\n      <td>0.847930</td>\n      <td>8.2261</td>\n      <td>0.369108</td>\n      <td>2.153512</td>\n      <td>2.6281</td>\n      <td>0.325000</td>\n      <td>2.500545</td>\n      <td>5.9521</td>\n      <td>0.029163</td>\n      <td>105.899232</td>\n      <td>2.768857</td>\n      <td>2.536051</td>\n      <td>0.640884</td>\n      <td>3.461543</td>\n      <td>3.035536</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1627</th>\n      <td>-68.192896</td>\n      <td>-72.706514</td>\n      <td>3.3783</td>\n      <td>43.0</td>\n      <td>-1.318496</td>\n      <td>-0.002719</td>\n      <td>0.440526</td>\n      <td>-0.150078</td>\n      <td>-0.1000</td>\n      <td>-46.058962</td>\n      <td>NaN</td>\n      <td>4.702748</td>\n      <td>3.919165</td>\n      <td>0.611406</td>\n      <td>13.0791</td>\n      <td>4.751450</td>\n      <td>4.851858</td>\n      <td>6.097074</td>\n      <td>12.500000</td>\n      <td>1.731850</td>\n      <td>-2.123096</td>\n      <td>0.045255</td>\n      <td>0.489898</td>\n      <td>0.076053</td>\n      <td>2.334026</td>\n      <td>NaN</td>\n      <td>7.3121</td>\n      <td>11.009174</td>\n      <td>NaN</td>\n      <td>4.7120</td>\n      <td>1.834862</td>\n      <td>NaN</td>\n      <td>17.5369</td>\n      <td>7.407407</td>\n      <td>NaN</td>\n      <td>7.3121</td>\n      <td>5.000000</td>\n      <td>-59.271061</td>\n      <td>3.662766</td>\n      <td>3.025033</td>\n      <td>0.676663</td>\n      <td>7.364649</td>\n      <td>7.438592</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 4. Missing values #\n\ncol_ignore = ['RET']\ncol_cat = ['ind']\ncol_num = [x for x in train.columns if x not in col_ignore+col_cat]\n\nfor col in col_num:\n    train[col] = train[col].fillna(train[col].median())\n    test[col] = test[col].fillna(train[col].median())\n\nfor col in col_cat:\n    train[col] = train[col].fillna(value=-1000)\n    test[col] = test[col].fillna(value=-1000)\n    \ndisplay(train.count())","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:30.168566Z","iopub.execute_input":"2022-08-25T15:36:30.169103Z","iopub.status.idle":"2022-08-25T15:36:30.562552Z","shell.execute_reply.started":"2022-08-25T15:36:30.169068Z","shell.execute_reply":"2022-08-25T15:36:30.561621Z"},"trusted":true},"execution_count":213,"outputs":[{"output_type":"display_data","data":{"text/plain":"mom482          152082\nmom242          152082\nRET             152082\nind             152082\nbm              152082\nop              152082\ngp              152082\ninv             152082\nmom11           152082\nmom122          152082\namhd            152082\nivol_capm       152082\nivol_ff5        152082\nbeta_bw         152082\nMAX             152082\nvol1m           152082\nvol6m           152082\nvol12m          152082\nBAspr           152082\nsize            152082\nlbm             152082\nlop             152082\nlgp             152082\nlinv            152082\nllme            152082\nl1amhd          152082\nl1MAX           152082\nl1BAspr         152082\nl3amhd          152082\nl3MAX           152082\nl3BAspr         152082\nl6amhd          152082\nl6MAX           152082\nl6BAspr         152082\nl12amhd         152082\nl12MAX          152082\nl12BAspr        152082\nl12mom122       152082\nl12ivol_capm    152082\nl12ivol_ff5     152082\nl12beta_bw      152082\nl12vol6m        152082\nl12vol12m       152082\namhd_miss       152082\nBAspr_miss      152082\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"# # [optional] Target Encoding\n\n# # first, do frequency encoding\n# freq_enc = (train.groupby('ind').size()) / len(train)\n# train['ind_fencoded'] = train['ind'].apply(lambda x : freq_enc[x])\n# test['ind_fencoded'] = test['ind'].apply(lambda x : freq_enc[x])\n\n# time1 = time.time()\n# encoder = CrossFoldEncoder(MEstimateEncoder, m=10)\n# train_encoded = encoder.fit_transform(train, train.RET, cols=col_cat)\n# test_encoded = encoder.transform(test)\n\n# train.drop(columns=col_cat, inplace=True)\n# test.drop(columns=col_cat,  inplace=True)\n# train = pd.concat([train, train_encoded], axis = 1)\n# test = pd.concat([test, test_encoded], axis = 1)\n\n# display(time.time()-time0, time.time()-time1)\n# display(train.shape, train.head(), train.count())\n# train0 = train.copy()\n# test0 = test.copy()","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:30.564021Z","iopub.execute_input":"2022-08-25T15:36:30.564569Z","iopub.status.idle":"2022-08-25T15:36:30.575957Z","shell.execute_reply.started":"2022-08-25T15:36:30.564532Z","shell.execute_reply":"2022-08-25T15:36:30.574748Z"},"trusted":true},"execution_count":214,"outputs":[]},{"cell_type":"code","source":"X_train = train.copy()\ny_train = X_train.pop('RET')\n\nX_test = test.copy()\ny_test = X_test.pop('RET')","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:30.577287Z","iopub.execute_input":"2022-08-25T15:36:30.577844Z","iopub.status.idle":"2022-08-25T15:36:30.645255Z","shell.execute_reply.started":"2022-08-25T15:36:30.577807Z","shell.execute_reply":"2022-08-25T15:36:30.643831Z"},"trusted":true},"execution_count":215,"outputs":[]},{"cell_type":"code","source":"# 5. Feature engineering #\n\ntime1 = time.time()\n\n# (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat),\n\nfeature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                        (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                        remainder=\"passthrough\")\n\nprint('Number of features before transformation: ', X_train.shape)\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nprint('time to do feature proprocessing: ', time.time()-time1)\nprint('Number of features after transformation: ', X_train.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:30.647600Z","iopub.execute_input":"2022-08-25T15:36:30.648484Z","iopub.status.idle":"2022-08-25T15:36:31.328944Z","shell.execute_reply.started":"2022-08-25T15:36:30.648442Z","shell.execute_reply":"2022-08-25T15:36:31.327964Z"},"trusted":true},"execution_count":216,"outputs":[{"name":"stdout","text":"Number of features before transformation:  (152082, 44)\ntime to do feature proprocessing:  0.664116382598877\nNumber of features after transformation:  (152082, 92)\n","output_type":"stream"}]},{"cell_type":"code","source":"# 6. Model fitting #\n\n# first, some trivial baselines:\nprint('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\nprint('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\ntime1 = time.time()\nxgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=300, max_depth=5, eta=0.03, colsample_bytree=0.6)\nxgb1.fit(X_train, y_train)\nprint('XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:31.332004Z","iopub.execute_input":"2022-08-25T15:36:31.332556Z","iopub.status.idle":"2022-08-25T15:36:34.643682Z","shell.execute_reply.started":"2022-08-25T15:36:31.332523Z","shell.execute_reply":"2022-08-25T15:36:34.642931Z"},"trusted":true},"execution_count":217,"outputs":[{"name":"stdout","text":"mae of a constant model 12.728897891302147\nR2 of a constant model 0.0\nXGB train: 12.366624062498225 0.05414099823420937 3.2824246883392334\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\nxgb = XGBRegressor(tree_method = 'gpu_hist')\nparam_grid = {'n_estimators':[400, 700], 'max_depth':[2,3,4], 'eta':[0.006, 0.012, 0.02],\n             'subsample':[0.6], 'colsample_bytree':[0.6]}\nxgbm = GridSearchCV(xgb, param_grid, cv=2, verbose=2, scoring='neg_mean_absolute_error')\nxgbm.fit(X_train, y_train)\nprint('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)\n# this runs for 40 min and finds \n# 'eta': 0.02, 'max_depth': 6, 'n_estimators': 500, 0.01095415380877135\nprint('XGB train:', mean_absolute_error(y_train, xgbm.predict(X_train)), r2_score(y_train, xgbm.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:36:34.647398Z","iopub.execute_input":"2022-08-25T15:36:34.649441Z","iopub.status.idle":"2022-08-25T15:37:39.500024Z","shell.execute_reply.started":"2022-08-25T15:36:34.649409Z","shell.execute_reply":"2022-08-25T15:37:39.499197Z"},"trusted":true},"execution_count":218,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 18 candidates, totalling 36 fits\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=400, subsample=0.6; total time=   1.0s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=400, subsample=0.6; total time=   1.0s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=2, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=400, subsample=0.6; total time=   1.2s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=400, subsample=0.6; total time=   1.3s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.9s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.9s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=400, subsample=0.6; total time=   1.6s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=400, subsample=0.6; total time=   1.6s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=700, subsample=0.6; total time=   2.5s\n[CV] END colsample_bytree=0.6, eta=0.006, max_depth=4, n_estimators=700, subsample=0.6; total time=   2.7s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=400, subsample=0.6; total time=   1.1s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=400, subsample=0.6; total time=   1.0s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=2, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=400, subsample=0.6; total time=   1.2s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=400, subsample=0.6; total time=   1.2s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=700, subsample=0.6; total time=   2.1s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.9s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=400, subsample=0.6; total time=   1.6s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=400, subsample=0.6; total time=   1.6s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=700, subsample=0.6; total time=   2.5s\n[CV] END colsample_bytree=0.6, eta=0.012, max_depth=4, n_estimators=700, subsample=0.6; total time=   2.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=400, subsample=0.6; total time=   1.2s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=400, subsample=0.6; total time=   1.0s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=2, n_estimators=700, subsample=0.6; total time=   1.5s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=400, subsample=0.6; total time=   1.2s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=400, subsample=0.6; total time=   1.2s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=700, subsample=0.6; total time=   1.9s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=3, n_estimators=700, subsample=0.6; total time=   2.2s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=400, subsample=0.6; total time=   1.6s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=400, subsample=0.6; total time=   1.6s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=700, subsample=0.6; total time=   2.6s\n[CV] END colsample_bytree=0.6, eta=0.02, max_depth=4, n_estimators=700, subsample=0.6; total time=   2.8s\nXGB {'colsample_bytree': 0.6, 'eta': 0.02, 'max_depth': 4, 'n_estimators': 700, 'subsample': 0.6} -12.58822289212777 62.402488470077515\nXGB train: 12.412174964481828 0.047041366689915765 64.84165334701538\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\n\ndef objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n    cv_regularizer=0.03\n    # Usually values between 0.1 and 0.2 work fine.\n\n    params = {\n        \"tree_method\": 'gpu_hist',\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1000),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 5),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.001, 0.05),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 0.95),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 0.95),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 30.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 200.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 10.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 10)    }\n    # usually it makes sense to resrtict hyperparameter space from some solutions which Optuna will find\n    # e.g., for tmx-joined data only (downsampled tmx), optuna keeps selecting depths of 2 and 3.\n    # for my purposes (smooth left side of prc, close to 1), those solutions are no good.\n\n    temp_out = []\n\n    for i in range(cv_runs):\n\n        X = X_train\n        y = y_train\n\n        model = XGBRegressor(**params, njobs=-1)\n        rkf = KFold(n_splits=n_splits, shuffle=True)\n        X_values = X.values\n        y_values = y.values\n        y_pred = np.zeros_like(y_values)\n        y_pred_train = np.zeros_like(y_values)\n        for train_index, test_index in rkf.split(X_values):\n            X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n            y_A, y_B = y_values[train_index], y_values[test_index]\n            model.fit(X_A, y_A, eval_set=[(X_B, y_B)], verbose = False)\n            y_pred[test_index] = model.predict(X_B)\n            y_pred_train[train_index] = model.predict(X_A)\n                      \n            \n        score_train = r2_score(y_train, y_pred_train)\n        score_test = r2_score(y_train, y_pred) \n        overfit = (score_train-score_test)\n        temp_out.append(score_test-cv_regularizer*overfit)\n        #temp_out.append(score_test)\n\n    return (np.mean(temp_out))\n\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=40)\n\nprint('Total time for hypermarameter optimization ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\n\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\n\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X_train, y_train)\nprint('Optuna XGB train:', \n      mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:38:42.796630Z","iopub.execute_input":"2022-08-25T15:38:42.797155Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2022-08-25 15:38:42,810]\u001b[0m A new study created in memory with name: no-name-d025d7cc-76b9-4ab5-8dba-36363d864620\u001b[0m\n\u001b[32m[I 2022-08-25 15:38:50,190]\u001b[0m Trial 0 finished with value: 0.014458207914420849 and parameters: {'n_estimators': 659, 'max_depth': 4, 'learning_rate': 0.013052060161372715, 'colsample_bytree': 0.6571130113731535, 'subsample': 0.7881604327770841, 'alpha': 0.19310032356171034, 'lambda': 0.6576837265534765, 'gamma': 3.6402778651217577e-09, 'min_child_weight': 0.8521415576636291}. Best is trial 0 with value: 0.014458207914420849.\u001b[0m\n\u001b[32m[I 2022-08-25 15:38:54,899]\u001b[0m Trial 1 finished with value: 0.01286501208344544 and parameters: {'n_estimators': 635, 'max_depth': 2, 'learning_rate': 0.02744269244663743, 'colsample_bytree': 0.8577592032896332, 'subsample': 0.709129855462602, 'alpha': 0.944508062671039, 'lambda': 197.9795520286687, 'gamma': 1.7677078333681452e-07, 'min_child_weight': 1.0434667705449268}. Best is trial 0 with value: 0.014458207914420849.\u001b[0m\n\u001b[32m[I 2022-08-25 15:39:00,744]\u001b[0m Trial 2 finished with value: 0.004755259832578923 and parameters: {'n_estimators': 685, 'max_depth': 3, 'learning_rate': 0.0018286487676202148, 'colsample_bytree': 0.1765470136733232, 'subsample': 0.6792909913836257, 'alpha': 3.473473700412708, 'lambda': 1.189342697170512, 'gamma': 3.4214474343451264e-06, 'min_child_weight': 0.12640967467392958}. Best is trial 0 with value: 0.014458207914420849.\u001b[0m\n\u001b[32m[I 2022-08-25 15:39:11,423]\u001b[0m Trial 3 finished with value: 0.01319626978075103 and parameters: {'n_estimators': 696, 'max_depth': 5, 'learning_rate': 0.030996884519320117, 'colsample_bytree': 0.7263699352596357, 'subsample': 0.8589889250617777, 'alpha': 3.9873949988465545, 'lambda': 99.48728788523543, 'gamma': 1.4958106068443456e-08, 'min_child_weight': 0.30325656580516414}. Best is trial 0 with value: 0.014458207914420849.\u001b[0m\n\u001b[32m[I 2022-08-25 15:39:16,969]\u001b[0m Trial 4 finished with value: 0.00979125242111091 and parameters: {'n_estimators': 793, 'max_depth': 2, 'learning_rate': 0.009488310243458389, 'colsample_bytree': 0.32721934252998586, 'subsample': 0.9054655532356832, 'alpha': 20.50731845748497, 'lambda': 0.14761545208230972, 'gamma': 0.000525669485926717, 'min_child_weight': 2.171288001301735}. Best is trial 0 with value: 0.014458207914420849.\u001b[0m\n\u001b[32m[I 2022-08-25 15:39:23,167]\u001b[0m Trial 5 finished with value: 0.014559176474461071 and parameters: {'n_estimators': 914, 'max_depth': 2, 'learning_rate': 0.0254796437581269, 'colsample_bytree': 0.747086190225672, 'subsample': 0.6799314547319699, 'alpha': 0.10008348297517204, 'lambda': 2.975612686765009, 'gamma': 0.03630216145464011, 'min_child_weight': 0.4105764840723095}. Best is trial 5 with value: 0.014559176474461071.\u001b[0m\n\u001b[32m[I 2022-08-25 15:39:30,137]\u001b[0m Trial 6 finished with value: 0.014247579256005418 and parameters: {'n_estimators': 593, 'max_depth': 4, 'learning_rate': 0.037004437610218543, 'colsample_bytree': 0.572393219159687, 'subsample': 0.5241239719904298, 'alpha': 0.16831796496246212, 'lambda': 24.924433031500296, 'gamma': 2.974277415409793, 'min_child_weight': 1.5040444783169824}. Best is trial 5 with value: 0.014559176474461071.\u001b[0m\n\u001b[32m[I 2022-08-25 15:39:41,060]\u001b[0m Trial 7 finished with value: 0.014115455368543361 and parameters: {'n_estimators': 922, 'max_depth': 4, 'learning_rate': 0.008748330265807158, 'colsample_bytree': 0.8706010740751192, 'subsample': 0.8070512680298986, 'alpha': 17.173672501591348, 'lambda': 18.953641872188303, 'gamma': 1.7232598865813436e-05, 'min_child_weight': 0.20405252384872352}. Best is trial 5 with value: 0.014559176474461071.\u001b[0m\n\u001b[32m[I 2022-08-25 15:39:46,779]\u001b[0m Trial 8 finished with value: 0.006368930186720285 and parameters: {'n_estimators': 840, 'max_depth': 2, 'learning_rate': 0.003102387058296642, 'colsample_bytree': 0.7071093018868261, 'subsample': 0.735653933418516, 'alpha': 0.6586039543222341, 'lambda': 4.214600150618966, 'gamma': 1.771762872384118e-08, 'min_child_weight': 0.22140583701330208}. Best is trial 5 with value: 0.014559176474461071.\u001b[0m\n\u001b[32m[I 2022-08-25 15:39:52,779]\u001b[0m Trial 9 finished with value: 0.013224879009473467 and parameters: {'n_estimators': 697, 'max_depth': 3, 'learning_rate': 0.024024942390720727, 'colsample_bytree': 0.11161124435175375, 'subsample': 0.5302380764010522, 'alpha': 0.1282209160816011, 'lambda': 3.3728193291141175, 'gamma': 7.355134740420984e-05, 'min_child_weight': 0.14057332405983144}. Best is trial 5 with value: 0.014559176474461071.\u001b[0m\n\u001b[32m[I 2022-08-25 15:40:01,055]\u001b[0m Trial 10 finished with value: 0.012510123136739503 and parameters: {'n_estimators': 999, 'max_depth': 3, 'learning_rate': 0.04988893584444937, 'colsample_bytree': 0.4221474292059725, 'subsample': 0.612414923063345, 'alpha': 0.43229643714693133, 'lambda': 0.14818011302054143, 'gamma': 0.11830695449944333, 'min_child_weight': 9.513456335139978}. Best is trial 5 with value: 0.014559176474461071.\u001b[0m\n\u001b[32m[I 2022-08-25 15:40:14,130]\u001b[0m Trial 11 finished with value: 0.013071595091052964 and parameters: {'n_estimators': 899, 'max_depth': 5, 'learning_rate': 0.017441917094622367, 'colsample_bytree': 0.6539416410916664, 'subsample': 0.7918393203833392, 'alpha': 0.10047387476294559, 'lambda': 0.708845316722423, 'gamma': 3.135751363922995e-10, 'min_child_weight': 0.439902989347341}. Best is trial 5 with value: 0.014559176474461071.\u001b[0m\n\u001b[32m[I 2022-08-25 15:40:20,280]\u001b[0m Trial 12 finished with value: 0.014091149104928197 and parameters: {'n_estimators': 542, 'max_depth': 4, 'learning_rate': 0.017398590606336677, 'colsample_bytree': 0.5410020241394118, 'subsample': 0.6283557242410868, 'alpha': 0.29841267057656645, 'lambda': 0.6482097357097728, 'gamma': 0.007085602070271426, 'min_child_weight': 0.5036783435118388}. Best is trial 5 with value: 0.014559176474461071.\u001b[0m\n\u001b[32m[I 2022-08-25 15:40:28,794]\u001b[0m Trial 13 finished with value: 0.014288496870138678 and parameters: {'n_estimators': 774, 'max_depth': 4, 'learning_rate': 0.019122329650488566, 'colsample_bytree': 0.7986147567749113, 'subsample': 0.7646380839654314, 'alpha': 0.2589530589507441, 'lambda': 1.6184673924461779, 'gamma': 7.993927624465956, 'min_child_weight': 3.4672066795645367}. Best is trial 5 with value: 0.014559176474461071.\u001b[0m\n\u001b[32m[I 2022-08-25 15:40:37,991]\u001b[0m Trial 14 finished with value: 0.01403462956324448 and parameters: {'n_estimators': 985, 'max_depth': 3, 'learning_rate': 0.03644401031880961, 'colsample_bytree': 0.9406912145528308, 'subsample': 0.6408412812929973, 'alpha': 1.7088073359855207, 'lambda': 12.099799284095454, 'gamma': 1.4440551239680416e-10, 'min_child_weight': 0.6323666312548366}. Best is trial 5 with value: 0.014559176474461071.\u001b[0m\n\u001b[32m[I 2022-08-25 15:40:46,012]\u001b[0m Trial 15 finished with value: 0.012716936740028405 and parameters: {'n_estimators': 518, 'max_depth': 5, 'learning_rate': 0.012101668131655106, 'colsample_bytree': 0.4458205135476697, 'subsample': 0.8572945891806131, 'alpha': 0.22290921922781973, 'lambda': 0.3429028006591023, 'gamma': 0.00994722968468138, 'min_child_weight': 0.9040604722609877}. Best is trial 5 with value: 0.014559176474461071.\u001b[0m\n\u001b[32m[I 2022-08-25 15:40:51,892]\u001b[0m Trial 16 finished with value: 0.015230675817438841 and parameters: {'n_estimators': 852, 'max_depth': 2, 'learning_rate': 0.04210279091400122, 'colsample_bytree': 0.6294119947010698, 'subsample': 0.9379557716415227, 'alpha': 0.5402470687041877, 'lambda': 8.000520664698527, 'gamma': 6.089964470954539e-07, 'min_child_weight': 4.268397559667533}. Best is trial 16 with value: 0.015230675817438841.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"# Evaluate performance of XGB models:\n\nprint('Constant guess: ', mean_absolute_error(y_test, np.ones(len(y_test))*y_test.mean()), \n      r2_score(y_test, np.ones(len(y_test))*y_test.mean()))\n\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\nprint('XGB GS test:', mean_absolute_error(y_test, xgbm.predict(X_test)), r2_score(y_test, xgbm.predict(X_test)))\n#print('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:37:39.514166Z","iopub.execute_input":"2022-08-25T15:37:39.514566Z","iopub.status.idle":"2022-08-25T15:37:39.603560Z","shell.execute_reply.started":"2022-08-25T15:37:39.514532Z","shell.execute_reply":"2022-08-25T15:37:39.602698Z"},"trusted":true},"execution_count":220,"outputs":[{"name":"stdout","text":"Constant guess:  11.71739125078972 0.0\nXGB test: 13.36810288645326 -0.30536535021911093\nXGB GS test: 13.361478497861068 -0.3070655453807609\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total time for a script: ', time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:37:39.606712Z","iopub.execute_input":"2022-08-25T15:37:39.607043Z","iopub.status.idle":"2022-08-25T15:37:39.615407Z","shell.execute_reply.started":"2022-08-25T15:37:39.607016Z","shell.execute_reply":"2022-08-25T15:37:39.614591Z"},"trusted":true},"execution_count":221,"outputs":[{"name":"stdout","text":"Total time for a script:  70.72457385063171\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_size = 0.1\n# df.reset_index(inplace=True, drop=True)\n# #random.seed(2)\n# test_index = random.sample(list(df.index), int(test_size*df.shape[0]))\n# train = df.iloc[list(set(df.index)-set(test_index))]\n# test = df.iloc[test_index]\n# train.reset_index(drop=True, inplace=True)\n# test.reset_index(drop=True, inplace=True)\n# display(train.shape, test.shape, train.head(3), test.head(3))","metadata":{"execution":{"iopub.status.busy":"2022-08-25T15:37:39.618783Z","iopub.execute_input":"2022-08-25T15:37:39.620748Z","iopub.status.idle":"2022-08-25T15:37:39.625993Z","shell.execute_reply.started":"2022-08-25T15:37:39.620719Z","shell.execute_reply":"2022-08-25T15:37:39.624984Z"},"trusted":true},"execution_count":222,"outputs":[]}]}
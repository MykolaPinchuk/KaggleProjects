{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### This is a new version of MLEAP scripts, started in late Aug 2022.\nIt will combine IProject_MLEAP_ANN and IP_MLEAP script, while improving them.","metadata":{}},{"cell_type":"markdown","source":"#### Outline\n\n1. Load libraries and data.\n2. pEDA. Look at feature distribution, fix them if they do not look right.\n3. Train-test split. Most likely couple years into test set. 2015-2018?. Impute missing values.\n4. Transform numerical features, add ohe for inds.\n5. Fit xgb as a baseline, then xgb optuna.\n6. Fit DL.\n\n","metadata":{}},{"cell_type":"code","source":"# 0. Import libraries #\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport os, time, math, re, warnings, random, gc, dill, optuna, pickle, shap\nimport statsmodels.api as sm\nfrom random import sample\n\nfrom sklearn.model_selection import train_test_split, KFold, PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNetCV\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import r2_score, mean_absolute_error\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBRegressor\nfrom optuna.integration import TFKerasPruningCallback\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import regularizers\n\nplt.style.use('seaborn-white')\nwarnings.simplefilter(action='ignore')\npd.set_option('display.max_columns', 110)\ngc.enable()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:57:01.722217Z","iopub.execute_input":"2022-09-07T17:57:01.722578Z","iopub.status.idle":"2022-09-07T17:57:01.734877Z","shell.execute_reply.started":"2022-09-07T17:57:01.722545Z","shell.execute_reply":"2022-09-07T17:57:01.733922Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:17:15.272911Z","iopub.execute_input":"2022-09-07T17:17:15.273557Z","iopub.status.idle":"2022-09-07T17:17:15.285376Z","shell.execute_reply.started":"2022-09-07T17:17:15.273519Z","shell.execute_reply":"2022-09-07T17:17:15.284110Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:17:15.287085Z","iopub.execute_input":"2022-09-07T17:17:15.287741Z","iopub.status.idle":"2022-09-07T17:17:15.310950Z","shell.execute_reply.started":"2022-09-07T17:17:15.287696Z","shell.execute_reply":"2022-09-07T17:17:15.309869Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"REPLICAS:  1\n","output_type":"stream"}]},{"cell_type":"code","source":"#min_prd_list = range(100, 676, 25)\nmin_prd_list = [250]\n# min_prd_list = [150, 250, 350, 450, 550, 650]\n#min_prd = min_prd_list[0]\nwindows_width = 3*12\ncv_regularizer=0.2\noptuna_trials = 20\ntime0 = time.time()\n\nresults = pd.DataFrame(columns = ['min_prd', 'xgbf_train', 'xgbf_val', 'xgbf_test', \n                                  'xgbgs_train', 'xgbgs_val', 'xgbgs_test', \n                                  'xgbo_train', 'xgbo_val', 'xgbo_test'])\nresults.min_prd = min_prd_list\n\nfor min_prd in min_prd_list:\n\n    with open('../input/mleap-46-preprocessed/MLEAP_46_v7.pkl', 'rb') as pickled_one:\n        df = pickle.load(pickled_one)\n    df = df[df.prd.isin(range(min_prd-1, min_prd+windows_width+10))]\n    df_cnt = df.count()\n    empty_cols = list(df_cnt[df_cnt<int(df.shape[0]/2)].index)\n    df.drop(columns=empty_cols, inplace=True)\n    #display(df.shape, df.head(), df.year.describe(), df.count())\n\n    features_miss_dummies = ['amhd', 'BAspr']\n    for col in features_miss_dummies:\n        if col in df.columns:\n            df[col+'_miss'] = df[col].isnull().astype(int)\n\n    temp_cols = ['PERMNO', 'year', 'prd']\n    df.reset_index(inplace=True, drop=True)\n    X = df.copy()\n    y = X.pop('RET')\n\n    train_indx = X.prd<(min_prd+windows_width-1)\n    val_indx = X['prd'].isin(range(min_prd+windows_width-1, min_prd+windows_width+2))\n    val_indx_extra = X['prd'].isin(range(min_prd+windows_width+5, min_prd+windows_width+8))\n    test_indx = X['prd'].isin(range(min_prd+windows_width+2, min_prd+windows_width+5))\n\n    X_train = X[train_indx]\n    X_val = X[val_indx]\n    X_val_extra = X[val_indx_extra]\n    X_test = X[test_indx]\n    y_train = y[train_indx]\n    y_val = y[val_indx]\n    y_val_extra = y[val_indx_extra]\n    y_test = y[test_indx]\n\n    #display(X_train.head(3), X_train.tail(3), y_train.head(3), y_train.tail(3))\n    display(X_train.shape, X_val.shape, X_test.shape, X_train.prd.describe(), X_val.prd.describe(), X_test.prd.describe())\n\n    X_train.drop(columns=temp_cols, inplace=True)\n    X_val.drop(columns=temp_cols, inplace=True)\n    X_val_extra.drop(columns=temp_cols, inplace=True)\n    X_test.drop(columns=temp_cols, inplace=True)\n\n    #display(X_train.tail())\n    col_cat = ['ind']\n    col_num = [x for x in X_train.columns if x not in col_cat]\n    for col in col_num:\n        X_train[col] = X_train[col].fillna(X_train[col].median())\n        X_val[col] = X_val[col].fillna(X_train[col].median())\n        X_val_extra[col] = X_val_extra[col].fillna(X_train[col].median())\n        X_test[col] = X_test[col].fillna(X_train[col].median())\n    for col in col_cat:\n        X_train[col] = X_train[col].fillna(value=-1000)\n        X_val[col] = X_val[col].fillna(value=-1000)\n        X_val_extra[col] = X_val_extra[col].fillna(value=-1000)\n        X_test[col] = X_test[col].fillna(value=-1000)\n\n    #display(X_train.tail())\n    feature_transformer = ColumnTransformer([('num', StandardScaler(), col_num),\n                                            (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), col_cat)], \n                                            remainder=\"passthrough\")\n\n    print('Number of features before transformation: ', X_train.shape)\n    train_index, val_index, val_index_extra, test_index = X_train.index, X_val.index, X_val_extra.index, X_test.index\n    X_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\n    X_val = pd.DataFrame(feature_transformer.transform(X_val), columns=feature_transformer.get_feature_names_out())\n    X_val_extra = pd.DataFrame(feature_transformer.transform(X_val_extra), columns=feature_transformer.get_feature_names_out())\n    X_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\n    print('time to do feature proprocessing: ')\n    print('Number of features after transformation: ', X_train.shape, X_val.shape, X_val_extra.shape, X_test.shape)\n    X_train.index = train_index\n    X_val.index = val_index\n    X_val_extra.index = val_index_extra\n    X_test.index = test_index\n    #display(X_train.tail())\n\n    X = pd.concat([X_train, X_val])\n    y = pd.concat([y_train, y_val])\n    #display(X,y)\n\n    X_ = pd.concat([X_train, X_val, X_val_extra])\n    y_ = pd.concat([y_train, y_val, y_val_extra])\n    #display(X,y, X_,y_)\n\n    print('mae of a constant model', mean_absolute_error(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n    print('R2 of a constant model', r2_score(df.RET, np.ones(df.shape[0])*(df.RET.mean())))\n\n    xgb1 = XGBRegressor(tree_method = 'gpu_hist', n_estimators=400, max_depth=4, eta=0.02, colsample_bytree=0.4, subsample=0.6)\n    xgb1.fit(X_train, y_train)\n    print('fixed XGB train:', mean_absolute_error(y_train, xgb1.predict(X_train)), r2_score(y_train, xgb1.predict(X_train)))\n    print('XGB val:', mean_absolute_error(y_val, xgb1.predict(X_val)), r2_score(y_val, xgb1.predict(X_val)))\n    print('XGB val extra:', mean_absolute_error(y_val_extra, xgb1.predict(X_val_extra)), r2_score(y_val_extra, xgb1.predict(X_val_extra)))\n    print('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_score(y_test, xgb1.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbf_train':'xgbf_test'] = \\\n    [r2_score(y_train, xgb1.predict(X_train)), \n    r2_score(y_val, xgb1.predict(X_val)),\n    r2_score(y_test, xgb1.predict(X_test))]\n\n    time1 = time.time()\n\n    # Create a list where train data indices are -1 and validation data indices are 0\n    split_index = [-1 if x in X_train.index else 0 for x in X.index]\n    pds = PredefinedSplit(test_fold = split_index)\n\n    xgb = XGBRegressor(tree_method = 'gpu_hist')\n    param_grid = {'n_estimators':[400, 600, 800], 'max_depth':[2,3,4,5], 'eta':[0.006, 0.012, 0.02], \n                  'subsample':[0.6], 'colsample_bytree':[0.6]}\n    xgbgs = GridSearchCV(estimator = xgb, cv=pds, param_grid=param_grid)\n\n    # Fit with all data\n    xgbgs.fit(X_, y_)\n\n    print('gs XGB', xgbgs.best_params_, xgbgs.best_score_, time.time()-time1)\n    print('XGB train:', mean_absolute_error(y_train, xgbgs.predict(X_train)), r2_score(y_train, xgbgs.predict(X_train)))\n    print('XGB validation:', mean_absolute_error(y_val, xgbgs.predict(X_val)), r2_score(y_val, xgbgs.predict(X_val)))\n    print('XGB validation extra:', mean_absolute_error(y_val_extra, xgbgs.predict(X_val_extra)), r2_score(y_val_extra, xgbgs.predict(X_val_extra)))\n    print('XGB test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_score(y_test, xgbgs.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbgs_train':'xgbgs_test'] = \\\n    [r2_score(y_train, xgbgs.predict(X_train)), \n    r2_score(y_val, xgbgs.predict(X_val)),\n    r2_score(y_test, xgbgs.predict(X_test))]\n\n    time1 = time.time()\n    def objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n        params = {\n        \"tree_method\": 'gpu_hist',\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 800, 1500),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.0005, 0.03),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.05, 0.95),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.1, 0.95),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 50.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 500.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 100.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50)    }\n\n        model = XGBRegressor(**params, njobs=-1)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=50, verbose = False)\n\n        score_train = r2_score(y_train, model.predict(X_train))\n        score_val = r2_score(y_val, model.predict(X_val))\n        score_val_extra = r2_score(y_val_extra, model.predict(X_val_extra)) \n        score_val = (score_val+score_val_extra)/2\n        overfit = np.abs(score_train-score_val)\n\n        return score_val-cv_regularizer*overfit\n\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(objective, n_trials=optuna_trials)\n    print('Total time for hypermarameter optimization ', time.time()-time1)\n    hp = study.best_params\n    for key, value in hp.items():\n        print(f\"{key:>20s} : {value}\")\n    print(f\"{'best objective value':>20s} : {study.best_value}\")\n    optuna_hyperpars = study.best_params\n    optuna_hyperpars['tree_method']='gpu_hist'\n    optuna_xgb = XGBRegressor(**optuna_hyperpars)\n    optuna_xgb.fit(X, y)\n    print('Optuna XGB train: \\n', \n          mean_absolute_error(y_train, optuna_xgb.predict(X_train)), r2_score(y_train, optuna_xgb.predict(X_train)), '\\nvalidation \\n',\n          mean_absolute_error(y_val, optuna_xgb.predict(X_val)), r2_score(y_val, optuna_xgb.predict(X_val)),\n          mean_absolute_error(y_val_extra, optuna_xgb.predict(X_val_extra)), r2_score(y_val_extra, optuna_xgb.predict(X_val_extra)), '\\ntest \\n',\n          mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_score(y_test, optuna_xgb.predict(X_test)))\n\n    results.loc[results.min_prd==min_prd,'xgbo_train':'xgbo_test'] = \\\n    [r2_score(y_train, optuna_xgb.predict(X_train)), \n    r2_score(y_val, optuna_xgb.predict(X_val)),\n    r2_score(y_test, optuna_xgb.predict(X_test))]\n\n    display(results)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:17:15.314844Z","iopub.execute_input":"2022-09-07T17:17:15.315877Z","iopub.status.idle":"2022-09-07T17:18:42.436533Z","shell.execute_reply.started":"2022-09-07T17:17:15.315840Z","shell.execute_reply":"2022-09-07T17:18:42.435529Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"(79162, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6405, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"(6311, 41)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    79162.000000\nmean       266.337409\nstd         10.396123\nmin        249.000000\n25%        257.000000\n50%        266.000000\n75%        275.000000\nmax        284.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    6405.000000\nmean      285.996565\nstd         0.816362\nmin       285.000000\n25%       285.000000\n50%       286.000000\n75%       287.000000\nmax       287.000000\nName: prd, dtype: float64"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"count    6311.000000\nmean      288.997782\nstd         0.816235\nmin       288.000000\n25%       288.000000\n50%       289.000000\n75%       290.000000\nmax       290.000000\nName: prd, dtype: float64"},"metadata":{}},{"name":"stdout","text":"Number of features before transformation:  (79162, 38)\ntime to do feature proprocessing: \nNumber of features after transformation:  (79162, 85) (6405, 85) (6616, 85) (6311, 85)\nmae of a constant model 8.616058689352922\nR2 of a constant model 0.0\nfixed XGB train: 8.094503280537975 0.08807399835850738\nXGB val: 7.80268307196985 -0.012211276662149384\nXGB val extra: 9.182509786895938 0.006809524790626353\nXGB test: 7.84572645024662 0.015254350212336232\ngs XGB {'colsample_bytree': 0.6, 'eta': 0.006, 'max_depth': 5, 'n_estimators': 400, 'subsample': 0.6} -0.009211110247068666 59.5159478187561\nXGB train: 8.232025093474888 0.060695330008433745\nXGB validation: 7.630334731974988 0.032497740585895496\nXGB validation extra: 8.985427732621966 0.05388213675932696\nXGB test: 7.787969462570123 0.02523901836111997\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-07 17:18:26,467]\u001b[0m A new study created in memory with name: no-name-44bb9e93-7dfe-4604-8850-65bd640ff05c\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:27,112]\u001b[0m Trial 0 finished with value: -0.001414592072799703 and parameters: {'n_estimators': 826, 'max_depth': 4, 'learning_rate': 0.003498941104432466, 'colsample_bytree': 0.5694761739947628, 'subsample': 0.5211493511587894, 'alpha': 0.5193423646858891, 'lambda': 1.0205932866775518, 'gamma': 0.0005441097389351968, 'min_child_weight': 0.10329063537082224}. Best is trial 0 with value: -0.001414592072799703.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:27,772]\u001b[0m Trial 1 finished with value: -0.0011726995494291792 and parameters: {'n_estimators': 865, 'max_depth': 5, 'learning_rate': 0.0039029533369027212, 'colsample_bytree': 0.8986659241488575, 'subsample': 0.10104743664147949, 'alpha': 44.91694825435624, 'lambda': 3.605442214753097, 'gamma': 6.865311436273415, 'min_child_weight': 0.24092611428586455}. Best is trial 1 with value: -0.0011726995494291792.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:28,500]\u001b[0m Trial 2 finished with value: -0.0010791896438250071 and parameters: {'n_estimators': 906, 'max_depth': 6, 'learning_rate': 0.029665409734482114, 'colsample_bytree': 0.5697885546160595, 'subsample': 0.6632932698994494, 'alpha': 8.498197272276892, 'lambda': 0.7514173442058966, 'gamma': 0.0372089631298453, 'min_child_weight': 1.110270890783641}. Best is trial 2 with value: -0.0010791896438250071.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:29,000]\u001b[0m Trial 3 finished with value: -0.0010423141643000288 and parameters: {'n_estimators': 1017, 'max_depth': 3, 'learning_rate': 0.005848481876256818, 'colsample_bytree': 0.439319685933996, 'subsample': 0.4209596330596076, 'alpha': 1.3599866142612846, 'lambda': 0.2377687187907435, 'gamma': 3.6616125433101203e-09, 'min_child_weight': 17.18283385946432}. Best is trial 3 with value: -0.0010423141643000288.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:29,747]\u001b[0m Trial 4 finished with value: -0.0008313287504827739 and parameters: {'n_estimators': 805, 'max_depth': 6, 'learning_rate': 0.016968552006846462, 'colsample_bytree': 0.6494549103847509, 'subsample': 0.4528983884416593, 'alpha': 0.5643595111167524, 'lambda': 111.71962869106571, 'gamma': 0.0001258468083788394, 'min_child_weight': 5.020467276688432}. Best is trial 4 with value: -0.0008313287504827739.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:30,278]\u001b[0m Trial 5 finished with value: -0.001064761892965471 and parameters: {'n_estimators': 1175, 'max_depth': 4, 'learning_rate': 0.0030922258598867986, 'colsample_bytree': 0.34899611730334107, 'subsample': 0.46988127458586726, 'alpha': 45.81418570810929, 'lambda': 137.0852298057998, 'gamma': 0.0042813244744537874, 'min_child_weight': 28.31249978433446}. Best is trial 4 with value: -0.0008313287504827739.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:30,820]\u001b[0m Trial 6 finished with value: -0.0010277028445619951 and parameters: {'n_estimators': 879, 'max_depth': 4, 'learning_rate': 0.0020141897309581113, 'colsample_bytree': 0.6711261181138454, 'subsample': 0.33373934136149763, 'alpha': 18.303556761534686, 'lambda': 253.53495921337557, 'gamma': 0.00013910278914584047, 'min_child_weight': 2.849018913477811}. Best is trial 4 with value: -0.0008313287504827739.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:31,441]\u001b[0m Trial 7 finished with value: -0.001028234196768163 and parameters: {'n_estimators': 833, 'max_depth': 5, 'learning_rate': 0.02453680886747501, 'colsample_bytree': 0.7634063019618393, 'subsample': 0.32261716267605206, 'alpha': 27.139455745364476, 'lambda': 226.2779547675689, 'gamma': 1.3078624532358336e-08, 'min_child_weight': 2.4955049658564277}. Best is trial 4 with value: -0.0008313287504827739.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:32,139]\u001b[0m Trial 8 finished with value: -0.000815612533661736 and parameters: {'n_estimators': 964, 'max_depth': 6, 'learning_rate': 0.003635599479346051, 'colsample_bytree': 0.1085559798571865, 'subsample': 0.7639167252325402, 'alpha': 6.463216362940282, 'lambda': 70.21624911010343, 'gamma': 4.603057253898225e-08, 'min_child_weight': 42.72214028502544}. Best is trial 8 with value: -0.000815612533661736.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:32,669]\u001b[0m Trial 9 finished with value: -0.0010239541834400345 and parameters: {'n_estimators': 1222, 'max_depth': 4, 'learning_rate': 0.023050428376717596, 'colsample_bytree': 0.22712155810138845, 'subsample': 0.22755349200820452, 'alpha': 0.6031835221865253, 'lambda': 185.0275671199973, 'gamma': 5.560459413608732e-08, 'min_child_weight': 15.975400876092332}. Best is trial 8 with value: -0.000815612533661736.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:33,213]\u001b[0m Trial 10 finished with value: -0.0008210924373974304 and parameters: {'n_estimators': 1420, 'max_depth': 2, 'learning_rate': 0.011268300478603617, 'colsample_bytree': 0.0865295271283326, 'subsample': 0.9470226215915808, 'alpha': 0.10019212217132767, 'lambda': 20.604741505318625, 'gamma': 1.313311451572362e-10, 'min_child_weight': 48.76187596793513}. Best is trial 8 with value: -0.000815612533661736.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:33,785]\u001b[0m Trial 11 finished with value: 2.437101140020378e-05 and parameters: {'n_estimators': 1455, 'max_depth': 2, 'learning_rate': 0.010554966353629248, 'colsample_bytree': 0.05446301244286159, 'subsample': 0.9123172322513737, 'alpha': 0.1400409828217791, 'lambda': 36.51404817173645, 'gamma': 1.2522435504837044e-10, 'min_child_weight': 43.18700359951894}. Best is trial 11 with value: 2.437101140020378e-05.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:34,394]\u001b[0m Trial 12 finished with value: -0.00026071343180662516 and parameters: {'n_estimators': 1496, 'max_depth': 2, 'learning_rate': 0.011953311699190972, 'colsample_bytree': 0.07304888730969594, 'subsample': 0.8920196686642757, 'alpha': 4.222748748327978, 'lambda': 23.4695057372977, 'gamma': 1.08157668323206e-06, 'min_child_weight': 9.118468357030315}. Best is trial 11 with value: 2.437101140020378e-05.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:34,901]\u001b[0m Trial 13 finished with value: -0.0012579999929277275 and parameters: {'n_estimators': 1473, 'max_depth': 2, 'learning_rate': 0.011688599386515865, 'colsample_bytree': 0.2682885942215454, 'subsample': 0.9399562257427, 'alpha': 0.12210473744628632, 'lambda': 18.986729409013872, 'gamma': 5.636439944921443e-06, 'min_child_weight': 7.702137120563978}. Best is trial 11 with value: 2.437101140020378e-05.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:35,602]\u001b[0m Trial 14 finished with value: -0.0018817540852511306 and parameters: {'n_estimators': 1354, 'max_depth': 3, 'learning_rate': 0.01477807532610283, 'colsample_bytree': 0.17977310605507024, 'subsample': 0.7888446145658001, 'alpha': 2.8930848167722525, 'lambda': 24.069936835319865, 'gamma': 1.918791464909495e-06, 'min_child_weight': 10.947727587997662}. Best is trial 11 with value: 2.437101140020378e-05.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:36,210]\u001b[0m Trial 15 finished with value: 6.681216376118736e-05 and parameters: {'n_estimators': 1301, 'max_depth': 2, 'learning_rate': 0.009268982312079108, 'colsample_bytree': 0.05600275199565251, 'subsample': 0.6665298413314573, 'alpha': 1.9761827571723936, 'lambda': 5.585601159175016, 'gamma': 1.1082169038411447e-10, 'min_child_weight': 0.8865122522418047}. Best is trial 15 with value: 6.681216376118736e-05.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:36,747]\u001b[0m Trial 16 finished with value: -0.0011526501136040145 and parameters: {'n_estimators': 1298, 'max_depth': 3, 'learning_rate': 0.008061029983133895, 'colsample_bytree': 0.3756999094568491, 'subsample': 0.6511830801394172, 'alpha': 0.23689804043724988, 'lambda': 4.945004507193158, 'gamma': 1.0579371277301847e-10, 'min_child_weight': 0.8320571067256461}. Best is trial 15 with value: 6.681216376118736e-05.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:37,261]\u001b[0m Trial 17 finished with value: -0.0010792541756947881 and parameters: {'n_estimators': 1296, 'max_depth': 2, 'learning_rate': 0.018654819078303755, 'colsample_bytree': 0.20589229383035335, 'subsample': 0.6452197370752895, 'alpha': 1.4655888826591952, 'lambda': 1.9936814987585996, 'gamma': 1.0526564982521015e-09, 'min_child_weight': 0.6446585255355757}. Best is trial 15 with value: 6.681216376118736e-05.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:37,831]\u001b[0m Trial 18 finished with value: -0.001378712269988358 and parameters: {'n_estimators': 1099, 'max_depth': 3, 'learning_rate': 0.007663359132124526, 'colsample_bytree': 0.29090414939794224, 'subsample': 0.8040635766472588, 'alpha': 0.18490056944214373, 'lambda': 10.397488601852485, 'gamma': 75.96194626388179, 'min_child_weight': 0.254931874318695}. Best is trial 15 with value: 6.681216376118736e-05.\u001b[0m\n\u001b[32m[I 2022-09-07 17:18:38,376]\u001b[0m Trial 19 finished with value: -0.001003314082847795 and parameters: {'n_estimators': 1386, 'max_depth': 2, 'learning_rate': 0.008736998472324403, 'colsample_bytree': 0.13296520227240774, 'subsample': 0.7088470680424912, 'alpha': 0.3338917433570806, 'lambda': 47.31418830849543, 'gamma': 2.799585552957603e-07, 'min_child_weight': 1.495743321054298}. Best is trial 15 with value: 6.681216376118736e-05.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Total time for hypermarameter optimization  11.910188913345337\n        n_estimators : 1301\n           max_depth : 2\n       learning_rate : 0.009268982312079108\n    colsample_bytree : 0.05600275199565251\n           subsample : 0.6665298413314573\n               alpha : 1.9761827571723936\n              lambda : 5.585601159175016\n               gamma : 1.1082169038411447e-10\n    min_child_weight : 0.8865122522418047\nbest objective value : 6.681216376118736e-05\nOptuna XGB train: \n 8.262250805377878 0.04952252330599172 \nvalidation \n 7.745393860745202 0.0032156773826091944 9.15082533948575 0.011952688587360427 \ntest \n 7.8266252847156865 0.019140564759762557\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"   min_prd xgbf_train  xgbf_val xgbf_test xgbgs_train xgbgs_val xgbgs_test  \\\n0      250   0.088074 -0.012211  0.015254    0.060695  0.032498   0.025239   \n\n  xgbo_train  xgbo_val xgbo_test  \n0   0.049523  0.003216  0.019141  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>min_prd</th>\n      <th>xgbf_train</th>\n      <th>xgbf_val</th>\n      <th>xgbf_test</th>\n      <th>xgbgs_train</th>\n      <th>xgbgs_val</th>\n      <th>xgbgs_test</th>\n      <th>xgbo_train</th>\n      <th>xgbo_val</th>\n      <th>xgbo_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>250</td>\n      <td>0.088074</td>\n      <td>-0.012211</td>\n      <td>0.015254</td>\n      <td>0.060695</td>\n      <td>0.032498</td>\n      <td>0.025239</td>\n      <td>0.049523</td>\n      <td>0.003216</td>\n      <td>0.019141</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"display(results.iloc[:,1:].mean())\n# cv_regularizer = 0.5\n# optuna_trials = 80\nprint(time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:42.437968Z","iopub.execute_input":"2022-09-07T17:18:42.438604Z","iopub.status.idle":"2022-09-07T17:18:42.452535Z","shell.execute_reply.started":"2022-09-07T17:18:42.438564Z","shell.execute_reply":"2022-09-07T17:18:42.451164Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"xgbf_train     0.088074\nxgbf_val      -0.012211\nxgbf_test      0.015254\nxgbgs_train    0.060695\nxgbgs_val      0.032498\nxgbgs_test     0.025239\nxgbo_train     0.049523\nxgbo_val       0.003216\nxgbo_test      0.019141\ndtype: float64"},"metadata":{}},{"name":"stdout","text":"87.09774470329285\n","output_type":"stream"}]},{"cell_type":"code","source":"# general point:\n# compared to NN, xgb is harder to regularize\n# in NN, you can simply shrink coefficient towards constant prediction.\n# in xgb, you can not do that. the only way to regularize is via hyperparameters.\n# in other words, by tweaking hyperpars, in NN you can approach R^2=0.0 prediction from a constant model arbitrarily close\n# in xgb, you can not do that.\n# by setting eta as low as 0.1% you can bring r2 down to 0.1%, but lowering eta further actyally increases abs(r2).\n","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:42.454028Z","iopub.execute_input":"2022-09-07T17:18:42.454441Z","iopub.status.idle":"2022-09-07T17:18:42.459420Z","shell.execute_reply.started":"2022-09-07T17:18:42.454406Z","shell.execute_reply":"2022-09-07T17:18:42.458226Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"optuna_xgb","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:42.461424Z","iopub.execute_input":"2022-09-07T17:18:42.461847Z","iopub.status.idle":"2022-09-07T17:18:42.476561Z","shell.execute_reply.started":"2022-09-07T17:18:42.461814Z","shell.execute_reply":"2022-09-07T17:18:42.475737Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"XGBRegressor(alpha=1.9761827571723936, base_score=0.5, booster='gbtree',\n             callbacks=None, colsample_bylevel=1, colsample_bynode=1,\n             colsample_bytree=0.05600275199565251, early_stopping_rounds=None,\n             enable_categorical=False, eval_metric=None,\n             gamma=1.1082169038411447e-10, gpu_id=0, grow_policy='depthwise',\n             importance_type=None, interaction_constraints='',\n             lambda=5.585601159175016, learning_rate=0.009268982312079108,\n             max_bin=256, max_cat_to_onehot=4, max_delta_step=0, max_depth=2,\n             max_leaves=0, min_child_weight=0.8865122522418047, missing=nan,\n             monotone_constraints='()', n_estimators=1301, n_jobs=0,\n             num_parallel_tree=1, predictor='auto', random_state=0, ...)"},"metadata":{}}]},{"cell_type":"code","source":"explainerxgbc = shap.TreeExplainer(optuna_xgb)\nshap_values_XGBoost_test = explainerxgbc.shap_values(X_test)\n\nvals = np.abs(shap_values_XGBoost_test).mean(0)\nfeature_names = X_test.columns\nfeature_importance = pd.DataFrame(list(zip(feature_names, vals)),\n                                 columns=['col_name','feature_importance_vals'])\nfeature_importance.sort_values(by=['feature_importance_vals'],\n                              ascending=False, inplace=True)\n\nshap.summary_plot(shap_values_XGBoost_test, X_test, \n                  plot_type=\"bar\", plot_size=(6,6), max_display=20)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:42.479256Z","iopub.execute_input":"2022-09-07T17:18:42.480241Z","iopub.status.idle":"2022-09-07T17:18:43.146112Z","shell.execute_reply.started":"2022-09-07T17:18:42.480199Z","shell.execute_reply":"2022-09-07T17:18:43.145176Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 432x432 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate performance of XGB models:\nr2_xgb1 = r2_score(y_test, xgb1.predict(X_test))\nr2_xgbgs = r2_score(y_test, xgbgs.predict(X_test))\nr2_xgbo = r2_score(y_test, optuna_xgb.predict(X_test))\n\nprint('Min_prd: ', min_prd)\nprint('Constant guess: ', mean_absolute_error(y_test, np.ones(len(y_test))*y_test.mean()), \n      r2_score(y_test, np.ones(len(y_test))*y_test.mean()))\nprint('XGB test:', mean_absolute_error(y_test, xgb1.predict(X_test)), r2_xgb1)\nprint('XGB GS test:', mean_absolute_error(y_test, xgbgs.predict(X_test)), r2_xgbgs)\nprint('Optuna XGB test:', mean_absolute_error(y_test, optuna_xgb.predict(X_test)), r2_xgbo)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:43.147427Z","iopub.execute_input":"2022-09-07T17:18:43.149005Z","iopub.status.idle":"2022-09-07T17:18:43.436271Z","shell.execute_reply.started":"2022-09-07T17:18:43.148961Z","shell.execute_reply":"2022-09-07T17:18:43.435247Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Min_prd:  250\nConstant guess:  7.924561381034317 0.0\nXGB test: 7.84572645024662 0.015254350212336232\nXGB GS test: 7.787969462570123 0.02523901836111997\nOptuna XGB test: 7.8266252847156865 0.019140564759762557\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Total time for a script: ', time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:43.439831Z","iopub.execute_input":"2022-09-07T17:18:43.440110Z","iopub.status.idle":"2022-09-07T17:18:43.446321Z","shell.execute_reply.started":"2022-09-07T17:18:43.440084Z","shell.execute_reply":"2022-09-07T17:18:43.445309Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Total time for a script:  88.09067153930664\n","output_type":"stream"}]},{"cell_type":"code","source":"results.iloc[:,1:].mean()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:43.447727Z","iopub.execute_input":"2022-09-07T17:18:43.448923Z","iopub.status.idle":"2022-09-07T17:18:43.461403Z","shell.execute_reply.started":"2022-09-07T17:18:43.448863Z","shell.execute_reply":"2022-09-07T17:18:43.460462Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"xgbf_train     0.088074\nxgbf_val      -0.012211\nxgbf_test      0.015254\nxgbgs_train    0.060695\nxgbgs_val      0.032498\nxgbgs_test     0.025239\nxgbo_train     0.049523\nxgbo_val       0.003216\nxgbo_test      0.019141\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# 3yr window, trials=20, cv_reg=0.03: 0.88%. runs 1 hr.\n# 3yr, t=40, cv_reg=0.04: 0.96%.\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:43.462700Z","iopub.execute_input":"2022-09-07T17:18:43.463120Z","iopub.status.idle":"2022-09-07T17:18:43.468205Z","shell.execute_reply.started":"2022-09-07T17:18:43.463085Z","shell.execute_reply":"2022-09-07T17:18:43.466831Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"display(X_train, X_val, X_test)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:43.469786Z","iopub.execute_input":"2022-09-07T17:18:43.471853Z","iopub.status.idle":"2022-09-07T17:18:43.771980Z","shell.execute_reply.started":"2022-09-07T17:18:43.471827Z","shell.execute_reply":"2022-09-07T17:18:43.771107Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"        num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n0         -0.574564    -0.711081  0.472308 -0.134353 -1.129718 -0.392313   \n1         -0.653236    -0.688240  0.472308 -0.134353 -1.129718 -0.392313   \n2         -0.590031    -0.689546  0.472308 -0.134353 -1.129718 -0.392313   \n3         -0.457370    -0.638779  0.472308 -0.134353 -1.129718 -0.392313   \n4         -0.604114    -0.623601  0.472308 -0.134353 -1.129718 -0.392313   \n...             ...          ...       ...       ...       ...       ...   \n102816    -0.270172    -1.218055  0.551286 -0.697511  0.311149  1.488425   \n102817    -0.270172    -1.150990  0.551286 -0.697511  0.311149  1.488425   \n102818    -0.270172    -0.562256  0.551286 -0.697511  0.311149  1.488425   \n102819    -0.270172    -0.926197  0.551286 -0.697511  0.311149  1.488425   \n102820    -0.734731    -0.685920  0.551286 -0.697511  0.311149  1.488425   \n\n        num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n0        -0.291318    -0.566625  -0.711182       -0.877729      -0.777803   \n1        -0.256761    -0.281055  -0.725324       -0.863874      -0.841188   \n2        -0.247274    -0.277685  -0.762064       -1.125174      -1.126674   \n3         0.597571    -0.412356  -0.772104       -0.658278      -0.636945   \n4         0.539864    -0.469925  -0.767163       -0.716880      -0.679172   \n...            ...          ...        ...             ...            ...   \n102816   -0.312495    -0.249525   0.034249       -0.618104      -0.475864   \n102817   -0.671719    -0.495995   0.034249        0.058760       0.167137   \n102818   -0.982918    -0.591451   0.034249       -0.266176      -0.229980   \n102819   -0.240009    -1.148669   0.034249       -0.403694      -0.367751   \n102820   -0.175391    -0.711005   0.034249       -1.014199      -0.914380   \n\n        num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n0          -0.290435 -0.729070   -0.931525   -1.013542    -1.142541   \n1          -0.390012 -0.914071   -0.929305   -1.037940    -1.204140   \n2          -0.400091 -0.890002   -1.089513   -1.077085    -1.203867   \n3          -0.308280 -0.563560   -0.640612   -1.025729    -1.160449   \n4          -0.284042 -0.580173   -0.749459   -1.060253    -1.159392   \n...              ...       ...         ...         ...          ...   \n102816     -1.041670 -0.743071   -0.766494   -0.651260    -0.727762   \n102817     -1.056420 -0.446666    0.006264   -0.521203    -0.709822   \n102818     -0.873990 -0.422597   -0.223735   -0.465464    -0.634525   \n102819     -0.678645 -0.617907   -0.329376   -0.456320    -0.600934   \n102820     -0.637533 -0.997354   -0.949070   -0.554110    -0.672194   \n\n        num__size  num__lbm  num__lop  num__lgp  num__linv  num__llme  \\\n0        0.884642  0.282772 -0.127991 -1.106961  -0.536405   1.046549   \n1        0.878183  0.282772 -0.127991 -1.106961  -0.536405   0.976333   \n2        0.862794  0.282772 -0.127991 -1.106961  -0.536405   0.958378   \n3        0.905620  0.282772 -0.127991 -1.106961  -0.536405   0.978536   \n4        0.945342  0.282772 -0.127991 -1.106961  -0.536405   1.032828   \n...           ...       ...       ...       ...        ...        ...   \n102816  -0.639557  0.299861 -0.353771  0.805470  -0.300501  -0.639496   \n102817  -0.673780  0.299861 -0.353771  0.805470  -0.300501  -0.603743   \n102818  -0.616034  0.299861 -0.353771  0.805470  -0.300501  -0.615397   \n102819  -0.624737  0.299861 -0.353771  0.805470  -0.300501  -0.528594   \n102820  -0.624737  0.299861 -0.353771  0.805470  -0.300501  -0.647777   \n\n        num__l1amhd  num__l1MAX  num__l3amhd  num__l3MAX  num__l6amhd  \\\n0         -0.767426   -0.752765    -0.691502   -1.055559    -0.712044   \n1         -0.720364   -0.738622    -0.729562   -0.627579    -0.666427   \n2         -0.734527   -0.921749    -0.788583   -0.745667    -0.697685   \n3         -0.771321   -0.897924    -0.741403   -0.731403    -0.730937   \n4         -0.781376   -0.574787    -0.755602   -0.916098    -0.769223   \n...             ...         ...          ...         ...          ...   \n102816     0.033403    0.389474     0.033070    0.003453     0.033342   \n102817     0.033403   -0.752480     0.033070    0.216772     0.033342   \n102818     0.033403   -0.459078     0.033070    0.406350     0.033342   \n102819     0.033403   -0.435252     0.033070   -0.745380     0.033342   \n102820     0.033403   -0.628584     0.033070   -0.449466     0.033342   \n\n        num__l6MAX  num__l12amhd  num__l12MAX  num__l12mom122  \\\n0        -0.801846     -0.793977    -0.752765       -0.449638   \n1        -0.880118     -0.766839    -0.738622       -0.432148   \n2        -0.599916     -0.750813    -0.921749       -0.711138   \n3        -1.069933     -0.744892    -0.897924       -0.761080   \n4        -0.643464     -0.751251    -0.574787       -0.791735   \n...            ...           ...          ...             ...   \n102816    0.121567      0.032700     0.389474       -1.531515   \n102817   -0.292024      0.032700    -0.752480       -1.558278   \n102818   -0.649188      0.032700    -0.459078       -1.243116   \n102819   -0.014659      0.032700    -0.435252        0.116326   \n102820    0.197908      0.032700    -0.628584       -0.028734   \n\n        num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n0               -0.884437         -0.928455        -0.039010      -1.128689   \n1               -0.441625         -0.316339         0.071654      -1.100925   \n2               -1.087944         -1.068946         0.105760      -1.085436   \n3               -0.969431         -0.923707         0.116068      -1.079960   \n4               -0.734650         -0.897763        -0.004421      -1.041669   \n...                   ...               ...              ...            ...   \n102816          -0.753885         -0.703000        -0.028841      -0.259662   \n102817          -0.172813         -0.018349         0.025846      -0.199049   \n102818          -1.072910         -0.994798        -0.121067      -0.375617   \n102819          -0.712028         -0.716829        -0.570517      -0.482257   \n102820          -0.493426         -1.041235        -0.746679      -0.613168   \n\n        num__l12vol12m  num__amhd_miss  cat__ind_1.0  cat__ind_2.0  \\\n0            -1.198377       -0.850499           0.0           0.0   \n1            -1.122707       -0.850499           0.0           0.0   \n2            -1.158464       -0.850499           0.0           0.0   \n3            -1.170898       -0.850499           0.0           0.0   \n4            -1.155515       -0.850499           0.0           0.0   \n...                ...             ...           ...           ...   \n102816        0.317119        1.175781           0.0           0.0   \n102817        0.359077        1.175781           0.0           0.0   \n102818        0.329325        1.175781           0.0           0.0   \n102819       -0.070720        1.175781           0.0           0.0   \n102820       -0.438535        1.175781           0.0           0.0   \n\n        cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  cat__ind_7.0  \\\n0                0.0           0.0           0.0           0.0           0.0   \n1                0.0           0.0           0.0           0.0           0.0   \n2                0.0           0.0           0.0           0.0           0.0   \n3                0.0           0.0           0.0           0.0           0.0   \n4                0.0           0.0           0.0           0.0           0.0   \n...              ...           ...           ...           ...           ...   \n102816           0.0           0.0           0.0           0.0           0.0   \n102817           0.0           0.0           0.0           0.0           0.0   \n102818           0.0           0.0           0.0           0.0           0.0   \n102819           0.0           0.0           0.0           0.0           0.0   \n102820           0.0           0.0           0.0           0.0           0.0   \n\n        cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n0                0.0           0.0            0.0            0.0   \n1                0.0           0.0            0.0            0.0   \n2                0.0           0.0            0.0            0.0   \n3                0.0           0.0            0.0            0.0   \n4                0.0           0.0            0.0            0.0   \n...              ...           ...            ...            ...   \n102816           1.0           0.0            0.0            0.0   \n102817           1.0           0.0            0.0            0.0   \n102818           1.0           0.0            0.0            0.0   \n102819           1.0           0.0            0.0            0.0   \n102820           1.0           0.0            0.0            0.0   \n\n        cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102816            0.0            0.0            0.0            0.0   \n102817            0.0            0.0            0.0            0.0   \n102818            0.0            0.0            0.0            0.0   \n102819            0.0            0.0            0.0            0.0   \n102820            0.0            0.0            0.0            0.0   \n\n        cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102816            0.0            0.0            0.0            0.0   \n102817            0.0            0.0            0.0            0.0   \n102818            0.0            0.0            0.0            0.0   \n102819            0.0            0.0            0.0            0.0   \n102820            0.0            0.0            0.0            0.0   \n\n        cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102816            0.0            0.0            0.0            0.0   \n102817            0.0            0.0            0.0            0.0   \n102818            0.0            0.0            0.0            0.0   \n102819            0.0            0.0            0.0            0.0   \n102820            0.0            0.0            0.0            0.0   \n\n        cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n0                 0.0            1.0            0.0            0.0   \n1                 0.0            1.0            0.0            0.0   \n2                 0.0            1.0            0.0            0.0   \n3                 0.0            1.0            0.0            0.0   \n4                 0.0            1.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102816            0.0            0.0            0.0            0.0   \n102817            0.0            0.0            0.0            0.0   \n102818            0.0            0.0            0.0            0.0   \n102819            0.0            0.0            0.0            0.0   \n102820            0.0            0.0            0.0            0.0   \n\n        cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102816            0.0            0.0            0.0            0.0   \n102817            0.0            0.0            0.0            0.0   \n102818            0.0            0.0            0.0            0.0   \n102819            0.0            0.0            0.0            0.0   \n102820            0.0            0.0            0.0            0.0   \n\n        cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102816            0.0            0.0            0.0            0.0   \n102817            0.0            0.0            0.0            0.0   \n102818            0.0            0.0            0.0            0.0   \n102819            0.0            0.0            0.0            0.0   \n102820            0.0            0.0            0.0            0.0   \n\n        cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102816            0.0            0.0            0.0            0.0   \n102817            0.0            0.0            0.0            0.0   \n102818            0.0            0.0            0.0            0.0   \n102819            0.0            0.0            0.0            0.0   \n102820            0.0            0.0            0.0            0.0   \n\n        cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102816            0.0            0.0            0.0            0.0   \n102817            0.0            0.0            0.0            0.0   \n102818            0.0            0.0            0.0            0.0   \n102819            0.0            0.0            0.0            0.0   \n102820            0.0            0.0            0.0            0.0   \n\n        cat__ind_44.0  cat__ind_45.0  cat__ind_47.0  cat__ind_48.0  \\\n0                 0.0            0.0            0.0            0.0   \n1                 0.0            0.0            0.0            0.0   \n2                 0.0            0.0            0.0            0.0   \n3                 0.0            0.0            0.0            0.0   \n4                 0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102816            0.0            0.0            0.0            0.0   \n102817            0.0            0.0            0.0            0.0   \n102818            0.0            0.0            0.0            0.0   \n102819            0.0            0.0            0.0            0.0   \n102820            0.0            0.0            0.0            0.0   \n\n        cat__ind_49.0  \n0                 0.0  \n1                 0.0  \n2                 0.0  \n3                 0.0  \n4                 0.0  \n...               ...  \n102816            0.0  \n102817            0.0  \n102818            0.0  \n102819            0.0  \n102820            0.0  \n\n[79162 rows x 85 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-0.574564</td>\n      <td>-0.711081</td>\n      <td>0.472308</td>\n      <td>-0.134353</td>\n      <td>-1.129718</td>\n      <td>-0.392313</td>\n      <td>-0.291318</td>\n      <td>-0.566625</td>\n      <td>-0.711182</td>\n      <td>-0.877729</td>\n      <td>-0.777803</td>\n      <td>-0.290435</td>\n      <td>-0.729070</td>\n      <td>-0.931525</td>\n      <td>-1.013542</td>\n      <td>-1.142541</td>\n      <td>0.884642</td>\n      <td>0.282772</td>\n      <td>-0.127991</td>\n      <td>-1.106961</td>\n      <td>-0.536405</td>\n      <td>1.046549</td>\n      <td>-0.767426</td>\n      <td>-0.752765</td>\n      <td>-0.691502</td>\n      <td>-1.055559</td>\n      <td>-0.712044</td>\n      <td>-0.801846</td>\n      <td>-0.793977</td>\n      <td>-0.752765</td>\n      <td>-0.449638</td>\n      <td>-0.884437</td>\n      <td>-0.928455</td>\n      <td>-0.039010</td>\n      <td>-1.128689</td>\n      <td>-1.198377</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.653236</td>\n      <td>-0.688240</td>\n      <td>0.472308</td>\n      <td>-0.134353</td>\n      <td>-1.129718</td>\n      <td>-0.392313</td>\n      <td>-0.256761</td>\n      <td>-0.281055</td>\n      <td>-0.725324</td>\n      <td>-0.863874</td>\n      <td>-0.841188</td>\n      <td>-0.390012</td>\n      <td>-0.914071</td>\n      <td>-0.929305</td>\n      <td>-1.037940</td>\n      <td>-1.204140</td>\n      <td>0.878183</td>\n      <td>0.282772</td>\n      <td>-0.127991</td>\n      <td>-1.106961</td>\n      <td>-0.536405</td>\n      <td>0.976333</td>\n      <td>-0.720364</td>\n      <td>-0.738622</td>\n      <td>-0.729562</td>\n      <td>-0.627579</td>\n      <td>-0.666427</td>\n      <td>-0.880118</td>\n      <td>-0.766839</td>\n      <td>-0.738622</td>\n      <td>-0.432148</td>\n      <td>-0.441625</td>\n      <td>-0.316339</td>\n      <td>0.071654</td>\n      <td>-1.100925</td>\n      <td>-1.122707</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.590031</td>\n      <td>-0.689546</td>\n      <td>0.472308</td>\n      <td>-0.134353</td>\n      <td>-1.129718</td>\n      <td>-0.392313</td>\n      <td>-0.247274</td>\n      <td>-0.277685</td>\n      <td>-0.762064</td>\n      <td>-1.125174</td>\n      <td>-1.126674</td>\n      <td>-0.400091</td>\n      <td>-0.890002</td>\n      <td>-1.089513</td>\n      <td>-1.077085</td>\n      <td>-1.203867</td>\n      <td>0.862794</td>\n      <td>0.282772</td>\n      <td>-0.127991</td>\n      <td>-1.106961</td>\n      <td>-0.536405</td>\n      <td>0.958378</td>\n      <td>-0.734527</td>\n      <td>-0.921749</td>\n      <td>-0.788583</td>\n      <td>-0.745667</td>\n      <td>-0.697685</td>\n      <td>-0.599916</td>\n      <td>-0.750813</td>\n      <td>-0.921749</td>\n      <td>-0.711138</td>\n      <td>-1.087944</td>\n      <td>-1.068946</td>\n      <td>0.105760</td>\n      <td>-1.085436</td>\n      <td>-1.158464</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>-0.457370</td>\n      <td>-0.638779</td>\n      <td>0.472308</td>\n      <td>-0.134353</td>\n      <td>-1.129718</td>\n      <td>-0.392313</td>\n      <td>0.597571</td>\n      <td>-0.412356</td>\n      <td>-0.772104</td>\n      <td>-0.658278</td>\n      <td>-0.636945</td>\n      <td>-0.308280</td>\n      <td>-0.563560</td>\n      <td>-0.640612</td>\n      <td>-1.025729</td>\n      <td>-1.160449</td>\n      <td>0.905620</td>\n      <td>0.282772</td>\n      <td>-0.127991</td>\n      <td>-1.106961</td>\n      <td>-0.536405</td>\n      <td>0.978536</td>\n      <td>-0.771321</td>\n      <td>-0.897924</td>\n      <td>-0.741403</td>\n      <td>-0.731403</td>\n      <td>-0.730937</td>\n      <td>-1.069933</td>\n      <td>-0.744892</td>\n      <td>-0.897924</td>\n      <td>-0.761080</td>\n      <td>-0.969431</td>\n      <td>-0.923707</td>\n      <td>0.116068</td>\n      <td>-1.079960</td>\n      <td>-1.170898</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>-0.604114</td>\n      <td>-0.623601</td>\n      <td>0.472308</td>\n      <td>-0.134353</td>\n      <td>-1.129718</td>\n      <td>-0.392313</td>\n      <td>0.539864</td>\n      <td>-0.469925</td>\n      <td>-0.767163</td>\n      <td>-0.716880</td>\n      <td>-0.679172</td>\n      <td>-0.284042</td>\n      <td>-0.580173</td>\n      <td>-0.749459</td>\n      <td>-1.060253</td>\n      <td>-1.159392</td>\n      <td>0.945342</td>\n      <td>0.282772</td>\n      <td>-0.127991</td>\n      <td>-1.106961</td>\n      <td>-0.536405</td>\n      <td>1.032828</td>\n      <td>-0.781376</td>\n      <td>-0.574787</td>\n      <td>-0.755602</td>\n      <td>-0.916098</td>\n      <td>-0.769223</td>\n      <td>-0.643464</td>\n      <td>-0.751251</td>\n      <td>-0.574787</td>\n      <td>-0.791735</td>\n      <td>-0.734650</td>\n      <td>-0.897763</td>\n      <td>-0.004421</td>\n      <td>-1.041669</td>\n      <td>-1.155515</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>102816</th>\n      <td>-0.270172</td>\n      <td>-1.218055</td>\n      <td>0.551286</td>\n      <td>-0.697511</td>\n      <td>0.311149</td>\n      <td>1.488425</td>\n      <td>-0.312495</td>\n      <td>-0.249525</td>\n      <td>0.034249</td>\n      <td>-0.618104</td>\n      <td>-0.475864</td>\n      <td>-1.041670</td>\n      <td>-0.743071</td>\n      <td>-0.766494</td>\n      <td>-0.651260</td>\n      <td>-0.727762</td>\n      <td>-0.639557</td>\n      <td>0.299861</td>\n      <td>-0.353771</td>\n      <td>0.805470</td>\n      <td>-0.300501</td>\n      <td>-0.639496</td>\n      <td>0.033403</td>\n      <td>0.389474</td>\n      <td>0.033070</td>\n      <td>0.003453</td>\n      <td>0.033342</td>\n      <td>0.121567</td>\n      <td>0.032700</td>\n      <td>0.389474</td>\n      <td>-1.531515</td>\n      <td>-0.753885</td>\n      <td>-0.703000</td>\n      <td>-0.028841</td>\n      <td>-0.259662</td>\n      <td>0.317119</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102817</th>\n      <td>-0.270172</td>\n      <td>-1.150990</td>\n      <td>0.551286</td>\n      <td>-0.697511</td>\n      <td>0.311149</td>\n      <td>1.488425</td>\n      <td>-0.671719</td>\n      <td>-0.495995</td>\n      <td>0.034249</td>\n      <td>0.058760</td>\n      <td>0.167137</td>\n      <td>-1.056420</td>\n      <td>-0.446666</td>\n      <td>0.006264</td>\n      <td>-0.521203</td>\n      <td>-0.709822</td>\n      <td>-0.673780</td>\n      <td>0.299861</td>\n      <td>-0.353771</td>\n      <td>0.805470</td>\n      <td>-0.300501</td>\n      <td>-0.603743</td>\n      <td>0.033403</td>\n      <td>-0.752480</td>\n      <td>0.033070</td>\n      <td>0.216772</td>\n      <td>0.033342</td>\n      <td>-0.292024</td>\n      <td>0.032700</td>\n      <td>-0.752480</td>\n      <td>-1.558278</td>\n      <td>-0.172813</td>\n      <td>-0.018349</td>\n      <td>0.025846</td>\n      <td>-0.199049</td>\n      <td>0.359077</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102818</th>\n      <td>-0.270172</td>\n      <td>-0.562256</td>\n      <td>0.551286</td>\n      <td>-0.697511</td>\n      <td>0.311149</td>\n      <td>1.488425</td>\n      <td>-0.982918</td>\n      <td>-0.591451</td>\n      <td>0.034249</td>\n      <td>-0.266176</td>\n      <td>-0.229980</td>\n      <td>-0.873990</td>\n      <td>-0.422597</td>\n      <td>-0.223735</td>\n      <td>-0.465464</td>\n      <td>-0.634525</td>\n      <td>-0.616034</td>\n      <td>0.299861</td>\n      <td>-0.353771</td>\n      <td>0.805470</td>\n      <td>-0.300501</td>\n      <td>-0.615397</td>\n      <td>0.033403</td>\n      <td>-0.459078</td>\n      <td>0.033070</td>\n      <td>0.406350</td>\n      <td>0.033342</td>\n      <td>-0.649188</td>\n      <td>0.032700</td>\n      <td>-0.459078</td>\n      <td>-1.243116</td>\n      <td>-1.072910</td>\n      <td>-0.994798</td>\n      <td>-0.121067</td>\n      <td>-0.375617</td>\n      <td>0.329325</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102819</th>\n      <td>-0.270172</td>\n      <td>-0.926197</td>\n      <td>0.551286</td>\n      <td>-0.697511</td>\n      <td>0.311149</td>\n      <td>1.488425</td>\n      <td>-0.240009</td>\n      <td>-1.148669</td>\n      <td>0.034249</td>\n      <td>-0.403694</td>\n      <td>-0.367751</td>\n      <td>-0.678645</td>\n      <td>-0.617907</td>\n      <td>-0.329376</td>\n      <td>-0.456320</td>\n      <td>-0.600934</td>\n      <td>-0.624737</td>\n      <td>0.299861</td>\n      <td>-0.353771</td>\n      <td>0.805470</td>\n      <td>-0.300501</td>\n      <td>-0.528594</td>\n      <td>0.033403</td>\n      <td>-0.435252</td>\n      <td>0.033070</td>\n      <td>-0.745380</td>\n      <td>0.033342</td>\n      <td>-0.014659</td>\n      <td>0.032700</td>\n      <td>-0.435252</td>\n      <td>0.116326</td>\n      <td>-0.712028</td>\n      <td>-0.716829</td>\n      <td>-0.570517</td>\n      <td>-0.482257</td>\n      <td>-0.070720</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102820</th>\n      <td>-0.734731</td>\n      <td>-0.685920</td>\n      <td>0.551286</td>\n      <td>-0.697511</td>\n      <td>0.311149</td>\n      <td>1.488425</td>\n      <td>-0.175391</td>\n      <td>-0.711005</td>\n      <td>0.034249</td>\n      <td>-1.014199</td>\n      <td>-0.914380</td>\n      <td>-0.637533</td>\n      <td>-0.997354</td>\n      <td>-0.949070</td>\n      <td>-0.554110</td>\n      <td>-0.672194</td>\n      <td>-0.624737</td>\n      <td>0.299861</td>\n      <td>-0.353771</td>\n      <td>0.805470</td>\n      <td>-0.300501</td>\n      <td>-0.647777</td>\n      <td>0.033403</td>\n      <td>-0.628584</td>\n      <td>0.033070</td>\n      <td>-0.449466</td>\n      <td>0.033342</td>\n      <td>0.197908</td>\n      <td>0.032700</td>\n      <td>-0.628584</td>\n      <td>-0.028734</td>\n      <td>-0.493426</td>\n      <td>-1.041235</td>\n      <td>-0.746679</td>\n      <td>-0.613168</td>\n      <td>-0.438535</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>79162 rows × 85 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"        num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n36        -0.585324    -0.644216  0.379757 -0.197630 -1.066823 -0.365342   \n37        -0.726781    -0.772511  0.379757 -0.197630 -1.066823 -0.365342   \n38        -0.719618    -0.431708  0.379757 -0.197630 -1.066823 -0.365342   \n83        -0.303435    -1.207074  0.139332 -0.120186 -0.301792  0.217784   \n84        -0.380104    -1.020176  0.139332 -0.120186 -0.301792  0.217784   \n...             ...          ...       ...       ...       ...       ...   \n102776    -1.361045    -1.630072 -1.351392 -0.490214  1.020231  0.092509   \n102777    -1.417092    -1.460491 -1.351392 -0.490214  1.020231  0.092509   \n102821    -0.766522    -0.872933  0.551286 -0.697511  0.311149  1.488425   \n102822    -0.756676    -0.511559  0.551286 -0.697511  0.311149  1.488425   \n102823    -0.584435    -0.268739  0.551286 -0.697511  0.311149  1.488425   \n\n        num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n36        0.388211    -0.899505  -1.038144       -0.560815      -0.445949   \n37       -0.990431    -0.984266  -0.984656       -0.393243      -0.315458   \n38       -0.162117    -1.126915  -0.964176       -0.979260      -0.866734   \n83        0.398875    -1.046352   0.301660        0.074126       0.016825   \n84        0.633951    -0.901490   0.324640        0.380243       0.591793   \n...            ...          ...        ...             ...            ...   \n102776   -1.837099    -1.931340   0.034249        0.805335       0.926578   \n102777   -0.162117    -1.951134   0.034249       -1.145549      -1.136099   \n102821   -0.673905    -0.355883   0.034249       -0.981337      -0.920224   \n102822    2.272577    -0.897724   0.034249        1.117335       1.050890   \n102823   -0.759452    -0.241247   0.034249       -0.639876      -0.776739   \n\n        num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n36         -0.416552 -0.303738   -0.659472   -0.982810    -1.166545   \n37         -0.244301 -0.295420   -0.124462   -0.839646    -1.104340   \n38         -0.289412 -0.906472   -0.742458   -0.828203    -1.116244   \n83         -0.417008  0.411358   -0.015756   -0.663543    -0.590345   \n84         -0.650563  0.690118    0.252041   -0.472592    -0.656507   \n...              ...       ...         ...         ...          ...   \n102776     -0.234380 -0.201900    0.624382    3.131960     2.815371   \n102777     -0.142543 -1.053763   -1.157854    1.794312     2.751182   \n102821     -0.685018 -0.987238   -1.053961   -0.770215    -0.701528   \n102822     -0.967766  1.596088    0.879360   -0.321175    -0.533085   \n102823     -0.916927 -0.707591    0.492733   -0.419310    -0.535245   \n\n        num__size  num__lbm  num__lop  num__lgp  num__linv  num__llme  \\\n36       1.057885  0.654601 -0.320691 -1.112279  -0.276956   1.216944   \n37       1.005449  0.654601 -0.320691 -1.112279  -0.276956   1.272056   \n38       1.005449  0.654601 -0.320691 -1.112279  -0.276956   1.260143   \n83       0.318093 -0.045407  0.416153 -0.030640   0.230781   0.503442   \n84       0.362862 -0.045407  0.416153 -0.030640   0.230781   0.498180   \n...           ...       ...       ...       ...        ...        ...   \n102776  -1.947690 -0.751215 -0.158184  1.032225  -1.586794  -1.485186   \n102777  -1.947690 -0.751215 -0.158184  1.032225  -1.586794  -1.545261   \n102821  -0.653294  0.299861 -0.353771  0.805470  -0.300501  -0.723550   \n102822  -0.525666  0.299861 -0.353771  0.805470  -0.300501  -0.631003   \n102823  -0.567501  0.299861 -0.353771  0.805470  -0.300501  -0.651628   \n\n        num__l1amhd  num__l1MAX  num__l3amhd  num__l3MAX  num__l6amhd  \\\n36        -1.089773   -0.902409    -1.187112   -0.977416    -1.333498   \n37        -1.047810   -0.317597    -1.150335   -0.484600    -1.303997   \n38        -0.994242   -0.309362    -1.111737   -0.896592    -1.309644   \n83         0.282370   -0.546595     0.196414   -0.252899    -0.010498   \n84         0.293974    0.390257     0.240561   -0.466554     0.052464   \n...             ...         ...          ...         ...          ...   \n102776     0.033403   -1.060026     0.033070    3.666174     0.033342   \n102777     0.033403   -0.216790     0.033070   -0.656969     0.033342   \n102821     0.033403   -1.004189     0.033070   -0.425436     0.033342   \n102822     0.033403   -0.994174     0.033070   -0.620423     0.033342   \n102823     0.033403    1.562990     0.033070   -0.999243     0.033342   \n\n        num__l6MAX  num__l12amhd  num__l12MAX  num__l12mom122  \\\n36       -0.924382     -1.189411    -0.902409        0.539004   \n37       -0.450954     -1.195741    -0.317597       -0.186181   \n38       -1.069933     -1.210686    -0.309362        0.331343   \n83       -0.775373      0.086514    -0.546595       -0.519835   \n84       -0.501467      0.075144     0.390257       -1.119754   \n...            ...           ...          ...             ...   \n102776    3.635136      0.032700    -1.060026        1.413904   \n102777    3.635136      0.032700    -0.216790       -0.047116   \n102821    0.386816      0.032700    -1.004189       -0.312594   \n102822   -0.760849      0.032700    -0.994174       -0.815843   \n102823   -0.465979      0.032700     1.562990        0.106954   \n\n        num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n36              -0.589160         -0.613450         0.138360      -0.773293   \n37              -0.615879         -0.481990         0.060410      -0.744144   \n38              -0.885376         -1.056955         0.024423      -0.842861   \n83               0.360269          0.421111        -0.446306       0.136028   \n84               0.484527          0.439798        -0.325219       0.270547   \n...                   ...               ...              ...            ...   \n102776          -0.316709         -0.278509        -0.150128      -0.022308   \n102777           0.799297          0.964181        -0.237083       0.219417   \n102821          -0.639769         -0.663403        -0.719451      -0.773289   \n102822          -0.361648         -0.391422        -0.694251      -0.631448   \n102823          -0.524419         -0.613246        -0.741032      -0.737642   \n\n        num__l12vol12m  num__amhd_miss  cat__ind_1.0  cat__ind_2.0  \\\n36           -0.616222       -0.850499           0.0           0.0   \n37           -0.643334       -0.850499           0.0           0.0   \n38           -0.659589       -0.850499           0.0           0.0   \n83           -0.091275       -0.850499           0.0           0.0   \n84            0.027074       -0.850499           0.0           0.0   \n...                ...             ...           ...           ...   \n102776        1.671264        1.175781           0.0           0.0   \n102777        1.083606        1.175781           0.0           0.0   \n102821       -0.503950        1.175781           0.0           0.0   \n102822       -0.445321        1.175781           0.0           0.0   \n102823       -0.435178        1.175781           0.0           0.0   \n\n        cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  cat__ind_7.0  \\\n36               0.0           0.0           0.0           0.0           0.0   \n37               0.0           0.0           0.0           0.0           0.0   \n38               0.0           0.0           0.0           0.0           0.0   \n83               0.0           0.0           0.0           0.0           0.0   \n84               0.0           0.0           0.0           0.0           0.0   \n...              ...           ...           ...           ...           ...   \n102776           0.0           0.0           0.0           0.0           0.0   \n102777           0.0           0.0           0.0           0.0           0.0   \n102821           0.0           0.0           0.0           0.0           0.0   \n102822           0.0           0.0           0.0           0.0           0.0   \n102823           0.0           0.0           0.0           0.0           0.0   \n\n        cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n36               0.0           0.0            0.0            0.0   \n37               0.0           0.0            0.0            0.0   \n38               0.0           0.0            0.0            0.0   \n83               0.0           0.0            0.0            0.0   \n84               0.0           0.0            0.0            0.0   \n...              ...           ...            ...            ...   \n102776           0.0           0.0            0.0            0.0   \n102777           0.0           0.0            0.0            0.0   \n102821           1.0           0.0            0.0            0.0   \n102822           1.0           0.0            0.0            0.0   \n102823           1.0           0.0            0.0            0.0   \n\n        cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n36                0.0            0.0            0.0            0.0   \n37                0.0            0.0            0.0            0.0   \n38                0.0            0.0            0.0            0.0   \n83                0.0            0.0            0.0            0.0   \n84                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102776            0.0            0.0            0.0            0.0   \n102777            0.0            0.0            0.0            0.0   \n102821            0.0            0.0            0.0            0.0   \n102822            0.0            0.0            0.0            0.0   \n102823            0.0            0.0            0.0            0.0   \n\n        cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n36                0.0            0.0            0.0            0.0   \n37                0.0            0.0            0.0            0.0   \n38                0.0            0.0            0.0            0.0   \n83                0.0            0.0            0.0            0.0   \n84                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102776            0.0            0.0            0.0            0.0   \n102777            0.0            0.0            0.0            0.0   \n102821            0.0            0.0            0.0            0.0   \n102822            0.0            0.0            0.0            0.0   \n102823            0.0            0.0            0.0            0.0   \n\n        cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n36                0.0            0.0            0.0            0.0   \n37                0.0            0.0            0.0            0.0   \n38                0.0            0.0            0.0            0.0   \n83                0.0            1.0            0.0            0.0   \n84                0.0            1.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102776            0.0            0.0            0.0            0.0   \n102777            0.0            0.0            0.0            0.0   \n102821            0.0            0.0            0.0            0.0   \n102822            0.0            0.0            0.0            0.0   \n102823            0.0            0.0            0.0            0.0   \n\n        cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n36                0.0            1.0            0.0            0.0   \n37                0.0            1.0            0.0            0.0   \n38                0.0            1.0            0.0            0.0   \n83                0.0            0.0            0.0            0.0   \n84                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102776            0.0            0.0            0.0            0.0   \n102777            0.0            0.0            0.0            0.0   \n102821            0.0            0.0            0.0            0.0   \n102822            0.0            0.0            0.0            0.0   \n102823            0.0            0.0            0.0            0.0   \n\n        cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n36                0.0            0.0            0.0            0.0   \n37                0.0            0.0            0.0            0.0   \n38                0.0            0.0            0.0            0.0   \n83                0.0            0.0            0.0            0.0   \n84                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102776            0.0            0.0            0.0            0.0   \n102777            0.0            0.0            0.0            0.0   \n102821            0.0            0.0            0.0            0.0   \n102822            0.0            0.0            0.0            0.0   \n102823            0.0            0.0            0.0            0.0   \n\n        cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n36                0.0            0.0            0.0            0.0   \n37                0.0            0.0            0.0            0.0   \n38                0.0            0.0            0.0            0.0   \n83                0.0            0.0            0.0            0.0   \n84                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102776            0.0            0.0            1.0            0.0   \n102777            0.0            0.0            1.0            0.0   \n102821            0.0            0.0            0.0            0.0   \n102822            0.0            0.0            0.0            0.0   \n102823            0.0            0.0            0.0            0.0   \n\n        cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n36                0.0            0.0            0.0            0.0   \n37                0.0            0.0            0.0            0.0   \n38                0.0            0.0            0.0            0.0   \n83                0.0            0.0            0.0            0.0   \n84                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102776            0.0            0.0            0.0            0.0   \n102777            0.0            0.0            0.0            0.0   \n102821            0.0            0.0            0.0            0.0   \n102822            0.0            0.0            0.0            0.0   \n102823            0.0            0.0            0.0            0.0   \n\n        cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n36                0.0            0.0            0.0            0.0   \n37                0.0            0.0            0.0            0.0   \n38                0.0            0.0            0.0            0.0   \n83                0.0            0.0            0.0            0.0   \n84                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102776            0.0            0.0            0.0            0.0   \n102777            0.0            0.0            0.0            0.0   \n102821            0.0            0.0            0.0            0.0   \n102822            0.0            0.0            0.0            0.0   \n102823            0.0            0.0            0.0            0.0   \n\n        cat__ind_44.0  cat__ind_45.0  cat__ind_47.0  cat__ind_48.0  \\\n36                0.0            0.0            0.0            0.0   \n37                0.0            0.0            0.0            0.0   \n38                0.0            0.0            0.0            0.0   \n83                0.0            0.0            0.0            0.0   \n84                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102776            0.0            0.0            0.0            0.0   \n102777            0.0            0.0            0.0            0.0   \n102821            0.0            0.0            0.0            0.0   \n102822            0.0            0.0            0.0            0.0   \n102823            0.0            0.0            0.0            0.0   \n\n        cat__ind_49.0  \n36                0.0  \n37                0.0  \n38                0.0  \n83                0.0  \n84                0.0  \n...               ...  \n102776            0.0  \n102777            0.0  \n102821            0.0  \n102822            0.0  \n102823            0.0  \n\n[6405 rows x 85 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>36</th>\n      <td>-0.585324</td>\n      <td>-0.644216</td>\n      <td>0.379757</td>\n      <td>-0.197630</td>\n      <td>-1.066823</td>\n      <td>-0.365342</td>\n      <td>0.388211</td>\n      <td>-0.899505</td>\n      <td>-1.038144</td>\n      <td>-0.560815</td>\n      <td>-0.445949</td>\n      <td>-0.416552</td>\n      <td>-0.303738</td>\n      <td>-0.659472</td>\n      <td>-0.982810</td>\n      <td>-1.166545</td>\n      <td>1.057885</td>\n      <td>0.654601</td>\n      <td>-0.320691</td>\n      <td>-1.112279</td>\n      <td>-0.276956</td>\n      <td>1.216944</td>\n      <td>-1.089773</td>\n      <td>-0.902409</td>\n      <td>-1.187112</td>\n      <td>-0.977416</td>\n      <td>-1.333498</td>\n      <td>-0.924382</td>\n      <td>-1.189411</td>\n      <td>-0.902409</td>\n      <td>0.539004</td>\n      <td>-0.589160</td>\n      <td>-0.613450</td>\n      <td>0.138360</td>\n      <td>-0.773293</td>\n      <td>-0.616222</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>-0.726781</td>\n      <td>-0.772511</td>\n      <td>0.379757</td>\n      <td>-0.197630</td>\n      <td>-1.066823</td>\n      <td>-0.365342</td>\n      <td>-0.990431</td>\n      <td>-0.984266</td>\n      <td>-0.984656</td>\n      <td>-0.393243</td>\n      <td>-0.315458</td>\n      <td>-0.244301</td>\n      <td>-0.295420</td>\n      <td>-0.124462</td>\n      <td>-0.839646</td>\n      <td>-1.104340</td>\n      <td>1.005449</td>\n      <td>0.654601</td>\n      <td>-0.320691</td>\n      <td>-1.112279</td>\n      <td>-0.276956</td>\n      <td>1.272056</td>\n      <td>-1.047810</td>\n      <td>-0.317597</td>\n      <td>-1.150335</td>\n      <td>-0.484600</td>\n      <td>-1.303997</td>\n      <td>-0.450954</td>\n      <td>-1.195741</td>\n      <td>-0.317597</td>\n      <td>-0.186181</td>\n      <td>-0.615879</td>\n      <td>-0.481990</td>\n      <td>0.060410</td>\n      <td>-0.744144</td>\n      <td>-0.643334</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>-0.719618</td>\n      <td>-0.431708</td>\n      <td>0.379757</td>\n      <td>-0.197630</td>\n      <td>-1.066823</td>\n      <td>-0.365342</td>\n      <td>-0.162117</td>\n      <td>-1.126915</td>\n      <td>-0.964176</td>\n      <td>-0.979260</td>\n      <td>-0.866734</td>\n      <td>-0.289412</td>\n      <td>-0.906472</td>\n      <td>-0.742458</td>\n      <td>-0.828203</td>\n      <td>-1.116244</td>\n      <td>1.005449</td>\n      <td>0.654601</td>\n      <td>-0.320691</td>\n      <td>-1.112279</td>\n      <td>-0.276956</td>\n      <td>1.260143</td>\n      <td>-0.994242</td>\n      <td>-0.309362</td>\n      <td>-1.111737</td>\n      <td>-0.896592</td>\n      <td>-1.309644</td>\n      <td>-1.069933</td>\n      <td>-1.210686</td>\n      <td>-0.309362</td>\n      <td>0.331343</td>\n      <td>-0.885376</td>\n      <td>-1.056955</td>\n      <td>0.024423</td>\n      <td>-0.842861</td>\n      <td>-0.659589</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>-0.303435</td>\n      <td>-1.207074</td>\n      <td>0.139332</td>\n      <td>-0.120186</td>\n      <td>-0.301792</td>\n      <td>0.217784</td>\n      <td>0.398875</td>\n      <td>-1.046352</td>\n      <td>0.301660</td>\n      <td>0.074126</td>\n      <td>0.016825</td>\n      <td>-0.417008</td>\n      <td>0.411358</td>\n      <td>-0.015756</td>\n      <td>-0.663543</td>\n      <td>-0.590345</td>\n      <td>0.318093</td>\n      <td>-0.045407</td>\n      <td>0.416153</td>\n      <td>-0.030640</td>\n      <td>0.230781</td>\n      <td>0.503442</td>\n      <td>0.282370</td>\n      <td>-0.546595</td>\n      <td>0.196414</td>\n      <td>-0.252899</td>\n      <td>-0.010498</td>\n      <td>-0.775373</td>\n      <td>0.086514</td>\n      <td>-0.546595</td>\n      <td>-0.519835</td>\n      <td>0.360269</td>\n      <td>0.421111</td>\n      <td>-0.446306</td>\n      <td>0.136028</td>\n      <td>-0.091275</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>84</th>\n      <td>-0.380104</td>\n      <td>-1.020176</td>\n      <td>0.139332</td>\n      <td>-0.120186</td>\n      <td>-0.301792</td>\n      <td>0.217784</td>\n      <td>0.633951</td>\n      <td>-0.901490</td>\n      <td>0.324640</td>\n      <td>0.380243</td>\n      <td>0.591793</td>\n      <td>-0.650563</td>\n      <td>0.690118</td>\n      <td>0.252041</td>\n      <td>-0.472592</td>\n      <td>-0.656507</td>\n      <td>0.362862</td>\n      <td>-0.045407</td>\n      <td>0.416153</td>\n      <td>-0.030640</td>\n      <td>0.230781</td>\n      <td>0.498180</td>\n      <td>0.293974</td>\n      <td>0.390257</td>\n      <td>0.240561</td>\n      <td>-0.466554</td>\n      <td>0.052464</td>\n      <td>-0.501467</td>\n      <td>0.075144</td>\n      <td>0.390257</td>\n      <td>-1.119754</td>\n      <td>0.484527</td>\n      <td>0.439798</td>\n      <td>-0.325219</td>\n      <td>0.270547</td>\n      <td>0.027074</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>102776</th>\n      <td>-1.361045</td>\n      <td>-1.630072</td>\n      <td>-1.351392</td>\n      <td>-0.490214</td>\n      <td>1.020231</td>\n      <td>0.092509</td>\n      <td>-1.837099</td>\n      <td>-1.931340</td>\n      <td>0.034249</td>\n      <td>0.805335</td>\n      <td>0.926578</td>\n      <td>-0.234380</td>\n      <td>-0.201900</td>\n      <td>0.624382</td>\n      <td>3.131960</td>\n      <td>2.815371</td>\n      <td>-1.947690</td>\n      <td>-0.751215</td>\n      <td>-0.158184</td>\n      <td>1.032225</td>\n      <td>-1.586794</td>\n      <td>-1.485186</td>\n      <td>0.033403</td>\n      <td>-1.060026</td>\n      <td>0.033070</td>\n      <td>3.666174</td>\n      <td>0.033342</td>\n      <td>3.635136</td>\n      <td>0.032700</td>\n      <td>-1.060026</td>\n      <td>1.413904</td>\n      <td>-0.316709</td>\n      <td>-0.278509</td>\n      <td>-0.150128</td>\n      <td>-0.022308</td>\n      <td>1.671264</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102777</th>\n      <td>-1.417092</td>\n      <td>-1.460491</td>\n      <td>-1.351392</td>\n      <td>-0.490214</td>\n      <td>1.020231</td>\n      <td>0.092509</td>\n      <td>-0.162117</td>\n      <td>-1.951134</td>\n      <td>0.034249</td>\n      <td>-1.145549</td>\n      <td>-1.136099</td>\n      <td>-0.142543</td>\n      <td>-1.053763</td>\n      <td>-1.157854</td>\n      <td>1.794312</td>\n      <td>2.751182</td>\n      <td>-1.947690</td>\n      <td>-0.751215</td>\n      <td>-0.158184</td>\n      <td>1.032225</td>\n      <td>-1.586794</td>\n      <td>-1.545261</td>\n      <td>0.033403</td>\n      <td>-0.216790</td>\n      <td>0.033070</td>\n      <td>-0.656969</td>\n      <td>0.033342</td>\n      <td>3.635136</td>\n      <td>0.032700</td>\n      <td>-0.216790</td>\n      <td>-0.047116</td>\n      <td>0.799297</td>\n      <td>0.964181</td>\n      <td>-0.237083</td>\n      <td>0.219417</td>\n      <td>1.083606</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102821</th>\n      <td>-0.766522</td>\n      <td>-0.872933</td>\n      <td>0.551286</td>\n      <td>-0.697511</td>\n      <td>0.311149</td>\n      <td>1.488425</td>\n      <td>-0.673905</td>\n      <td>-0.355883</td>\n      <td>0.034249</td>\n      <td>-0.981337</td>\n      <td>-0.920224</td>\n      <td>-0.685018</td>\n      <td>-0.987238</td>\n      <td>-1.053961</td>\n      <td>-0.770215</td>\n      <td>-0.701528</td>\n      <td>-0.653294</td>\n      <td>0.299861</td>\n      <td>-0.353771</td>\n      <td>0.805470</td>\n      <td>-0.300501</td>\n      <td>-0.723550</td>\n      <td>0.033403</td>\n      <td>-1.004189</td>\n      <td>0.033070</td>\n      <td>-0.425436</td>\n      <td>0.033342</td>\n      <td>0.386816</td>\n      <td>0.032700</td>\n      <td>-1.004189</td>\n      <td>-0.312594</td>\n      <td>-0.639769</td>\n      <td>-0.663403</td>\n      <td>-0.719451</td>\n      <td>-0.773289</td>\n      <td>-0.503950</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102822</th>\n      <td>-0.756676</td>\n      <td>-0.511559</td>\n      <td>0.551286</td>\n      <td>-0.697511</td>\n      <td>0.311149</td>\n      <td>1.488425</td>\n      <td>2.272577</td>\n      <td>-0.897724</td>\n      <td>0.034249</td>\n      <td>1.117335</td>\n      <td>1.050890</td>\n      <td>-0.967766</td>\n      <td>1.596088</td>\n      <td>0.879360</td>\n      <td>-0.321175</td>\n      <td>-0.533085</td>\n      <td>-0.525666</td>\n      <td>0.299861</td>\n      <td>-0.353771</td>\n      <td>0.805470</td>\n      <td>-0.300501</td>\n      <td>-0.631003</td>\n      <td>0.033403</td>\n      <td>-0.994174</td>\n      <td>0.033070</td>\n      <td>-0.620423</td>\n      <td>0.033342</td>\n      <td>-0.760849</td>\n      <td>0.032700</td>\n      <td>-0.994174</td>\n      <td>-0.815843</td>\n      <td>-0.361648</td>\n      <td>-0.391422</td>\n      <td>-0.694251</td>\n      <td>-0.631448</td>\n      <td>-0.445321</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102823</th>\n      <td>-0.584435</td>\n      <td>-0.268739</td>\n      <td>0.551286</td>\n      <td>-0.697511</td>\n      <td>0.311149</td>\n      <td>1.488425</td>\n      <td>-0.759452</td>\n      <td>-0.241247</td>\n      <td>0.034249</td>\n      <td>-0.639876</td>\n      <td>-0.776739</td>\n      <td>-0.916927</td>\n      <td>-0.707591</td>\n      <td>0.492733</td>\n      <td>-0.419310</td>\n      <td>-0.535245</td>\n      <td>-0.567501</td>\n      <td>0.299861</td>\n      <td>-0.353771</td>\n      <td>0.805470</td>\n      <td>-0.300501</td>\n      <td>-0.651628</td>\n      <td>0.033403</td>\n      <td>1.562990</td>\n      <td>0.033070</td>\n      <td>-0.999243</td>\n      <td>0.033342</td>\n      <td>-0.465979</td>\n      <td>0.032700</td>\n      <td>1.562990</td>\n      <td>0.106954</td>\n      <td>-0.524419</td>\n      <td>-0.613246</td>\n      <td>-0.741032</td>\n      <td>-0.737642</td>\n      <td>-0.435178</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6405 rows × 85 columns</p>\n</div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"        num__mom482  num__mom242   num__bm   num__op   num__gp  num__inv  \\\n39        -0.867368    -0.499066  0.379757 -0.197630 -1.066823 -0.365342   \n40        -0.813073    -0.496274  0.379757 -0.197630 -1.066823 -0.365342   \n41        -0.929999    -0.655791  0.379757 -0.197630 -1.066823 -0.365342   \n86        -0.653287    -0.592435  0.139332 -0.120186 -0.301792  0.217784   \n87        -0.774069    -0.803011  0.139332 -0.120186 -0.301792  0.217784   \n...             ...          ...       ...       ...       ...       ...   \n102779    -1.300270    -1.472369 -1.351392 -0.490214  1.020231  0.092509   \n102780    -1.303046    -1.578372 -1.351392 -0.490214  1.020231  0.092509   \n102824    -0.849383    -0.468005  0.551286 -0.697511  0.311149  1.488425   \n102825    -0.960291    -0.697860  0.551286 -0.697511  0.311149  1.488425   \n102826    -1.146700    -0.821739  0.551286 -0.697511  0.311149  1.488425   \n\n        num__mom11  num__mom122  num__amhd  num__ivol_capm  num__ivol_ff5  \\\n39       -0.719913    -1.112204  -0.951812       -0.823485      -0.823200   \n40        0.469989    -1.102434  -0.895303       -0.483593      -0.413036   \n41       -0.795177    -1.090620  -0.873778       -0.981461      -0.907385   \n86       -0.368547    -0.800143   0.278860       -0.656207      -0.539955   \n87        0.220612    -1.037389   0.297423       -0.625074      -0.481491   \n...            ...          ...        ...             ...            ...   \n102779    2.272577    -1.839320   0.034249        2.114800       1.876220   \n102780   -0.174506    -1.215467   0.034249       -1.145549      -1.136099   \n102824   -1.165481    -0.245963   0.034249       -0.180854      -0.119445   \n102825   -0.180700    -0.668499   0.034249       -0.317436      -0.468010   \n102826   -0.153506    -0.809007   0.034249       -0.078191       0.039278   \n\n        num__beta_bw  num__MAX  num__vol1m  num__vol6m  num__vol12m  \\\n39         -0.263365 -0.847810   -0.759358   -0.939433    -1.113617   \n40         -0.226876 -0.565454   -0.561918   -0.899538    -1.067529   \n41         -0.229098 -0.911075   -1.073475   -0.913637    -1.073300   \n86         -0.640559 -0.583769   -0.865856   -0.558238    -0.730289   \n87         -0.609086 -0.852556   -0.668448   -0.591883    -0.773978   \n...              ...       ...         ...         ...          ...   \n102779      0.255016  2.021061    2.098712    0.803448     2.955418   \n102780      0.119976 -1.053763   -1.157854    0.761057     2.686886   \n102824     -0.742812  0.425478    0.028259   -0.377168    -0.472695   \n102825     -0.713194 -0.184712   -0.377822   -0.379995    -0.472371   \n102826     -0.494235  0.812333   -0.014013   -0.238286    -0.441773   \n\n        num__size  num__lbm  num__lop  num__lgp  num__linv  num__llme  \\\n39       0.960933  0.654601 -0.320691 -1.112279  -0.276956   1.250689   \n40       0.998287  0.654601 -0.320691 -1.112279  -0.276956   1.214034   \n41       0.949190  0.654601 -0.320691 -1.112279  -0.276956   1.241065   \n86       0.327357 -0.045407  0.416153 -0.030640   0.230781   0.499802   \n87       0.342443 -0.045407  0.416153 -0.030640   0.230781   0.542960   \n...           ...       ...       ...       ...        ...        ...   \n102779  -1.682934 -0.751215 -0.158184  1.032225  -1.586794  -1.739517   \n102780  -1.682934 -0.751215 -0.158184  1.032225  -1.586794  -1.686939   \n102824  -0.630500  0.299861 -0.353771  0.805470  -0.300501  -0.686379   \n102825  -0.630500  0.299861 -0.353771  0.805470  -0.300501  -0.660106   \n102826  -0.634981  0.299861 -0.353771  0.805470  -0.300501  -0.631003   \n\n        num__l1amhd  num__l1MAX  num__l3amhd  num__l3MAX  num__l6amhd  \\\n39        -0.973732   -0.914227    -1.069669   -0.306774    -1.229480   \n40        -0.961350   -0.856158    -1.015967   -0.298469    -1.192486   \n41        -0.904757   -0.576662    -0.995405   -0.908511    -1.153659   \n86         0.281703   -0.862803     0.275475    0.407140     0.162234   \n87         0.271140   -0.594792     0.298546    0.685439     0.206642   \n...             ...         ...          ...         ...          ...   \n102779     0.033403    0.079365     0.033070   -0.205104     0.033342   \n102780     0.033403    1.983659     0.033070   -1.055559     0.033342   \n102824     0.033403   -0.717359     0.033070   -0.989143     0.033342   \n102825     0.033403    0.404234     0.033070    1.589911     0.033342   \n102826     0.033403   -0.199775     0.033070   -0.709958     0.033342   \n\n        num__l6MAX  num__l12amhd  num__l12MAX  num__l12mom122  \\\n39       -0.992066     -1.251140    -0.914227        0.966894   \n40       -0.500990     -1.317560    -0.856158        1.063903   \n41       -0.911528     -1.374790    -0.576662        0.470431   \n86       -0.270107      0.109854    -0.862803       -0.407803   \n87       -0.483007      0.032981    -0.594792        0.107700   \n...            ...           ...          ...             ...   \n102779    3.635136      0.032700     0.079365       -0.689944   \n102780   -0.672751      0.032700     1.983659       -1.553356   \n102824   -0.442035      0.032700    -0.717359        0.278820   \n102825   -0.636333      0.032700     0.404234       -0.062456   \n102826   -1.013816      0.032700    -0.199775       -0.336790   \n\n        num__l12ivol_capm  num__l12ivol_ff5  num__l12beta_bw  num__l12vol6m  \\\n39              -0.782169         -0.887040        -0.205925      -0.916214   \n40              -0.936993         -0.912128        -0.326780      -0.911090   \n41              -1.003227         -1.016788        -0.395446      -1.059311   \n86              -0.389782         -0.263555        -0.248717       0.162718   \n87              -0.168599         -0.665567        -0.120194       0.042497   \n...                   ...               ...              ...            ...   \n102779           0.619338          0.512810        -0.510974      -0.035492   \n102780           3.292455          2.883474        -0.416459       0.871126   \n102824          -0.871179         -0.810220        -0.767302      -0.709822   \n102825          -0.224705         -0.403947        -0.695827      -0.599545   \n102826          -0.041263         -0.230526        -0.882927      -0.604291   \n\n        num__l12vol12m  num__amhd_miss  cat__ind_1.0  cat__ind_2.0  \\\n39           -0.819440       -0.850499           0.0           0.0   \n40           -0.921379       -0.850499           0.0           0.0   \n41           -0.963268       -0.850499           0.0           0.0   \n86            0.044085       -0.850499           0.0           0.0   \n87           -0.044168       -0.850499           0.0           0.0   \n...                ...             ...           ...           ...   \n102779        0.539557        1.175781           0.0           0.0   \n102780        0.940838        1.175781           0.0           0.0   \n102824       -0.534761        1.175781           0.0           0.0   \n102825       -0.557256        1.175781           0.0           0.0   \n102826       -0.628839        1.175781           0.0           0.0   \n\n        cat__ind_3.0  cat__ind_4.0  cat__ind_5.0  cat__ind_6.0  cat__ind_7.0  \\\n39               0.0           0.0           0.0           0.0           0.0   \n40               0.0           0.0           0.0           0.0           0.0   \n41               0.0           0.0           0.0           0.0           0.0   \n86               0.0           0.0           0.0           0.0           0.0   \n87               0.0           0.0           0.0           0.0           0.0   \n...              ...           ...           ...           ...           ...   \n102779           0.0           0.0           0.0           0.0           0.0   \n102780           0.0           0.0           0.0           0.0           0.0   \n102824           0.0           0.0           0.0           0.0           0.0   \n102825           0.0           0.0           0.0           0.0           0.0   \n102826           0.0           0.0           0.0           0.0           0.0   \n\n        cat__ind_8.0  cat__ind_9.0  cat__ind_10.0  cat__ind_11.0  \\\n39               0.0           0.0            0.0            0.0   \n40               0.0           0.0            0.0            0.0   \n41               0.0           0.0            0.0            0.0   \n86               0.0           0.0            0.0            0.0   \n87               0.0           0.0            0.0            0.0   \n...              ...           ...            ...            ...   \n102779           0.0           0.0            0.0            0.0   \n102780           0.0           0.0            0.0            0.0   \n102824           1.0           0.0            0.0            0.0   \n102825           1.0           0.0            0.0            0.0   \n102826           1.0           0.0            0.0            0.0   \n\n        cat__ind_12.0  cat__ind_13.0  cat__ind_14.0  cat__ind_15.0  \\\n39                0.0            0.0            0.0            0.0   \n40                0.0            0.0            0.0            0.0   \n41                0.0            0.0            0.0            0.0   \n86                0.0            0.0            0.0            0.0   \n87                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102779            0.0            0.0            0.0            0.0   \n102780            0.0            0.0            0.0            0.0   \n102824            0.0            0.0            0.0            0.0   \n102825            0.0            0.0            0.0            0.0   \n102826            0.0            0.0            0.0            0.0   \n\n        cat__ind_16.0  cat__ind_17.0  cat__ind_18.0  cat__ind_19.0  \\\n39                0.0            0.0            0.0            0.0   \n40                0.0            0.0            0.0            0.0   \n41                0.0            0.0            0.0            0.0   \n86                0.0            0.0            0.0            0.0   \n87                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102779            0.0            0.0            0.0            0.0   \n102780            0.0            0.0            0.0            0.0   \n102824            0.0            0.0            0.0            0.0   \n102825            0.0            0.0            0.0            0.0   \n102826            0.0            0.0            0.0            0.0   \n\n        cat__ind_20.0  cat__ind_21.0  cat__ind_22.0  cat__ind_23.0  \\\n39                0.0            0.0            0.0            0.0   \n40                0.0            0.0            0.0            0.0   \n41                0.0            0.0            0.0            0.0   \n86                0.0            1.0            0.0            0.0   \n87                0.0            1.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102779            0.0            0.0            0.0            0.0   \n102780            0.0            0.0            0.0            0.0   \n102824            0.0            0.0            0.0            0.0   \n102825            0.0            0.0            0.0            0.0   \n102826            0.0            0.0            0.0            0.0   \n\n        cat__ind_24.0  cat__ind_25.0  cat__ind_26.0  cat__ind_27.0  \\\n39                0.0            1.0            0.0            0.0   \n40                0.0            1.0            0.0            0.0   \n41                0.0            1.0            0.0            0.0   \n86                0.0            0.0            0.0            0.0   \n87                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102779            0.0            0.0            0.0            0.0   \n102780            0.0            0.0            0.0            0.0   \n102824            0.0            0.0            0.0            0.0   \n102825            0.0            0.0            0.0            0.0   \n102826            0.0            0.0            0.0            0.0   \n\n        cat__ind_28.0  cat__ind_29.0  cat__ind_30.0  cat__ind_31.0  \\\n39                0.0            0.0            0.0            0.0   \n40                0.0            0.0            0.0            0.0   \n41                0.0            0.0            0.0            0.0   \n86                0.0            0.0            0.0            0.0   \n87                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102779            0.0            0.0            0.0            0.0   \n102780            0.0            0.0            0.0            0.0   \n102824            0.0            0.0            0.0            0.0   \n102825            0.0            0.0            0.0            0.0   \n102826            0.0            0.0            0.0            0.0   \n\n        cat__ind_32.0  cat__ind_33.0  cat__ind_34.0  cat__ind_35.0  \\\n39                0.0            0.0            0.0            0.0   \n40                0.0            0.0            0.0            0.0   \n41                0.0            0.0            0.0            0.0   \n86                0.0            0.0            0.0            0.0   \n87                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102779            0.0            0.0            1.0            0.0   \n102780            0.0            0.0            1.0            0.0   \n102824            0.0            0.0            0.0            0.0   \n102825            0.0            0.0            0.0            0.0   \n102826            0.0            0.0            0.0            0.0   \n\n        cat__ind_36.0  cat__ind_37.0  cat__ind_38.0  cat__ind_39.0  \\\n39                0.0            0.0            0.0            0.0   \n40                0.0            0.0            0.0            0.0   \n41                0.0            0.0            0.0            0.0   \n86                0.0            0.0            0.0            0.0   \n87                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102779            0.0            0.0            0.0            0.0   \n102780            0.0            0.0            0.0            0.0   \n102824            0.0            0.0            0.0            0.0   \n102825            0.0            0.0            0.0            0.0   \n102826            0.0            0.0            0.0            0.0   \n\n        cat__ind_40.0  cat__ind_41.0  cat__ind_42.0  cat__ind_43.0  \\\n39                0.0            0.0            0.0            0.0   \n40                0.0            0.0            0.0            0.0   \n41                0.0            0.0            0.0            0.0   \n86                0.0            0.0            0.0            0.0   \n87                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102779            0.0            0.0            0.0            0.0   \n102780            0.0            0.0            0.0            0.0   \n102824            0.0            0.0            0.0            0.0   \n102825            0.0            0.0            0.0            0.0   \n102826            0.0            0.0            0.0            0.0   \n\n        cat__ind_44.0  cat__ind_45.0  cat__ind_47.0  cat__ind_48.0  \\\n39                0.0            0.0            0.0            0.0   \n40                0.0            0.0            0.0            0.0   \n41                0.0            0.0            0.0            0.0   \n86                0.0            0.0            0.0            0.0   \n87                0.0            0.0            0.0            0.0   \n...               ...            ...            ...            ...   \n102779            0.0            0.0            0.0            0.0   \n102780            0.0            0.0            0.0            0.0   \n102824            0.0            0.0            0.0            0.0   \n102825            0.0            0.0            0.0            0.0   \n102826            0.0            0.0            0.0            0.0   \n\n        cat__ind_49.0  \n39                0.0  \n40                0.0  \n41                0.0  \n86                0.0  \n87                0.0  \n...               ...  \n102779            0.0  \n102780            0.0  \n102824            0.0  \n102825            0.0  \n102826            0.0  \n\n[6311 rows x 85 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>num__mom482</th>\n      <th>num__mom242</th>\n      <th>num__bm</th>\n      <th>num__op</th>\n      <th>num__gp</th>\n      <th>num__inv</th>\n      <th>num__mom11</th>\n      <th>num__mom122</th>\n      <th>num__amhd</th>\n      <th>num__ivol_capm</th>\n      <th>num__ivol_ff5</th>\n      <th>num__beta_bw</th>\n      <th>num__MAX</th>\n      <th>num__vol1m</th>\n      <th>num__vol6m</th>\n      <th>num__vol12m</th>\n      <th>num__size</th>\n      <th>num__lbm</th>\n      <th>num__lop</th>\n      <th>num__lgp</th>\n      <th>num__linv</th>\n      <th>num__llme</th>\n      <th>num__l1amhd</th>\n      <th>num__l1MAX</th>\n      <th>num__l3amhd</th>\n      <th>num__l3MAX</th>\n      <th>num__l6amhd</th>\n      <th>num__l6MAX</th>\n      <th>num__l12amhd</th>\n      <th>num__l12MAX</th>\n      <th>num__l12mom122</th>\n      <th>num__l12ivol_capm</th>\n      <th>num__l12ivol_ff5</th>\n      <th>num__l12beta_bw</th>\n      <th>num__l12vol6m</th>\n      <th>num__l12vol12m</th>\n      <th>num__amhd_miss</th>\n      <th>cat__ind_1.0</th>\n      <th>cat__ind_2.0</th>\n      <th>cat__ind_3.0</th>\n      <th>cat__ind_4.0</th>\n      <th>cat__ind_5.0</th>\n      <th>cat__ind_6.0</th>\n      <th>cat__ind_7.0</th>\n      <th>cat__ind_8.0</th>\n      <th>cat__ind_9.0</th>\n      <th>cat__ind_10.0</th>\n      <th>cat__ind_11.0</th>\n      <th>cat__ind_12.0</th>\n      <th>cat__ind_13.0</th>\n      <th>cat__ind_14.0</th>\n      <th>cat__ind_15.0</th>\n      <th>cat__ind_16.0</th>\n      <th>cat__ind_17.0</th>\n      <th>cat__ind_18.0</th>\n      <th>cat__ind_19.0</th>\n      <th>cat__ind_20.0</th>\n      <th>cat__ind_21.0</th>\n      <th>cat__ind_22.0</th>\n      <th>cat__ind_23.0</th>\n      <th>cat__ind_24.0</th>\n      <th>cat__ind_25.0</th>\n      <th>cat__ind_26.0</th>\n      <th>cat__ind_27.0</th>\n      <th>cat__ind_28.0</th>\n      <th>cat__ind_29.0</th>\n      <th>cat__ind_30.0</th>\n      <th>cat__ind_31.0</th>\n      <th>cat__ind_32.0</th>\n      <th>cat__ind_33.0</th>\n      <th>cat__ind_34.0</th>\n      <th>cat__ind_35.0</th>\n      <th>cat__ind_36.0</th>\n      <th>cat__ind_37.0</th>\n      <th>cat__ind_38.0</th>\n      <th>cat__ind_39.0</th>\n      <th>cat__ind_40.0</th>\n      <th>cat__ind_41.0</th>\n      <th>cat__ind_42.0</th>\n      <th>cat__ind_43.0</th>\n      <th>cat__ind_44.0</th>\n      <th>cat__ind_45.0</th>\n      <th>cat__ind_47.0</th>\n      <th>cat__ind_48.0</th>\n      <th>cat__ind_49.0</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>39</th>\n      <td>-0.867368</td>\n      <td>-0.499066</td>\n      <td>0.379757</td>\n      <td>-0.197630</td>\n      <td>-1.066823</td>\n      <td>-0.365342</td>\n      <td>-0.719913</td>\n      <td>-1.112204</td>\n      <td>-0.951812</td>\n      <td>-0.823485</td>\n      <td>-0.823200</td>\n      <td>-0.263365</td>\n      <td>-0.847810</td>\n      <td>-0.759358</td>\n      <td>-0.939433</td>\n      <td>-1.113617</td>\n      <td>0.960933</td>\n      <td>0.654601</td>\n      <td>-0.320691</td>\n      <td>-1.112279</td>\n      <td>-0.276956</td>\n      <td>1.250689</td>\n      <td>-0.973732</td>\n      <td>-0.914227</td>\n      <td>-1.069669</td>\n      <td>-0.306774</td>\n      <td>-1.229480</td>\n      <td>-0.992066</td>\n      <td>-1.251140</td>\n      <td>-0.914227</td>\n      <td>0.966894</td>\n      <td>-0.782169</td>\n      <td>-0.887040</td>\n      <td>-0.205925</td>\n      <td>-0.916214</td>\n      <td>-0.819440</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>-0.813073</td>\n      <td>-0.496274</td>\n      <td>0.379757</td>\n      <td>-0.197630</td>\n      <td>-1.066823</td>\n      <td>-0.365342</td>\n      <td>0.469989</td>\n      <td>-1.102434</td>\n      <td>-0.895303</td>\n      <td>-0.483593</td>\n      <td>-0.413036</td>\n      <td>-0.226876</td>\n      <td>-0.565454</td>\n      <td>-0.561918</td>\n      <td>-0.899538</td>\n      <td>-1.067529</td>\n      <td>0.998287</td>\n      <td>0.654601</td>\n      <td>-0.320691</td>\n      <td>-1.112279</td>\n      <td>-0.276956</td>\n      <td>1.214034</td>\n      <td>-0.961350</td>\n      <td>-0.856158</td>\n      <td>-1.015967</td>\n      <td>-0.298469</td>\n      <td>-1.192486</td>\n      <td>-0.500990</td>\n      <td>-1.317560</td>\n      <td>-0.856158</td>\n      <td>1.063903</td>\n      <td>-0.936993</td>\n      <td>-0.912128</td>\n      <td>-0.326780</td>\n      <td>-0.911090</td>\n      <td>-0.921379</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>-0.929999</td>\n      <td>-0.655791</td>\n      <td>0.379757</td>\n      <td>-0.197630</td>\n      <td>-1.066823</td>\n      <td>-0.365342</td>\n      <td>-0.795177</td>\n      <td>-1.090620</td>\n      <td>-0.873778</td>\n      <td>-0.981461</td>\n      <td>-0.907385</td>\n      <td>-0.229098</td>\n      <td>-0.911075</td>\n      <td>-1.073475</td>\n      <td>-0.913637</td>\n      <td>-1.073300</td>\n      <td>0.949190</td>\n      <td>0.654601</td>\n      <td>-0.320691</td>\n      <td>-1.112279</td>\n      <td>-0.276956</td>\n      <td>1.241065</td>\n      <td>-0.904757</td>\n      <td>-0.576662</td>\n      <td>-0.995405</td>\n      <td>-0.908511</td>\n      <td>-1.153659</td>\n      <td>-0.911528</td>\n      <td>-1.374790</td>\n      <td>-0.576662</td>\n      <td>0.470431</td>\n      <td>-1.003227</td>\n      <td>-1.016788</td>\n      <td>-0.395446</td>\n      <td>-1.059311</td>\n      <td>-0.963268</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>86</th>\n      <td>-0.653287</td>\n      <td>-0.592435</td>\n      <td>0.139332</td>\n      <td>-0.120186</td>\n      <td>-0.301792</td>\n      <td>0.217784</td>\n      <td>-0.368547</td>\n      <td>-0.800143</td>\n      <td>0.278860</td>\n      <td>-0.656207</td>\n      <td>-0.539955</td>\n      <td>-0.640559</td>\n      <td>-0.583769</td>\n      <td>-0.865856</td>\n      <td>-0.558238</td>\n      <td>-0.730289</td>\n      <td>0.327357</td>\n      <td>-0.045407</td>\n      <td>0.416153</td>\n      <td>-0.030640</td>\n      <td>0.230781</td>\n      <td>0.499802</td>\n      <td>0.281703</td>\n      <td>-0.862803</td>\n      <td>0.275475</td>\n      <td>0.407140</td>\n      <td>0.162234</td>\n      <td>-0.270107</td>\n      <td>0.109854</td>\n      <td>-0.862803</td>\n      <td>-0.407803</td>\n      <td>-0.389782</td>\n      <td>-0.263555</td>\n      <td>-0.248717</td>\n      <td>0.162718</td>\n      <td>0.044085</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>87</th>\n      <td>-0.774069</td>\n      <td>-0.803011</td>\n      <td>0.139332</td>\n      <td>-0.120186</td>\n      <td>-0.301792</td>\n      <td>0.217784</td>\n      <td>0.220612</td>\n      <td>-1.037389</td>\n      <td>0.297423</td>\n      <td>-0.625074</td>\n      <td>-0.481491</td>\n      <td>-0.609086</td>\n      <td>-0.852556</td>\n      <td>-0.668448</td>\n      <td>-0.591883</td>\n      <td>-0.773978</td>\n      <td>0.342443</td>\n      <td>-0.045407</td>\n      <td>0.416153</td>\n      <td>-0.030640</td>\n      <td>0.230781</td>\n      <td>0.542960</td>\n      <td>0.271140</td>\n      <td>-0.594792</td>\n      <td>0.298546</td>\n      <td>0.685439</td>\n      <td>0.206642</td>\n      <td>-0.483007</td>\n      <td>0.032981</td>\n      <td>-0.594792</td>\n      <td>0.107700</td>\n      <td>-0.168599</td>\n      <td>-0.665567</td>\n      <td>-0.120194</td>\n      <td>0.042497</td>\n      <td>-0.044168</td>\n      <td>-0.850499</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>102779</th>\n      <td>-1.300270</td>\n      <td>-1.472369</td>\n      <td>-1.351392</td>\n      <td>-0.490214</td>\n      <td>1.020231</td>\n      <td>0.092509</td>\n      <td>2.272577</td>\n      <td>-1.839320</td>\n      <td>0.034249</td>\n      <td>2.114800</td>\n      <td>1.876220</td>\n      <td>0.255016</td>\n      <td>2.021061</td>\n      <td>2.098712</td>\n      <td>0.803448</td>\n      <td>2.955418</td>\n      <td>-1.682934</td>\n      <td>-0.751215</td>\n      <td>-0.158184</td>\n      <td>1.032225</td>\n      <td>-1.586794</td>\n      <td>-1.739517</td>\n      <td>0.033403</td>\n      <td>0.079365</td>\n      <td>0.033070</td>\n      <td>-0.205104</td>\n      <td>0.033342</td>\n      <td>3.635136</td>\n      <td>0.032700</td>\n      <td>0.079365</td>\n      <td>-0.689944</td>\n      <td>0.619338</td>\n      <td>0.512810</td>\n      <td>-0.510974</td>\n      <td>-0.035492</td>\n      <td>0.539557</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102780</th>\n      <td>-1.303046</td>\n      <td>-1.578372</td>\n      <td>-1.351392</td>\n      <td>-0.490214</td>\n      <td>1.020231</td>\n      <td>0.092509</td>\n      <td>-0.174506</td>\n      <td>-1.215467</td>\n      <td>0.034249</td>\n      <td>-1.145549</td>\n      <td>-1.136099</td>\n      <td>0.119976</td>\n      <td>-1.053763</td>\n      <td>-1.157854</td>\n      <td>0.761057</td>\n      <td>2.686886</td>\n      <td>-1.682934</td>\n      <td>-0.751215</td>\n      <td>-0.158184</td>\n      <td>1.032225</td>\n      <td>-1.586794</td>\n      <td>-1.686939</td>\n      <td>0.033403</td>\n      <td>1.983659</td>\n      <td>0.033070</td>\n      <td>-1.055559</td>\n      <td>0.033342</td>\n      <td>-0.672751</td>\n      <td>0.032700</td>\n      <td>1.983659</td>\n      <td>-1.553356</td>\n      <td>3.292455</td>\n      <td>2.883474</td>\n      <td>-0.416459</td>\n      <td>0.871126</td>\n      <td>0.940838</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102824</th>\n      <td>-0.849383</td>\n      <td>-0.468005</td>\n      <td>0.551286</td>\n      <td>-0.697511</td>\n      <td>0.311149</td>\n      <td>1.488425</td>\n      <td>-1.165481</td>\n      <td>-0.245963</td>\n      <td>0.034249</td>\n      <td>-0.180854</td>\n      <td>-0.119445</td>\n      <td>-0.742812</td>\n      <td>0.425478</td>\n      <td>0.028259</td>\n      <td>-0.377168</td>\n      <td>-0.472695</td>\n      <td>-0.630500</td>\n      <td>0.299861</td>\n      <td>-0.353771</td>\n      <td>0.805470</td>\n      <td>-0.300501</td>\n      <td>-0.686379</td>\n      <td>0.033403</td>\n      <td>-0.717359</td>\n      <td>0.033070</td>\n      <td>-0.989143</td>\n      <td>0.033342</td>\n      <td>-0.442035</td>\n      <td>0.032700</td>\n      <td>-0.717359</td>\n      <td>0.278820</td>\n      <td>-0.871179</td>\n      <td>-0.810220</td>\n      <td>-0.767302</td>\n      <td>-0.709822</td>\n      <td>-0.534761</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102825</th>\n      <td>-0.960291</td>\n      <td>-0.697860</td>\n      <td>0.551286</td>\n      <td>-0.697511</td>\n      <td>0.311149</td>\n      <td>1.488425</td>\n      <td>-0.180700</td>\n      <td>-0.668499</td>\n      <td>0.034249</td>\n      <td>-0.317436</td>\n      <td>-0.468010</td>\n      <td>-0.713194</td>\n      <td>-0.184712</td>\n      <td>-0.377822</td>\n      <td>-0.379995</td>\n      <td>-0.472371</td>\n      <td>-0.630500</td>\n      <td>0.299861</td>\n      <td>-0.353771</td>\n      <td>0.805470</td>\n      <td>-0.300501</td>\n      <td>-0.660106</td>\n      <td>0.033403</td>\n      <td>0.404234</td>\n      <td>0.033070</td>\n      <td>1.589911</td>\n      <td>0.033342</td>\n      <td>-0.636333</td>\n      <td>0.032700</td>\n      <td>0.404234</td>\n      <td>-0.062456</td>\n      <td>-0.224705</td>\n      <td>-0.403947</td>\n      <td>-0.695827</td>\n      <td>-0.599545</td>\n      <td>-0.557256</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>102826</th>\n      <td>-1.146700</td>\n      <td>-0.821739</td>\n      <td>0.551286</td>\n      <td>-0.697511</td>\n      <td>0.311149</td>\n      <td>1.488425</td>\n      <td>-0.153506</td>\n      <td>-0.809007</td>\n      <td>0.034249</td>\n      <td>-0.078191</td>\n      <td>0.039278</td>\n      <td>-0.494235</td>\n      <td>0.812333</td>\n      <td>-0.014013</td>\n      <td>-0.238286</td>\n      <td>-0.441773</td>\n      <td>-0.634981</td>\n      <td>0.299861</td>\n      <td>-0.353771</td>\n      <td>0.805470</td>\n      <td>-0.300501</td>\n      <td>-0.631003</td>\n      <td>0.033403</td>\n      <td>-0.199775</td>\n      <td>0.033070</td>\n      <td>-0.709958</td>\n      <td>0.033342</td>\n      <td>-1.013816</td>\n      <td>0.032700</td>\n      <td>-0.199775</td>\n      <td>-0.336790</td>\n      <td>-0.041263</td>\n      <td>-0.230526</td>\n      <td>-0.882927</td>\n      <td>-0.604291</td>\n      <td>-0.628839</td>\n      <td>1.175781</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>6311 rows × 85 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neurons_base = 16\ndropout_rate = 0.05\n# n_b=8 was ok with small overfit.\n# n_b=32 starts clearly overfitting. \n# 128 fits clearly slower than 64 and becomes somewhat unstable. regularization could make it work, but i see no reason to go wider.\n# 64 seems to have nice balance of flexibility and runtime, but its variance may be too large. dropout makes variance vene worse.\n# 6 hidden layers is probably most this architecture can hold\n\n# in this framework the optimal model seems to have width of 16 or 32, somehow regularized. try l1/l2?\n# w32 can take at most 0.03 dropout.\n# w16 looks good w/o dropout.\n\n# more general point:\n# main drawback of dropout is in incresing variance\n# for textbook problems with high s/n ratio (e.g., mnist) this may be ok.\n# for application like this with very low s/n ratio dropout may be a bad idea.\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*32, activation=\"selu\", kernel_initializer=\"lecun_normal\", input_shape=X_train.shape[1:]),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*16, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\"),\n    tf.keras.layers.AlphaDropout(dropout_rate),\n    Dense(1)])\n\nprint(model_snn.count_params())","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:43.772943Z","iopub.execute_input":"2022-09-07T17:18:43.773875Z","iopub.status.idle":"2022-09-07T17:18:50.303909Z","shell.execute_reply.started":"2022-09-07T17:18:43.773839Z","shell.execute_reply":"2022-09-07T17:18:50.302897Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stderr","text":"2022-09-07 17:18:43.812428: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n","output_type":"stream"},{"name":"stdout","text":"219137\n","output_type":"stream"},{"name":"stderr","text":"2022-09-07 17:18:43.813723: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-07 17:18:43.814580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-07 17:18:43.815528: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-09-07 17:18:43.815878: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-07 17:18:43.816594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-07 17:18:43.817305: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-07 17:18:49.880745: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-07 17:18:49.881663: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-07 17:18:49.882444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-09-07 17:18:49.883066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14669 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"neurons_base = 16\nl2_reg_rate = 0.4\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*8, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          input_shape=X_train.shape[1:],\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn.count_params())\n\n# similar problem as before: model seems ok in terms of flexibility and variance, but adding dropout breaks it before i can fix overfitting.\n# the solution is to either use smaller models or to use laternative regularizers (which do not increase variance.)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:50.305372Z","iopub.execute_input":"2022-09-07T17:18:50.306025Z","iopub.status.idle":"2022-09-07T17:18:50.357362Z","shell.execute_reply.started":"2022-09-07T17:18:50.305987Z","shell.execute_reply":"2022-09-07T17:18:50.356392Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"26049\n","output_type":"stream"}]},{"cell_type":"code","source":"neurons_base = 8\nl2_reg_rate = 0.3\n\nmodel_snn = Sequential([\n    tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                          kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n    tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                         kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n    Dense(1)])\n\nprint(model_snn.count_params())\n\n# snn: 32-16-8-4 with 30%l2 reg seems to converge to 5.1% test r2.","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:50.358811Z","iopub.execute_input":"2022-09-07T17:18:50.359163Z","iopub.status.idle":"2022-09-07T17:18:50.404575Z","shell.execute_reply.started":"2022-09-07T17:18:50.359130Z","shell.execute_reply":"2022-09-07T17:18:50.403531Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"3457\n","output_type":"stream"}]},{"cell_type":"code","source":"early_stopping50 = EarlyStopping(patience=50, restore_best_weights=True)\ntime1 = time.time()\noptimizer_adam = tf.keras.optimizers.Adam()\nmodel_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\nhistory = model_snn.fit(X_train, y_train, validation_data=(X_val, y_val), \n                         batch_size=2048, epochs=1000, verbose=2, callbacks=[early_stopping50])\nhistory_df = pd.DataFrame(history.history)\nhistory_df.loc[:, ['loss', 'val_loss']].plot()\nprint(\"Minimum Validation Loss: {:0.4f}\".format(history_df['val_loss'].min()))\n\nprint([r2_score(y_train, model_snn.predict(X_train)), \n       r2_score(y_val, model_snn.predict(X_val)),\n       r2_score(y_test, model_snn.predict(X_test))])\nprint(time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:18:50.405768Z","iopub.execute_input":"2022-09-07T17:18:50.406091Z","iopub.status.idle":"2022-09-07T17:19:35.580552Z","shell.execute_reply.started":"2022-09-07T17:18:50.406057Z","shell.execute_reply":"2022-09-07T17:19:35.579594Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"2022-09-07 17:18:50.626814: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/1000\n39/39 - 2s - loss: 150.1036 - mean_squared_error: 134.9699 - val_loss: 134.3322 - val_mean_squared_error: 121.3434\nEpoch 2/1000\n39/39 - 0s - loss: 144.5756 - mean_squared_error: 133.1774 - val_loss: 131.2944 - val_mean_squared_error: 121.3750\nEpoch 3/1000\n39/39 - 0s - loss: 141.0446 - mean_squared_error: 132.1308 - val_loss: 128.3966 - val_mean_squared_error: 120.3830\nEpoch 4/1000\n39/39 - 0s - loss: 138.1486 - mean_squared_error: 130.6927 - val_loss: 126.3911 - val_mean_squared_error: 119.4095\nEpoch 5/1000\n39/39 - 0s - loss: 135.9481 - mean_squared_error: 129.2906 - val_loss: 125.4822 - val_mean_squared_error: 119.1435\nEpoch 6/1000\n39/39 - 0s - loss: 134.4322 - mean_squared_error: 128.3903 - val_loss: 124.9000 - val_mean_squared_error: 119.1915\nEpoch 7/1000\n39/39 - 0s - loss: 133.3543 - mean_squared_error: 127.8983 - val_loss: 125.1949 - val_mean_squared_error: 119.9692\nEpoch 8/1000\n39/39 - 0s - loss: 132.7072 - mean_squared_error: 127.6958 - val_loss: 124.0461 - val_mean_squared_error: 119.2602\nEpoch 9/1000\n39/39 - 0s - loss: 132.1924 - mean_squared_error: 127.5811 - val_loss: 124.7907 - val_mean_squared_error: 120.3461\nEpoch 10/1000\n39/39 - 0s - loss: 131.7572 - mean_squared_error: 127.4701 - val_loss: 124.4468 - val_mean_squared_error: 120.2819\nEpoch 11/1000\n39/39 - 0s - loss: 131.4478 - mean_squared_error: 127.3897 - val_loss: 123.3036 - val_mean_squared_error: 119.3594\nEpoch 12/1000\n39/39 - 0s - loss: 131.1907 - mean_squared_error: 127.3308 - val_loss: 122.8968 - val_mean_squared_error: 119.1443\nEpoch 13/1000\n39/39 - 0s - loss: 130.8887 - mean_squared_error: 127.1981 - val_loss: 122.9216 - val_mean_squared_error: 119.3360\nEpoch 14/1000\n39/39 - 0s - loss: 130.5904 - mean_squared_error: 127.0735 - val_loss: 123.5718 - val_mean_squared_error: 120.1037\nEpoch 15/1000\n39/39 - 0s - loss: 130.6240 - mean_squared_error: 127.2334 - val_loss: 124.2110 - val_mean_squared_error: 120.8964\nEpoch 16/1000\n39/39 - 0s - loss: 130.3072 - mean_squared_error: 127.0163 - val_loss: 123.9501 - val_mean_squared_error: 120.7018\nEpoch 17/1000\n39/39 - 0s - loss: 130.2224 - mean_squared_error: 127.0259 - val_loss: 124.0317 - val_mean_squared_error: 120.8963\nEpoch 18/1000\n39/39 - 0s - loss: 130.0888 - mean_squared_error: 126.9812 - val_loss: 122.4128 - val_mean_squared_error: 119.3657\nEpoch 19/1000\n39/39 - 0s - loss: 129.9224 - mean_squared_error: 126.8970 - val_loss: 123.0580 - val_mean_squared_error: 120.0945\nEpoch 20/1000\n39/39 - 0s - loss: 129.8457 - mean_squared_error: 126.8827 - val_loss: 123.8496 - val_mean_squared_error: 120.9334\nEpoch 21/1000\n39/39 - 0s - loss: 129.7426 - mean_squared_error: 126.8503 - val_loss: 123.8317 - val_mean_squared_error: 120.9793\nEpoch 22/1000\n39/39 - 0s - loss: 129.7592 - mean_squared_error: 126.9214 - val_loss: 121.8382 - val_mean_squared_error: 119.0342\nEpoch 23/1000\n39/39 - 0s - loss: 129.6002 - mean_squared_error: 126.8168 - val_loss: 123.9394 - val_mean_squared_error: 121.1612\nEpoch 24/1000\n39/39 - 0s - loss: 129.6144 - mean_squared_error: 126.8654 - val_loss: 122.6706 - val_mean_squared_error: 119.9443\nEpoch 25/1000\n39/39 - 0s - loss: 129.4916 - mean_squared_error: 126.7733 - val_loss: 123.0182 - val_mean_squared_error: 120.3393\nEpoch 26/1000\n39/39 - 0s - loss: 129.4608 - mean_squared_error: 126.7997 - val_loss: 123.8039 - val_mean_squared_error: 121.1593\nEpoch 27/1000\n39/39 - 0s - loss: 129.2820 - mean_squared_error: 126.6371 - val_loss: 121.0999 - val_mean_squared_error: 118.4853\nEpoch 28/1000\n39/39 - 0s - loss: 129.3880 - mean_squared_error: 126.7932 - val_loss: 121.5621 - val_mean_squared_error: 118.9989\nEpoch 29/1000\n39/39 - 0s - loss: 129.2834 - mean_squared_error: 126.6871 - val_loss: 121.6925 - val_mean_squared_error: 119.1569\nEpoch 30/1000\n39/39 - 0s - loss: 129.2193 - mean_squared_error: 126.6787 - val_loss: 122.0929 - val_mean_squared_error: 119.5741\nEpoch 31/1000\n39/39 - 0s - loss: 129.0770 - mean_squared_error: 126.5528 - val_loss: 123.2020 - val_mean_squared_error: 120.6887\nEpoch 32/1000\n39/39 - 0s - loss: 129.1136 - mean_squared_error: 126.6312 - val_loss: 122.4293 - val_mean_squared_error: 119.9345\nEpoch 33/1000\n39/39 - 0s - loss: 129.1479 - mean_squared_error: 126.6686 - val_loss: 123.7209 - val_mean_squared_error: 121.2793\nEpoch 34/1000\n39/39 - 0s - loss: 128.9135 - mean_squared_error: 126.4554 - val_loss: 122.6944 - val_mean_squared_error: 120.2593\nEpoch 35/1000\n39/39 - 0s - loss: 129.0091 - mean_squared_error: 126.5778 - val_loss: 122.2065 - val_mean_squared_error: 119.7891\nEpoch 36/1000\n39/39 - 0s - loss: 128.9704 - mean_squared_error: 126.5723 - val_loss: 122.3366 - val_mean_squared_error: 119.9558\nEpoch 37/1000\n39/39 - 0s - loss: 128.8848 - mean_squared_error: 126.4780 - val_loss: 121.9115 - val_mean_squared_error: 119.5510\nEpoch 38/1000\n39/39 - 0s - loss: 128.8999 - mean_squared_error: 126.5322 - val_loss: 124.2457 - val_mean_squared_error: 121.8481\nEpoch 39/1000\n39/39 - 0s - loss: 128.8758 - mean_squared_error: 126.5250 - val_loss: 122.3842 - val_mean_squared_error: 120.0434\nEpoch 40/1000\n39/39 - 0s - loss: 128.7259 - mean_squared_error: 126.3737 - val_loss: 123.5948 - val_mean_squared_error: 121.2392\nEpoch 41/1000\n39/39 - 0s - loss: 128.8751 - mean_squared_error: 126.5507 - val_loss: 124.6908 - val_mean_squared_error: 122.3698\nEpoch 42/1000\n39/39 - 0s - loss: 128.7360 - mean_squared_error: 126.4250 - val_loss: 123.9185 - val_mean_squared_error: 121.5710\nEpoch 43/1000\n39/39 - 0s - loss: 128.6990 - mean_squared_error: 126.3967 - val_loss: 124.4999 - val_mean_squared_error: 122.1893\nEpoch 44/1000\n39/39 - 0s - loss: 128.6632 - mean_squared_error: 126.3792 - val_loss: 121.6760 - val_mean_squared_error: 119.4039\nEpoch 45/1000\n39/39 - 0s - loss: 128.5835 - mean_squared_error: 126.3068 - val_loss: 121.5564 - val_mean_squared_error: 119.2696\nEpoch 46/1000\n39/39 - 0s - loss: 128.5985 - mean_squared_error: 126.3211 - val_loss: 122.3314 - val_mean_squared_error: 120.0877\nEpoch 47/1000\n39/39 - 0s - loss: 128.5713 - mean_squared_error: 126.3269 - val_loss: 122.0338 - val_mean_squared_error: 119.7836\nEpoch 48/1000\n39/39 - 0s - loss: 128.7489 - mean_squared_error: 126.5167 - val_loss: 122.6823 - val_mean_squared_error: 120.4548\nEpoch 49/1000\n39/39 - 0s - loss: 128.6175 - mean_squared_error: 126.3848 - val_loss: 121.1140 - val_mean_squared_error: 118.9007\nEpoch 50/1000\n39/39 - 0s - loss: 128.4328 - mean_squared_error: 126.2071 - val_loss: 121.8997 - val_mean_squared_error: 119.6758\nEpoch 51/1000\n39/39 - 0s - loss: 128.4497 - mean_squared_error: 126.2413 - val_loss: 121.5247 - val_mean_squared_error: 119.3112\nEpoch 52/1000\n39/39 - 0s - loss: 128.4346 - mean_squared_error: 126.2303 - val_loss: 121.0377 - val_mean_squared_error: 118.8547\nEpoch 53/1000\n39/39 - 0s - loss: 128.5224 - mean_squared_error: 126.3363 - val_loss: 122.6958 - val_mean_squared_error: 120.5069\nEpoch 54/1000\n39/39 - 0s - loss: 128.3828 - mean_squared_error: 126.2024 - val_loss: 121.1516 - val_mean_squared_error: 118.9849\nEpoch 55/1000\n39/39 - 0s - loss: 128.3707 - mean_squared_error: 126.2061 - val_loss: 120.9728 - val_mean_squared_error: 118.8135\nEpoch 56/1000\n39/39 - 0s - loss: 128.3185 - mean_squared_error: 126.1520 - val_loss: 122.8762 - val_mean_squared_error: 120.7000\nEpoch 57/1000\n39/39 - 0s - loss: 128.3727 - mean_squared_error: 126.2182 - val_loss: 121.4434 - val_mean_squared_error: 119.2983\nEpoch 58/1000\n39/39 - 0s - loss: 128.3785 - mean_squared_error: 126.2418 - val_loss: 122.8450 - val_mean_squared_error: 120.7178\nEpoch 59/1000\n39/39 - 0s - loss: 128.3251 - mean_squared_error: 126.1919 - val_loss: 122.4785 - val_mean_squared_error: 120.3451\nEpoch 60/1000\n39/39 - 0s - loss: 128.1996 - mean_squared_error: 126.0837 - val_loss: 123.1394 - val_mean_squared_error: 120.9863\nEpoch 61/1000\n39/39 - 0s - loss: 128.3527 - mean_squared_error: 126.2322 - val_loss: 121.8573 - val_mean_squared_error: 119.7771\nEpoch 62/1000\n39/39 - 0s - loss: 128.3811 - mean_squared_error: 126.2837 - val_loss: 122.6659 - val_mean_squared_error: 120.5717\nEpoch 63/1000\n39/39 - 0s - loss: 128.1979 - mean_squared_error: 126.0958 - val_loss: 123.0732 - val_mean_squared_error: 120.9828\nEpoch 64/1000\n39/39 - 0s - loss: 128.1227 - mean_squared_error: 126.0230 - val_loss: 120.5284 - val_mean_squared_error: 118.4266\nEpoch 65/1000\n39/39 - 0s - loss: 128.2062 - mean_squared_error: 126.1151 - val_loss: 121.6446 - val_mean_squared_error: 119.5541\nEpoch 66/1000\n39/39 - 0s - loss: 128.0971 - mean_squared_error: 126.0202 - val_loss: 122.0076 - val_mean_squared_error: 119.9417\nEpoch 67/1000\n39/39 - 0s - loss: 128.2435 - mean_squared_error: 126.1707 - val_loss: 124.5726 - val_mean_squared_error: 122.4907\nEpoch 68/1000\n39/39 - 0s - loss: 128.0843 - mean_squared_error: 126.0120 - val_loss: 124.1811 - val_mean_squared_error: 122.1306\nEpoch 69/1000\n39/39 - 0s - loss: 128.1117 - mean_squared_error: 126.0537 - val_loss: 123.8320 - val_mean_squared_error: 121.7477\nEpoch 70/1000\n39/39 - 0s - loss: 128.1304 - mean_squared_error: 126.0634 - val_loss: 121.1624 - val_mean_squared_error: 119.1236\nEpoch 71/1000\n39/39 - 0s - loss: 128.2434 - mean_squared_error: 126.2121 - val_loss: 124.8920 - val_mean_squared_error: 122.8526\nEpoch 72/1000\n39/39 - 0s - loss: 128.1496 - mean_squared_error: 126.1171 - val_loss: 124.4898 - val_mean_squared_error: 122.4519\nEpoch 73/1000\n39/39 - 0s - loss: 128.1305 - mean_squared_error: 126.1011 - val_loss: 122.1367 - val_mean_squared_error: 120.1028\nEpoch 74/1000\n39/39 - 0s - loss: 128.0807 - mean_squared_error: 126.0556 - val_loss: 122.6414 - val_mean_squared_error: 120.6457\nEpoch 75/1000\n39/39 - 0s - loss: 128.1135 - mean_squared_error: 126.1058 - val_loss: 120.8576 - val_mean_squared_error: 118.8443\nEpoch 76/1000\n39/39 - 0s - loss: 128.0252 - mean_squared_error: 126.0316 - val_loss: 122.5762 - val_mean_squared_error: 120.5563\nEpoch 77/1000\n39/39 - 0s - loss: 127.9568 - mean_squared_error: 125.9253 - val_loss: 121.9173 - val_mean_squared_error: 119.8948\nEpoch 78/1000\n39/39 - 0s - loss: 128.0770 - mean_squared_error: 126.0745 - val_loss: 123.5116 - val_mean_squared_error: 121.5382\nEpoch 79/1000\n39/39 - 0s - loss: 127.9745 - mean_squared_error: 125.9717 - val_loss: 120.3866 - val_mean_squared_error: 118.3893\nEpoch 80/1000\n39/39 - 0s - loss: 127.9001 - mean_squared_error: 125.8946 - val_loss: 121.3187 - val_mean_squared_error: 119.3250\nEpoch 81/1000\n39/39 - 0s - loss: 128.1105 - mean_squared_error: 126.1322 - val_loss: 120.8800 - val_mean_squared_error: 118.9207\nEpoch 82/1000\n39/39 - 0s - loss: 127.9864 - mean_squared_error: 126.0025 - val_loss: 121.4349 - val_mean_squared_error: 119.4746\nEpoch 83/1000\n39/39 - 0s - loss: 127.8990 - mean_squared_error: 125.9046 - val_loss: 121.6727 - val_mean_squared_error: 119.7138\nEpoch 84/1000\n39/39 - 0s - loss: 128.0308 - mean_squared_error: 126.0628 - val_loss: 122.9907 - val_mean_squared_error: 121.0519\nEpoch 85/1000\n39/39 - 0s - loss: 127.9133 - mean_squared_error: 125.9711 - val_loss: 123.3790 - val_mean_squared_error: 121.4005\nEpoch 86/1000\n39/39 - 0s - loss: 127.8753 - mean_squared_error: 125.9028 - val_loss: 120.4261 - val_mean_squared_error: 118.4791\nEpoch 87/1000\n39/39 - 0s - loss: 127.8228 - mean_squared_error: 125.8704 - val_loss: 121.5608 - val_mean_squared_error: 119.5986\nEpoch 88/1000\n39/39 - 0s - loss: 128.0809 - mean_squared_error: 126.1320 - val_loss: 122.0472 - val_mean_squared_error: 120.1219\nEpoch 89/1000\n39/39 - 0s - loss: 128.0254 - mean_squared_error: 126.0943 - val_loss: 121.3386 - val_mean_squared_error: 119.3976\nEpoch 90/1000\n39/39 - 0s - loss: 127.8033 - mean_squared_error: 125.8671 - val_loss: 120.5147 - val_mean_squared_error: 118.5869\nEpoch 91/1000\n39/39 - 0s - loss: 127.9299 - mean_squared_error: 125.9984 - val_loss: 122.5174 - val_mean_squared_error: 120.5885\nEpoch 92/1000\n39/39 - 0s - loss: 127.7842 - mean_squared_error: 125.8539 - val_loss: 123.5657 - val_mean_squared_error: 121.6434\nEpoch 93/1000\n39/39 - 0s - loss: 127.8711 - mean_squared_error: 125.9375 - val_loss: 120.7659 - val_mean_squared_error: 118.8609\nEpoch 94/1000\n39/39 - 0s - loss: 127.8322 - mean_squared_error: 125.9117 - val_loss: 121.1209 - val_mean_squared_error: 119.2153\nEpoch 95/1000\n39/39 - 0s - loss: 127.8346 - mean_squared_error: 125.9129 - val_loss: 122.9281 - val_mean_squared_error: 121.0327\nEpoch 96/1000\n39/39 - 0s - loss: 127.8662 - mean_squared_error: 125.9577 - val_loss: 120.1448 - val_mean_squared_error: 118.2308\nEpoch 97/1000\n39/39 - 0s - loss: 127.9082 - mean_squared_error: 126.0097 - val_loss: 123.5519 - val_mean_squared_error: 121.6424\nEpoch 98/1000\n39/39 - 0s - loss: 127.6792 - mean_squared_error: 125.7642 - val_loss: 121.3919 - val_mean_squared_error: 119.4814\nEpoch 99/1000\n39/39 - 0s - loss: 127.8499 - mean_squared_error: 125.9514 - val_loss: 122.5453 - val_mean_squared_error: 120.6345\nEpoch 100/1000\n39/39 - 0s - loss: 127.8840 - mean_squared_error: 125.9932 - val_loss: 122.7751 - val_mean_squared_error: 120.8734\nEpoch 101/1000\n39/39 - 0s - loss: 127.7612 - mean_squared_error: 125.8697 - val_loss: 121.6360 - val_mean_squared_error: 119.7479\nEpoch 102/1000\n39/39 - 0s - loss: 127.6972 - mean_squared_error: 125.7918 - val_loss: 121.3099 - val_mean_squared_error: 119.4219\nEpoch 103/1000\n39/39 - 0s - loss: 127.6653 - mean_squared_error: 125.7879 - val_loss: 122.8640 - val_mean_squared_error: 120.9680\nEpoch 104/1000\n39/39 - 0s - loss: 127.9065 - mean_squared_error: 126.0313 - val_loss: 121.9220 - val_mean_squared_error: 120.0425\nEpoch 105/1000\n39/39 - 0s - loss: 127.6483 - mean_squared_error: 125.7691 - val_loss: 121.5665 - val_mean_squared_error: 119.6928\nEpoch 106/1000\n39/39 - 0s - loss: 127.6826 - mean_squared_error: 125.8009 - val_loss: 124.8775 - val_mean_squared_error: 122.9899\nEpoch 107/1000\n39/39 - 0s - loss: 127.8383 - mean_squared_error: 125.9882 - val_loss: 122.1531 - val_mean_squared_error: 120.2806\nEpoch 108/1000\n39/39 - 0s - loss: 127.6044 - mean_squared_error: 125.7164 - val_loss: 122.1087 - val_mean_squared_error: 120.2344\nEpoch 109/1000\n39/39 - 0s - loss: 127.6044 - mean_squared_error: 125.7470 - val_loss: 120.7873 - val_mean_squared_error: 118.9263\nEpoch 110/1000\n39/39 - 0s - loss: 127.6134 - mean_squared_error: 125.7404 - val_loss: 122.7717 - val_mean_squared_error: 120.9203\nEpoch 111/1000\n39/39 - 0s - loss: 127.7593 - mean_squared_error: 125.9124 - val_loss: 123.6125 - val_mean_squared_error: 121.7571\nEpoch 112/1000\n39/39 - 0s - loss: 127.5761 - mean_squared_error: 125.7172 - val_loss: 122.4520 - val_mean_squared_error: 120.5868\nEpoch 113/1000\n39/39 - 0s - loss: 127.6243 - mean_squared_error: 125.7670 - val_loss: 122.7207 - val_mean_squared_error: 120.8629\nEpoch 114/1000\n39/39 - 0s - loss: 127.6033 - mean_squared_error: 125.7499 - val_loss: 121.7598 - val_mean_squared_error: 119.9177\nEpoch 115/1000\n39/39 - 0s - loss: 127.6287 - mean_squared_error: 125.7674 - val_loss: 121.5945 - val_mean_squared_error: 119.7712\nEpoch 116/1000\n39/39 - 0s - loss: 127.5645 - mean_squared_error: 125.7352 - val_loss: 123.0075 - val_mean_squared_error: 121.1592\nEpoch 117/1000\n39/39 - 0s - loss: 127.7313 - mean_squared_error: 125.8787 - val_loss: 123.5479 - val_mean_squared_error: 121.7185\nEpoch 118/1000\n39/39 - 0s - loss: 127.6210 - mean_squared_error: 125.8036 - val_loss: 121.0663 - val_mean_squared_error: 119.2404\nEpoch 119/1000\n39/39 - 0s - loss: 127.6016 - mean_squared_error: 125.7672 - val_loss: 123.8775 - val_mean_squared_error: 122.0512\nEpoch 120/1000\n39/39 - 0s - loss: 127.6989 - mean_squared_error: 125.8862 - val_loss: 121.3550 - val_mean_squared_error: 119.5181\nEpoch 121/1000\n39/39 - 0s - loss: 127.4952 - mean_squared_error: 125.6623 - val_loss: 123.3848 - val_mean_squared_error: 121.5777\nEpoch 122/1000\n39/39 - 0s - loss: 127.4876 - mean_squared_error: 125.6588 - val_loss: 123.0389 - val_mean_squared_error: 121.2004\nEpoch 123/1000\n39/39 - 0s - loss: 127.6004 - mean_squared_error: 125.7841 - val_loss: 121.1665 - val_mean_squared_error: 119.3828\nEpoch 124/1000\n39/39 - 0s - loss: 127.4513 - mean_squared_error: 125.6407 - val_loss: 120.9042 - val_mean_squared_error: 119.0909\nEpoch 125/1000\n39/39 - 0s - loss: 127.4412 - mean_squared_error: 125.6174 - val_loss: 120.8729 - val_mean_squared_error: 119.0804\nEpoch 126/1000\n39/39 - 0s - loss: 127.6941 - mean_squared_error: 125.9073 - val_loss: 119.8245 - val_mean_squared_error: 118.0237\nEpoch 127/1000\n39/39 - 0s - loss: 127.7845 - mean_squared_error: 125.9942 - val_loss: 123.5334 - val_mean_squared_error: 121.7670\nEpoch 128/1000\n39/39 - 0s - loss: 127.5708 - mean_squared_error: 125.7831 - val_loss: 122.4192 - val_mean_squared_error: 120.6209\nEpoch 129/1000\n39/39 - 0s - loss: 127.4602 - mean_squared_error: 125.6528 - val_loss: 121.1841 - val_mean_squared_error: 119.4077\nEpoch 130/1000\n39/39 - 0s - loss: 127.3188 - mean_squared_error: 125.5154 - val_loss: 122.6179 - val_mean_squared_error: 120.8129\nEpoch 131/1000\n39/39 - 0s - loss: 127.7467 - mean_squared_error: 125.9630 - val_loss: 121.5556 - val_mean_squared_error: 119.7925\nEpoch 132/1000\n39/39 - 0s - loss: 127.5075 - mean_squared_error: 125.7326 - val_loss: 120.8882 - val_mean_squared_error: 119.1196\nEpoch 133/1000\n39/39 - 0s - loss: 127.6211 - mean_squared_error: 125.8453 - val_loss: 120.1938 - val_mean_squared_error: 118.4339\nEpoch 134/1000\n39/39 - 0s - loss: 127.6922 - mean_squared_error: 125.9131 - val_loss: 121.4026 - val_mean_squared_error: 119.6399\nEpoch 135/1000\n39/39 - 0s - loss: 127.4508 - mean_squared_error: 125.6729 - val_loss: 120.5357 - val_mean_squared_error: 118.7798\nEpoch 136/1000\n39/39 - 0s - loss: 127.5092 - mean_squared_error: 125.7403 - val_loss: 120.7192 - val_mean_squared_error: 118.9470\nEpoch 137/1000\n39/39 - 0s - loss: 127.4339 - mean_squared_error: 125.6695 - val_loss: 122.4380 - val_mean_squared_error: 120.6587\nEpoch 138/1000\n39/39 - 0s - loss: 127.3952 - mean_squared_error: 125.6161 - val_loss: 123.9934 - val_mean_squared_error: 122.2245\nEpoch 139/1000\n39/39 - 0s - loss: 127.3742 - mean_squared_error: 125.6086 - val_loss: 123.5284 - val_mean_squared_error: 121.7543\nEpoch 140/1000\n39/39 - 0s - loss: 127.3387 - mean_squared_error: 125.5771 - val_loss: 121.5955 - val_mean_squared_error: 119.8219\nEpoch 141/1000\n39/39 - 0s - loss: 127.2658 - mean_squared_error: 125.4985 - val_loss: 122.3413 - val_mean_squared_error: 120.5721\nEpoch 142/1000\n39/39 - 0s - loss: 127.4680 - mean_squared_error: 125.7101 - val_loss: 123.6721 - val_mean_squared_error: 121.9215\nEpoch 143/1000\n39/39 - 0s - loss: 127.3958 - mean_squared_error: 125.6414 - val_loss: 121.2841 - val_mean_squared_error: 119.5265\nEpoch 144/1000\n39/39 - 0s - loss: 127.3660 - mean_squared_error: 125.6041 - val_loss: 122.5411 - val_mean_squared_error: 120.7828\nEpoch 145/1000\n39/39 - 0s - loss: 127.3685 - mean_squared_error: 125.6256 - val_loss: 122.8826 - val_mean_squared_error: 121.1314\nEpoch 146/1000\n39/39 - 0s - loss: 127.3446 - mean_squared_error: 125.5712 - val_loss: 122.6291 - val_mean_squared_error: 120.8963\nEpoch 147/1000\n39/39 - 0s - loss: 127.4676 - mean_squared_error: 125.7247 - val_loss: 123.4417 - val_mean_squared_error: 121.7042\nEpoch 148/1000\n39/39 - 0s - loss: 127.4166 - mean_squared_error: 125.6733 - val_loss: 123.9243 - val_mean_squared_error: 122.1744\nEpoch 149/1000\n39/39 - 0s - loss: 127.5229 - mean_squared_error: 125.7948 - val_loss: 121.6370 - val_mean_squared_error: 119.9198\nEpoch 150/1000\n39/39 - 0s - loss: 127.3288 - mean_squared_error: 125.5938 - val_loss: 122.3418 - val_mean_squared_error: 120.5950\nEpoch 151/1000\n39/39 - 0s - loss: 127.3047 - mean_squared_error: 125.5606 - val_loss: 122.5399 - val_mean_squared_error: 120.7947\nEpoch 152/1000\n39/39 - 0s - loss: 127.1783 - mean_squared_error: 125.4323 - val_loss: 121.3649 - val_mean_squared_error: 119.6400\nEpoch 153/1000\n39/39 - 0s - loss: 127.3334 - mean_squared_error: 125.6030 - val_loss: 121.7892 - val_mean_squared_error: 120.0626\nEpoch 154/1000\n39/39 - 0s - loss: 127.3584 - mean_squared_error: 125.6210 - val_loss: 122.9404 - val_mean_squared_error: 121.1960\nEpoch 155/1000\n39/39 - 0s - loss: 127.3574 - mean_squared_error: 125.6236 - val_loss: 122.8600 - val_mean_squared_error: 121.1228\nEpoch 156/1000\n39/39 - 0s - loss: 127.2459 - mean_squared_error: 125.5207 - val_loss: 121.8600 - val_mean_squared_error: 120.1466\nEpoch 157/1000\n39/39 - 0s - loss: 127.5693 - mean_squared_error: 125.8422 - val_loss: 124.4196 - val_mean_squared_error: 122.7227\nEpoch 158/1000\n39/39 - 0s - loss: 127.4297 - mean_squared_error: 125.7180 - val_loss: 122.4887 - val_mean_squared_error: 120.7775\nEpoch 159/1000\n39/39 - 0s - loss: 127.4628 - mean_squared_error: 125.7562 - val_loss: 120.4107 - val_mean_squared_error: 118.6920\nEpoch 160/1000\n39/39 - 0s - loss: 127.2264 - mean_squared_error: 125.4989 - val_loss: 122.7862 - val_mean_squared_error: 121.0571\nEpoch 161/1000\n39/39 - 0s - loss: 127.2329 - mean_squared_error: 125.5172 - val_loss: 122.5348 - val_mean_squared_error: 120.8138\nEpoch 162/1000\n39/39 - 0s - loss: 127.3971 - mean_squared_error: 125.6824 - val_loss: 122.3238 - val_mean_squared_error: 120.6205\nEpoch 163/1000\n39/39 - 0s - loss: 127.2958 - mean_squared_error: 125.5901 - val_loss: 123.6752 - val_mean_squared_error: 121.9678\nEpoch 164/1000\n39/39 - 0s - loss: 127.3388 - mean_squared_error: 125.6368 - val_loss: 121.6873 - val_mean_squared_error: 119.9776\nEpoch 165/1000\n39/39 - 0s - loss: 127.2104 - mean_squared_error: 125.4944 - val_loss: 123.3510 - val_mean_squared_error: 121.6315\nEpoch 166/1000\n39/39 - 0s - loss: 127.3386 - mean_squared_error: 125.6206 - val_loss: 122.9240 - val_mean_squared_error: 121.2409\nEpoch 167/1000\n39/39 - 0s - loss: 127.2863 - mean_squared_error: 125.5862 - val_loss: 121.6994 - val_mean_squared_error: 119.9838\nEpoch 168/1000\n39/39 - 0s - loss: 127.1649 - mean_squared_error: 125.4625 - val_loss: 120.3449 - val_mean_squared_error: 118.6470\nEpoch 169/1000\n39/39 - 0s - loss: 127.2308 - mean_squared_error: 125.5316 - val_loss: 121.5978 - val_mean_squared_error: 119.9039\nEpoch 170/1000\n39/39 - 0s - loss: 127.3494 - mean_squared_error: 125.6541 - val_loss: 124.3694 - val_mean_squared_error: 122.6672\nEpoch 171/1000\n39/39 - 0s - loss: 127.1424 - mean_squared_error: 125.4422 - val_loss: 121.2709 - val_mean_squared_error: 119.5839\nEpoch 172/1000\n39/39 - 0s - loss: 127.0458 - mean_squared_error: 125.3587 - val_loss: 122.4096 - val_mean_squared_error: 120.6964\nEpoch 173/1000\n39/39 - 0s - loss: 127.1228 - mean_squared_error: 125.4081 - val_loss: 121.6372 - val_mean_squared_error: 119.9425\nEpoch 174/1000\n39/39 - 0s - loss: 127.3444 - mean_squared_error: 125.6641 - val_loss: 124.2322 - val_mean_squared_error: 122.5490\nEpoch 175/1000\n39/39 - 0s - loss: 127.2040 - mean_squared_error: 125.5116 - val_loss: 120.9190 - val_mean_squared_error: 119.2228\nEpoch 176/1000\n39/39 - 0s - loss: 127.3051 - mean_squared_error: 125.6086 - val_loss: 121.6754 - val_mean_squared_error: 119.9857\nMinimum Validation Loss: 119.8245\n[0.07855582411381679, -0.000670632190987952, 0.008224182550759629]\n45.06532573699951\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":"# try optuna, using this kaggle notebook: https://www.kaggle.com/code/mistag/keras-model-tuning-with-optuna\n\ndef create_snnn_model(trial):\n\n    neurons_base = trial.suggest_int(\"neurons\", 4, 16, 2)\n    l2_reg_rate = trial.suggest_float('l2_regularizer', 0, 0.5)\n\n    model_snn = Sequential([\n        tf.keras.layers.Dense(units=neurons_base*4, activation=\"selu\", kernel_initializer=\"lecun_normal\", \n                              kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate), input_shape=X_train.shape[1:]),\n        tf.keras.layers.Dense(units=neurons_base*2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n        tf.keras.layers.Dense(units=neurons_base/2, activation=\"selu\", kernel_initializer=\"lecun_normal\",\n                             kernel_regularizer=tf.keras.regularizers.l2(l=l2_reg_rate)),\n        Dense(1)])\n\n    optimizer_adam = tf.keras.optimizers.Adam()\n    model_snn.compile(loss= \"mean_squared_error\" , optimizer=optimizer_adam, metrics=[\"mean_squared_error\"])\n    \n    return model_snn\n\n    \nEPOCHS = 5 # number of epocs per trial\n\ndef objective(trial):\n    \n    tf.keras.backend.clear_session()\n    \n    with strategy.scope():\n        # Generate our trial model.\n        model = create_snnn_model(trial)\n        \n        callbacks = [\n        tf.keras.callbacks.EarlyStopping(patience=30),\n        TFKerasPruningCallback(trial, \"val_loss\"),\n    ]\n\n    \n        # Fit the model on the training data.\n        # The TFKerasPruningCallback checks for pruning condition every epoch.\n        \n        history = model_snn.fit(X_train, y_train, \n                                validation_data=(X_val, y_val),\n                                batch_size=2048, \n                                epochs=500, \n                                verbose=1, \n                                callbacks=callbacks)\n\n        # Evaluate the model accuracy on the validation set.\n        score = model.evaluate(X_val, y_val, verbose=0)\n        return score[1]\n    \nstudy = optuna.create_study(direction=\"minimize\", sampler=optuna.samplers.TPESampler(), pruner=optuna.pruners.HyperbandPruner())\nstudy.optimize(objective, n_trials=10)\npruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\ncomplete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n    \n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-09-07T18:06:56.463288Z","iopub.execute_input":"2022-09-07T18:06:56.463637Z","iopub.status.idle":"2022-09-07T18:07:46.621352Z","shell.execute_reply.started":"2022-09-07T18:06:56.463606Z","shell.execute_reply":"2022-09-07T18:07:46.619595Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2022-09-07 18:06:56,473]\u001b[0m A new study created in memory with name: no-name-99acfebe-bc6d-41c8-926f-2e2c3d995714\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n39/39 [==============================] - 0s 5ms/step - loss: 125.6054 - mean_squared_error: 124.0280 - val_loss: 123.3122 - val_mean_squared_error: 121.7350\nEpoch 2/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5234 - mean_squared_error: 123.9476 - val_loss: 118.3451 - val_mean_squared_error: 116.7806\nEpoch 3/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4503 - mean_squared_error: 123.8801 - val_loss: 122.5039 - val_mean_squared_error: 120.9308\nEpoch 4/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4550 - mean_squared_error: 123.8795 - val_loss: 123.4243 - val_mean_squared_error: 121.8470\nEpoch 5/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2653 - mean_squared_error: 123.6875 - val_loss: 124.5891 - val_mean_squared_error: 123.0060\nEpoch 6/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5795 - mean_squared_error: 124.0039 - val_loss: 122.3020 - val_mean_squared_error: 120.7266\nEpoch 7/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5839 - mean_squared_error: 124.0135 - val_loss: 123.5203 - val_mean_squared_error: 121.9473\nEpoch 8/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3868 - mean_squared_error: 123.8081 - val_loss: 122.9583 - val_mean_squared_error: 121.3830\nEpoch 9/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.6110 - mean_squared_error: 124.0351 - val_loss: 122.5476 - val_mean_squared_error: 120.9778\nEpoch 10/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.6371 - mean_squared_error: 124.0638 - val_loss: 124.3890 - val_mean_squared_error: 122.8169\nEpoch 11/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3967 - mean_squared_error: 123.8239 - val_loss: 121.2973 - val_mean_squared_error: 119.7171\nEpoch 12/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.6661 - mean_squared_error: 124.0868 - val_loss: 120.4558 - val_mean_squared_error: 118.8895\nEpoch 13/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5032 - mean_squared_error: 123.9312 - val_loss: 121.5745 - val_mean_squared_error: 120.0055\nEpoch 14/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5014 - mean_squared_error: 123.9237 - val_loss: 121.0753 - val_mean_squared_error: 119.4922\nEpoch 15/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4533 - mean_squared_error: 123.8753 - val_loss: 123.7299 - val_mean_squared_error: 122.1475\nEpoch 16/500\n39/39 [==============================] - 0s 7ms/step - loss: 125.5724 - mean_squared_error: 123.9971 - val_loss: 121.4846 - val_mean_squared_error: 119.9129\nEpoch 17/500\n39/39 [==============================] - 0s 5ms/step - loss: 125.7217 - mean_squared_error: 124.1473 - val_loss: 123.5271 - val_mean_squared_error: 121.9466\nEpoch 18/500\n39/39 [==============================] - 0s 5ms/step - loss: 125.2754 - mean_squared_error: 123.7005 - val_loss: 126.0753 - val_mean_squared_error: 124.4969\nEpoch 19/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5923 - mean_squared_error: 124.0122 - val_loss: 122.8950 - val_mean_squared_error: 121.3154\nEpoch 20/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.6483 - mean_squared_error: 124.0677 - val_loss: 124.1441 - val_mean_squared_error: 122.5695\nEpoch 21/500\n39/39 [==============================] - 0s 7ms/step - loss: 125.5788 - mean_squared_error: 124.0055 - val_loss: 124.2511 - val_mean_squared_error: 122.6739\nEpoch 22/500\n39/39 [==============================] - 0s 6ms/step - loss: 125.3891 - mean_squared_error: 123.8103 - val_loss: 123.8923 - val_mean_squared_error: 122.3144\nEpoch 23/500\n39/39 [==============================] - 0s 6ms/step - loss: 125.4300 - mean_squared_error: 123.8514 - val_loss: 123.6626 - val_mean_squared_error: 122.0839\nEpoch 24/500\n39/39 [==============================] - 0s 6ms/step - loss: 125.4687 - mean_squared_error: 123.8925 - val_loss: 123.6495 - val_mean_squared_error: 122.0616\nEpoch 25/500\n39/39 [==============================] - 0s 5ms/step - loss: 125.5751 - mean_squared_error: 123.9911 - val_loss: 122.4950 - val_mean_squared_error: 120.9291\nEpoch 26/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.6776 - mean_squared_error: 124.1088 - val_loss: 121.9798 - val_mean_squared_error: 120.4139\nEpoch 27/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4750 - mean_squared_error: 123.9014 - val_loss: 120.0620 - val_mean_squared_error: 118.4912\nEpoch 28/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3132 - mean_squared_error: 123.7362 - val_loss: 121.0815 - val_mean_squared_error: 119.5063\nEpoch 29/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.1772 - mean_squared_error: 123.5967 - val_loss: 124.8994 - val_mean_squared_error: 123.3199\nEpoch 30/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5043 - mean_squared_error: 123.9276 - val_loss: 121.9700 - val_mean_squared_error: 120.3965\nEpoch 31/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2328 - mean_squared_error: 123.6593 - val_loss: 122.9948 - val_mean_squared_error: 121.4148\nEpoch 32/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5986 - mean_squared_error: 124.0224 - val_loss: 123.3993 - val_mean_squared_error: 121.8292\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-07 18:07:07,561]\u001b[0m Trial 0 finished with value: 118.44977569580078 and parameters: {'neurons': 4, 'l2_regularizer': 0.20271051439998727}. Best is trial 0 with value: 118.44977569580078.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n39/39 [==============================] - 0s 6ms/step - loss: 125.5492 - mean_squared_error: 123.9750 - val_loss: 122.6149 - val_mean_squared_error: 121.0402\nEpoch 2/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4825 - mean_squared_error: 123.8994 - val_loss: 121.4142 - val_mean_squared_error: 119.8399\nEpoch 3/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.7383 - mean_squared_error: 124.1588 - val_loss: 122.7195 - val_mean_squared_error: 121.1424\nEpoch 4/500\n39/39 [==============================] - 0s 3ms/step - loss: 125.4926 - mean_squared_error: 123.9142 - val_loss: 121.9770 - val_mean_squared_error: 120.4073\nEpoch 5/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3709 - mean_squared_error: 123.7895 - val_loss: 120.5168 - val_mean_squared_error: 118.9386\nEpoch 6/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3633 - mean_squared_error: 123.7877 - val_loss: 127.2702 - val_mean_squared_error: 125.6879\nEpoch 7/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3357 - mean_squared_error: 123.7575 - val_loss: 123.5689 - val_mean_squared_error: 121.9899\nEpoch 8/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3187 - mean_squared_error: 123.7322 - val_loss: 121.0365 - val_mean_squared_error: 119.4436\nEpoch 9/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.6559 - mean_squared_error: 124.0721 - val_loss: 120.7805 - val_mean_squared_error: 119.2025\nEpoch 10/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3488 - mean_squared_error: 123.7635 - val_loss: 124.1939 - val_mean_squared_error: 122.6078\nEpoch 11/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5302 - mean_squared_error: 123.9464 - val_loss: 123.5308 - val_mean_squared_error: 121.9550\nEpoch 12/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.7297 - mean_squared_error: 124.1544 - val_loss: 122.0652 - val_mean_squared_error: 120.5033\nEpoch 13/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.6178 - mean_squared_error: 124.0476 - val_loss: 121.8758 - val_mean_squared_error: 120.2995\nEpoch 14/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2578 - mean_squared_error: 123.6772 - val_loss: 122.9231 - val_mean_squared_error: 121.3422\nEpoch 15/500\n39/39 [==============================] - 0s 6ms/step - loss: 125.6732 - mean_squared_error: 124.0950 - val_loss: 122.5523 - val_mean_squared_error: 120.9777\nEpoch 16/500\n39/39 [==============================] - 0s 5ms/step - loss: 125.3366 - mean_squared_error: 123.7675 - val_loss: 120.3171 - val_mean_squared_error: 118.7454\nEpoch 17/500\n39/39 [==============================] - 0s 5ms/step - loss: 125.3526 - mean_squared_error: 123.7692 - val_loss: 123.9053 - val_mean_squared_error: 122.3319\nEpoch 18/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3663 - mean_squared_error: 123.7949 - val_loss: 123.8908 - val_mean_squared_error: 122.3168\nEpoch 19/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.6609 - mean_squared_error: 124.0856 - val_loss: 126.7052 - val_mean_squared_error: 125.1337\nEpoch 20/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3197 - mean_squared_error: 123.7484 - val_loss: 123.2162 - val_mean_squared_error: 121.6354\nEpoch 21/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2355 - mean_squared_error: 123.6518 - val_loss: 122.5019 - val_mean_squared_error: 120.9277\nEpoch 22/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4159 - mean_squared_error: 123.8424 - val_loss: 122.6173 - val_mean_squared_error: 121.0417\nEpoch 23/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.1726 - mean_squared_error: 123.5907 - val_loss: 122.0852 - val_mean_squared_error: 120.4971\nEpoch 24/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4440 - mean_squared_error: 123.8635 - val_loss: 124.1527 - val_mean_squared_error: 122.5721\nEpoch 25/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2396 - mean_squared_error: 123.6575 - val_loss: 121.6763 - val_mean_squared_error: 120.0858\nEpoch 26/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4051 - mean_squared_error: 123.8159 - val_loss: 123.9530 - val_mean_squared_error: 122.3712\nEpoch 27/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5215 - mean_squared_error: 123.9402 - val_loss: 124.1205 - val_mean_squared_error: 122.5369\nEpoch 28/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4478 - mean_squared_error: 123.8686 - val_loss: 122.5082 - val_mean_squared_error: 120.9315\nEpoch 29/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5242 - mean_squared_error: 123.9439 - val_loss: 121.6955 - val_mean_squared_error: 120.1184\nEpoch 30/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.9967 - mean_squared_error: 124.4278 - val_loss: 120.8595 - val_mean_squared_error: 119.2914\nEpoch 31/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3578 - mean_squared_error: 123.7788 - val_loss: 123.0544 - val_mean_squared_error: 121.4789\nEpoch 32/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3292 - mean_squared_error: 123.7540 - val_loss: 122.2439 - val_mean_squared_error: 120.6658\nEpoch 33/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3398 - mean_squared_error: 123.7605 - val_loss: 123.7734 - val_mean_squared_error: 122.1931\nEpoch 34/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.1968 - mean_squared_error: 123.6127 - val_loss: 121.6559 - val_mean_squared_error: 120.0715\nEpoch 35/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4701 - mean_squared_error: 123.8874 - val_loss: 125.3634 - val_mean_squared_error: 123.7873\nEpoch 36/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3610 - mean_squared_error: 123.7844 - val_loss: 122.6029 - val_mean_squared_error: 121.0236\nEpoch 37/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3314 - mean_squared_error: 123.7545 - val_loss: 122.5213 - val_mean_squared_error: 120.9434\nEpoch 38/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.8370 - mean_squared_error: 124.2585 - val_loss: 124.4039 - val_mean_squared_error: 122.8370\nEpoch 39/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3349 - mean_squared_error: 123.7531 - val_loss: 119.9702 - val_mean_squared_error: 118.3840\nEpoch 40/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2631 - mean_squared_error: 123.6811 - val_loss: 120.5001 - val_mean_squared_error: 118.9271\nEpoch 41/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2460 - mean_squared_error: 123.6644 - val_loss: 123.6416 - val_mean_squared_error: 122.0676\nEpoch 42/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.7276 - mean_squared_error: 124.1539 - val_loss: 122.2876 - val_mean_squared_error: 120.7140\nEpoch 43/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4605 - mean_squared_error: 123.8853 - val_loss: 120.8740 - val_mean_squared_error: 119.3038\nEpoch 44/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5517 - mean_squared_error: 123.9737 - val_loss: 122.4227 - val_mean_squared_error: 120.8287\nEpoch 45/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4929 - mean_squared_error: 123.9048 - val_loss: 122.6550 - val_mean_squared_error: 121.0759\nEpoch 46/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3987 - mean_squared_error: 123.8186 - val_loss: 124.0651 - val_mean_squared_error: 122.4939\nEpoch 47/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3258 - mean_squared_error: 123.7521 - val_loss: 121.7637 - val_mean_squared_error: 120.1856\nEpoch 48/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3336 - mean_squared_error: 123.7550 - val_loss: 122.6661 - val_mean_squared_error: 121.0882\nEpoch 49/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.1954 - mean_squared_error: 123.6154 - val_loss: 125.0574 - val_mean_squared_error: 123.4775\nEpoch 50/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2994 - mean_squared_error: 123.7210 - val_loss: 120.2894 - val_mean_squared_error: 118.7095\nEpoch 51/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3654 - mean_squared_error: 123.7825 - val_loss: 119.6209 - val_mean_squared_error: 118.0446\nEpoch 52/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4421 - mean_squared_error: 123.8683 - val_loss: 121.3903 - val_mean_squared_error: 119.8148\nEpoch 53/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3683 - mean_squared_error: 123.7879 - val_loss: 123.4449 - val_mean_squared_error: 121.8679\nEpoch 54/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5660 - mean_squared_error: 123.9858 - val_loss: 123.5515 - val_mean_squared_error: 121.9728\nEpoch 55/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.9043 - mean_squared_error: 124.3360 - val_loss: 121.5089 - val_mean_squared_error: 119.9327\nEpoch 56/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3992 - mean_squared_error: 123.8188 - val_loss: 125.4446 - val_mean_squared_error: 123.8714\nEpoch 57/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3960 - mean_squared_error: 123.8148 - val_loss: 122.9640 - val_mean_squared_error: 121.3881\nEpoch 58/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3692 - mean_squared_error: 123.7922 - val_loss: 122.2861 - val_mean_squared_error: 120.7101\nEpoch 59/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4759 - mean_squared_error: 123.8997 - val_loss: 120.4549 - val_mean_squared_error: 118.8843\nEpoch 60/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5222 - mean_squared_error: 123.9378 - val_loss: 122.3783 - val_mean_squared_error: 120.7980\nEpoch 61/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3189 - mean_squared_error: 123.7403 - val_loss: 121.5671 - val_mean_squared_error: 119.9860\nEpoch 62/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.1940 - mean_squared_error: 123.6100 - val_loss: 124.2674 - val_mean_squared_error: 122.6785\nEpoch 63/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2807 - mean_squared_error: 123.6966 - val_loss: 124.2341 - val_mean_squared_error: 122.6493\nEpoch 64/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4118 - mean_squared_error: 123.8261 - val_loss: 122.9966 - val_mean_squared_error: 121.4184\nEpoch 65/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4169 - mean_squared_error: 123.8361 - val_loss: 124.8563 - val_mean_squared_error: 123.2804\nEpoch 66/500\n39/39 [==============================] - 0s 4ms/step - loss: 126.3074 - mean_squared_error: 124.7378 - val_loss: 119.6329 - val_mean_squared_error: 118.0617\nEpoch 67/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.7204 - mean_squared_error: 124.1450 - val_loss: 123.7473 - val_mean_squared_error: 122.1707\nEpoch 68/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3350 - mean_squared_error: 123.7571 - val_loss: 122.9749 - val_mean_squared_error: 121.3963\nEpoch 69/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3457 - mean_squared_error: 123.7742 - val_loss: 125.0953 - val_mean_squared_error: 123.5177\nEpoch 70/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5429 - mean_squared_error: 123.9643 - val_loss: 126.1470 - val_mean_squared_error: 124.5756\nEpoch 71/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5087 - mean_squared_error: 123.9380 - val_loss: 121.7262 - val_mean_squared_error: 120.1469\nEpoch 72/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4537 - mean_squared_error: 123.8796 - val_loss: 128.1178 - val_mean_squared_error: 126.5380\nEpoch 73/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5851 - mean_squared_error: 124.0094 - val_loss: 122.6459 - val_mean_squared_error: 121.0725\nEpoch 74/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.8244 - mean_squared_error: 124.2538 - val_loss: 123.4776 - val_mean_squared_error: 121.9047\nEpoch 75/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5425 - mean_squared_error: 123.9694 - val_loss: 121.4940 - val_mean_squared_error: 119.9185\nEpoch 76/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4534 - mean_squared_error: 123.8785 - val_loss: 123.3877 - val_mean_squared_error: 121.8118\nEpoch 77/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4938 - mean_squared_error: 123.9201 - val_loss: 121.1588 - val_mean_squared_error: 119.5903\nEpoch 78/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4283 - mean_squared_error: 123.8496 - val_loss: 122.7801 - val_mean_squared_error: 121.1960\nEpoch 79/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2549 - mean_squared_error: 123.6707 - val_loss: 128.9588 - val_mean_squared_error: 127.3814\nEpoch 80/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5606 - mean_squared_error: 123.9828 - val_loss: 121.7616 - val_mean_squared_error: 120.1840\nEpoch 81/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3490 - mean_squared_error: 123.7661 - val_loss: 126.7490 - val_mean_squared_error: 125.1649\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-07 18:07:28,918]\u001b[0m Trial 1 finished with value: 119.5679931640625 and parameters: {'neurons': 10, 'l2_regularizer': 0.36294872349247087}. Best is trial 0 with value: 118.44977569580078.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n39/39 [==============================] - 0s 6ms/step - loss: 125.4902 - mean_squared_error: 123.9087 - val_loss: 119.9050 - val_mean_squared_error: 118.3328\nEpoch 2/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3421 - mean_squared_error: 123.7600 - val_loss: 122.4661 - val_mean_squared_error: 120.8813\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-07 18:07:29,659]\u001b[0m Trial 2 pruned. Trial was pruned at epoch 1.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n39/39 [==============================] - 0s 5ms/step - loss: 125.4849 - mean_squared_error: 123.9016 - val_loss: 122.4004 - val_mean_squared_error: 120.8251\nEpoch 2/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.6070 - mean_squared_error: 124.0337 - val_loss: 123.5386 - val_mean_squared_error: 121.9734\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-07 18:07:30,301]\u001b[0m Trial 3 pruned. Trial was pruned at epoch 1.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2489 - mean_squared_error: 123.6733 - val_loss: 123.2266 - val_mean_squared_error: 121.6472\nEpoch 2/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4007 - mean_squared_error: 123.8206 - val_loss: 122.5853 - val_mean_squared_error: 121.0089\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-07 18:07:31,224]\u001b[0m Trial 4 pruned. Trial was pruned at epoch 1.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n39/39 [==============================] - 1s 10ms/step - loss: 125.4491 - mean_squared_error: 123.8699 - val_loss: 124.4775 - val_mean_squared_error: 122.9016\nEpoch 2/500\n39/39 [==============================] - 0s 12ms/step - loss: 125.6923 - mean_squared_error: 124.1184 - val_loss: 123.5757 - val_mean_squared_error: 121.9946\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-07 18:07:32,740]\u001b[0m Trial 5 pruned. Trial was pruned at epoch 1.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n39/39 [==============================] - 0s 5ms/step - loss: 125.3544 - mean_squared_error: 123.7741 - val_loss: 122.0177 - val_mean_squared_error: 120.4396\nEpoch 2/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.1717 - mean_squared_error: 123.5903 - val_loss: 121.4472 - val_mean_squared_error: 119.8696\nEpoch 3/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5395 - mean_squared_error: 123.9680 - val_loss: 124.1937 - val_mean_squared_error: 122.6212\nEpoch 4/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.0489 - mean_squared_error: 123.4663 - val_loss: 125.0250 - val_mean_squared_error: 123.4431\nEpoch 5/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3011 - mean_squared_error: 123.7229 - val_loss: 124.0176 - val_mean_squared_error: 122.4315\nEpoch 6/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4444 - mean_squared_error: 123.8671 - val_loss: 124.3992 - val_mean_squared_error: 122.8230\nEpoch 7/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4817 - mean_squared_error: 123.9057 - val_loss: 119.9321 - val_mean_squared_error: 118.3526\nEpoch 8/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3735 - mean_squared_error: 123.7930 - val_loss: 123.5673 - val_mean_squared_error: 121.9908\nEpoch 9/500\n39/39 [==============================] - 0s 3ms/step - loss: 125.4649 - mean_squared_error: 123.8869 - val_loss: 120.9496 - val_mean_squared_error: 119.3744\nEpoch 10/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4285 - mean_squared_error: 123.8514 - val_loss: 123.4383 - val_mean_squared_error: 121.8596\nEpoch 11/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.1681 - mean_squared_error: 123.5835 - val_loss: 122.8846 - val_mean_squared_error: 121.2946\nEpoch 12/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3047 - mean_squared_error: 123.7192 - val_loss: 123.5460 - val_mean_squared_error: 121.9634\nEpoch 13/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4711 - mean_squared_error: 123.8894 - val_loss: 123.4002 - val_mean_squared_error: 121.8168\nEpoch 14/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5559 - mean_squared_error: 123.9747 - val_loss: 122.2252 - val_mean_squared_error: 120.6421\nEpoch 15/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.6275 - mean_squared_error: 124.0548 - val_loss: 121.6143 - val_mean_squared_error: 120.0396\nEpoch 16/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.0872 - mean_squared_error: 123.5007 - val_loss: 123.4256 - val_mean_squared_error: 121.8458\nEpoch 17/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3387 - mean_squared_error: 123.7575 - val_loss: 121.0566 - val_mean_squared_error: 119.4818\nEpoch 18/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.7876 - mean_squared_error: 124.2187 - val_loss: 125.2902 - val_mean_squared_error: 123.7170\nEpoch 19/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5714 - mean_squared_error: 123.9911 - val_loss: 122.4480 - val_mean_squared_error: 120.8793\nEpoch 20/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5379 - mean_squared_error: 123.9613 - val_loss: 123.9126 - val_mean_squared_error: 122.3223\nEpoch 21/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4186 - mean_squared_error: 123.8268 - val_loss: 121.5895 - val_mean_squared_error: 120.0005\nEpoch 22/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4351 - mean_squared_error: 123.8474 - val_loss: 122.3184 - val_mean_squared_error: 120.7390\nEpoch 23/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.8177 - mean_squared_error: 124.2387 - val_loss: 121.4985 - val_mean_squared_error: 119.9189\nEpoch 24/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.7157 - mean_squared_error: 124.1343 - val_loss: 122.8954 - val_mean_squared_error: 121.3119\nEpoch 25/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5392 - mean_squared_error: 123.9550 - val_loss: 120.8769 - val_mean_squared_error: 119.2925\nEpoch 26/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5138 - mean_squared_error: 123.9329 - val_loss: 123.9216 - val_mean_squared_error: 122.3421\nEpoch 27/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3476 - mean_squared_error: 123.7661 - val_loss: 122.7990 - val_mean_squared_error: 121.2088\nEpoch 28/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3498 - mean_squared_error: 123.7666 - val_loss: 124.3837 - val_mean_squared_error: 122.8039\nEpoch 29/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4015 - mean_squared_error: 123.8175 - val_loss: 120.0279 - val_mean_squared_error: 118.4478\nEpoch 30/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5231 - mean_squared_error: 123.9389 - val_loss: 123.9829 - val_mean_squared_error: 122.4101\nEpoch 31/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.3329 - mean_squared_error: 123.7537 - val_loss: 121.9206 - val_mean_squared_error: 120.3417\nEpoch 32/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5011 - mean_squared_error: 123.9236 - val_loss: 122.0597 - val_mean_squared_error: 120.4845\nEpoch 33/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5664 - mean_squared_error: 123.9913 - val_loss: 120.8445 - val_mean_squared_error: 119.2720\nEpoch 34/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.5529 - mean_squared_error: 123.9769 - val_loss: 120.2020 - val_mean_squared_error: 118.6268\nEpoch 35/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4185 - mean_squared_error: 123.8426 - val_loss: 121.1184 - val_mean_squared_error: 119.5535\nEpoch 36/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.1926 - mean_squared_error: 123.6209 - val_loss: 125.2746 - val_mean_squared_error: 123.6972\nEpoch 37/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4179 - mean_squared_error: 123.8391 - val_loss: 122.6986 - val_mean_squared_error: 121.1201\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-07 18:07:44,023]\u001b[0m Trial 6 finished with value: 116.81756591796875 and parameters: {'neurons': 12, 'l2_regularizer': 0.08217877798544859}. Best is trial 6 with value: 116.81756591796875.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n39/39 [==============================] - 0s 7ms/step - loss: 125.6376 - mean_squared_error: 124.0680 - val_loss: 121.0347 - val_mean_squared_error: 119.4623\nEpoch 2/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.4596 - mean_squared_error: 123.8836 - val_loss: 123.9579 - val_mean_squared_error: 122.3837\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-07 18:07:44,766]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 1.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n39/39 [==============================] - 0s 5ms/step - loss: 125.2811 - mean_squared_error: 123.7077 - val_loss: 127.1043 - val_mean_squared_error: 125.5221\nEpoch 2/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.6276 - mean_squared_error: 124.0551 - val_loss: 123.3421 - val_mean_squared_error: 121.7654\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-07 18:07:45,409]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 1.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/500\n39/39 [==============================] - 0s 5ms/step - loss: 125.1512 - mean_squared_error: 123.5714 - val_loss: 124.1072 - val_mean_squared_error: 122.5291\nEpoch 2/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2756 - mean_squared_error: 123.6911 - val_loss: 120.1671 - val_mean_squared_error: 118.5805\nEpoch 3/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.1646 - mean_squared_error: 123.5819 - val_loss: 120.0325 - val_mean_squared_error: 118.4559\nEpoch 4/500\n39/39 [==============================] - 0s 4ms/step - loss: 125.2839 - mean_squared_error: 123.6970 - val_loss: 126.4807 - val_mean_squared_error: 124.8960\n","output_type":"stream"},{"name":"stderr","text":"\u001b[32m[I 2022-09-07 18:07:46,593]\u001b[0m Trial 9 pruned. Trial was pruned at epoch 3.\u001b[0m\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/1518389537.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"minimize\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msampler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamplers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPESampler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpruners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHyperbandPruner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mpruned_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPRUNED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mcomplete_trials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOMPLETE\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'TrialState' is not defined"],"ename":"NameError","evalue":"name 'TrialState' is not defined","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# try optuna for NN:\n\ndef objective(trial):\n\n    n_layers = trial.suggest_int('n_layers', 1, 3)\n    model = tf.keras.Sequential()\n    for i in range(n_layers):\n        num_hidden = trial.suggest_int(f'n_units_l{i}', 4, 128, log=True)\n        model.add(tf.keras.layers.Dense(num_hidden, activation='relu'))\n    model.add(tf.keras.layers.Dense(1))\n    display(model.summary())\n    return accuracy\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=10)\n","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:19:35.581847Z","iopub.execute_input":"2022-09-07T17:19:35.582526Z","iopub.status.idle":"2022-09-07T17:19:36.000100Z","shell.execute_reply.started":"2022-09-07T17:19:35.582487Z","shell.execute_reply":"2022-09-07T17:19:35.998556Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2022-09-07 17:19:35,586]\u001b[0m A new study created in memory with name: no-name-4822100f-578e-42d5-a866-45c76aec136c\u001b[0m\n\u001b[33m[W 2022-09-07 17:19:35,597]\u001b[0m Trial 0 failed because of the following error: ValueError('This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.')\u001b[0m\nTraceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\", line 213, in _run_trial\n    value_or_values = func(trial)\n  File \"/tmp/ipykernel_18/643329396.py\", line 11, in objective\n    display(model.summary())\n  File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 2521, in summary\n    raise ValueError('This model has not yet been built. '\nValueError: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_18/643329396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mgc_after_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgc_after_trial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshow_progress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         )\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mreseed_sampler_rng\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mtime_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0mprogress_bar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogress_bar\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mtrial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTrialState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFAIL\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfunc_err\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_18/643329396.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   2519\u001b[0m     \"\"\"\n\u001b[1;32m   2520\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m       raise ValueError('This model has not yet been built. '\n\u001b[0m\u001b[1;32m   2522\u001b[0m                        \u001b[0;34m'Build the model first by calling `build()` or calling '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m                        \u001b[0;34m'`fit()` with some data, or specify '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build."],"ename":"ValueError","evalue":"This model has not yet been built. Build the model first by calling `build()` or calling `fit()` with some data, or specify an `input_shape` argument in the first layer(s) for automatic build.","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_snn.summary()","metadata":{"execution":{"iopub.status.busy":"2022-09-07T17:20:32.813369Z","iopub.execute_input":"2022-09-07T17:20:32.813781Z","iopub.status.idle":"2022-09-07T17:20:32.820571Z","shell.execute_reply.started":"2022-09-07T17:20:32.813743Z","shell.execute_reply":"2022-09-07T17:20:32.819242Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Model: \"sequential_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ndense_13 (Dense)             (None, 32)                2752      \n_________________________________________________________________\ndense_14 (Dense)             (None, 16)                528       \n_________________________________________________________________\ndense_15 (Dense)             (None, 8)                 136       \n_________________________________________________________________\ndense_16 (Dense)             (None, 4)                 36        \n_________________________________________________________________\ndense_17 (Dense)             (None, 1)                 5         \n=================================================================\nTotal params: 3,457\nTrainable params: 3,457\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# on nns:\n# - try classic regularizers (l1, l2 etc)\n# - try different architecture (not snnn)\n# classic architecture:\n# He initialization, elu activation, batch norm, l2 reg, adam.\n\n# - try exotic architecture, e.g., wide'n'deep\n# \n","metadata":{"execution":{"iopub.status.busy":"2022-09-07T15:05:46.575321Z","iopub.execute_input":"2022-09-07T15:05:46.576937Z","iopub.status.idle":"2022-09-07T15:05:46.583045Z","shell.execute_reply.started":"2022-09-07T15:05:46.576877Z","shell.execute_reply":"2022-09-07T15:05:46.581733Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# usually self-norm seems better: it overfits less and runs faster\n\n","metadata":{"execution":{"iopub.status.busy":"2022-09-07T15:06:34.958520Z","iopub.execute_input":"2022-09-07T15:06:34.959694Z","iopub.status.idle":"2022-09-07T15:06:34.964481Z","shell.execute_reply.started":"2022-09-07T15:06:34.959652Z","shell.execute_reply":"2022-09-07T15:06:34.963248Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
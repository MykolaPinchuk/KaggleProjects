{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This script is developed based on 'KProject_HousePrice_i5'","metadata":{}},{"cell_type":"markdown","source":"### Outline:\n0. Load libraries and custom functions.\n1. Load data.\n2. Preliminary data analysis: explore features and a target, delete unneeded features, create new features.\n3. Train-test split.\n4. Missing values. In some cases it may be useful to explore skew and perform log-transform before imputing missing values.\n5. Feature engineering. Transform skewed variables, do OHC and scaling.\n6. Fit models.\n7. Evaluate models.\n8. Feature importance, error analysis. Based on the results, go to 2. and iterate.\n9. Make predictions.","metadata":{}},{"cell_type":"code","source":"# 0. Load libraries #\n\nimport numpy as np\nimport pandas as pd\nimport os, time, warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, r2_score, mean_squared_error\nfrom sklearn.inspection import permutation_importance\nfrom scipy.special import inv_boxcox\nfrom xgboost import XGBClassifier, XGBRegressor\n\npd.set_option('display.max_columns', 20)\npd.set_option('mode.chained_assignment', None)\npd.set_option('display.expand_frame_repr', False)\nwarnings.filterwarnings('ignore')\n\ndef draw_histograms(df, variables, n_rows, n_cols):\n    # stolen from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n    fig.tight_layout()  \n    plt.show()\n\n\ndef fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode'):\n    \"\"\"This function speeds up filling missing values for 3 main datasets using different imputation methods.\n    Later may replace it with some subclass.\n    Example: fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\"\"\"\n    # set df_pred to None if it does not exist\n    if (cat_fill=='mode'):\n    \n        df_train[cat_features] = df_train[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n        df_test[cat_features] = df_test[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n        if (df_pred is not None):\n            df_pred[cat_features] = df_pred[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n            \n    if (cat_fill=='missing'):\n    \n        df_train[cat_features] = df_train[cat_features].fillna(value='missing')\n        df_test[cat_features] = df_test[cat_features].fillna(value='missing')\n        if (df_pred is not None):\n            df_pred[cat_features] = df_pred[cat_features].fillna(value='missing')\n        \n    if (num_fill=='median'):\n        df_train[num_features] = df_train[num_features].fillna(value=df_train[num_features].median())\n        df_test[num_features] = df_test[num_features].fillna(value=df_train[num_features].median())\n        if (df_pred is not None):\n            df_pred[num_features] = df_pred[num_features].fillna(value=df_train[num_features].median())    \n    \n    all_good = (\n    (np.prod(df_train[num_features+cat_features].shape)==df_train[num_features+cat_features].count().sum()) and \n    (np.prod(df_test[num_features+cat_features].shape) == df_test[num_features+cat_features].count().sum()) and \n    (np.prod(df_pred[num_features+cat_features].shape) == df_pred[num_features+cat_features].count().sum()))\n    if (all_good):\n        print('Missing values imputed successfully')\n    else:\n        print('There are still some missing values...')\n    \n    \n    \ndef add_misDummy_mp_i1(df_train, df_test, df_pred, features):\n    \"\"\"This function creates new dummy columns for missing features.\n    Example: add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\"\"\"\n    # set df_pred to None if it does not exist\n    for feature_name in features:\n        misColName = 'mis'+feature_name\n        df_train.loc[df_train[feature_name].isnull(), misColName]=1\n        df_train.loc[df_train[feature_name].notnull(), misColName]=0\n        df_test.loc[df_test[feature_name].isnull(), misColName]=1\n        df_test.loc[df_test[feature_name].notnull(), misColName]=0\n        if (df_pred is not None):\n            df_pred.loc[df_pred[feature_name].isnull(), misColName]=1\n            df_pred.loc[df_pred[feature_name].notnull(), misColName]=0\n   \n\ndef discretize_mp_i1(df_train, df_test, df_pred, feature, ntiles, delete_feature=False):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: discretize_mp_i1(X_train, X_test, X_pred, 'Age', 15)\"\"\"\n    # set df_pred to None if it does not exist\n    _,bin = pd.qcut(df_train[feature], ntiles, retbins = True, labels = False, duplicates = 'drop')\n    df_train[feature+'Ntile'] = pd.cut(df_train[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    df_test[feature+'Ntile'] = pd.cut(df_test[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (df_pred is not None):\n        df_pred[feature+'Ntile'] = pd.cut(df_pred[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (delete_feature==True):\n        df_train.drop(columns=[feature], inplace=True)\n        df_test.drop(columns=[feature], inplace=True)\n        df_pred.drop(columns=[feature], inplace=True)\n    print('Discretized ',feature, ' into ', len(bin)-1, ' bins')\n\n\ndef log_transformer_mp_i1(df_train, df_test, df_pred, feature_subset=False, min_skew=3):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: log_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\"\"\"\n    # set df_pred to None if it does not exist\n    if (feature_subset==False):\n        features_totransform = df_train.columns\n    else:\n        features_totransform = feature_subset.copy()\n    skewed_vars = list(df_train.skew()[abs(df_train.skew())>min_skew].index)\n    for col in list(set(skewed_vars)&set(features_totransform)):\n        df_train[col] = np.log1p(df_train[col])\n        df_test[col] = np.log1p(df_test[col])\n        if (df_pred is not None):\n            df_pred[col] = np.log1p(df_pred[col])\n    print('Skewed columns log-transformed: ', list(set(skewed_vars)&set(features_totransform)))\n    \n    \ndef add_dummyfeatures(df_train, df_test, df_pred, feature_dict):\n    \"\"\"This function adds dummy feature when some feature is equal to value, specified in a dictionary.\n    Example: add_dummyfeatures(X_train, X_test, X_pred, {'RoomService':0, 'Spa':0, 'VRDeck':0, 'ShoppingMall':0})\"\"\"\n    input_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n    for i in range(len(list(feature_dict.items()))):\n        feature,value = list(feature_dict.keys())[i], list(feature_dict.values())[i]\n        df_train.loc[df_train[feature]==value,(str(feature)+str(value))]=1\n        df_train.loc[df_train[feature]!=value,(str(feature)+str(value))]=0\n        df_test.loc[df_test[feature]==value,(str(feature)+str(value))]=1\n        df_test.loc[df_test[feature]!=value,(str(feature)+str(value))]=0\n        df_pred.loc[df_pred[feature]==value,(str(feature)+str(value))]=1\n        df_pred.loc[df_pred[feature]!=value,(str(feature)+str(value))]=0\n    output_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n    print(output_dimensions-input_dimensions, ' variables created') \n    ","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:38:11.774541Z","iopub.execute_input":"2022-10-27T22:38:11.774918Z","iopub.status.idle":"2022-10-27T22:38:11.810661Z","shell.execute_reply.started":"2022-10-27T22:38:11.774880Z","shell.execute_reply":"2022-10-27T22:38:11.809942Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# 1. Load data #\n\ntime0 = time.time()\npath = '../input/house-prices-advanced-regression-techniques/train.csv'\ndf = pd.read_csv(path) \ndf0 = df.copy()\n\npred=pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')\npred0 = pred.copy()\n\nprint(df.shape, pred.shape)\ndf\n\nirrelevant_features = pd.read_csv('../input/homeprice-features30/KP20_irrel_features_30.csv')\n\n# 2. pEDA #\n\ncols_tokeep = ['Id', 'SalePrice', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'ExterCond', \n               'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', 'HeatingQC', '1stFlrSF', '2ndFlrSF', 'GrLivArea',  \n               'KitchenQual', 'GarageArea', 'GarageCars', 'TotRmsAbvGrd', 'BedroomAbvGr', 'FullBath', \n               'HalfBath', 'MiscVal', 'LotFrontage', \n               'ExterQual', 'MSSubClass', 'MSZoning', 'LotShape', 'LandContour', 'LotConfig', 'Neighborhood',\n               'Condition1', 'BldgType', 'HouseStyle', 'RoofStyle', 'Exterior1st', 'Exterior2nd',\n               'Foundation', 'Heating', 'CentralAir', 'Electrical', 'Functional', 'PavedDrive',\n               'SaleType', 'SaleCondition', 'MasVnrType', 'MasVnrArea', 'BsmtQual', 'BsmtCond', \n               'BsmtExposure', 'BsmtFinType1', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\ndf = df[cols_tokeep]\nX_pred = pred[list(set(cols_tokeep) - set(['SalePrice']))]\n\n# preliminary feature engineering:\ndf['GrLivArea_log'] = np.log1p(df['GrLivArea'])\nX_pred['GrLivArea_log'] = np.log1p(X_pred['GrLivArea'])\n# w/o logtransform, scatterplot looks better. not sure whether log tranform helps.\n\n\n# 3. train-test split #\n\ntrain_y = df['SalePrice']\ntrain_x = df.drop(columns = ['SalePrice'])\n\nord_cols = ['ExterCond', 'HeatingQC', 'KitchenQual', 'ExterQual', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond']\n#num_cols = [col for col in train_x.columns if train_x[col].nunique() > 12]\nnum_cols = ['Id', 'LotArea', 'YearBuilt', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', \n            'GrLivArea', 'GarageArea', 'MiscVal', 'LotFrontage', 'MasVnrArea',\n           'TotRmsAbvGrd', 'GarageCars', 'BedroomAbvGr', 'OverallCond', 'OverallQual', 'GrLivArea_log']\ncat_cols = list(set(train_x.columns)-set(num_cols)-set(ord_cols))\n# for now, view ordinal features as categorical features\nprint(\"Numerical features \", num_cols, \"\\n\",\n      'Ordinal features', ord_cols, '\\n',\n      \"Categorical features \", cat_cols)\n\ntrain_x[ord_cols] = train_x[ord_cols].replace(['Po', 'Fa', 'TA', 'Gd', 'Ex'], [1,2,3,4,5])\nX_pred[ord_cols] = X_pred[ord_cols].replace(['Po', 'Fa', 'TA', 'Gd', 'Ex'], [1,2,3,4,5])\n\nX_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.01, random_state=2)\nprint(X_train.shape, X_test.shape, y_train.shape, X_pred.shape)\n\n# 4. Missing values #\n\nfillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols, num_fill = 'median', cat_fill='missing')\n\n# fill na for ordinal columns. missing values if those columns ususally mean that that feature DNE, so 0.\nX_train[ord_cols] = X_train[ord_cols].fillna(value=0)\nX_test[ord_cols] = X_test[ord_cols].fillna(value=0)\nX_pred[ord_cols] = X_pred[ord_cols].fillna(value=0)\n\n# 5. Feature engineering #\n\n# add dummy features\nadd_dummyfeatures(X_train, X_test, X_pred, {'OverallQual':1, 'OverallQual':8, 'OverallQual':9, 'OverallQual':10})\n\nlog_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\n\nfeature_transformer = ColumnTransformer([\n    (\"num\", StandardScaler(), num_cols+ord_cols),\n    (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\"), cat_cols),\n    ])\n\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nX_pred = pd.DataFrame(feature_transformer.transform(X_pred), columns=feature_transformer.get_feature_names_out())\n\n# there are many dummies... may wish to use pca here later.\n\nprint(X_train.shape, X_test.shape, y_train.shape, X_pred.shape)\n# another way to deal with redundant features is to delete those, which do not help in feature importance:\ncols = list(X_train.columns)\ncols_few = list(set(cols)-set(list(irrelevant_features.loc[irrelevant_features.freq>19, 'col'])))\ncols_veryfew = list(set(cols)-set(list(irrelevant_features.loc[irrelevant_features.freq>15, 'col'])))\ncols_veryveryfew = list(set(cols)-set(list(irrelevant_features.loc[irrelevant_features.freq>9, 'col'])))\nprint('Feature sets: ', len(cols), len(cols_few), len(cols_veryfew), len(cols_veryveryfew))\n# after running _v5 of this script for like 30 times, \n# I believe that the feature set of 56 features is the best due to decreasing overfitting.","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:38:11.812831Z","iopub.execute_input":"2022-10-27T22:38:11.813644Z","iopub.status.idle":"2022-10-27T22:38:12.037362Z","shell.execute_reply.started":"2022-10-27T22:38:11.813612Z","shell.execute_reply":"2022-10-27T22:38:12.036416Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(1460, 81) (1459, 80)\nNumerical features  ['Id', 'LotArea', 'YearBuilt', 'BsmtFinSF1', 'BsmtFinSF2', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'MiscVal', 'LotFrontage', 'MasVnrArea', 'TotRmsAbvGrd', 'GarageCars', 'BedroomAbvGr', 'OverallCond', 'OverallQual', 'GrLivArea_log'] \n Ordinal features ['ExterCond', 'HeatingQC', 'KitchenQual', 'ExterQual', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond'] \n Categorical features  ['SaleType', 'FullBath', 'MSSubClass', 'Heating', 'Electrical', 'LotShape', 'BsmtExposure', 'RoofStyle', 'Condition1', 'LandContour', 'BldgType', 'MasVnrType', 'GarageFinish', 'Foundation', 'PavedDrive', 'HouseStyle', 'HalfBath', 'Exterior1st', 'GarageType', 'MSZoning', 'Exterior2nd', 'Neighborhood', 'Functional', 'BsmtFinType1', 'SaleCondition', 'LotConfig', 'CentralAir']\n(1445, 54) (15, 54) (1445,) (1459, 54)\nMissing values imputed successfully\n[1 1 1]  variables created\nSkewed columns log-transformed:  ['LotArea', 'BsmtFinSF2', 'MiscVal']\n(1445, 224) (15, 224) (1445,) (1459, 224)\nFeature sets:  224 121 91 55\n","output_type":"stream"}]},{"cell_type":"code","source":"# 6. Model Fitting #\n\nprint(X_train.shape)\n\nlr = LinearRegression()\nlr.fit(X_train[cols_veryveryfew], y_train)\nprint('OLS ', mean_squared_error(y_train, lr.predict(X_train[cols_veryveryfew])))\n\ntime1 = time.time()\nsvr4 = SVR()\ngrid_param = {'C': [50000, 100000, 200000, 400000, 600000, 900000]}\nsvrm4 = GridSearchCV(svr4, grid_param, cv=8, scoring='neg_root_mean_squared_error')\nsvrm4.fit(X_train[cols_veryveryfew], y_train)\nprint('SVR 56 cols', \n      svrm4.best_params_, \n      svrm4.best_score_, \n      np.sqrt(mean_squared_error(y_train, svrm4.predict(X_train[cols_veryveryfew]))), \n      time.time()-time1)\n\nxgb4 = XGBRegressor()\ngrid_param = {'n_estimators':[200], \n              'max_depth':[2,3,4,5], \n              'eta':[0.04, 0.06, 0.08, 0.1],\n             'subsample':[0.7], \n              'colsample_bytree':[0.5]}\nxgbm4 = GridSearchCV(xgb4, grid_param, cv=8, scoring='neg_root_mean_squared_error')\nxgbm4.fit(X_train[cols_veryveryfew], y_train)\nprint('XGB 56 cols', \n      xgbm4.best_params_, \n      xgbm4.best_score_, \n      np.sqrt(mean_squared_error(y_train, xgbm4.predict(X_train[cols_veryveryfew]))), \n      time.time()-time1)\n\n# 7. Model Evaluation #\n\nprint('SVR 56', np.sqrt(mean_squared_error(y_test, svrm4.predict(X_test[cols_veryveryfew]))))\nprint('XGB 56', np.sqrt(mean_squared_error(y_test, xgbm4.predict(X_test[cols_veryveryfew]))))\n\n# sometimes ridge may fail really bad.\nprint('Total Time is ', time.time()-time0)\n\n# all 3 models perform best with the smallest features set (56 features)","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:44:04.294632Z","iopub.execute_input":"2022-10-27T22:44:04.294898Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"(1445, 224)\nOLS  940569570.3653979\nSVR 56 cols {'C': 400000} -24838.943023766722 9407.464303121382 22.546037673950195\n","output_type":"stream"}]},{"cell_type":"code","source":"# time1 = time.time()\n# xgb3 = XGBRegressor()\n# grid_param = {'n_estimators':[400,500], 'max_depth':[3,4,5], 'eta':[0.025, 0.035, 0.05, 0.06, 0.07, 0.08], 'subsample':[0.6],\n#              'colsample_bytree':[0.2]}\n# xgbm3 = GridSearchCV(xgb3, grid_param, scoring='neg_root_mean_squared_error', cv=4, verbose=1)\n# xgbm3.fit(X_train[cols_veryveryfew], y_train)\n# print('XGB 56 cols', xgbm3.best_params_, xgbm3.best_score_, np.sqrt(mean_squared_error(y_train, xgbm3.predict(X_train[cols_veryveryfew]))), time.time()-time1)\n# print('XGB 56', np.sqrt(mean_squared_error(y_test, xgbm3.predict(X_test[cols_veryveryfew]))))\n","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:39:44.305309Z","iopub.status.idle":"2022-10-27T22:39:44.306073Z","shell.execute_reply.started":"2022-10-27T22:39:44.305870Z","shell.execute_reply":"2022-10-27T22:39:44.305890Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('train lr 56', np.sqrt(mean_squared_error(y_train, lr.predict(X_train[cols_veryveryfew]))))\nprint('train SVR 56', np.sqrt(mean_squared_error(y_train, svrm4.predict(X_train[cols_veryveryfew]))))\nprint('test lr 56', np.sqrt(mean_squared_error(y_test, lr.predict(X_test[cols_veryveryfew]))))\nprint('test SVR 56', np.sqrt(mean_squared_error(y_test, svrm4.predict(X_test[cols_veryveryfew]))))","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:42:41.875572Z","iopub.execute_input":"2022-10-27T22:42:41.875880Z","iopub.status.idle":"2022-10-27T22:42:41.904100Z","shell.execute_reply.started":"2022-10-27T22:42:41.875848Z","shell.execute_reply":"2022-10-27T22:42:41.902806Z"},"trusted":true},"execution_count":8,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_17/3129255952.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train lr 56'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_veryveryfew\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train SVR 56'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvrm4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_veryveryfew\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test lr 56'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_veryveryfew\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test SVR 56'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvrm4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols_veryveryfew\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \"\"\"\n\u001b[0;32m--> 362\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0m_preprocess_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_preprocess_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/_base.py\u001b[0m in \u001b[0;36m_decision_function\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"csc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"coo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdense_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             raise ValueError(\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m                 \u001b[0;34mf\"is expecting {self.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m             )\n","\u001b[0;31mValueError\u001b[0m: X has 55 features, but LinearRegression is expecting 224 features as input."],"ename":"ValueError","evalue":"X has 55 features, but LinearRegression is expecting 224 features as input.","output_type":"error"}]},{"cell_type":"code","source":"# 8. Feature importance #\n\nresults = permutation_importance(xgbm, X_test, y_test, n_jobs=-1)\nfi = pd.DataFrame({'col':X_test.columns, 'FI':results.importances_mean})\nfi = fi.sort_values('FI', ascending = False)\nfi\n# OverallQual and GrLivArea ate the two most important features","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:39:44.307075Z","iopub.status.idle":"2022-10-27T22:39:44.307542Z","shell.execute_reply.started":"2022-10-27T22:39:44.307345Z","shell.execute_reply":"2022-10-27T22:39:44.307367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:39:44.308409Z","iopub.status.idle":"2022-10-27T22:39:44.309108Z","shell.execute_reply.started":"2022-10-27T22:39:44.308740Z","shell.execute_reply":"2022-10-27T22:39:44.308783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#submission_df_vc = pd.DataFrame({'Id': pred.Id, 'SalePrice': yhat}, columns=['Id', 'SalePrice'])\n#submission_df_svm = pd.DataFrame({'Id': pred.Id, 'SalePrice': svrm4.predict(X_pred[cols_veryveryfew])}, columns=['Id', 'SalePrice'])\nsubmission_df_bt = pd.DataFrame({'Id': pred.Id, 'SalePrice': xgbm3.predict(X_pred[cols_veryveryfew])}, columns=['Id', 'SalePrice'])\n\n#submission_df_vc.to_csv('KP11_vc.csv',index=False)\n#submission_df_svm.to_csv('KP20_svr.csv',index=False)\n#submission_df_rf.to_csv('KP11_rf.csv',index=False)\nsubmission_df_bt.to_csv('KP20_bt.csv',index=False)\n\nos.chdir(r'/kaggle/working')\n\nfrom IPython.display import FileLink\nFileLink(r'KP20_bt.csv')","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:39:44.310088Z","iopub.status.idle":"2022-10-27T22:39:44.310378Z","shell.execute_reply.started":"2022-10-27T22:39:44.310227Z","shell.execute_reply":"2022-10-27T22:39:44.310241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.barplot(data=df, x='OverallQual', y='SalePrice')","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:39:44.311067Z","iopub.status.idle":"2022-10-27T22:39:44.311313Z","shell.execute_reply.started":"2022-10-27T22:39:44.311180Z","shell.execute_reply":"2022-10-27T22:39:44.311194Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data=df, x='GrLivArea', y='SalePrice')\n# transformed","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:39:44.312165Z","iopub.status.idle":"2022-10-27T22:39:44.312409Z","shell.execute_reply.started":"2022-10-27T22:39:44.312278Z","shell.execute_reply":"2022-10-27T22:39:44.312291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(data=df, x='GrLivArea', y='SalePrice')\n# not transformed","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:39:44.312971Z","iopub.status.idle":"2022-10-27T22:39:44.313211Z","shell.execute_reply.started":"2022-10-27T22:39:44.313084Z","shell.execute_reply":"2022-10-27T22:39:44.313097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x[['OverallQual', 'GrLivArea']].skew()","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:39:44.314411Z","iopub.status.idle":"2022-10-27T22:39:44.314667Z","shell.execute_reply.started":"2022-10-27T22:39:44.314521Z","shell.execute_reply":"2022-10-27T22:39:44.314534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_x[['GrLivArea']].hist()","metadata":{"execution":{"iopub.status.busy":"2022-10-27T22:39:44.315315Z","iopub.status.idle":"2022-10-27T22:39:44.315556Z","shell.execute_reply.started":"2022-10-27T22:39:44.315428Z","shell.execute_reply":"2022-10-27T22:39:44.315442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Optuna: A hyperparameter optimization framework","metadata":{}},{"cell_type":"markdown","source":"### This is modified public notebook from Kaggle, showing how to use Optuna.\n### I have modified it to do cross-validation within Optuna.","metadata":{"execution":{"iopub.status.busy":"2022-06-04T19:50:29.687081Z","iopub.execute_input":"2022-06-04T19:50:29.687439Z","iopub.status.idle":"2022-06-04T19:50:29.691828Z","shell.execute_reply.started":"2022-06-04T19:50:29.687407Z","shell.execute_reply":"2022-06-04T19:50:29.690941Z"}}},{"cell_type":"markdown","source":"* [1.Basic Concepts](#chapter1)\n* [2. Let's build our optimization function using optuna](#chapter2)\n* [3. XGBoost using Optuna](#chapter3)\n* [5. Submission](#chapter5)","metadata":{}},{"cell_type":"markdown","source":"* <h4> In This Kernel I will use an amazing framework called <b>Optuna</b> to find the best hyparameters of our XGBoost and CatBoost </h4>","metadata":{}},{"cell_type":"markdown","source":"**So, Optuna is an automatic hyperparameter optimization software framework, particularly designed for machine learning. It features an imperative, define-by-run style user API.<br> The code written with Optuna enjoys high modularity, and the user of Optuna can dynamically construct the search spaces for the hyperparameters.** \n* To learn more about Optuna check this [link](https://optuna.org/)","metadata":{}},{"cell_type":"markdown","source":"MP: Good fast introduction to Optuna: https://towardsdatascience.com/why-is-everyone-at-kaggle-obsessed-with-optuna-for-hyperparameter-tuning-7608fdca337c","metadata":{}},{"cell_type":"markdown","source":"# 1. Basic Concepts <a class=\"anchor\" id=\"chapter1\"></a>\nSo, We use the terms study and trial as follows:\n* <b>Study</b> : optimization based on an objective function\n* <b>Trial</b> : a single execution of the objective function","metadata":{}},{"cell_type":"code","source":"#import optuna \nimport optuna\n\nfrom xgboost import XGBRegressor\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold, GridSearchCV, KFold, RepeatedKFold\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport time, warnings\n#from optuna.visualization.matplotlib import plot_param_importances\n\nwarnings.filterwarnings('ignore')\n\ntrain = pd.read_csv('../input/tabular-playground-series-jan-2021/train.csv')\ntest  = pd.read_csv('../input/tabular-playground-series-jan-2021/test.csv')\nsub = pd.read_csv('../input/tabular-playground-series-jan-2021/sample_submission.csv')\n\ntrain.head()\n\nprint(train.shape)\ncolumns = [col for col in train.columns.to_list() if col not in ['id','target']]\n\ndata=train[columns]\ntarget=train['target']\n\nX_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.15,random_state=5)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:48.983728Z","iopub.execute_input":"2022-06-04T21:11:48.984111Z","iopub.status.idle":"2022-06-04T21:11:50.284755Z","shell.execute_reply.started":"2022-06-04T21:11:48.984079Z","shell.execute_reply":"2022-06-04T21:11:50.283427Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"(300000, 16)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# 2. Let's build our optimization function using optuna <a class=\"anchor\" id=\"chapter2\"></a>","metadata":{}},{"cell_type":"markdown","source":"### The following optimization function uses XGBoostRegressor model, so it takes the following arguments:\n* the data\n* the target\n* trial (How many executions we will do)  \n### and returns\n* RMSE (Root Mean Squared Rrror)","metadata":{}},{"cell_type":"markdown","source":"## Notes:\n* Note that I used some XGBoostRegressor hyperparameters from Xgboost official site. \n* So if you like to add more parameters or change them, check this [link](https://xgboost.readthedocs.io/en/latest/parameter.html) \n* Also I used early_stopping_rounds to avoid overfiting\n* to speedup the training process we can use the GPU or you can comment the first param argument (the training process will takes a lot of time by only using the cpu ðŸ˜©) ","metadata":{}},{"cell_type":"markdown","source":"# 3. XGBoost using Optuna <a class=\"anchor\" id=\"chapter3\"></a>","metadata":{}},{"cell_type":"code","source":"def evaluate_model_rkf(model, X_df, y_df, n_splits=4, random_state=3):\n    X_values = X_df.values\n    y_values = y_df.values\n    rkf = KFold(n_splits=n_splits, random_state=random_state)\n    y_pred = np.zeros_like(y_values)\n    for train_index, test_index in rkf.split(X_values):\n        X_t, X_v = X_values[train_index, :], X_values[test_index, :]\n        y_t = y_values[train_index]\n        model.fit(\n            X_t, y_t,\n        )\n        y_pred[test_index] += model.predict(X_v)\n    y_pred\n    return np.sqrt(mean_squared_error(y_train, y_pred))\n\n\n# First, try raw XGBoost to make sure that evaluate_model_rkf works:\n\nmodel = XGBRegressor(tree_method = 'gpu_hist', gpu_id=0, max_depth=8, eta=0.03, n_estimators=200)\nevaluate_model_rkf(model, X_train, y_train, n_splits=4, random_state=2)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:50.286798Z","iopub.execute_input":"2022-06-04T21:11:50.287083Z","iopub.status.idle":"2022-06-04T21:11:54.522406Z","shell.execute_reply.started":"2022-06-04T21:11:50.287054Z","shell.execute_reply":"2022-06-04T21:11:54.519142Z"},"trusted":true},"execution_count":16,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-85bb465f612c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree_method\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'gpu_hist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.03\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mevaluate_model_rkf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-85bb465f612c>\u001b[0m in \u001b[0;36mevaluate_model_rkf\u001b[0;34m(model, X_df, y_df, n_splits, random_state)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0my_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_values\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         model.fit(\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mX_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         )\n\u001b[1;32m     12\u001b[0m         \u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[1;32m    546\u001b[0m                               \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m                               callbacks=callbacks)\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevals_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m   1159\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[1;32m   1160\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m   1162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"def objective(trial, random_state=1, n_splits=4, n_jobs=-1, early_stopping_rounds=50):\n    params = {\n        \"tree_method\": 'gpu_hist',\n        \"gpu_id\": 0,\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"objective\": \"reg:squarederror\",\n        \"n_estimators\": 300,\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 15),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.01, 0.09),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.2, 1),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 1),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.01, 10.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n        \"gamma\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n        \"min_child_weight\": trial.suggest_uniform(\"min_child_weight\", 10, 1000),\n        \"seed\": random_state,\n        \"n_jobs\": n_jobs,\n    }\n\n    X = X_train\n    y = y_train\n    \n    model = XGBRegressor(**params)\n    rkf = KFold(n_splits=n_splits, random_state=random_state)\n    X_values = X.values\n    y_values = y.values\n    y_pred = np.zeros_like(y_values)\n    for train_index, test_index in rkf.split(X_values):\n        X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n        y_A, y_B = y_values[train_index], y_values[test_index]\n        model.fit(X_A, y_A, eval_set=[(X_B, y_B)],\n            eval_metric=\"rmse\", early_stopping_rounds=early_stopping_rounds, verbose = False)\n        y_pred[test_index] += model.predict(X_B)\n    return (mean_squared_error(y_train, y_pred, squared=False))\n\n\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=25)\nprint('Total time ', time.time()-time1)\n\n# display params\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.523492Z","iopub.status.idle":"2022-06-04T21:11:54.523994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compare timing with GridSearchCV on XGBoost:\n\ntime1 = time.time()\nxgbb = XGBRegressor(tree_method='gpu_hist', gpu_id=0)\nparam_grid = {'n_estimators':[300], 'eta':[0.02, 0.04, 0.06], 'max_depth':[4,7,10,12,15]}\nxgbm = GridSearchCV(xgbb, param_grid, cv=4, scoring='neg_root_mean_squared_error')\nxgbm.fit(X_train, y_train)\nprint('XGB ', xgbm.best_params_, xgbm.best_score_, time.time()-time1)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.525337Z","iopub.status.idle":"2022-06-04T21:11:54.527394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_hyperpars['gpu_id']=0\noptuna_hyperpars['n_estimators']=400\n#optuna_hyperpars\noptuna_xgb = XGBRegressor(**optuna_hyperpars)\noptuna_xgb.fit(X_train, y_train)\n\nprint('Optuna model:', mean_squared_error(optuna_xgb.predict(X_test), y_test, squared = False))\nprint('GS model:', mean_squared_error(xgbm.predict(X_test), y_test, squared = False))","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.529050Z","iopub.status.idle":"2022-06-04T21:11:54.530059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"study.trials_dataframe()","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.531690Z","iopub.status.idle":"2022-06-04T21:11:54.532698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Let's do some Quick Visualization for Hyperparameter Optimization Analysis\n#### Optuna provides various visualization features in optuna.visualization to analyze optimization results visually","metadata":{}},{"cell_type":"code","source":"#plot_optimization_histor: shows the scores from all trials as well as the best score so far at each point.\noptuna.visualization.plot_optimization_history(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.534176Z","iopub.status.idle":"2022-06-04T21:11:54.535304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_parallel_coordinate: interactively visualizes the hyperparameters and scores\noptuna.visualization.plot_parallel_coordinate(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.537212Z","iopub.status.idle":"2022-06-04T21:11:54.538256Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''plot_slice: shows the evolution of the search. You can see where in the hyperparameter space your search\nwent and which parts of the space were explored more.'''\noptuna.visualization.plot_slice(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.539867Z","iopub.status.idle":"2022-06-04T21:11:54.541004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#plot_contour: plots parameter interactions on an interactive chart. You can choose which hyperparameters you would like to explore.\noptuna.visualization.plot_contour(study, params=['alpha',\n                            #'max_depth',\n                            'lambda',\n                            'subsample',\n                            'learning_rate',\n                            'subsample'])","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.542844Z","iopub.status.idle":"2022-06-04T21:11:54.543888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.545657Z","iopub.status.idle":"2022-06-04T21:11:54.546710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize empirical distribution function\noptuna.visualization.plot_edf(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.548271Z","iopub.status.idle":"2022-06-04T21:11:54.549337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Optuna with cross-validation code was developed using this: https://aetperf.github.io/2021/02/16/Optuna-+-XGBoost-on-a-tabular-dataset.html","metadata":{}},{"cell_type":"markdown","source":"# Let's create an XGBoostRegressor model with the best hyperparameters","metadata":{}},{"cell_type":"code","source":"#Best_trial = study.best_trial.params\n#Best_trial[\"n_estimators\"], Best_trial[\"tree_method\"] = 10000, 'gpu_hist'\n#Best_trial","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.550938Z","iopub.status.idle":"2022-06-04T21:11:54.551956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#preds = np.zeros(test.shape[0])\n#kf = KFold(n_splits=5,random_state=48,shuffle=True)\n#rmse=[]  # list contains rmse for each fold\n#n=0\n#for trn_idx, test_idx in kf.split(train[columns],train['target']):\n#    X_tr,X_val=train[columns].iloc[trn_idx],train[columns].iloc[test_idx]\n#    y_tr,y_val=train['target'].iloc[trn_idx],train['target'].iloc[test_idx]\n#    model = xgb.XGBRegressor(**Best_trial)\n#    model.fit(X_tr,y_tr,eval_set=[(X_val,y_val)],early_stopping_rounds=100,verbose=False)\n#    preds+=model.predict(test[columns])/kf.n_splits\n#    rmse.append(mean_squared_error(y_val, model.predict(X_val), squared=False))\n#    print(f\"fold: {n+1} ==> rmse: {rmse[n]}\")\n#    n+=1","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.553695Z","iopub.status.idle":"2022-06-04T21:11:54.554815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#np.mean(rmse)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.556388Z","iopub.status.idle":"2022-06-04T21:11:54.557468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 5. Submission <a class=\"anchor\" id=\"chapter5\"></a>","metadata":{}},{"cell_type":"code","source":"#sub['target']=preds\n#sub.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-06-04T21:11:54.559044Z","iopub.status.idle":"2022-06-04T21:11:54.560096Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Outline:\n\n0. Load libraries and custom functions.\n1. Load data.\n2. Preliminary data analysis: explore features and a target, delete unneeded features, create new features.\n3. Train-test split.\n4. Missing values. In some cases it may be useful to explore skew and perform log-transform before imputing missing values.\n5. Feature engineering. Transform skewed variables, do OHC and scaling.\n6. Fit models.\n7. Evaluate models.\n8. Feature importance, error analysis. Based on the results, go to 2. and iterate.\n9. Make predictions.","metadata":{}},{"cell_type":"markdown","source":"To do: \n- try fitting separate XGBoost for each feature.\n\n","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os, time, warnings, random\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, train_test_split, RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\n\npd.set_option('display.max_columns', 20)\npd.set_option('mode.chained_assignment', None)\npd.set_option('display.expand_frame_repr', False)\nwarnings.filterwarnings('ignore')\n\ndef draw_histograms(df, variables, n_rows, n_cols):\n    # stolen from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n    fig.tight_layout()  \n    plt.show()\n\n\ndef fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode'):\n    \"\"\"This function speeds up filling missing values for 3 main datasets using different imputation methods.\n    Later may replace it with some subclass.\n    Example: fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\"\"\"\n    # set df_pred to None if it does not exist\n    if not ((cat_fill=='mode') and (num_fill=='median')):\n        print ('Imputation method not Implemented yet!')\n        return None\n    \n    df_train[num_features] = df_train[num_features].fillna(value=df_train[num_features].median())\n    df_test[num_features] = df_test[num_features].fillna(value=df_train[num_features].median())\n    df_train[cat_features] = df_train[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n    df_test[cat_features] = df_test[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n    if (df_pred is not None):\n        df_pred[num_features] = df_pred[num_features].fillna(value=df_train[num_features].median())\n        df_pred[cat_features] = df_pred[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n    df_train[num_features+cat_features].count\n    \n    all_good = (\n    (np.prod(df_train[num_features+cat_features].shape)==df_train[num_features+cat_features].count().sum()) and \n    (np.prod(df_test[num_features+cat_features].shape) == df_test[num_features+cat_features].count().sum()) and \n    (np.prod(df_pred[num_features+cat_features].shape) == df_pred[num_features+cat_features].count().sum()))\n    if (all_good):\n        print('Missing values imputed successfully')\n    else:\n        print('There are still some missing values...')\n    \ndef add_misDummy_mp_i1(df_train, df_test, df_pred, features):\n    \"\"\"This function creates new dummy columns for missing features.\n    Example: add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\"\"\"\n    # set df_pred to None if it does not exist\n    for feature_name in features:\n        misColName = 'mis'+feature_name\n        df_train.loc[df_train[feature_name].isnull(), misColName]=1\n        df_train.loc[df_train[feature_name].notnull(), misColName]=0\n        df_test.loc[df_test[feature_name].isnull(), misColName]=1\n        df_test.loc[df_test[feature_name].notnull(), misColName]=0\n        if (df_pred is not None):\n            df_pred.loc[df_pred[feature_name].isnull(), misColName]=1\n            df_pred.loc[df_pred[feature_name].notnull(), misColName]=0\n   \n\ndef discretize_mp_i1(df_train, df_test, df_pred, feature, ntiles, delete_feature=False):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: discretize_mp_i1(X_train, X_test, X_pred, 'Age', 15)\"\"\"\n    # set df_pred to None if it does not exist\n    _,bin = pd.qcut(df_train[feature], ntiles, retbins = True, labels = False, duplicates = 'drop')\n    df_train[feature+'Ntile'] = pd.cut(df_train[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    df_test[feature+'Ntile'] = pd.cut(df_test[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (df_pred is not None):\n        df_pred[feature+'Ntile'] = pd.cut(df_pred[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (delete_feature==True):\n        df_train.drop(columns=[feature], inplace=True)\n        df_test.drop(columns=[feature], inplace=True)\n        df_pred.drop(columns=[feature], inplace=True)\n    print('Discretized ',feature, ' into ', len(bin)-1, ' bins')\n\n\ndef log_transformer_mp_i1(df_train, df_test, df_pred, feature_subset=False, min_skew=3):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: log_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\"\"\"\n    # set df_pred to None if it does not exist\n    if (feature_subset==False):\n        features_totransform = df_train.columns\n    else:\n        features_totransform = feature_subset.copy()\n    skewed_vars = list(df_train.skew()[abs(df_train.skew())>min_skew].index)\n    for col in list(set(skewed_vars)&set(features_totransform)):\n        df_train[col] = np.log1p(df_train[col])\n        df_test[col] = np.log1p(df_test[col])\n        if (df_pred is not None):\n            df_pred[col] = np.log1p(df_pred[col])\n    print('Skewed columns log-transformed: ', list(set(skewed_vars)&set(features_totransform)))\n    \n    \ndef add_dummyfeatures(df_train, df_test, df_pred, feature_dict):\n    \"\"\"This function adds dummy feature when some feature is equal to value, specified in a dictionary.\n    Example: add_dummyfeatures(X_train, X_test, X_pred, {'RoomService':0, 'Spa':0, 'VRDeck':0, 'ShoppingMall':0})\"\"\"\n    input_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n    for i in range(len(list(feature_dict.items()))):\n        feature,value = list(feature_dict.keys())[i], list(feature_dict.values())[i]\n        df_train.loc[df_train[feature]==value,(str(feature)+str(value))]=1\n        df_train.loc[df_train[feature]!=value,(str(feature)+str(value))]=0\n        df_test.loc[df_test[feature]==value,(str(feature)+str(value))]=1\n        df_test.loc[df_test[feature]!=value,(str(feature)+str(value))]=0\n        df_pred.loc[df_pred[feature]==value,(str(feature)+str(value))]=1\n        df_pred.loc[df_pred[feature]!=value,(str(feature)+str(value))]=0\n    output_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n    print(output_dimensions-input_dimensions, ' variables created') \n    \n\n# 1. Import data #\n\ntime0 = time.time()\n\npath = '../input/santander-customer-transaction-prediction/train.csv'\ndf = pd.read_csv(path) \ndf0 = df.copy()\n#df = df.sample(50000)\n\n#df.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],inplace=True)\npred=pd.read_csv('../input/santander-customer-transaction-prediction/test.csv')\npred0 = pred.copy()\n#pred.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],inplace=True)\n\nprint(df.shape, pred.shape)\nprint(df.target.mean())\n# unbalanced responsed variable\ndf.head()\n\n# 2. EDA #\n\n# with all features unnamed normally-distributed features, there is not much EDA and feature engineering to do.\ndf.drop(columns = ['ID_code'], inplace= True)\n\n# 3. Train-test split #\n\ntrain_y = df[['target']]\ntrain_x = df.drop(columns = ['target'])\nX_pred = pred.copy()\nX_pred.drop(columns = ['ID_code'], inplace= True)\n#train_x = train_x[features_i1]\n#X_pred = X_pred[features_i1]\n\nX_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.02, random_state=10)\nprint(X_train.shape, X_test.shape, y_train.shape, X_pred.shape)\n\nX_train.count().sum() == np.prod(X_train.shape)\n# no missing values, good.\n\n# 5. feature engineering #\n\nss = StandardScaler()\n\nfor col in X_train.columns:\n    X_train[[col]] = ss.fit_transform(X_train[[col]])\n    X_test[[col]] = ss.transform(X_test[[col]])\n    X_pred[[col]] = ss.transform(X_pred[[col]])\n\n#X_test.iloc[:,:30].describe()\nrandom.seed(1)\nfewfeatures = random.sample(list(X_train.columns),5)\n\n# 5.1 try PCA \n\n#pca = PCA(n_components=100)\n#X_train = pca.fit_transform(X_train)\n#X_test = pca.transform(X_test)\n\n\n# 6. Fit models #\n\ntime1 = time.time()\nlr = LogisticRegression()\nparam_grid = {'C':[0.0001, 0.0003, 0.001, 0.003, 0.01, 0.03, 0.1]}\nlrm = GridSearchCV(lr, param_grid, cv=4, scoring='roc_auc')\nlrm.fit(X_train, y_train)\n#print('Logistic f1 ', lrm.best_params_, lrm.best_score_, f1_score(y_train, lrm.predict(X_train)), time.time()-time1)\nprint('Logistic roc_auc ', lrm.best_params_, lrm.best_score_, \n      roc_auc_score(y_train, lrm.predict_proba(X_train)[:,1]), time.time()-time1)\n\n\n#time1 = time.time()\n#knn = KNeighborsClassifier(n_jobs=-1)\n#param_grid = dict(n_neighbors=range(10, 41, 10))\n#knnm = GridSearchCV(knn, param_grid, cv=2)\n#knnm.fit(X_train[fewfeatures], y_train)\n#print('KNN ', knnm.best_params_, f1_score(y_train, knnm.predict(X_train[fewfeatures])), time.time()-time1)\n\n#time1 = time.time()\n#rf = RandomForestClassifier(n_jobs=-1)\n#param_grid = {'n_estimators':[100], 'max_depth':[4,5,6], 'max_features':[10]}\n#rfm = GridSearchCV(rf, param_grid, cv=2, scoring = 'f1')\n#rfm.fit(X_train, y_train)\n#print('RF ', rfm.best_params_, rfm.best_score_, f1_score(y_train, rfm.predict(X_train)), time.time()-time1)\n\ntime1 = time.time()\nparam_grid_nb = {\n    'var_smoothing': np.logspace(-3,-7, num=17)\n}\nnb = GaussianNB()\nnbm = GridSearchCV(nb, param_grid_nb, cv=4, scoring='roc_auc')\nnbm.fit(X_train, y_train)\nprint('NB ', nbm.best_params_, nbm.best_score_, \n      roc_auc_score(y_train, nbm.predict_proba(X_train)[:,1]), time.time()-time1)\n\n\ntime1 = time.time()\nxgb = XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1)\nparam_grid = {'n_estimators':[8000], 'max_depth':[3], 'eta':[0.02],\n'subsample':[0.5],'colsample_bytree':[0.005]}\nxgbm = GridSearchCV(xgb, param_grid, cv=4, scoring='roc_auc')\nxgbm.fit(X_train, y_train)\nprint('XGB ', xgbm.best_params_, xgbm.best_score_, \n      roc_auc_score(y_train, xgbm.predict_proba(X_train)[:,1]), time.time()-time1)\n\n# 7. accuracy #\n\nprint('Out of Sample:')\nprint('Logistic ', roc_auc_score(y_test, lrm.predict_proba(X_test)[:,1]))\n#print('SVM ', accuracy_score(y_test, svmm.predict(X_test)))\n#print('KNN ', accuracy_score(y_test, knnm.predict(X_test[fewfeatures])))\nprint('Bayes ', roc_auc_score(y_test, nbm.predict_proba(X_test)[:,1]))\n#print('RF ', f1_score(y_test, rfm.predict(X_test)))\nprint('XGB ', roc_auc_score(y_test, xgbm.predict_proba(X_test)[:,1]))\n\n# VotingClassifier:\n\nestimator1 = []\nestimator1.append(('LR', LogisticRegression(C=0.001)))\nestimator1.append(('NB1', GaussianNB(var_smoothing = 1e-6)))\n#estimator1.append(('NB2', GaussianNB(var_smoothing = 1e-7)))\n#estimator.append(('RF', RandomForestClassifier(max_depth=4, max_features=10, n_estimators=100)))\nestimator1.append(('XGB', XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1,\n                                       eta=0.02, max_depth=3, n_estimators=8000, \n                                       subsample=0.5, colsample_bytree=0.005)))\nvot_soft = VotingClassifier(estimators = estimator1, voting ='soft')\nvot_soft.fit(X_train, y_train)\nvot_hard = VotingClassifier(estimators = estimator1, voting ='hard')\nvot_hard.fit(X_train, y_train)\nprint('VotingClassifiers3 in sample', roc_auc_score(y_train, vot_soft.predict_proba(X_train)[:,1]), f1_score(y_train, vot_hard.predict(X_train)))\nprint('VotingClassifiers3 out of sample', roc_auc_score(y_test, vot_soft.predict_proba(X_test)[:,1]), f1_score(y_test, vot_hard.predict(X_test)))\n\nestimator3 = []\n#estimator3.append(('LR', LogisticRegression(C=0.001)))\nestimator3.append(('NB1', GaussianNB(var_smoothing = 1e-6)))\n#estimator1.append(('NB2', GaussianNB(var_smoothing = 1e-7)))\n#estimator.append(('RF', RandomForestClassifier(max_depth=4, max_features=10, n_estimators=100)))\nestimator3.append(('XGB', XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1,\n                                       eta=0.02, max_depth=3, n_estimators=8000, \n                                       subsample=0.5, colsample_bytree=0.005)))\nvot_soft = VotingClassifier(estimators = estimator3, voting ='soft')\nvot_soft.fit(X_train, y_train)\nvot_hard = VotingClassifier(estimators = estimator3, voting ='hard')\nvot_hard.fit(X_train, y_train)\nprint('VotingClassifiers3 in sample', roc_auc_score(y_train, vot_soft.predict_proba(X_train)[:,1]), f1_score(y_train, vot_hard.predict(X_train)))\nprint('VotingClassifiers3 out of sample', roc_auc_score(y_test, vot_soft.predict_proba(X_test)[:,1]), f1_score(y_test, vot_hard.predict(X_test)))\nprint('Total time ', time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T22:16:50.209474Z","iopub.execute_input":"2022-05-25T22:16:50.210017Z","iopub.status.idle":"2022-05-25T22:24:53.077386Z","shell.execute_reply.started":"2022-05-25T22:16:50.209970Z","shell.execute_reply":"2022-05-25T22:24:53.076667Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(200000, 202) (200000, 201)\n0.10049\n(196000, 200) (4000, 200) (196000, 1) (200000, 200)\nLogistic roc_auc  {'C': 0.0001} 0.8592985828598789 0.8608310903421739 25.896098375320435\nNB  {'var_smoothing': 3.162277660168379e-07} 0.8880007953994473 0.8896255974500569 49.0909423828125\nXGB  {'colsample_bytree': 0.005, 'eta': 0.02, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.5} 0.8975434400723943 0.918000991580137 181.88582372665405\nOut of Sample:\nLogistic  0.8790826625674636\nBayes  0.9059843858656327\nXGB  0.9141771610204088\nVotingClassifiers3 in sample 0.8993361964337245 0.4099182273806383\nVotingClassifiers3 out of sample 0.9089518033959194 0.43333333333333335\nVotingClassifiers3 in sample 0.9073576197427422 0.3162098537077518\nVotingClassifiers3 out of sample 0.9136872837431129 0.2929936305732484\nTotal time  482.79882979393005\n","output_type":"stream"}]},{"cell_type":"code","source":"xgb = XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1,\n                   n_estimators=200, max_depth=2, eta=0.3, subsample = 0.4, colsample_bytree=0.3)\nxgbm.fit(X_train, y_train)\nprint('XGB ', roc_auc_score(y_test, xgbm.predict(X_test)))\n\n# VotingClassifier:\n\nestimator1 = []\nestimator1.append(('LR', LogisticRegression(C=0.002)))\nestimator1.append(('NB1', GaussianNB(var_smoothing = 1e-6)))\n#estimator1.append(('NB2', GaussianNB(var_smoothing = 1e-7)))\n#estimator.append(('RF', RandomForestClassifier(max_depth=4, max_features=10, n_estimators=100)))\nestimator1.append(('XGB', XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1,\n                                       eta=0.3, max_depth=2, n_estimators=200, \n                                       subsample=0.4, colsample_bytree=0.3)))\nvot_soft = VotingClassifier(estimators = estimator1, voting ='soft')\nvot_soft.fit(X_train, y_train)\nvot_hard = VotingClassifier(estimators = estimator1, voting ='hard')\nvot_hard.fit(X_train, y_train)\nprint('VotingClassifiers3 in sample', roc_auc_score(y_train, vot_soft.predict_proba(X_train)[:,1]), f1_score(y_train, vot_hard.predict(X_train)))\nprint('VotingClassifiers3 out of sample', roc_auc_score(y_test, vot_soft.predict_proba(X_test)[:,1]), f1_score(y_test, vot_hard.predict(X_test)))\n\nestimator2 = []\nestimator2.append(('LR', LogisticRegression(C=0.002)))\nestimator2.append(('NB1', GaussianNB(var_smoothing = 1e-5)))\nestimator2.append(('NB2', GaussianNB(var_smoothing = 1e-8)))\n#estimator.append(('RF', RandomForestClassifier(max_depth=4, max_features=10, n_estimators=100)))\nestimator2.append(('XGB', XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1,\n                                       eta=0.3, max_depth=2, n_estimators=200, \n                                       subsample=0.4, colsample_bytree=0.3)))\nvot_soft = VotingClassifier(estimators = estimator2, voting ='soft')\nvot_soft.fit(X_train, y_train)\nvot_hard = VotingClassifier(estimators = estimator2, voting ='hard')\nvot_hard.fit(X_train, y_train)\nprint('VotingClassifiers3 in sample', roc_auc_score(y_train, vot_soft.predict_proba(X_train)[:,1]), f1_score(y_train, vot_hard.predict(X_train)))\nprint('VotingClassifiers3 out of sample', roc_auc_score(y_test, vot_soft.predict_proba(X_test)[:,1]), f1_score(y_test, vot_hard.predict(X_test)))\nprint('total time is ', time.time()-time0)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:05:38.130504Z","iopub.status.idle":"2022-05-25T18:05:38.130977Z","shell.execute_reply.started":"2022-05-25T18:05:38.130730Z","shell.execute_reply":"2022-05-25T18:05:38.130755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time1 = time.time()\nxgb = XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1)\nparam_grid = {'n_estimators':[200], 'max_depth':[2], 'eta':[0.3],\n'subsample':[0.5],'colsample_bytree':[0.1]}\nxgbm = GridSearchCV(xgb, param_grid, cv=2, scoring='roc_auc')\nxgbm.fit(X_train, y_train)\nprint('XGB ', xgbm.best_params_, xgbm.best_score_, \nroc_auc_score(y_train, xgbm.predict_proba(X_train)[:,1]), time.time()-time1)\nprint('XGB ', roc_auc_score(y_test, xgbm.predict_proba(X_test)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:05:38.132124Z","iopub.status.idle":"2022-05-25T18:05:38.132661Z","shell.execute_reply.started":"2022-05-25T18:05:38.132338Z","shell.execute_reply":"2022-05-25T18:05:38.132377Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time1 = time.time()\nxgb = XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1)\nparam_grid = {'n_estimators':[2000, 3000], 'max_depth':[3,4,5,6,8], 'eta':[0.01, 0.03, 0.05, 0.1],\n'subsample':[0.4,0.5,0.6],'colsample_bytree':[0.005]}\nxgbm = RandomizedSearchCV(xgb, param_grid, random_state=0, cv=2, n_iter=10, scoring='roc_auc')\nxgbm.fit(X_train, y_train)\nprint('XGB ', xgbm.best_params_, xgbm.best_score_, \nroc_auc_score(y_train, xgbm.predict_proba(X_train)[:,1]), time.time()-time1)\nprint('XGB ', roc_auc_score(y_test, xgbm.predict_proba(X_test)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:05:38.133757Z","iopub.status.idle":"2022-05-25T18:05:38.134249Z","shell.execute_reply.started":"2022-05-25T18:05:38.133941Z","shell.execute_reply":"2022-05-25T18:05:38.133965Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time1 = time.time()\nxgb = XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1)\nparam_grid = {'n_estimators':[3000], 'max_depth':[3,4,5,6], 'eta':[0.05, 0.1, 0.15, 0.2],\n'subsample':[0.5,0.6],'colsample_bytree':[0.005]}\nxgbm = RandomizedSearchCV(xgb, param_grid, random_state=0, cv=2, n_iter=10, scoring='roc_auc')\nxgbm.fit(X_train, y_train)\nprint('XGB ', xgbm.best_params_, xgbm.best_score_, \nroc_auc_score(y_train, xgbm.predict_proba(X_train)[:,1]), time.time()-time1)\nprint('XGB ', roc_auc_score(y_test, xgbm.predict_proba(X_test)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:05:38.135352Z","iopub.status.idle":"2022-05-25T18:05:38.135804Z","shell.execute_reply.started":"2022-05-25T18:05:38.135556Z","shell.execute_reply":"2022-05-25T18:05:38.135583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time1 = time.time()\nxgb = XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1)\nparam_grid = {'n_estimators':[3000], 'max_depth':[3,4], 'eta':[0.05, 0.1, 0.15, 0.2],\n'subsample':[0.5,0.6],'colsample_bytree':[0.005]}\nxgbm = RandomizedSearchCV(xgb, param_grid, random_state=0, cv=2, n_iter=8, scoring='roc_auc')\nxgbm.fit(X_train, y_train)\nprint('XGB ', xgbm.best_params_, xgbm.best_score_, \nroc_auc_score(y_train, xgbm.predict_proba(X_train)[:,1]), time.time()-time1)\nprint('XGB ', roc_auc_score(y_test, xgbm.predict_proba(X_test)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:05:38.136870Z","iopub.status.idle":"2022-05-25T18:05:38.137316Z","shell.execute_reply.started":"2022-05-25T18:05:38.137054Z","shell.execute_reply":"2022-05-25T18:05:38.137078Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time1 = time.time()\nxgb = XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1)\nparam_grid = {'n_estimators':[5000], 'max_depth':[3], 'eta':[0.06, 0.08, 0.1],\n'subsample':[0.6],'colsample_bytree':[0.005]}\nxgbm = GridSearchCV(xgb, param_grid, cv=4, scoring='roc_auc')\nxgbm.fit(X_train, y_train)\nprint('XGB ', xgbm.best_params_, xgbm.best_score_, \n      roc_auc_score(y_train, xgbm.predict_proba(X_train)[:,1]), time.time()-time1)\nprint('XGB ', roc_auc_score(y_test, xgbm.predict_proba(X_test)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:49:26.242347Z","iopub.execute_input":"2022-05-25T18:49:26.242708Z","iopub.status.idle":"2022-05-25T18:52:40.282045Z","shell.execute_reply.started":"2022-05-25T18:49:26.242655Z","shell.execute_reply":"2022-05-25T18:52:40.280992Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"XGB  {'colsample_bytree': 0.005, 'eta': 0.08, 'max_depth': 3, 'n_estimators': 3000, 'subsample': 0.6} 0.8954880884107757 0.9193106050903986 193.8829221725464\nXGB  0.8882499359150693\n","output_type":"stream"}]},{"cell_type":"code","source":"time1 = time.time()\nxgb = XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1)\nparam_grid = {'n_estimators':[5000], 'max_depth':[3], 'eta':[0.03, 0.04, 0.05],\n'subsample':[0.6],'colsample_bytree':[0.005]}\nxgbm = GridSearchCV(xgb, param_grid, cv=4, scoring='roc_auc')\nxgbm.fit(X_train, y_train)\nprint('XGB 5000', xgbm.best_params_, xgbm.best_score_, \n      roc_auc_score(y_train, xgbm.predict_proba(X_train)[:,1]), time.time()-time1)\nprint('XGB ', roc_auc_score(y_test, xgbm.predict_proba(X_test)[:,1]))\n\ntime1 = time.time()\nxgb = XGBClassifier(tree_method='gpu_hist', gpu_id=0, min_child_weight=50, n_jobs=-1)\nparam_grid = {'n_estimators':[8000], 'max_depth':[3], 'eta':[0.015, 0.02, 0.025],\n'subsample':[0.5],'colsample_bytree':[0.005]}\nxgbm = GridSearchCV(xgb, param_grid, cv=4, scoring='roc_auc')\nxgbm.fit(X_train, y_train)\nprint('XGB 8000', xgbm.best_params_, xgbm.best_score_, \nroc_auc_score(y_train, xgbm.predict_proba(X_train)[:,1]), time.time()-time1)\nprint('XGB ', roc_auc_score(y_test, xgbm.predict_proba(X_test)[:,1]))","metadata":{"execution":{"iopub.status.busy":"2022-05-25T21:35:36.149984Z","iopub.execute_input":"2022-05-25T21:35:36.150240Z","iopub.status.idle":"2022-05-25T21:47:49.370726Z","shell.execute_reply.started":"2022-05-25T21:35:36.150211Z","shell.execute_reply":"2022-05-25T21:47:49.369908Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"XGB 5000 {'colsample_bytree': 0.005, 'eta': 0.03, 'max_depth': 3, 'n_estimators': 5000, 'subsample': 0.6} 0.8963482411450597 0.9167435137473001 285.34923791885376\nXGB  0.9139258877617958\nXGB 8000 {'colsample_bytree': 0.005, 'eta': 0.02, 'max_depth': 3, 'n_estimators': 8000, 'subsample': 0.5} 0.8975434400723943 0.918000991580137 447.38973736763\nXGB  0.9141771610204088\n","output_type":"stream"}]},{"cell_type":"code","source":"# 8. feature importance #\n\nresults = permutation_importance(xgbm, X_test, y_test, scoring='f1', n_jobs=-1)\nfi_lr = pd.DataFrame({'col':X_test.columns, 'FI':results.importances_mean})\nfi_lr.sort_values('FI', ascending = False)[:20]","metadata":{"execution":{"iopub.status.busy":"2022-05-25T22:13:13.321720Z","iopub.execute_input":"2022-05-25T22:13:13.322481Z","iopub.status.idle":"2022-05-25T22:15:09.653810Z","shell.execute_reply.started":"2022-05-25T22:13:13.322442Z","shell.execute_reply":"2022-05-25T22:15:09.651472Z"},"trusted":true},"execution_count":11,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/1086613687.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 8. feature importance #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpermutation_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxgbm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfi_lr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'col'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'FI'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimportances_mean\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfi_lr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FI'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/inspection/_permutation_importance.py\u001b[0m in \u001b[0;36mpermutation_importance\u001b[0;34m(estimator, X, y, scoring, n_repeats, n_jobs, random_state, sample_weight, max_samples)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mmax_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         )\n\u001b[0;32m--> 271\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mcol_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    272\u001b[0m     )\n\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"temp = fi_lr.sort_values('FI', ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:05:38.157454Z","iopub.status.idle":"2022-05-25T18:05:38.158092Z","shell.execute_reply.started":"2022-05-25T18:05:38.157755Z","shell.execute_reply":"2022-05-25T18:05:38.157786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_i1 = list(temp.loc[temp.FI>0]['col'])\nfeatures_i1","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:05:38.166695Z","iopub.status.idle":"2022-05-25T18:05:38.167280Z","shell.execute_reply.started":"2022-05-25T18:05:38.166981Z","shell.execute_reply":"2022-05-25T18:05:38.167024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:05:38.169415Z","iopub.status.idle":"2022-05-25T18:05:38.169998Z","shell.execute_reply.started":"2022-05-25T18:05:38.169678Z","shell.execute_reply":"2022-05-25T18:05:38.169706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 9. predictions #\n\nsubmission_df_vc = pd.DataFrame({'ID_code': pred0.ID_code, 'target': vot_soft.predict_proba(X_pred)[:,1]}, columns=['ID_code', 'target'])\nsubmission_df_bt = pd.DataFrame({'ID_code': pred0.ID_code, 'target': xgbm.predict_proba(X_pred)[:,1]}, columns=['ID_code', 'target'])\n\n#submission_df_bt.Transported = np.array([bool(x) for x in submission_df_bt.Transported])\n\nsubmission_df_vc.to_csv('KP12_vc.csv',index=False)\n#submission_df_svm.to_csv('KP11_svm.csv',index=False)\n#submission_df_rf.to_csv('KP11_rf.csv',index=False)\nsubmission_df_bt.to_csv('KP12_bt.csv',index=False)\n\nos.chdir(r'/kaggle/working')\n\nfrom IPython.display import FileLink\nFileLink(r'KP12_vc.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-25T22:28:04.914442Z","iopub.execute_input":"2022-05-25T22:28:04.914724Z","iopub.status.idle":"2022-05-25T22:28:34.819389Z","shell.execute_reply.started":"2022-05-25T22:28:04.914673Z","shell.execute_reply":"2022-05-25T22:28:34.818591Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/KP12_vc.csv","text/html":"<a href='KP12_vc.csv' target='_blank'>KP12_vc.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"X_pred","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:05:38.180322Z","iopub.status.idle":"2022-05-25T18:05:38.180855Z","shell.execute_reply.started":"2022-05-25T18:05:38.180552Z","shell.execute_reply":"2022-05-25T18:05:38.180581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:05:38.189701Z","iopub.status.idle":"2022-05-25T18:05:38.190307Z","shell.execute_reply.started":"2022-05-25T18:05:38.189993Z","shell.execute_reply":"2022-05-25T18:05:38.190022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.logspace(-3,-7, num=17)","metadata":{"execution":{"iopub.status.busy":"2022-05-25T18:05:38.192316Z","iopub.status.idle":"2022-05-25T18:05:38.192879Z","shell.execute_reply.started":"2022-05-25T18:05:38.192563Z","shell.execute_reply":"2022-05-25T18:05:38.192593Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is December 2021 Tabulat Playground series","metadata":{}},{"cell_type":"markdown","source":"### Outline:\n0. Load libraries and custom functions.\n1. Load data.\n2. Preliminary data analysis: explore features and a target, delete unneeded features, create new features.\n3. Train-test split.\n4. Missing values. In some cases it may be useful to explore skew and perform log-transform before imputing missing values.\n5. Feature engineering. Transform skewed variables, do OHC and scaling.\n6. Fit models.\n7. Evaluate models.\n8. Feature importance, error analysis. Based on the results, go to 2. and iterate.\n9. Make predictions.","metadata":{}},{"cell_type":"code","source":"# 0. Load libraries #\n\nimport numpy as np\nimport pandas as pd\nimport os, time, warnings, optuna, gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, r2_score, mean_squared_error, make_scorer\nfrom sklearn.inspection import permutation_importance\nfrom scipy.special import inv_boxcox\nfrom xgboost import XGBClassifier, XGBRegressor\nimport lightgbm as lgb\n\npd.set_option('display.max_columns', 20)\npd.set_option('mode.chained_assignment', None)\npd.set_option('display.expand_frame_repr', False)\nwarnings.filterwarnings('ignore')\n\ndef draw_histograms(df, variables, n_rows, n_cols):\n    # stolen from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n    fig.tight_layout()  \n    plt.show()\n\n\ndef fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode'):\n    \"\"\"This function speeds up filling missing values for 3 main datasets using different imputation methods.\n    Later may replace it with some subclass.\n    Example: \n    fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\"\"\"\n    \n    # set df_pred to None if it does not exist\n    if (cat_features is not None):\n        if (cat_fill=='mode'):\n\n            df_train[cat_features] = df_train[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n            df_test[cat_features] = df_test[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n            if (df_pred is not None):\n                df_pred[cat_features] = df_pred[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n\n        if (cat_fill=='missing'):\n\n            df_train[cat_features] = df_train[cat_features].fillna(value='missing')\n            df_test[cat_features] = df_test[cat_features].fillna(value='missing')\n            if (df_pred is not None):\n                df_pred[cat_features] = df_pred[cat_features].fillna(value='missing')\n        \n    if (num_fill=='median'):\n        df_train[num_features] = df_train[num_features].fillna(value=df_train[num_features].median())\n        df_test[num_features] = df_test[num_features].fillna(value=df_train[num_features].median())\n        if (df_pred is not None):\n            df_pred[num_features] = df_pred[num_features].fillna(value=df_train[num_features].median())    \n    \n    if (cat_features is not None):\n        all_good = (\n        (np.prod(df_train[num_features+cat_features].shape)==df_train[num_features+cat_features].count().sum()) and \n        (np.prod(df_test[num_features+cat_features].shape) == df_test[num_features+cat_features].count().sum()))\n        if (all_good):\n            print('Missing values imputed successfully')\n        else:\n            print('There are still some missing values...')\n    else:\n        all_good = (\n        (np.prod(df_train[num_features].shape)==df_train[num_features].count().sum()) and \n        (np.prod(df_test[num_features].shape) == df_test[num_features].count().sum()))\n        if (all_good):\n            print('Missing values imputed successfully')\n        else:\n            print('There are still some missing values...')\n# END\n\n    \ndef add_misDummy_mp_i1(df_train, df_test, df_pred, features):\n    \"\"\"This function creates new dummy columns for missing features.\n    Example: add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\"\"\"\n    # set df_pred to None if it does not exist\n    \n    columns_before = df_train.shape[1]\n    \n    for feature_name in features:\n        \n        if df_train[feature_name].count()==df_train.shape[0]:\n            continue\n        \n        misColName = 'mis'+feature_name\n        df_train.loc[df_train[feature_name].isnull(), misColName]=1\n        df_train.loc[df_train[feature_name].notnull(), misColName]=0\n        df_test.loc[df_test[feature_name].isnull(), misColName]=1\n        df_test.loc[df_test[feature_name].notnull(), misColName]=0\n        if (df_pred is not None):\n            df_pred.loc[df_pred[feature_name].isnull(), misColName]=1\n            df_pred.loc[df_pred[feature_name].notnull(), misColName]=0\n            \n        columns_after = df_train.shape[1]\n            \n    print(columns_after-columns_before, ' dummy features added')\n# END\n   \n\ndef discretize_mp_i1(df_train, df_test, df_pred, feature, ntiles, delete_feature=False):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: discretize_mp_i1(X_train, X_test, X_pred, 'Age', 15)\"\"\"\n    # set df_pred to None if it does not exist\n    _,bin = pd.qcut(df_train[feature], ntiles, retbins = True, labels = False, duplicates = 'drop')\n    df_train[feature+'Ntile'] = pd.cut(df_train[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    df_test[feature+'Ntile'] = pd.cut(df_test[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (df_pred is not None):\n        df_pred[feature+'Ntile'] = pd.cut(df_pred[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (delete_feature==True):\n        df_train.drop(columns=[feature], inplace=True)\n        df_test.drop(columns=[feature], inplace=True)\n        df_pred.drop(columns=[feature], inplace=True)\n    print('Discretized ',feature, ' into ', len(bin)-1, ' bins')\n# END\n\n\ndef log_transformer_mp_i1(df_train, df_test, df_pred=None, feature_subset=False, max_skew=3):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: log_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\"\"\"\n    # set df_pred to None if it does not exist\n    if (feature_subset==False):\n        features_totransform = df_train.columns\n    else:\n        features_totransform = feature_subset.copy()\n    skewed_vars = list(df_train.skew()[(df_train.skew())>max_skew].index)\n    for col in list(set(skewed_vars)&set(features_totransform)):\n        df_train[col] = np.log1p(df_train[col])\n        df_test[col] = np.log1p(df_test[col])\n        if (df_pred is not None):\n            df_pred[col] = np.log1p(df_pred[col])\n    print('Skewed columns log-transformed: ', list(set(skewed_vars)&set(features_totransform)))\n# END\n    \n    \ndef add_dummyfeatures(df_train, df_test, df_pred, feature_dict):\n    \"\"\"This function adds dummy feature when some feature is equal to value, specified in a dictionary.\n    Example: add_dummyfeatures(X_train, X_test, X_pred, {'RoomService':0, 'Spa':0, 'VRDeck':0, 'ShoppingMall':0})\"\"\"\n    input_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n    for i in range(len(list(feature_dict.items()))):\n        feature,value = list(feature_dict.keys())[i], list(feature_dict.values())[i]\n        df_train.loc[df_train[feature]==value,(str(feature)+str(value))]=1\n        df_train.loc[df_train[feature]!=value,(str(feature)+str(value))]=0\n        df_test.loc[df_test[feature]==value,(str(feature)+str(value))]=1\n        df_test.loc[df_test[feature]!=value,(str(feature)+str(value))]=0\n        df_pred.loc[df_pred[feature]==value,(str(feature)+str(value))]=1\n        df_pred.loc[df_pred[feature]!=value,(str(feature)+str(value))]=0\n    output_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n    print(output_dimensions-input_dimensions, ' variables created') \n# END\n\n\n\ntime0 = time.time()\n\n\n#1. Load data #\n\ndf = pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv')\npred = pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv')\npred0 = pred.copy()\nprint(df.shape, pred.shape)\n\ndf.drop(columns = ['Slope','Aspect','Soil_Type5','Soil_Type7','Soil_Type15',\n                  'Hillshade_3pm', 'Hillshade_9am', 'Soil_Type1', 'Soil_Type3'], inplace = True)\n\n\n# 2. pEDA #\n\ndf = df.sample(1000000, random_state=3)\n\ndf.drop(columns = ['Id'], inplace = True)\npred.drop(columns = ['Id'], inplace = True)\nprint(df.Cover_Type.value_counts())\n#df.head()\n\n#[[col, df[col].nunique()] for col in df.columns]\n#df.count()\ndf.Cover_Type.value_counts()\n#df.skew()\n\n# 3. Train-test split #\n\ntrain_y = df[['Cover_Type']]\ntrain_y.replace([1, 2, 3, 4, 6, 7], [0,1,2,3,4,5], inplace = True)\ntrain_x = df.drop(columns = ['Cover_Type'])\n\nX_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.05, random_state = 1)\n\nprint(X_train.shape, X_test.shape, y_train.shape, pred.shape)\n\n\n# 4. Missing values #\n\n#X_train.count()\n\n#X_train.skew()\n\n# 5. Feature engineering #\n\n#log_transformer_mp_i1(X_train, X_test, pred)\n#X_train.skew()\n# all skewed variables are already ohc-ed, so their skew does not matter.\n\nss = StandardScaler()\n\nfor col in X_train.columns:\n    X_train[[col]] = ss.fit_transform(X_train[[col]])\n    X_test[[col]] = ss.transform(X_test[[col]])\n    pred[[col]] = ss.transform(pred[[col]])\n    \ngc.collect()\nprint(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T01:22:29.635168Z","iopub.execute_input":"2022-06-09T01:22:29.635543Z","iopub.status.idle":"2022-06-09T01:23:02.028957Z","shell.execute_reply.started":"2022-06-09T01:22:29.635513Z","shell.execute_reply":"2022-06-09T01:23:02.027967Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"(4000000, 56) (1000000, 55)\n2    566215\n1    366466\n3     48854\n7     15486\n6      2896\n4        83\nName: Cover_Type, dtype: int64\n(950000, 45) (50000, 45) (950000, 1) (1000000, 54)\n(950000, 45)\n","output_type":"stream"}]},{"cell_type":"code","source":"# 6. Model fitting #\n\nf1w = make_scorer(f1_score , average='weighted')\n\ntime1 = time.time()\nlr = LogisticRegression()\nparam_grid = {'C':[1, 10, 100]}\nlrm = GridSearchCV(lr, param_grid, cv=2, scoring=f1w)\nlrm.fit(X_train, y_train)\nprint('Logistic', lrm.best_params_, lrm.best_score_, time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T20:32:30.874637Z","iopub.execute_input":"2022-06-05T20:32:30.875001Z","iopub.status.idle":"2022-06-05T20:33:19.806219Z","shell.execute_reply.started":"2022-06-05T20:32:30.87497Z","shell.execute_reply":"2022-06-05T20:33:19.805323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit XGBoost using Optuna\n\ntime1 = time.time()\n\ndef objective(trial, n_splits=2, n_jobs=-1, early_stopping_rounds=50):\n    params = {\n        \"tree_method\": 'gpu_hist',\n        \"gpu_id\": 0,\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": 500,\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.02, 0.3),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.2, 1),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 1),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.001, 10.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-8, 10.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 200),\n        \"n_jobs\": n_jobs,\n    }\n\n    X = X_train\n    y = y_train\n    \n    model = XGBClassifier(**params)\n    rkf = KFold(n_splits=n_splits)\n    X_values = X.values\n    y_values = y.values\n    y_pred = np.zeros_like(y_values)\n    for train_index, test_index in rkf.split(X_values):\n        X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n        y_A, y_B = y_values[train_index], y_values[test_index]\n        model.fit(X_A, y_A, eval_set=[(X_B, y_B)],\n                  early_stopping_rounds=early_stopping_rounds, verbose = False)\n        y_pred[test_index] += model.predict(X_B).reshape(-1,1)\n    return (f1_score(y_train, y_pred, average='weighted'))\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\nprint('Total time ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\n\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_hyperpars['gpu_id']=0\noptuna_hyperpars['n_estimators']=500\n#optuna_hyperpars\noptuna_xgb = XGBClassifier(**optuna_hyperpars)\noptuna_xgb.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T01:23:48.506702Z","iopub.execute_input":"2022-06-09T01:23:48.507162Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"\u001b[32m[I 2022-06-09 01:23:48,520]\u001b[0m A new study created in memory with name: no-name-069c839f-700b-47ef-80f8-e23f6754cfff\u001b[0m\n\u001b[32m[I 2022-06-09 01:24:55,989]\u001b[0m Trial 0 finished with value: 0.9584389417187137 and parameters: {'max_depth': 5, 'learning_rate': 0.29535515718703187, 'colsample_bytree': 0.5066221863279529, 'subsample': 0.6826572392683761, 'alpha': 4.509393388813394, 'lambda': 0.0005266980188326377, 'gamma': 1.2764179485806539e-06, 'min_child_weight': 6.937299033354326}. Best is trial 0 with value: 0.9584389417187137.\u001b[0m\n\u001b[32m[I 2022-06-09 01:26:06,442]\u001b[0m Trial 1 finished with value: 0.9556603229937607 and parameters: {'max_depth': 6, 'learning_rate': 0.20367503092088726, 'colsample_bytree': 0.43423686736648603, 'subsample': 0.3191481405440985, 'alpha': 4.156795098890419, 'lambda': 0.010015128712165837, 'gamma': 2.0898745712528777e-06, 'min_child_weight': 46.373549320787355}. Best is trial 0 with value: 0.9584389417187137.\u001b[0m\n","output_type":"stream"}]},{"cell_type":"code","source":"f1w = make_scorer(f1_score , average='weighted')\n\ntime1 = time.time()\n\nxgb = XGBClassifier(tree_method = 'gpu_hist', gpu_id = 0)\nparam_grid = {'eta':[0.1, 0.2, 0.3], 'max_depth':[4,6,8], 'n_estimators':[300]}\nxgbm = GridSearchCV(xgb, param_grid, cv=2, scoring=f1w, verbose=1)\n\nxgbm.fit(X_train, y_train)\nprint('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T01:15:53.159470Z","iopub.execute_input":"2022-06-09T01:15:53.159824Z","iopub.status.idle":"2022-06-09T01:16:55.143269Z","shell.execute_reply.started":"2022-06-09T01:15:53.159794Z","shell.execute_reply":"2022-06-09T01:16:55.141708Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Fitting 2 folds for each of 9 candidates, totalling 18 fits\nXGB {'eta': 0.2, 'max_depth': 8, 'n_estimators': 100} 0.9552287013306059 61.97619009017944\n","output_type":"stream"}]},{"cell_type":"code","source":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T01:34:41.163815Z","iopub.execute_input":"2022-06-05T01:34:41.164197Z","iopub.status.idle":"2022-06-05T01:34:42.475186Z","shell.execute_reply.started":"2022-06-05T01:34:41.164166Z","shell.execute_reply":"2022-06-05T01:34:42.474314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_slice(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T01:35:18.793448Z","iopub.execute_input":"2022-06-05T01:35:18.793935Z","iopub.status.idle":"2022-06-05T01:35:19.124632Z","shell.execute_reply.started":"2022-06-05T01:35:18.793896Z","shell.execute_reply":"2022-06-05T01:35:19.1239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"# 7. Model evaluation #\n\n#print('Logistic IS', f1_score(y_train, lrm.predict(X_train), average='weighted'), \n#      f1_score(y_test, lrm.predict(X_test), average='weighted'))\n\nprint('XGB_gs IS', f1_score(y_train, xgbm.predict(X_train), average='weighted'), \n      f1_score(y_test, xgbm.predict(X_test), average='weighted'))\n\nprint('XGB_Optuna IS', f1_score(y_train, optuna_xgb.predict(X_train), average='weighted'), \n      f1_score(y_test, optuna_xgb.predict(X_test), average='weighted'))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-09T01:17:30.739944Z","iopub.execute_input":"2022-06-09T01:17:30.740772Z","iopub.status.idle":"2022-06-09T01:17:33.035064Z","shell.execute_reply.started":"2022-06-09T01:17:30.740733Z","shell.execute_reply":"2022-06-09T01:17:33.034070Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"XGB_gs IS 0.9815944233398258 0.95826016940016\n","output_type":"stream"}]},{"cell_type":"code","source":"# feature importance #\n\nresults = permutation_importance(optuna_xgb, X_test, y_test, n_jobs=-1)\nfi = pd.DataFrame({'col':X_test.columns, 'FI':results.importances_mean})\nfi = fi.sort_values('FI', ascending = False)\nfi","metadata":{"execution":{"iopub.status.busy":"2022-06-09T01:17:51.405037Z","iopub.execute_input":"2022-06-09T01:17:51.405999Z","iopub.status.idle":"2022-06-09T01:18:24.452719Z","shell.execute_reply.started":"2022-06-09T01:17:51.405899Z","shell.execute_reply":"2022-06-09T01:18:24.451856Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"                                   col        FI\n0                            Elevation  0.462276\n3      Horizontal_Distance_To_Roadways  0.050991\n5   Horizontal_Distance_To_Fire_Points  0.031500\n8                     Wilderness_Area3  0.020405\n2       Vertical_Distance_To_Hydrology  0.017207\n6                     Wilderness_Area1  0.011685\n1     Horizontal_Distance_To_Hydrology  0.011141\n43                         Soil_Type39  0.007698\n42                         Soil_Type38  0.006796\n15                         Soil_Type10  0.005608\n44                         Soil_Type40  0.005404\n27                         Soil_Type23  0.003564\n26                         Soil_Type22  0.003511\n37                         Soil_Type33  0.002958\n39                         Soil_Type35  0.002623\n9                     Wilderness_Area4  0.002591\n41                         Soil_Type37  0.002416\n34                         Soil_Type30  0.001966\n4                       Hillshade_Noon  0.001839\n36                         Soil_Type32  0.001763\n18                         Soil_Type13  0.001759\n35                         Soil_Type31  0.001732\n21                         Soil_Type17  0.001517\n28                         Soil_Type24  0.001441\n16                         Soil_Type11  0.001317\n31                         Soil_Type27  0.001263\n10                          Soil_Type2  0.001244\n38                         Soil_Type34  0.001233\n23                         Soil_Type19  0.001154\n40                         Soil_Type36  0.001105\n30                         Soil_Type26  0.001018\n11                          Soil_Type4  0.000938\n20                         Soil_Type16  0.000850\n24                         Soil_Type20  0.000816\n25                         Soil_Type21  0.000800\n17                         Soil_Type12  0.000723\n19                         Soil_Type14  0.000578\n22                         Soil_Type18  0.000568\n33                         Soil_Type29  0.000479\n14                          Soil_Type9  0.000442\n32                         Soil_Type28  0.000240\n7                     Wilderness_Area2  0.000097\n29                         Soil_Type25  0.000092\n13                          Soil_Type8  0.000042\n12                          Soil_Type6 -0.000024","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>col</th>\n      <th>FI</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Elevation</td>\n      <td>0.462276</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Horizontal_Distance_To_Roadways</td>\n      <td>0.050991</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>Horizontal_Distance_To_Fire_Points</td>\n      <td>0.031500</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>Wilderness_Area3</td>\n      <td>0.020405</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Vertical_Distance_To_Hydrology</td>\n      <td>0.017207</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>Wilderness_Area1</td>\n      <td>0.011685</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Horizontal_Distance_To_Hydrology</td>\n      <td>0.011141</td>\n    </tr>\n    <tr>\n      <th>43</th>\n      <td>Soil_Type39</td>\n      <td>0.007698</td>\n    </tr>\n    <tr>\n      <th>42</th>\n      <td>Soil_Type38</td>\n      <td>0.006796</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>Soil_Type10</td>\n      <td>0.005608</td>\n    </tr>\n    <tr>\n      <th>44</th>\n      <td>Soil_Type40</td>\n      <td>0.005404</td>\n    </tr>\n    <tr>\n      <th>27</th>\n      <td>Soil_Type23</td>\n      <td>0.003564</td>\n    </tr>\n    <tr>\n      <th>26</th>\n      <td>Soil_Type22</td>\n      <td>0.003511</td>\n    </tr>\n    <tr>\n      <th>37</th>\n      <td>Soil_Type33</td>\n      <td>0.002958</td>\n    </tr>\n    <tr>\n      <th>39</th>\n      <td>Soil_Type35</td>\n      <td>0.002623</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Wilderness_Area4</td>\n      <td>0.002591</td>\n    </tr>\n    <tr>\n      <th>41</th>\n      <td>Soil_Type37</td>\n      <td>0.002416</td>\n    </tr>\n    <tr>\n      <th>34</th>\n      <td>Soil_Type30</td>\n      <td>0.001966</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Hillshade_Noon</td>\n      <td>0.001839</td>\n    </tr>\n    <tr>\n      <th>36</th>\n      <td>Soil_Type32</td>\n      <td>0.001763</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>Soil_Type13</td>\n      <td>0.001759</td>\n    </tr>\n    <tr>\n      <th>35</th>\n      <td>Soil_Type31</td>\n      <td>0.001732</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>Soil_Type17</td>\n      <td>0.001517</td>\n    </tr>\n    <tr>\n      <th>28</th>\n      <td>Soil_Type24</td>\n      <td>0.001441</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Soil_Type11</td>\n      <td>0.001317</td>\n    </tr>\n    <tr>\n      <th>31</th>\n      <td>Soil_Type27</td>\n      <td>0.001263</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>Soil_Type2</td>\n      <td>0.001244</td>\n    </tr>\n    <tr>\n      <th>38</th>\n      <td>Soil_Type34</td>\n      <td>0.001233</td>\n    </tr>\n    <tr>\n      <th>23</th>\n      <td>Soil_Type19</td>\n      <td>0.001154</td>\n    </tr>\n    <tr>\n      <th>40</th>\n      <td>Soil_Type36</td>\n      <td>0.001105</td>\n    </tr>\n    <tr>\n      <th>30</th>\n      <td>Soil_Type26</td>\n      <td>0.001018</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>Soil_Type4</td>\n      <td>0.000938</td>\n    </tr>\n    <tr>\n      <th>20</th>\n      <td>Soil_Type16</td>\n      <td>0.000850</td>\n    </tr>\n    <tr>\n      <th>24</th>\n      <td>Soil_Type20</td>\n      <td>0.000816</td>\n    </tr>\n    <tr>\n      <th>25</th>\n      <td>Soil_Type21</td>\n      <td>0.000800</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>Soil_Type12</td>\n      <td>0.000723</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>Soil_Type14</td>\n      <td>0.000578</td>\n    </tr>\n    <tr>\n      <th>22</th>\n      <td>Soil_Type18</td>\n      <td>0.000568</td>\n    </tr>\n    <tr>\n      <th>33</th>\n      <td>Soil_Type29</td>\n      <td>0.000479</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>Soil_Type9</td>\n      <td>0.000442</td>\n    </tr>\n    <tr>\n      <th>32</th>\n      <td>Soil_Type28</td>\n      <td>0.000240</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>Wilderness_Area2</td>\n      <td>0.000097</td>\n    </tr>\n    <tr>\n      <th>29</th>\n      <td>Soil_Type25</td>\n      <td>0.000092</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>Soil_Type8</td>\n      <td>0.000042</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>Soil_Type6</td>\n      <td>-0.000024</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-06-09T01:04:18.397072Z","iopub.execute_input":"2022-06-09T01:04:18.398098Z","iopub.status.idle":"2022-06-09T01:04:18.443430Z","shell.execute_reply.started":"2022-06-09T01:04:18.398041Z","shell.execute_reply":"2022-06-09T01:04:18.442564Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"         Elevation    Aspect     Slope  Horizontal_Distance_To_Hydrology  Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  Hillshade_9am  Hillshade_Noon  Hillshade_3pm  Horizontal_Distance_To_Fire_Points  ...  Soil_Type31  Soil_Type32  Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  Soil_Type39  Soil_Type40\n366266    0.911931 -1.361045  0.803411                          3.098012                        2.478975                        -0.198322       1.078583       -0.941120      -0.059149                            0.406448  ...    -0.169528     -0.19819    -0.197285     -0.11282    -0.126101    -0.104283    -0.112389     -0.20703    -0.203746    -0.178611\n2727183  -2.170251 -0.105683  0.102755                          0.560129                       -0.317980                        -0.527786       0.329476        0.807619      -0.173422                           -1.270972  ...    -0.169528     -0.19819    -0.197285     -0.11282    -0.126101    -0.104283    -0.112389     -0.20703    -0.203746    -0.178611\n2481996  -0.965083 -0.123876  0.336307                         -1.011152                        0.920255                         0.959359       0.948303       -1.927589       0.237960                            0.020987  ...    -0.169528     -0.19819    -0.197285     -0.11282    -0.126101    -0.104283    -0.112389     -0.20703    -0.203746    -0.178611\n2223972  -0.490635 -1.179108  0.219531                         -0.905223                       -0.419953                        -1.069050       1.241432       -1.389515      -0.059149                           -0.486758  ...    -0.169528     -0.19819    -0.197285     -0.11282    -0.126101    -0.104283    -0.112389     -0.20703    -0.203746    -0.178611\n137685    0.430557  1.395294  1.387292                          0.551301                        1.838006                         0.267787       1.404281        1.345692      -0.493385                            1.058631  ...    -0.169528     -0.19819    -0.197285     -0.11282    -0.126101    -0.104283    -0.112389     -0.20703    -0.203746    -0.178611\n...            ...       ...       ...                               ...                             ...                              ...            ...             ...            ...                                 ...  ...          ...          ...          ...          ...          ...          ...          ...          ...          ...          ...\n3518093  -0.507951  1.604521 -0.130797                          0.405649                       -0.347115                        -0.111780      -0.940747        0.090187       1.472103                            0.417968  ...    -0.169528     -0.19819    -0.197285     -0.11282    -0.126101    -0.104283    -0.112389     -0.20703    -0.203746    -0.178611\n1235702  -0.646476 -1.288270 -1.648886                         -0.565367                        2.449840                        -0.276513       0.818024        1.480211      -0.219131                           -0.806646  ...    -0.169528     -0.19819    -0.197285     -0.11282    -0.126101    -0.104283    -0.112389     -0.20703    -0.203746    -0.178611\n3761920  -0.767685  0.303675 -0.597901                         -0.424128                       -0.623897                        -0.825367      -0.484770        1.076656      -1.384711                           -0.235101  ...    -0.169528     -0.19819    -0.197285     -0.11282    -0.126101    -0.104283    -0.112389     -0.20703    -0.203746    -0.178611\n223019    0.711070  1.604521  0.219531                         -0.706606                       -0.419953                        -0.082174       0.427186       -0.941120      -0.607657                            0.044026  ...    -0.169528     -0.19819    -0.197285     -0.11282    -0.126101    -0.104283    -0.112389     -0.20703    -0.203746    -0.178611\n1296925  -1.446458 -0.178457  0.569859                         -1.033220                       -0.507358                        -0.706183       0.394616       -0.851442      -1.247584                           -0.036610  ...    -0.169528     -0.19819    -0.197285     -0.11282    -0.126101    -0.104283    -0.112389     -0.20703    -0.203746    -0.178611\n\n[190000 rows x 54 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Elevation</th>\n      <th>Aspect</th>\n      <th>Slope</th>\n      <th>Horizontal_Distance_To_Hydrology</th>\n      <th>Vertical_Distance_To_Hydrology</th>\n      <th>Horizontal_Distance_To_Roadways</th>\n      <th>Hillshade_9am</th>\n      <th>Hillshade_Noon</th>\n      <th>Hillshade_3pm</th>\n      <th>Horizontal_Distance_To_Fire_Points</th>\n      <th>...</th>\n      <th>Soil_Type31</th>\n      <th>Soil_Type32</th>\n      <th>Soil_Type33</th>\n      <th>Soil_Type34</th>\n      <th>Soil_Type35</th>\n      <th>Soil_Type36</th>\n      <th>Soil_Type37</th>\n      <th>Soil_Type38</th>\n      <th>Soil_Type39</th>\n      <th>Soil_Type40</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>366266</th>\n      <td>0.911931</td>\n      <td>-1.361045</td>\n      <td>0.803411</td>\n      <td>3.098012</td>\n      <td>2.478975</td>\n      <td>-0.198322</td>\n      <td>1.078583</td>\n      <td>-0.941120</td>\n      <td>-0.059149</td>\n      <td>0.406448</td>\n      <td>...</td>\n      <td>-0.169528</td>\n      <td>-0.19819</td>\n      <td>-0.197285</td>\n      <td>-0.11282</td>\n      <td>-0.126101</td>\n      <td>-0.104283</td>\n      <td>-0.112389</td>\n      <td>-0.20703</td>\n      <td>-0.203746</td>\n      <td>-0.178611</td>\n    </tr>\n    <tr>\n      <th>2727183</th>\n      <td>-2.170251</td>\n      <td>-0.105683</td>\n      <td>0.102755</td>\n      <td>0.560129</td>\n      <td>-0.317980</td>\n      <td>-0.527786</td>\n      <td>0.329476</td>\n      <td>0.807619</td>\n      <td>-0.173422</td>\n      <td>-1.270972</td>\n      <td>...</td>\n      <td>-0.169528</td>\n      <td>-0.19819</td>\n      <td>-0.197285</td>\n      <td>-0.11282</td>\n      <td>-0.126101</td>\n      <td>-0.104283</td>\n      <td>-0.112389</td>\n      <td>-0.20703</td>\n      <td>-0.203746</td>\n      <td>-0.178611</td>\n    </tr>\n    <tr>\n      <th>2481996</th>\n      <td>-0.965083</td>\n      <td>-0.123876</td>\n      <td>0.336307</td>\n      <td>-1.011152</td>\n      <td>0.920255</td>\n      <td>0.959359</td>\n      <td>0.948303</td>\n      <td>-1.927589</td>\n      <td>0.237960</td>\n      <td>0.020987</td>\n      <td>...</td>\n      <td>-0.169528</td>\n      <td>-0.19819</td>\n      <td>-0.197285</td>\n      <td>-0.11282</td>\n      <td>-0.126101</td>\n      <td>-0.104283</td>\n      <td>-0.112389</td>\n      <td>-0.20703</td>\n      <td>-0.203746</td>\n      <td>-0.178611</td>\n    </tr>\n    <tr>\n      <th>2223972</th>\n      <td>-0.490635</td>\n      <td>-1.179108</td>\n      <td>0.219531</td>\n      <td>-0.905223</td>\n      <td>-0.419953</td>\n      <td>-1.069050</td>\n      <td>1.241432</td>\n      <td>-1.389515</td>\n      <td>-0.059149</td>\n      <td>-0.486758</td>\n      <td>...</td>\n      <td>-0.169528</td>\n      <td>-0.19819</td>\n      <td>-0.197285</td>\n      <td>-0.11282</td>\n      <td>-0.126101</td>\n      <td>-0.104283</td>\n      <td>-0.112389</td>\n      <td>-0.20703</td>\n      <td>-0.203746</td>\n      <td>-0.178611</td>\n    </tr>\n    <tr>\n      <th>137685</th>\n      <td>0.430557</td>\n      <td>1.395294</td>\n      <td>1.387292</td>\n      <td>0.551301</td>\n      <td>1.838006</td>\n      <td>0.267787</td>\n      <td>1.404281</td>\n      <td>1.345692</td>\n      <td>-0.493385</td>\n      <td>1.058631</td>\n      <td>...</td>\n      <td>-0.169528</td>\n      <td>-0.19819</td>\n      <td>-0.197285</td>\n      <td>-0.11282</td>\n      <td>-0.126101</td>\n      <td>-0.104283</td>\n      <td>-0.112389</td>\n      <td>-0.20703</td>\n      <td>-0.203746</td>\n      <td>-0.178611</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3518093</th>\n      <td>-0.507951</td>\n      <td>1.604521</td>\n      <td>-0.130797</td>\n      <td>0.405649</td>\n      <td>-0.347115</td>\n      <td>-0.111780</td>\n      <td>-0.940747</td>\n      <td>0.090187</td>\n      <td>1.472103</td>\n      <td>0.417968</td>\n      <td>...</td>\n      <td>-0.169528</td>\n      <td>-0.19819</td>\n      <td>-0.197285</td>\n      <td>-0.11282</td>\n      <td>-0.126101</td>\n      <td>-0.104283</td>\n      <td>-0.112389</td>\n      <td>-0.20703</td>\n      <td>-0.203746</td>\n      <td>-0.178611</td>\n    </tr>\n    <tr>\n      <th>1235702</th>\n      <td>-0.646476</td>\n      <td>-1.288270</td>\n      <td>-1.648886</td>\n      <td>-0.565367</td>\n      <td>2.449840</td>\n      <td>-0.276513</td>\n      <td>0.818024</td>\n      <td>1.480211</td>\n      <td>-0.219131</td>\n      <td>-0.806646</td>\n      <td>...</td>\n      <td>-0.169528</td>\n      <td>-0.19819</td>\n      <td>-0.197285</td>\n      <td>-0.11282</td>\n      <td>-0.126101</td>\n      <td>-0.104283</td>\n      <td>-0.112389</td>\n      <td>-0.20703</td>\n      <td>-0.203746</td>\n      <td>-0.178611</td>\n    </tr>\n    <tr>\n      <th>3761920</th>\n      <td>-0.767685</td>\n      <td>0.303675</td>\n      <td>-0.597901</td>\n      <td>-0.424128</td>\n      <td>-0.623897</td>\n      <td>-0.825367</td>\n      <td>-0.484770</td>\n      <td>1.076656</td>\n      <td>-1.384711</td>\n      <td>-0.235101</td>\n      <td>...</td>\n      <td>-0.169528</td>\n      <td>-0.19819</td>\n      <td>-0.197285</td>\n      <td>-0.11282</td>\n      <td>-0.126101</td>\n      <td>-0.104283</td>\n      <td>-0.112389</td>\n      <td>-0.20703</td>\n      <td>-0.203746</td>\n      <td>-0.178611</td>\n    </tr>\n    <tr>\n      <th>223019</th>\n      <td>0.711070</td>\n      <td>1.604521</td>\n      <td>0.219531</td>\n      <td>-0.706606</td>\n      <td>-0.419953</td>\n      <td>-0.082174</td>\n      <td>0.427186</td>\n      <td>-0.941120</td>\n      <td>-0.607657</td>\n      <td>0.044026</td>\n      <td>...</td>\n      <td>-0.169528</td>\n      <td>-0.19819</td>\n      <td>-0.197285</td>\n      <td>-0.11282</td>\n      <td>-0.126101</td>\n      <td>-0.104283</td>\n      <td>-0.112389</td>\n      <td>-0.20703</td>\n      <td>-0.203746</td>\n      <td>-0.178611</td>\n    </tr>\n    <tr>\n      <th>1296925</th>\n      <td>-1.446458</td>\n      <td>-0.178457</td>\n      <td>0.569859</td>\n      <td>-1.033220</td>\n      <td>-0.507358</td>\n      <td>-0.706183</td>\n      <td>0.394616</td>\n      <td>-0.851442</td>\n      <td>-1.247584</td>\n      <td>-0.036610</td>\n      <td>...</td>\n      <td>-0.169528</td>\n      <td>-0.19819</td>\n      <td>-0.197285</td>\n      <td>-0.11282</td>\n      <td>-0.126101</td>\n      <td>-0.104283</td>\n      <td>-0.112389</td>\n      <td>-0.20703</td>\n      <td>-0.203746</td>\n      <td>-0.178611</td>\n    </tr>\n  </tbody>\n</table>\n<p>190000 rows × 54 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"pred","metadata":{"execution":{"iopub.status.busy":"2022-06-05T20:46:28.82292Z","iopub.execute_input":"2022-06-05T20:46:28.823291Z","iopub.status.idle":"2022-06-05T20:46:28.981601Z","shell.execute_reply.started":"2022-06-05T20:46:28.823244Z","shell.execute_reply":"2022-06-05T20:46:28.980699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp = optuna_xgb.predict(pred)\ntemp = pd.DataFrame(temp, columns = ['Cover_Type'])\nprint(temp.head())\ntemp.replace([0,1,2,3,4,5], [1, 2, 3, 4, 6, 7], inplace = True)\nprint(temp.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-05T20:57:08.429715Z","iopub.execute_input":"2022-06-05T20:57:08.430082Z","iopub.status.idle":"2022-06-05T20:57:33.073914Z","shell.execute_reply.started":"2022-06-05T20:57:08.43005Z","shell.execute_reply":"2022-06-05T20:57:33.073115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#yhat = optuna_xgb.predict(pred)\n\nsubmission_df_xgb = pd.DataFrame({'Id': pred0.Id, 'Cover_Type': temp['Cover_Type']}, columns=['Id', 'Cover_Type'])\n#submission_df_bt.Transported = np.array([bool(x) for x in submission_df_bt.Transported])\nsubmission_df_xgb.to_csv('KP14_xgb.csv',index=False)\n\nos.chdir(r'/kaggle/working')\n\nfrom IPython.display import FileLink\nFileLink(r'KP14_xgb.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-05T20:58:00.533531Z","iopub.execute_input":"2022-06-05T20:58:00.534219Z","iopub.status.idle":"2022-06-05T20:58:02.010941Z","shell.execute_reply.started":"2022-06-05T20:58:00.534181Z","shell.execute_reply":"2022-06-05T20:58:02.010115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
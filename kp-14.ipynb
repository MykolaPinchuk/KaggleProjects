{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is December 2021 Tabulat Playground series","metadata":{}},{"cell_type":"markdown","source":"### Outline:\n0. Load libraries and custom functions.\n1. Load data.\n2. Preliminary data analysis: explore features and a target, delete unneeded features, create new features.\n3. Train-test split.\n4. Missing values. In some cases it may be useful to explore skew and perform log-transform before imputing missing values.\n5. Feature engineering. Transform skewed variables, do OHC and scaling.\n6. Fit models.\n7. Evaluate models.\n8. Feature importance, error analysis. Based on the results, go to 2. and iterate.\n9. Make predictions.","metadata":{}},{"cell_type":"code","source":"# 0. Load libraries #\n\nimport numpy as np\nimport pandas as pd\nimport os, time, warnings, optuna, gc\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, KFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, r2_score, mean_squared_error, make_scorer\nfrom sklearn.inspection import permutation_importance\nfrom scipy.special import inv_boxcox\nfrom xgboost import XGBClassifier, XGBRegressor\nimport lightgbm as lgb\n\npd.set_option('display.max_columns', 20)\npd.set_option('mode.chained_assignment', None)\npd.set_option('display.expand_frame_repr', False)\nwarnings.filterwarnings('ignore')\n\ndef draw_histograms(df, variables, n_rows, n_cols):\n    # stolen from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n    fig.tight_layout()  \n    plt.show()\n\n\ndef fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode'):\n    \"\"\"This function speeds up filling missing values for 3 main datasets using different imputation methods.\n    Later may replace it with some subclass.\n    Example: \n    fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\"\"\"\n    \n    # set df_pred to None if it does not exist\n    if (cat_features is not None):\n        if (cat_fill=='mode'):\n\n            df_train[cat_features] = df_train[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n            df_test[cat_features] = df_test[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n            if (df_pred is not None):\n                df_pred[cat_features] = df_pred[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n\n        if (cat_fill=='missing'):\n\n            df_train[cat_features] = df_train[cat_features].fillna(value='missing')\n            df_test[cat_features] = df_test[cat_features].fillna(value='missing')\n            if (df_pred is not None):\n                df_pred[cat_features] = df_pred[cat_features].fillna(value='missing')\n        \n    if (num_fill=='median'):\n        df_train[num_features] = df_train[num_features].fillna(value=df_train[num_features].median())\n        df_test[num_features] = df_test[num_features].fillna(value=df_train[num_features].median())\n        if (df_pred is not None):\n            df_pred[num_features] = df_pred[num_features].fillna(value=df_train[num_features].median())    \n    \n    if (cat_features is not None):\n        all_good = (\n        (np.prod(df_train[num_features+cat_features].shape)==df_train[num_features+cat_features].count().sum()) and \n        (np.prod(df_test[num_features+cat_features].shape) == df_test[num_features+cat_features].count().sum()))\n        if (all_good):\n            print('Missing values imputed successfully')\n        else:\n            print('There are still some missing values...')\n    else:\n        all_good = (\n        (np.prod(df_train[num_features].shape)==df_train[num_features].count().sum()) and \n        (np.prod(df_test[num_features].shape) == df_test[num_features].count().sum()))\n        if (all_good):\n            print('Missing values imputed successfully')\n        else:\n            print('There are still some missing values...')\n# END\n\n    \ndef add_misDummy_mp_i1(df_train, df_test, df_pred, features):\n    \"\"\"This function creates new dummy columns for missing features.\n    Example: add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\"\"\"\n    # set df_pred to None if it does not exist\n    \n    columns_before = df_train.shape[1]\n    \n    for feature_name in features:\n        \n        if df_train[feature_name].count()==df_train.shape[0]:\n            continue\n        \n        misColName = 'mis'+feature_name\n        df_train.loc[df_train[feature_name].isnull(), misColName]=1\n        df_train.loc[df_train[feature_name].notnull(), misColName]=0\n        df_test.loc[df_test[feature_name].isnull(), misColName]=1\n        df_test.loc[df_test[feature_name].notnull(), misColName]=0\n        if (df_pred is not None):\n            df_pred.loc[df_pred[feature_name].isnull(), misColName]=1\n            df_pred.loc[df_pred[feature_name].notnull(), misColName]=0\n            \n        columns_after = df_train.shape[1]\n            \n    print(columns_after-columns_before, ' dummy features added')\n# END\n   \n\ndef discretize_mp_i1(df_train, df_test, df_pred, feature, ntiles, delete_feature=False):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: discretize_mp_i1(X_train, X_test, X_pred, 'Age', 15)\"\"\"\n    # set df_pred to None if it does not exist\n    _,bin = pd.qcut(df_train[feature], ntiles, retbins = True, labels = False, duplicates = 'drop')\n    df_train[feature+'Ntile'] = pd.cut(df_train[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    df_test[feature+'Ntile'] = pd.cut(df_test[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (df_pred is not None):\n        df_pred[feature+'Ntile'] = pd.cut(df_pred[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (delete_feature==True):\n        df_train.drop(columns=[feature], inplace=True)\n        df_test.drop(columns=[feature], inplace=True)\n        df_pred.drop(columns=[feature], inplace=True)\n    print('Discretized ',feature, ' into ', len(bin)-1, ' bins')\n# END\n\n\ndef log_transformer_mp_i1(df_train, df_test, df_pred=None, feature_subset=False, max_skew=3):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: log_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\"\"\"\n    # set df_pred to None if it does not exist\n    if (feature_subset==False):\n        features_totransform = df_train.columns\n    else:\n        features_totransform = feature_subset.copy()\n    skewed_vars = list(df_train.skew()[(df_train.skew())>max_skew].index)\n    for col in list(set(skewed_vars)&set(features_totransform)):\n        df_train[col] = np.log1p(df_train[col])\n        df_test[col] = np.log1p(df_test[col])\n        if (df_pred is not None):\n            df_pred[col] = np.log1p(df_pred[col])\n    print('Skewed columns log-transformed: ', list(set(skewed_vars)&set(features_totransform)))\n# END\n    \n    \ndef add_dummyfeatures(df_train, df_test, df_pred, feature_dict):\n    \"\"\"This function adds dummy feature when some feature is equal to value, specified in a dictionary.\n    Example: add_dummyfeatures(X_train, X_test, X_pred, {'RoomService':0, 'Spa':0, 'VRDeck':0, 'ShoppingMall':0})\"\"\"\n    input_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n    for i in range(len(list(feature_dict.items()))):\n        feature,value = list(feature_dict.keys())[i], list(feature_dict.values())[i]\n        df_train.loc[df_train[feature]==value,(str(feature)+str(value))]=1\n        df_train.loc[df_train[feature]!=value,(str(feature)+str(value))]=0\n        df_test.loc[df_test[feature]==value,(str(feature)+str(value))]=1\n        df_test.loc[df_test[feature]!=value,(str(feature)+str(value))]=0\n        df_pred.loc[df_pred[feature]==value,(str(feature)+str(value))]=1\n        df_pred.loc[df_pred[feature]!=value,(str(feature)+str(value))]=0\n    output_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n    print(output_dimensions-input_dimensions, ' variables created') \n# END\n\n\n\ntime0 = time.time()\n\n\n#1. Load data #\n\ndf = pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv')\npred = pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv')\npred0 = pred.copy()\nprint(df.shape, pred.shape)\n\ndf.drop(columns = ['Slope','Aspect','Soil_Type5','Soil_Type7','Soil_Type15',\n                  'Hillshade_3pm', 'Hillshade_9am', 'Soil_Type1', 'Soil_Type3'], inplace = True)\n\n\n# 2. pEDA #\n\ndf = df.sample(20000, random_state=4)\n# this should be 1000000\n\ndf.drop(columns = ['Id'], inplace = True)\npred.drop(columns = ['Id'], inplace = True)\nprint(df.Cover_Type.value_counts())\n#df.head()\n\n#[[col, df[col].nunique()] for col in df.columns]\n#df.count()\ndf.Cover_Type.value_counts()\n#df.skew()\n\n# 3. Train-test split #\n\ntrain_y = df[['Cover_Type']]\ntrain_y.replace([1, 2, 3, 4, 6, 7], [0,1,2,3,4,5], inplace = True)\ntrain_x = df.drop(columns = ['Cover_Type'])\n\nX_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.05, random_state = 1)\n\nprint(X_train.shape, X_test.shape, y_train.shape, pred.shape)\n\n\n# 4. Missing values #\n\n#X_train.count()\n\n#X_train.skew()\n\n# 5. Feature engineering #\n\n#log_transformer_mp_i1(X_train, X_test, pred)\n#X_train.skew()\n# all skewed variables are already ohc-ed, so their skew does not matter.\n\nss = StandardScaler()\n\nfor col in X_train.columns:\n    X_train[[col]] = ss.fit_transform(X_train[[col]])\n    X_test[[col]] = ss.transform(X_test[[col]])\n    pred[[col]] = ss.transform(pred[[col]])\n    \ngc.collect()\nprint(X_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-22T16:57:42.123056Z","iopub.execute_input":"2022-06-22T16:57:42.123411Z","iopub.status.idle":"2022-06-22T16:58:05.168299Z","shell.execute_reply.started":"2022-06-22T16:57:42.123381Z","shell.execute_reply":"2022-06-22T16:58:05.167467Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"(4000000, 56) (1000000, 55)\n2    11220\n1     7404\n3      991\n7      330\n6       51\n4        4\nName: Cover_Type, dtype: int64\n(19000, 45) (1000, 45) (19000, 1) (1000000, 54)\n(19000, 45)\n","output_type":"stream"}]},{"cell_type":"code","source":"nvidia-smi\nnvcc --version\n\n# they both fail yet gpu works just fine.","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:04:14.069323Z","iopub.execute_input":"2022-06-22T17:04:14.069697Z","iopub.status.idle":"2022-06-22T17:04:14.094730Z","shell.execute_reply.started":"2022-06-22T17:04:14.069666Z","shell.execute_reply":"2022-06-22T17:04:14.093668Z"},"trusted":true},"execution_count":21,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/177488625.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnvcc\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'nvcc' is not defined"],"ename":"NameError","evalue":"name 'nvcc' is not defined","output_type":"error"}]},{"cell_type":"code","source":"### temporary ###\n# test of XGBoost witg gpu: do I need gpu_id = 0?#\n\ntime2 = time.time()\nxgbm = XGBClassifier(tree_method = 'gpu_hist', gpu_id=0, n_estimators=100, max_depth=2)\nxgbm.fit(X_train, y_train)\nprint(time.time()-time2)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-22T17:03:50.312996Z","iopub.execute_input":"2022-06-22T17:03:50.313668Z","iopub.status.idle":"2022-06-22T17:03:50.345007Z","shell.execute_reply.started":"2022-06-22T17:03:50.313632Z","shell.execute_reply":"2022-06-22T17:03:50.343449Z"},"trusted":true},"execution_count":19,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/3863697772.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# test of XGBoost witg gpu: do I need gpu_id = 0?#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnvidia\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0msmi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnvcc\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nvidia' is not defined"],"ename":"NameError","evalue":"name 'nvidia' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# 6. Model fitting #\n\nf1w = make_scorer(f1_score , average='weighted')\n\ntime1 = time.time()\nlr = LogisticRegression()\nparam_grid = {'C':[1, 10, 100]}\nlrm = GridSearchCV(lr, param_grid, cv=2, scoring=f1w)\nlrm.fit(X_train, y_train)\nprint('Logistic', lrm.best_params_, lrm.best_score_, time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-06-05T20:32:30.874637Z","iopub.execute_input":"2022-06-05T20:32:30.875001Z","iopub.status.idle":"2022-06-05T20:33:19.806219Z","shell.execute_reply.started":"2022-06-05T20:32:30.87497Z","shell.execute_reply":"2022-06-05T20:33:19.805323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Fit XGBoost using Optuna\n\ntime1 = time.time()\n\ndef objective(trial, n_splits=2, n_jobs=-1, early_stopping_rounds=50):\n    params = {\n        \"tree_method\": 'gpu_hist',\n        \"gpu_id\": 0,\n        \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n        \"n_estimators\": 500,\n        \"max_depth\": trial.suggest_int(\"max_depth\", 2, 10),\n        \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.02, 0.3),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.2, 1),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0.3, 1),\n        \"alpha\": trial.suggest_loguniform(\"alpha\", 0.001, 10.0),\n        \"lambda\": trial.suggest_loguniform(\"lambda\", 1e-8, 10.0),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-8, 10.0),\n        \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 50),\n        \"n_jobs\": n_jobs,\n    }\n\n    X = X_train\n    y = y_train\n    \n    model = XGBClassifier(**params)\n    rkf = KFold(n_splits=n_splits)\n    X_values = X.values\n    y_values = y.values\n    y_pred = np.zeros_like(y_values)\n    for train_index, test_index in rkf.split(X_values):\n        X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n        y_A, y_B = y_values[train_index], y_values[test_index]\n        model.fit(X_A, y_A, eval_set=[(X_B, y_B)],\n                  early_stopping_rounds=early_stopping_rounds, verbose = False)\n        y_pred[test_index] += model.predict(X_B).reshape(-1,1)\n    return (f1_score(y_train, y_pred, average='weighted'))\n\ntime1 = time.time()\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=30)\nprint('Total time ', time.time()-time1)\nhp = study.best_params\nfor key, value in hp.items():\n    print(f\"{key:>20s} : {value}\")\nprint(f\"{'best objective value':>20s} : {study.best_value}\")\n\noptuna_hyperpars = study.best_params\noptuna_hyperpars['tree_method']='gpu_hist'\noptuna_hyperpars['gpu_id']=0\noptuna_hyperpars['n_estimators']=500\n#optuna_hyperpars\noptuna_xgb = XGBClassifier(**optuna_hyperpars)\noptuna_xgb.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T01:23:48.506702Z","iopub.execute_input":"2022-06-09T01:23:48.507162Z","iopub.status.idle":"2022-06-09T02:43:11.67614Z","shell.execute_reply.started":"2022-06-09T01:23:48.507128Z","shell.execute_reply":"2022-06-09T02:43:11.67537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1w = make_scorer(f1_score , average='weighted')\n\ntime1 = time.time()\n\nxgb = XGBClassifier(tree_method = 'gpu_hist', gpu_id = 0)\nparam_grid = {'eta':[0.1, 0.2, 0.3], 'max_depth':[4,6,8], 'n_estimators':[300]}\nxgbm = GridSearchCV(xgb, param_grid, cv=2, scoring=f1w, verbose=1)\n\nxgbm.fit(X_train, y_train)\nprint('XGB', xgbm.best_params_, xgbm.best_score_, time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T02:50:07.962677Z","iopub.execute_input":"2022-06-09T02:50:07.963069Z","iopub.status.idle":"2022-06-09T02:58:14.045859Z","shell.execute_reply.started":"2022-06-09T02:50:07.963036Z","shell.execute_reply":"2022-06-09T02:58:14.044943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize parameter importances.\noptuna.visualization.plot_param_importances(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:01:26.133513Z","iopub.execute_input":"2022-06-09T03:01:26.133875Z","iopub.status.idle":"2022-06-09T03:01:27.585234Z","shell.execute_reply.started":"2022-06-09T03:01:26.133846Z","shell.execute_reply":"2022-06-09T03:01:27.584362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optuna.visualization.plot_slice(study)","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:01:41.055256Z","iopub.execute_input":"2022-06-09T03:01:41.055725Z","iopub.status.idle":"2022-06-09T03:01:41.356245Z","shell.execute_reply.started":"2022-06-09T03:01:41.055691Z","shell.execute_reply":"2022-06-09T03:01:41.355416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"# 7. Model evaluation #\n\n#print('Logistic IS', f1_score(y_train, lrm.predict(X_train), average='weighted'), \n#      f1_score(y_test, lrm.predict(X_test), average='weighted'))\n\nprint('XGB_gs IS', f1_score(y_train, xgbm.predict(X_train), average='weighted'), \n      f1_score(y_test, xgbm.predict(X_test), average='weighted'))\n\nprint('XGB_Optuna IS', f1_score(y_train, optuna_xgb.predict(X_train), average='weighted'), \n      f1_score(y_test, optuna_xgb.predict(X_test), average='weighted'))\n","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:02:19.841774Z","iopub.execute_input":"2022-06-09T03:02:19.842156Z","iopub.status.idle":"2022-06-09T03:04:09.765334Z","shell.execute_reply.started":"2022-06-09T03:02:19.842123Z","shell.execute_reply":"2022-06-09T03:04:09.764495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# feature importance #\n\nresults = permutation_importance(optuna_xgb, X_test, y_test, n_jobs=-1)\nfi = pd.DataFrame({'col':X_test.columns, 'FI':results.importances_mean})\nfi = fi.sort_values('FI', ascending = False)\nfi","metadata":{"execution":{"iopub.status.busy":"2022-06-09T01:17:51.405037Z","iopub.execute_input":"2022-06-09T01:17:51.405999Z","iopub.status.idle":"2022-06-09T01:18:24.452719Z","shell.execute_reply.started":"2022-06-09T01:17:51.405899Z","shell.execute_reply":"2022-06-09T01:18:24.451856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"execution":{"iopub.status.busy":"2022-06-09T01:04:18.397072Z","iopub.execute_input":"2022-06-09T01:04:18.398098Z","iopub.status.idle":"2022-06-09T01:04:18.44343Z","shell.execute_reply.started":"2022-06-09T01:04:18.398041Z","shell.execute_reply":"2022-06-09T01:04:18.442564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"execution":{"iopub.status.busy":"2022-06-05T20:46:28.82292Z","iopub.execute_input":"2022-06-05T20:46:28.823291Z","iopub.status.idle":"2022-06-05T20:46:28.981601Z","shell.execute_reply.started":"2022-06-05T20:46:28.823244Z","shell.execute_reply":"2022-06-05T20:46:28.980699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.drop(columns = ['Slope','Aspect','Soil_Type5','Soil_Type7','Soil_Type15',\n                  'Hillshade_3pm', 'Hillshade_9am', 'Soil_Type1', 'Soil_Type3'], inplace = True)\ntemp = optuna_xgb.predict(pred)\ntemp = pd.DataFrame(temp, columns = ['Cover_Type'])\nprint(temp.head())\ntemp.replace([0,1,2,3,4,5], [1, 2, 3, 4, 6, 7], inplace = True)\nprint(temp.head())","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:07:14.044291Z","iopub.execute_input":"2022-06-09T03:07:14.044831Z","iopub.status.idle":"2022-06-09T03:08:19.101183Z","shell.execute_reply.started":"2022-06-09T03:07:14.044786Z","shell.execute_reply":"2022-06-09T03:08:19.100347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n#yhat = optuna_xgb.predict(pred)\n\nsubmission_df_xgb = pd.DataFrame({'Id': pred0.Id, 'Cover_Type': temp['Cover_Type']}, columns=['Id', 'Cover_Type'])\n#submission_df_bt.Transported = np.array([bool(x) for x in submission_df_bt.Transported])\nsubmission_df_xgb.to_csv('KP14_xgb.csv',index=False)\n\nos.chdir(r'/kaggle/working')\n\nfrom IPython.display import FileLink\nFileLink(r'KP14_xgb.csv')","metadata":{"execution":{"iopub.status.busy":"2022-06-09T03:13:15.441082Z","iopub.execute_input":"2022-06-09T03:13:15.441639Z","iopub.status.idle":"2022-06-09T03:13:16.953415Z","shell.execute_reply.started":"2022-06-09T03:13:15.441598Z","shell.execute_reply":"2022-06-09T03:13:16.95265Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This is December 2021 Tabulat Playground series","metadata":{}},{"cell_type":"markdown","source":"### Outline:\n0. Load libraries and custom functions.\n1. Load data.\n2. Preliminary data analysis: explore features and a target, delete unneeded features, create new features.\n3. Train-test split.\n4. Missing values. In some cases it may be useful to explore skew and perform log-transform before imputing missing values.\n5. Feature engineering. Transform skewed variables, do OHC and scaling.\n6. Fit models.\n7. Evaluate models.\n8. Feature importance, error analysis. Based on the results, go to 2. and iterate.\n9. Make predictions.","metadata":{}},{"cell_type":"code","source":"# 0. Load libraries #\n\nimport numpy as np\nimport pandas as pd\nimport os, time, warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.svm import SVC, SVR\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression, Lasso, Ridge\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score, r2_score, mean_squared_error\nfrom sklearn.inspection import permutation_importance\nfrom scipy.special import inv_boxcox\nfrom xgboost import XGBClassifier, XGBRegressor\n\npd.set_option('display.max_columns', 20)\npd.set_option('mode.chained_assignment', None)\npd.set_option('display.expand_frame_repr', False)\nwarnings.filterwarnings('ignore')\n\ndef draw_histograms(df, variables, n_rows, n_cols):\n    # stolen from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n    fig.tight_layout()  \n    plt.show()\n\n\ndef fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode'):\n    \"\"\"This function speeds up filling missing values for 3 main datasets using different imputation methods.\n    Later may replace it with some subclass.\n    Example: fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\"\"\"\n    # set df_pred to None if it does not exist\n    if (cat_fill=='mode'):\n    \n        df_train[cat_features] = df_train[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n        df_test[cat_features] = df_test[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n        if (df_pred is not None):\n            df_pred[cat_features] = df_pred[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n            \n    if (cat_fill=='missing'):\n    \n        df_train[cat_features] = df_train[cat_features].fillna(value='missing')\n        df_test[cat_features] = df_test[cat_features].fillna(value='missing')\n        if (df_pred is not None):\n            df_pred[cat_features] = df_pred[cat_features].fillna(value='missing')\n        \n    if (num_fill=='median'):\n        df_train[num_features] = df_train[num_features].fillna(value=df_train[num_features].median())\n        df_test[num_features] = df_test[num_features].fillna(value=df_train[num_features].median())\n        if (df_pred is not None):\n            df_pred[num_features] = df_pred[num_features].fillna(value=df_train[num_features].median())    \n    \n    all_good = (\n    (np.prod(df_train[num_features+cat_features].shape)==df_train[num_features+cat_features].count().sum()) and \n    (np.prod(df_test[num_features+cat_features].shape) == df_test[num_features+cat_features].count().sum()) and \n    (np.prod(df_pred[num_features+cat_features].shape) == df_pred[num_features+cat_features].count().sum()))\n    if (all_good):\n        print('Missing values imputed successfully')\n    else:\n        print('There are still some missing values...')\n    \n    \n    \ndef add_misDummy_mp_i1(df_train, df_test, df_pred, features):\n    \"\"\"This function creates new dummy columns for missing features.\n    Example: add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\"\"\"\n    # set df_pred to None if it does not exist\n    for feature_name in features:\n        misColName = 'mis'+feature_name\n        df_train.loc[df_train[feature_name].isnull(), misColName]=1\n        df_train.loc[df_train[feature_name].notnull(), misColName]=0\n        df_test.loc[df_test[feature_name].isnull(), misColName]=1\n        df_test.loc[df_test[feature_name].notnull(), misColName]=0\n        if (df_pred is not None):\n            df_pred.loc[df_pred[feature_name].isnull(), misColName]=1\n            df_pred.loc[df_pred[feature_name].notnull(), misColName]=0\n   \n\ndef discretize_mp_i1(df_train, df_test, df_pred, feature, ntiles, delete_feature=False):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: discretize_mp_i1(X_train, X_test, X_pred, 'Age', 15)\"\"\"\n    # set df_pred to None if it does not exist\n    _,bin = pd.qcut(df_train[feature], ntiles, retbins = True, labels = False, duplicates = 'drop')\n    df_train[feature+'Ntile'] = pd.cut(df_train[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    df_test[feature+'Ntile'] = pd.cut(df_test[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (df_pred is not None):\n        df_pred[feature+'Ntile'] = pd.cut(df_pred[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (delete_feature==True):\n        df_train.drop(columns=[feature], inplace=True)\n        df_test.drop(columns=[feature], inplace=True)\n        df_pred.drop(columns=[feature], inplace=True)\n    print('Discretized ',feature, ' into ', len(bin)-1, ' bins')\n\n\ndef log_transformer_mp_i1(df_train, df_test, df_pred, feature_subset=False, max_skew=3):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: log_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\"\"\"\n    # set df_pred to None if it does not exist\n    if (feature_subset==False):\n        features_totransform = df_train.columns\n    else:\n        features_totransform = feature_subset.copy()\n    skewed_vars = list(df_train.skew()[abs(df_train.skew())>max_skew].index)\n    for col in list(set(skewed_vars)&set(features_totransform)):\n        df_train[col] = np.log1p(df_train[col])\n        df_test[col] = np.log1p(df_test[col])\n        if (df_pred is not None):\n            df_pred[col] = np.log1p(df_pred[col])\n    print('Skewed columns log-transformed: ', list(set(skewed_vars)&set(features_totransform)))\n    \n    \ndef add_dummyfeatures(df_train, df_test, df_pred, feature_dict):\n    \"\"\"This function adds dummy feature when some feature is equal to value, specified in a dictionary.\n    Example: add_dummyfeatures(X_train, X_test, X_pred, {'RoomService':0, 'Spa':0, 'VRDeck':0, 'ShoppingMall':0})\"\"\"\n    input_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n    for i in range(len(list(feature_dict.items()))):\n        feature,value = list(feature_dict.keys())[i], list(feature_dict.values())[i]\n        df_train.loc[df_train[feature]==value,(str(feature)+str(value))]=1\n        df_train.loc[df_train[feature]!=value,(str(feature)+str(value))]=0\n        df_test.loc[df_test[feature]==value,(str(feature)+str(value))]=1\n        df_test.loc[df_test[feature]!=value,(str(feature)+str(value))]=0\n        df_pred.loc[df_pred[feature]==value,(str(feature)+str(value))]=1\n        df_pred.loc[df_pred[feature]!=value,(str(feature)+str(value))]=0\n    output_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n    print(output_dimensions-input_dimensions, ' variables created') \n    ","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:26:06.265401Z","iopub.execute_input":"2022-06-02T03:26:06.266855Z","iopub.status.idle":"2022-06-02T03:26:06.311004Z","shell.execute_reply.started":"2022-06-02T03:26:06.266800Z","shell.execute_reply":"2022-06-02T03:26:06.309671Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"#1. Load data #\n\ndf = pd.read_csv('../input/tabular-playground-series-dec-2021/train.csv')\npred = pd.read_csv('../input/tabular-playground-series-dec-2021/test.csv')\nprint(df.shape, pred.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:26:37.695998Z","iopub.execute_input":"2022-06-02T03:26:37.696524Z","iopub.status.idle":"2022-06-02T03:26:59.246939Z","shell.execute_reply.started":"2022-06-02T03:26:37.696484Z","shell.execute_reply":"2022-06-02T03:26:59.245956Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"(4000000, 56) (1000000, 55)\n","output_type":"stream"}]},{"cell_type":"code","source":"# 2. pEDA #\n\ndf = df.sample(100000)\n\ndf.drop(columns = ['Id'], inplace = True)\ndf.Cover_Type.value_counts()\n#df.head()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:27:07.108881Z","iopub.execute_input":"2022-06-02T03:27:07.109363Z","iopub.status.idle":"2022-06-02T03:27:07.468472Z","shell.execute_reply.started":"2022-06-02T03:27:07.109324Z","shell.execute_reply":"2022-06-02T03:27:07.467098Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"         Elevation  Aspect  Slope  Horizontal_Distance_To_Hydrology  Vertical_Distance_To_Hydrology  Horizontal_Distance_To_Roadways  Hillshade_9am  Hillshade_Noon  Hillshade_3pm  Horizontal_Distance_To_Fire_Points  ...  Soil_Type32  Soil_Type33  Soil_Type34  Soil_Type35  Soil_Type36  Soil_Type37  Soil_Type38  Soil_Type39  Soil_Type40  Cover_Type\n804010        3062     346     15                               219                              41                             1899            245             234             96                                1060  ...            0            0            0            0            0            0            0            0            0           2\n3492244       3074      52      9                                59                              -5                             1728            245             190             41                                1653  ...            0            0            0            0            0            0            0            0            0           1\n2672459       3499     148     35                               344                              18                              948            182             248            138                                2029  ...            0            0            0            0            0            0            0            0            0           1\n201794        2774     192     15                                38                              25                             3179            201             207            155                                2949  ...            0            0            0            0            0            0            0            0            0           2\n505345        2927     130      8                               397                               3                             1487            208             226            110                                1339  ...            0            0            0            0            0            1            0            0            0           2\n\n[5 rows x 55 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Elevation</th>\n      <th>Aspect</th>\n      <th>Slope</th>\n      <th>Horizontal_Distance_To_Hydrology</th>\n      <th>Vertical_Distance_To_Hydrology</th>\n      <th>Horizontal_Distance_To_Roadways</th>\n      <th>Hillshade_9am</th>\n      <th>Hillshade_Noon</th>\n      <th>Hillshade_3pm</th>\n      <th>Horizontal_Distance_To_Fire_Points</th>\n      <th>...</th>\n      <th>Soil_Type32</th>\n      <th>Soil_Type33</th>\n      <th>Soil_Type34</th>\n      <th>Soil_Type35</th>\n      <th>Soil_Type36</th>\n      <th>Soil_Type37</th>\n      <th>Soil_Type38</th>\n      <th>Soil_Type39</th>\n      <th>Soil_Type40</th>\n      <th>Cover_Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>804010</th>\n      <td>3062</td>\n      <td>346</td>\n      <td>15</td>\n      <td>219</td>\n      <td>41</td>\n      <td>1899</td>\n      <td>245</td>\n      <td>234</td>\n      <td>96</td>\n      <td>1060</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3492244</th>\n      <td>3074</td>\n      <td>52</td>\n      <td>9</td>\n      <td>59</td>\n      <td>-5</td>\n      <td>1728</td>\n      <td>245</td>\n      <td>190</td>\n      <td>41</td>\n      <td>1653</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2672459</th>\n      <td>3499</td>\n      <td>148</td>\n      <td>35</td>\n      <td>344</td>\n      <td>18</td>\n      <td>948</td>\n      <td>182</td>\n      <td>248</td>\n      <td>138</td>\n      <td>2029</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>201794</th>\n      <td>2774</td>\n      <td>192</td>\n      <td>15</td>\n      <td>38</td>\n      <td>25</td>\n      <td>3179</td>\n      <td>201</td>\n      <td>207</td>\n      <td>155</td>\n      <td>2949</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>505345</th>\n      <td>2927</td>\n      <td>130</td>\n      <td>8</td>\n      <td>397</td>\n      <td>3</td>\n      <td>1487</td>\n      <td>208</td>\n      <td>226</td>\n      <td>110</td>\n      <td>1339</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 55 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#[[col, df[col].nunique()] for col in df.columns]\n#df.count()\ndf.skew()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:27:10.308498Z","iopub.execute_input":"2022-06-02T03:27:10.309043Z","iopub.status.idle":"2022-06-02T03:27:10.387625Z","shell.execute_reply.started":"2022-06-02T03:27:10.309004Z","shell.execute_reply":"2022-06-02T03:27:10.386310Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"Elevation                             -0.060282\nAspect                                 0.499600\nSlope                                  0.747234\nHorizontal_Distance_To_Hydrology       1.743528\nVertical_Distance_To_Hydrology         1.795471\nHorizontal_Distance_To_Roadways        1.235741\nHillshade_9am                         -1.325788\nHillshade_Noon                        -1.150376\nHillshade_3pm                         -0.309561\nHorizontal_Distance_To_Fire_Points     1.557500\nWilderness_Area1                       1.093493\nWilderness_Area2                       4.565378\nWilderness_Area3                      -0.648043\nWilderness_Area4                       6.588287\nSoil_Type1                             7.528919\nSoil_Type2                             5.387291\nSoil_Type3                            15.314730\nSoil_Type4                             4.860912\nSoil_Type5                             7.717326\nSoil_Type6                            11.271671\nSoil_Type7                             0.000000\nSoil_Type8                            19.573310\nSoil_Type9                             9.576554\nSoil_Type10                            3.952292\nSoil_Type11                            5.651059\nSoil_Type12                            7.072122\nSoil_Type13                            5.379714\nSoil_Type14                            8.149409\nSoil_Type15                            0.000000\nSoil_Type16                            7.707211\nSoil_Type17                            6.780108\nSoil_Type18                            8.418497\nSoil_Type19                            8.224224\nSoil_Type20                            7.330829\nSoil_Type21                            9.122562\nSoil_Type22                            5.305462\nSoil_Type23                            4.168135\nSoil_Type24                            6.057538\nSoil_Type25                           17.648738\nSoil_Type26                            8.386282\nSoil_Type27                            8.834711\nSoil_Type28                            9.705408\nSoil_Type29                            6.415220\nSoil_Type30                            5.629627\nSoil_Type31                            5.763849\nSoil_Type32                            4.900531\nSoil_Type33                            4.893871\nSoil_Type34                            8.827287\nSoil_Type35                            7.753025\nSoil_Type36                            9.525451\nSoil_Type37                            8.849611\nSoil_Type38                            4.660040\nSoil_Type39                            4.698917\nSoil_Type40                            5.363707\nCover_Type                             3.303490\ndtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"# 3. Train-test split #\n\ntrain_y = df[['Cover_Type']]\ntrain_x = df.drop(columns = ['Cover_Type'])\n\nX_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.05, random_state = 1)\n\nprint(X_train.shape, X_test.shape, y_train.shape, pred.shape)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:27:13.449442Z","iopub.execute_input":"2022-06-02T03:27:13.449920Z","iopub.status.idle":"2022-06-02T03:27:13.529981Z","shell.execute_reply.started":"2022-06-02T03:27:13.449885Z","shell.execute_reply":"2022-06-02T03:27:13.528879Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"(95000, 54) (5000, 54) (95000, 1) (1000000, 55)\n","output_type":"stream"}]},{"cell_type":"code","source":"# 4. Missing values #","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:21:25.590847Z","iopub.execute_input":"2022-06-02T03:21:25.591271Z","iopub.status.idle":"2022-06-02T03:21:25.596771Z","shell.execute_reply.started":"2022-06-02T03:21:25.591240Z","shell.execute_reply":"2022-06-02T03:21:25.595325Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# 5. Feature engineering #\n\n#log_transformer_mp_i1(X_train, X_test, pred)\n#X_train.skew()\n\nss = StandardScaler()\n\nfor col in X_train.columns:\n    X_train[[col]] = ss.fit_transform(X_train[[col]])\n    X_test[[col]] = ss.transform(X_test[[col]])\n    pred[[col]] = ss.transform(pred[[col]])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:29:04.750702Z","iopub.execute_input":"2022-06-02T03:29:04.751091Z","iopub.status.idle":"2022-06-02T03:29:18.887258Z","shell.execute_reply.started":"2022-06-02T03:29:04.751062Z","shell.execute_reply":"2022-06-02T03:29:18.885961Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"y_train","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:29:31.639505Z","iopub.execute_input":"2022-06-02T03:29:31.639955Z","iopub.status.idle":"2022-06-02T03:29:31.655867Z","shell.execute_reply.started":"2022-06-02T03:29:31.639921Z","shell.execute_reply":"2022-06-02T03:29:31.654524Z"},"trusted":true},"execution_count":35,"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"         Cover_Type\n2606908           1\n764458            2\n2171574           1\n3319445           1\n2299173           1\n...             ...\n1465348           2\n2071470           2\n3433591           2\n3021388           1\n3500053           2\n\n[95000 rows x 1 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Cover_Type</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2606908</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>764458</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2171574</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3319445</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2299173</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1465348</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2071470</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3433591</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3021388</th>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3500053</th>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>95000 rows × 1 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# 6. Model fitting #\n\ntime1 = time.time()\nlr = LogisticRegression()\nparam_grid = {'C':[0.1, 1, 10, 100]}\nlrm = GridSearchCV(lr, param_grid, cv=2)\nlrm.fit(X_train, y_train)\nprint('Logistic', lrm.best_params_, lrm.best_score_, time.time()-time1)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T03:34:07.026678Z","iopub.execute_input":"2022-06-02T03:34:07.027131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"code","source":"# 7. Model evaluation #\n\nprint('Logistic', accuracy_score(y_train, lrm.predict(X_train)), accuracy_score(y_test, lrm.predict(X_test)))\n","metadata":{},"execution_count":null,"outputs":[]}]}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06891c9a",
   "metadata": {
    "papermill": {
     "duration": 0.005875,
     "end_time": "2022-06-03T13:24:02.804986",
     "exception": false,
     "start_time": "2022-06-03T13:24:02.799111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### This is my library of Python functions to speed up standard ML analysis of tabular data.\n",
    "\n",
    "Author: Mykola Pinchuk\n",
    "\n",
    "Started on 05/21/2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56f4f50c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T13:24:02.817429Z",
     "iopub.status.busy": "2022-06-03T13:24:02.817087Z",
     "iopub.status.idle": "2022-06-03T13:24:02.850192Z",
     "shell.execute_reply": "2022-06-03T13:24:02.849252Z"
    },
    "papermill": {
     "duration": 0.043226,
     "end_time": "2022-06-03T13:24:02.853260",
     "exception": false,
     "start_time": "2022-06-03T13:24:02.810034",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode'):\n",
    "    \"\"\"This function speeds up filling missing values for 3 main datasets using different imputation methods.\n",
    "    Later may replace it with some subclass.\n",
    "    Example: \n",
    "    fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\"\"\"\n",
    "    \n",
    "    # set df_pred to None if it does not exist\n",
    "    if (cat_features is not None):\n",
    "        if (cat_fill=='mode'):\n",
    "\n",
    "            df_train[cat_features] = df_train[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "            df_test[cat_features] = df_test[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "            if (df_pred is not None):\n",
    "                df_pred[cat_features] = df_pred[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n",
    "\n",
    "        if (cat_fill=='missing'):\n",
    "\n",
    "            df_train[cat_features] = df_train[cat_features].fillna(value='missing')\n",
    "            df_test[cat_features] = df_test[cat_features].fillna(value='missing')\n",
    "            if (df_pred is not None):\n",
    "                df_pred[cat_features] = df_pred[cat_features].fillna(value='missing')\n",
    "        \n",
    "    if (num_fill=='median'):\n",
    "        df_train[num_features] = df_train[num_features].fillna(value=df_train[num_features].median())\n",
    "        df_test[num_features] = df_test[num_features].fillna(value=df_train[num_features].median())\n",
    "        if (df_pred is not None):\n",
    "            df_pred[num_features] = df_pred[num_features].fillna(value=df_train[num_features].median())    \n",
    "    \n",
    "    if (cat_features is not None):\n",
    "        all_good = (\n",
    "        (np.prod(df_train[num_features+cat_features].shape)==df_train[num_features+cat_features].count().sum()) and \n",
    "        (np.prod(df_test[num_features+cat_features].shape) == df_test[num_features+cat_features].count().sum()))\n",
    "        if (all_good):\n",
    "            print('Missing values imputed successfully')\n",
    "        else:\n",
    "            print('There are still some missing values...')\n",
    "    else:\n",
    "        all_good = (\n",
    "        (np.prod(df_train[num_features].shape)==df_train[num_features].count().sum()) and \n",
    "        (np.prod(df_test[num_features].shape) == df_test[num_features].count().sum()))\n",
    "        if (all_good):\n",
    "            print('Missing values imputed successfully')\n",
    "        else:\n",
    "            print('There are still some missing values...')\n",
    "# END\n",
    "\n",
    "    \n",
    "def add_misDummy_mp_i1(df_train, df_test, df_pred, features):\n",
    "    \"\"\"This function creates new dummy columns for missing features.\n",
    "    Example: add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    \n",
    "    columns_before = df_train.shape[1]\n",
    "    \n",
    "    for feature_name in features:\n",
    "        \n",
    "        if df_train[feature_name].count()==df_train.shape[0]:\n",
    "            continue\n",
    "        \n",
    "        misColName = 'mis'+feature_name\n",
    "        df_train.loc[df_train[feature_name].isnull(), misColName]=1\n",
    "        df_train.loc[df_train[feature_name].notnull(), misColName]=0\n",
    "        df_test.loc[df_test[feature_name].isnull(), misColName]=1\n",
    "        df_test.loc[df_test[feature_name].notnull(), misColName]=0\n",
    "        if (df_pred is not None):\n",
    "            df_pred.loc[df_pred[feature_name].isnull(), misColName]=1\n",
    "            df_pred.loc[df_pred[feature_name].notnull(), misColName]=0\n",
    "            \n",
    "        columns_after = df_train.shape[1]\n",
    "            \n",
    "    print(columns_after-columns_before, ' dummy features added')\n",
    "# END\n",
    "   \n",
    "\n",
    "def discretize_mp_i1(df_train, df_test, df_pred, feature, ntiles, delete_feature=False):\n",
    "    \"\"\"This function divides a continuous feature into quantile groups.\n",
    "    Example: discretize_mp_i1(X_train, X_test, X_pred, 'Age', 15)\"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    _,bin = pd.qcut(df_train[feature], ntiles, retbins = True, labels = False, duplicates = 'drop')\n",
    "    df_train[feature+'Ntile'] = pd.cut(df_train[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n",
    "    df_test[feature+'Ntile'] = pd.cut(df_test[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n",
    "    if (df_pred is not None):\n",
    "        df_pred[feature+'Ntile'] = pd.cut(df_pred[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n",
    "    if (delete_feature==True):\n",
    "        df_train.drop(columns=[feature], inplace=True)\n",
    "        df_test.drop(columns=[feature], inplace=True)\n",
    "        df_pred.drop(columns=[feature], inplace=True)\n",
    "    print('Discretized ',feature, ' into ', len(bin)-1, ' bins')\n",
    "# END\n",
    "\n",
    "\n",
    "def log_transformer_mp_i1(df_train, df_test, df_pred=None, feature_subset=False, max_skew=3):\n",
    "    \"\"\"This function divides a continuous feature into quantile groups.\n",
    "    Example: log_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\"\"\"\n",
    "    # set df_pred to None if it does not exist\n",
    "    if (feature_subset==False):\n",
    "        features_totransform = df_train.columns\n",
    "    else:\n",
    "        features_totransform = feature_subset.copy()\n",
    "    skewed_vars = list(df_train.skew()[(df_train.skew())>max_skew].index)\n",
    "    for col in list(set(skewed_vars)&set(features_totransform)):\n",
    "        df_train[col] = np.log1p(df_train[col])\n",
    "        df_test[col] = np.log1p(df_test[col])\n",
    "        if (df_pred is not None):\n",
    "            df_pred[col] = np.log1p(df_pred[col])\n",
    "    print('Skewed columns log-transformed: ', list(set(skewed_vars)&set(features_totransform)))\n",
    "# END\n",
    "    \n",
    "    \n",
    "def add_dummyfeatures(df_train, df_test, df_pred, feature_dict):\n",
    "    \"\"\"This function adds dummy feature when some feature is equal to value, specified in a dictionary.\n",
    "    Example: add_dummyfeatures(X_train, X_test, X_pred, {'RoomService':0, 'Spa':0, 'VRDeck':0, 'ShoppingMall':0})\"\"\"\n",
    "    input_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n",
    "    for i in range(len(list(feature_dict.items()))):\n",
    "        feature,value = list(feature_dict.keys())[i], list(feature_dict.values())[i]\n",
    "        df_train.loc[df_train[feature]==value,(str(feature)+str(value))]=1\n",
    "        df_train.loc[df_train[feature]!=value,(str(feature)+str(value))]=0\n",
    "        df_test.loc[df_test[feature]==value,(str(feature)+str(value))]=1\n",
    "        df_test.loc[df_test[feature]!=value,(str(feature)+str(value))]=0\n",
    "        df_pred.loc[df_pred[feature]==value,(str(feature)+str(value))]=1\n",
    "        df_pred.loc[df_pred[feature]!=value,(str(feature)+str(value))]=0\n",
    "    output_dimensions = np.array([df_train.shape[1], df_test.shape[1], df_pred.shape[1]])\n",
    "    print(output_dimensions-input_dimensions, ' variables created') \n",
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74be274d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T13:24:02.864735Z",
     "iopub.status.busy": "2022-06-03T13:24:02.864451Z",
     "iopub.status.idle": "2022-06-03T13:24:02.868106Z",
     "shell.execute_reply": "2022-06-03T13:24:02.867234Z"
    },
    "papermill": {
     "duration": 0.011456,
     "end_time": "2022-06-03T13:24:02.870104",
     "exception": false,
     "start_time": "2022-06-03T13:24:02.858648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# need to add function to add dummy feature where continuous feature is equal to some number, e.g., 0.\n",
    "# see SpaceTitanic for raw code.\n",
    "\n",
    "# add function to delete columns from all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ec282e8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-03T13:24:02.882374Z",
     "iopub.status.busy": "2022-06-03T13:24:02.882069Z",
     "iopub.status.idle": "2022-06-03T13:24:02.887304Z",
     "shell.execute_reply": "2022-06-03T13:24:02.886382Z"
    },
    "papermill": {
     "duration": 0.014393,
     "end_time": "2022-06-03T13:24:02.890210",
     "exception": false,
     "start_time": "2022-06-03T13:24:02.875817",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function fillna_mp_i1 in module __main__:\n",
      "\n",
      "fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode')\n",
      "    This function speeds up filling missing values for 3 main datasets using different imputation methods.\n",
      "    Later may replace it with some subclass.\n",
      "    Example: \n",
      "    fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(fillna_mp_i1)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8a1fe47",
   "metadata": {
    "papermill": {
     "duration": 0.005728,
     "end_time": "2022-06-03T13:24:02.902672",
     "exception": false,
     "start_time": "2022-06-03T13:24:02.896944",
     "status": "completed"
    },
    "tags": []
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 10.065267,
   "end_time": "2022-06-03T13:24:03.530750",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-06-03T13:23:53.465483",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

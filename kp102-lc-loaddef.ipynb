{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/pmykola/lending-club-loan-default-prediction?scriptVersionId=103801415\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Modeling Credit Risk with Lending Club data\n\nInternal name: KP102\n\n\n### Outline\n\n1. Business problem and summary of results\n2. Data import.\n3. Data preprocessing.\n4. EDA.\n5. Sample split, missing values and feature engineering.\n6. Modeling.\n7. Interpretation of the results.\n8. Business implications.\n\n\n\n### 1. Business problem\n\nLending Club was a peer to peer lending platform. Over 2007-2020 it issued at least 2.9M loans. Loans varied in size between \\\\$1,000 and \\\\$40,000 and were used to refinance consumer loans or to cover midsize expenses.\n\nAfter accepting an application for a loan, Lending Club used internal risk model to assign credit rating and an interest rate to a loan. Then it placed the loan on online platform, where investors could invest in it. Investors fully bore credit risk in return for getting interest payments. \n\nThus the key problem of investors in Lending Club was to determine riskiness of a loan. Investors' objective is to avoid risky loans, in which interest rate is not enough to cover expected credit loss. While Lending Club no longer engages in peer to peer lending, there is a number of P2P lending platforms, operating right now. The goal of this project is to solve investors' problem by building ML model.\n\n\n#### Objective: Predict delinquency of a loan borrower.\n\n\n#### Metric: Precision at 10% Recall.\n\n\n#### Summary of Results\nâ€‹\n**I build XGBoost model to predict loan delinquencies. The model has 60.3% precision at 10% recall. It allows Lending Club investors to save \\\\$59.7M by avoiding the riskiest loans. The savings come from allocating investment funds into risk-free rate instead of the riskiest loans.** \n","metadata":{}},{"cell_type":"markdown","source":"##### Notes:\n\nOne big question about this data is which features are available at the origination time and are never updated afterwards. It seems that most features are pulled at loan application/origination. \nThe only features, pulled later, belong to the two groups:\n- features, related to loan performance/payments.\n- features, which clearly mention this in name/description. E.g., word 'last' in feature name.\n\nI will use only features, known before loan issuance.","metadata":{}},{"cell_type":"code","source":"!pip install pycodestyle","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 2. Data import\n\nTo make analysis manageable, I use 30% sample of the original dataset with 2.9M loans. This is a random sample. If you ramdomly sample original data and use this code, you should get very similar results.","metadata":{"execution":{"iopub.status.busy":"2022-08-13T20:35:49.837008Z","iopub.execute_input":"2022-08-13T20:35:49.837358Z","iopub.status.idle":"2022-08-13T20:36:00.182203Z","shell.execute_reply.started":"2022-08-13T20:35:49.83733Z","shell.execute_reply":"2022-08-13T20:36:00.180868Z"}}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport os, time, warnings, gzip, gc, random, math, shap, pickle, optuna, csv, sys, re\nfrom IPython.display import display\nfrom matplotlib_venn import venn2, venn2_circles, venn2_unweighted\nfrom matplotlib_venn import venn3, venn3_circles\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split, KFold\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, precision_recall_curve, auc\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, roc_auc_score\nfrom sklearn.inspection import permutation_importance\nfrom category_encoders import MEstimateEncoder\nfrom xgboost import XGBClassifier\n\npd.set_option('display.max_columns', 5000)\npd.set_option('display.max_rows', 400)\n\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:03.631769Z","iopub.execute_input":"2022-08-16T14:05:03.632177Z","iopub.status.idle":"2022-08-16T14:05:03.645983Z","shell.execute_reply.started":"2022-08-16T14:05:03.632143Z","shell.execute_reply":"2022-08-16T14:05:03.64463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### target encoding ###\n# source: https://www.kaggle.com/code/ryanholbrook/feature-engineering-for-house-prices/notebook\n\nclass CrossFoldEncoder:\n    def __init__(self, encoder, **kwargs):\n        self.encoder_ = encoder\n        self.kwargs_ = kwargs  # keyword arguments for the encoder\n        self.cv_ = KFold(n_splits=4)\n\n    # Fit an encoder on one split and transform the feature on the\n    # other. Iterating over the splits in all folds gives a complete\n    # transformation. We also now have one trained encoder on each\n    # fold.\n    def fit_transform(self, X, y, cols):\n        self.fitted_encoders_ = []\n        self.cols_ = cols\n        X_encoded = []\n        for idx_encode, idx_train in self.cv_.split(X):\n            fitted_encoder = self.encoder_(cols=cols, **self.kwargs_)\n            fitted_encoder.fit(\n                X.iloc[idx_encode, :], y.iloc[idx_encode],\n            )\n            X_encoded.append(fitted_encoder.transform(X.iloc[idx_train, :])[cols])\n            self.fitted_encoders_.append(fitted_encoder)\n        X_encoded = pd.concat(X_encoded)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded\n\n    # To transform the test data, average the encodings learned from\n    # each fold.\n    def transform(self, X):\n        from functools import reduce\n\n        X_encoded_list = []\n        for fitted_encoder in self.fitted_encoders_:\n            X_encoded = fitted_encoder.transform(X)\n            X_encoded_list.append(X_encoded[self.cols_])\n        X_encoded = reduce(\n            lambda x, y: x.add(y, fill_value=0), X_encoded_list\n        ) / len(X_encoded_list)\n        X_encoded.columns = [name + \"_encoded\" for name in X_encoded.columns]\n        return X_encoded","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:03.648028Z","iopub.execute_input":"2022-08-16T14:05:03.64851Z","iopub.status.idle":"2022-08-16T14:05:03.661352Z","shell.execute_reply.started":"2022-08-16T14:05:03.648473Z","shell.execute_reply":"2022-08-16T14:05:03.660408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"time0 = time.time()\n\nwith open('../input/lc-800k-sample/LCLoans_141_800k.pkl', 'rb') as pickled_one:\n    df = pickle.load(pickled_one)\n    \ndisplay(df.head())\n\nfeatures_tokeep = ['id', 'loan_status',\n 'loan_amnt', 'funded_amnt', 'funded_amnt_inv', 'term', 'int_rate', 'installment','issue_d',\n 'purpose', 'title', 'initial_list_status', 'application_type',\n 'grade', 'sub_grade', 'fico_range_high',\n 'emp_title', 'emp_length', 'home_ownership', 'annual_inc', 'zip_code', 'addr_state',\n 'dti',           \n 'verification_status', \n 'mo_sin_rcnt_tl', 'mths_since_last_delinq', 'mths_since_last_major_derog', 'mths_since_last_record',\n 'mths_since_recent_bc_dlq', 'mths_since_recent_revol_delinq',\n 'num_tl_op_past_12m', \n 'earliest_cr_line', 'inq_last_6mths', 'inq_fi', 'inq_last_12m',\n 'open_acc', 'acc_open_past_24mths', 'mort_acc', 'total_acc',\n 'avg_cur_bal', 'il_util', 'tot_cur_bal', \n 'revol_bal', 'revol_util', 'max_bal_bc', 'bc_open_to_buy', 'mo_sin_rcnt_rev_tl_op', 'num_actv_rev_tl', 'num_op_rev_tl', 'total_rev_hi_lim',               \n 'delinq_2yrs', 'acc_now_delinq', 'delinq_amnt', 'pub_rec', 'pub_rec_bankruptcies',\n 'annual_inc_joint', 'dti_joint', 'verification_status_joint',\n 'total_bal_ex_mort', 'tot_coll_amt', 'tax_liens', 'percent_bc_gt_75', 'pct_tl_nvr_dlq', \n 'open_rv_12m', 'open_il_12m', 'num_tl_90g_dpd_24m', 'num_tl_30dpd', 'num_tl_120dpd_2m',\n 'num_accts_ever_120_pd',\n 'recoveries', 'total_rec_prncp', 'total_rec_int']\n\ndf = df[features_tokeep]\ngc.collect()\n\nrecoveries = df[df.loan_status.isin(['Charged Off', 'Default'])][[\n    'id', 'loan_status', 'recoveries', 'loan_amnt', 'int_rate', 'total_rec_prncp', 'total_rec_int']]\n\ndf.drop(columns = ['recoveries', 'total_rec_prncp', 'total_rec_int'], inplace=True)\n# this removes all features, not known to investors ex ante.\n\ndf.drop(columns = ['il_util', 'max_bal_bc'], inplace=True)\n# these are useful features, which I will preprocess later\n\ndf.issue_d = df.issue_d.astype('O')\ndf.issue_d = pd.to_datetime(df.issue_d, format='%b-%Y')\ndf['year_issued']=df.issue_d.dt.year\n    \n#df = df.sample(200000, random_state=1)\ndf.reset_index(inplace=True, drop=True)\ndisplay(df.shape, time.time()-time0, df.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:03.681023Z","iopub.execute_input":"2022-08-16T14:05:03.681321Z","iopub.status.idle":"2022-08-16T14:05:05.440858Z","shell.execute_reply.started":"2022-08-16T14:05:03.681295Z","shell.execute_reply":"2022-08-16T14:05:05.43993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Data preprocessing\n\nI preprocess main features and create a target. To get better separation between repaid and defaulted loans I look only at loans, which are Fully paid, Charged off or in Default.","metadata":{}},{"cell_type":"code","source":"### feature description:\n\n#f_desc = pd.read_excel('../input/lending-club-20072020q1/LCDataDictionary.xlsx')\n#f_desc.columns = ['colname','desc']\n#display(f_desc.head())\n#display(f_desc)\n#display(f_desc.loc[f_desc.colname=='open_acc','desc'], f_desc.loc[f_desc.colname=='total_acc','desc'])","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:05.442707Z","iopub.execute_input":"2022-08-16T14:05:05.443687Z","iopub.status.idle":"2022-08-16T14:05:05.448598Z","shell.execute_reply.started":"2022-08-16T14:05:05.443645Z","shell.execute_reply":"2022-08-16T14:05:05.447446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# remove some very rare loan types:\n\ndf = df[~df.purpose.isin(['educational', 'renewable_energy', 'wedding'])]\ndf.purpose = df.purpose.cat.remove_categories(['educational', 'renewable_energy', 'wedding'])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:05.449882Z","iopub.execute_input":"2022-08-16T14:05:05.450691Z","iopub.status.idle":"2022-08-16T14:05:05.685541Z","shell.execute_reply.started":"2022-08-16T14:05:05.450638Z","shell.execute_reply":"2022-08-16T14:05:05.684561Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clean time features\n\ndf.earliest_cr_line = df.earliest_cr_line.astype('O')\ndf.earliest_cr_line = pd.to_datetime(df.earliest_cr_line, format='%b-%Y')\ndf['month_issued']=df.issue_d.dt.month\ndf['year_earliest']=df.issue_d.dt.year\ndf['years_borrowing'] = (df.issue_d - df.earliest_cr_line)/ np.timedelta64(1, 'Y')\ndf['pub_rec_pa'] = df.pub_rec/df.years_borrowing\ndisplay(df.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:05.688259Z","iopub.execute_input":"2022-08-16T14:05:05.688644Z","iopub.status.idle":"2022-08-16T14:05:05.993161Z","shell.execute_reply.started":"2022-08-16T14:05:05.688606Z","shell.execute_reply":"2022-08-16T14:05:05.992018Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# create a target variable\n\ndisplay(df.loan_status.value_counts())\ndf.target=np.nan\n#df.loc[df.loan_status.isin(['Fully Paid', 'Does not meet the credit policy. Status:Fully Paid']), 'target']=0\n#df.loc[df.loan_status.isin(['Charged Off', 'Late (31-120 days)', 'Does not meet the credit policy. Status:Charged Off', 'Default']), 'target']=1\ndf.loc[df.loan_status.isin(['Fully Paid']), 'target']=0\ndf.loc[df.loan_status.isin(['Charged Off', 'Default']), 'target']=1\ndf=df[df['target'].isin([0,1])]\ndisplay(df.shape,df.loan_status.value_counts(), df.count(), sys.getsizeof(df)/1048576)\ndf.drop(columns='loan_status',inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:05.994574Z","iopub.execute_input":"2022-08-16T14:05:05.995553Z","iopub.status.idle":"2022-08-16T14:05:06.424933Z","shell.execute_reply.started":"2022-08-16T14:05:05.995513Z","shell.execute_reply":"2022-08-16T14:05:06.424079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Feature Preprocessing\n\nThere are many features, related to the loan size, which are measured in dollars. To make these features comparable across loans, we should rescale them by income of borrowers.","metadata":{}},{"cell_type":"code","source":"# add key loan features, scaled by borrower's income:\n\ndf.loc[df.annual_inc<1,'annual_inc']=1\ndf['lti']=df.loan_amnt/df.annual_inc\ndf['iti']=(df.installment*12)/df.annual_inc\ndf.loc[df.lti==np.inf, 'lti']=np.nan\ndf.loc[df.lti>1.5, 'lti']=1.5\ndf.loc[df.iti==np.inf, 'iti']=np.nan\ndf.loc[df.iti>1, 'iti']=1\ndf.loc[df.revol_util>100,'revol_util']=100\ndf.loc[df.dti>100, 'dti']=100\ndf.loc[df.dti<0, 'dti']=0\n\ndf['revol_balance_income'] = df.revol_bal/df.annual_inc\ndf['avg_cur_bal_inc'] = df.avg_cur_bal/df.annual_inc\ndf['tot_cur_bal_inc'] = df.tot_cur_bal/df.annual_inc\ndf['total_bal_ex_mort_inc'] = df.total_bal_ex_mort/df.annual_inc\ndf['total_rev_inc'] = df.total_rev_hi_lim/df.annual_inc\ndf['open_cl_ratio']=df.open_acc/df.total_acc\n\n# add more features\n\ndf['zip_code'] = df.zip_code.str.rstrip('xx').astype(int)\ndf['joint'] = df.dti_joint.notnull().astype(int)\ndf['emp_length'] = df.emp_length.str.rstrip(' years')\ndf.loc[df.emp_length=='< 1','emp_length'] = 0\ndf.loc[df.emp_length=='10+','emp_length'] = 10\ndf['emp_length'] = df.emp_length.astype(np.float32)\ndisplay(df.emp_length.value_counts())\ndf.amnt_same = (df.loan_amnt == df.funded_amnt_inv).astype(int)\ndf['low_fico'] = (df.fico_range_high<=659).astype(int)\ndf.loc[df.home_ownership.isin(['ANY','NONE','OTHER']), 'home_ownership'] = 'OTHER'\ndf['was_bankrupt'] = (df.pub_rec_bankruptcies>0).astype(int)\n\ndf.drop(columns = ['annual_inc_joint', 'dti_joint', 'verification_status_joint', 'earliest_cr_line', 'issue_d'], inplace=True)\n\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:06.426202Z","iopub.execute_input":"2022-08-16T14:05:06.426825Z","iopub.status.idle":"2022-08-16T14:05:06.837469Z","shell.execute_reply.started":"2022-08-16T14:05:06.426793Z","shell.execute_reply":"2022-08-16T14:05:06.83512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For features like 'time_since some credit event', treat NA as never and fill those values with 100 years equivalent\n\ndf.mo_sin_rcnt_tl = df.mo_sin_rcnt_tl.fillna(value=120)\ndf.num_tl_op_past_12m = df.num_tl_op_past_12m.fillna(value=0)\n\nmonths_since_col = ['mths_since_last_delinq', 'mths_since_last_major_derog', \n                    'mths_since_last_record', 'mths_since_recent_bc_dlq', 'mths_since_recent_revol_delinq']\n\nfor col in months_since_col:\n    df[col] = df[col].fillna(value=1200)\n\n#display(df.count())\n\ndf.inq_fi = df.inq_fi.fillna(value=0)\ndf.inq_last_12m = df.inq_last_12m.fillna(value=0)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:06.838884Z","iopub.execute_input":"2022-08-16T14:05:06.840109Z","iopub.status.idle":"2022-08-16T14:05:06.87094Z","shell.execute_reply.started":"2022-08-16T14:05:06.840039Z","shell.execute_reply":"2022-08-16T14:05:06.870066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('Bankruptcy incidence is the sample is: ', df.target.mean())","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:06.872341Z","iopub.execute_input":"2022-08-16T14:05:06.873425Z","iopub.status.idle":"2022-08-16T14:05:06.881133Z","shell.execute_reply.started":"2022-08-16T14:05:06.873364Z","shell.execute_reply":"2022-08-16T14:05:06.880086Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Loan delinquencies account for 19.5% instances. This is an imbalanced dataset. We should keep this in mind while performing model evaluation.","metadata":{}},{"cell_type":"code","source":"cat_features_te = ['sub_grade', 'emp_title', 'purpose', 'title', 'zip_code', 'addr_state', 'grade', 'home_ownership']\ncat_features_ohe = ['verification_status', 'initial_list_status', 'application_type']","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:06.88267Z","iopub.execute_input":"2022-08-16T14:05:06.884699Z","iopub.status.idle":"2022-08-16T14:05:06.890239Z","shell.execute_reply.started":"2022-08-16T14:05:06.884661Z","shell.execute_reply":"2022-08-16T14:05:06.889168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The code above lists categorical columns, for which I will use target encoding or one-hot encoding.","metadata":{"execution":{"iopub.status.busy":"2022-08-14T23:15:15.771521Z","iopub.execute_input":"2022-08-14T23:15:15.771958Z","iopub.status.idle":"2022-08-14T23:15:15.789189Z","shell.execute_reply.started":"2022-08-14T23:15:15.771895Z","shell.execute_reply":"2022-08-14T23:15:15.788078Z"}}},{"cell_type":"markdown","source":"### 4. EDA\n\nIn this subsection I look at the distribution of key loan features.","metadata":{}},{"cell_type":"code","source":"display(df[['loan_amnt', 'funded_amnt', 'funded_amnt_inv']].describe())\ndisplay(df.term.value_counts())","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:06.895044Z","iopub.execute_input":"2022-08-16T14:05:06.896338Z","iopub.status.idle":"2022-08-16T14:05:06.980728Z","shell.execute_reply.started":"2022-08-16T14:05:06.896301Z","shell.execute_reply":"2022-08-16T14:05:06.979732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"So the three amount features above are basically identical.","metadata":{}},{"cell_type":"code","source":"sns.countplot(y='purpose', data=df)\nplt.show()\nsns.countplot(y='emp_length', data=df)\nplt.show()\nsns.histplot(x='lti', data=df).set(title='Loan LTI')\nplt.show()\nsns.histplot(x='iti', data=df).set(title='Loan ITI')\nplt.show()\nsns.histplot(x='annual_inc', data=df, log_scale=True).set(title='Annual Income')\nplt.show()\nsns.histplot(x='dti', data=df).set(title='Loan DTI')\nplt.show()\nsns.histplot(x='grade', data=df).set(title='Loan Grade')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:06.982406Z","iopub.execute_input":"2022-08-16T14:05:06.983089Z","iopub.status.idle":"2022-08-16T14:05:13.412065Z","shell.execute_reply.started":"2022-08-16T14:05:06.983034Z","shell.execute_reply":"2022-08-16T14:05:13.411012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Debt consolidation and credit card refinancing are the most frequent purposes for loans. It makes sense given maximum \\\\$40,000 loan size. Annual income is very approximately Gaussian and is centered around \\\\$100,000.\nKey credit metrics as a fraction of income have pretty tight distributions with few outliers. It makes sense, since people with little to no income are unlikely to be good borrowers.","metadata":{"execution":{"iopub.status.busy":"2022-08-16T02:30:18.327437Z","iopub.execute_input":"2022-08-16T02:30:18.328133Z","iopub.status.idle":"2022-08-16T02:30:18.335573Z","shell.execute_reply.started":"2022-08-16T02:30:18.328091Z","shell.execute_reply":"2022-08-16T02:30:18.333933Z"}}},{"cell_type":"markdown","source":"### EDA with target\n\nHere I explore univariate relations between target and key features.","metadata":{}},{"cell_type":"code","source":"df.open_acc = df.open_acc.astype(int)\ndf.delinq_2yrs = df.delinq_2yrs.astype(int)\ndf.pub_rec = df.pub_rec.astype(int)\ndf.fico_range_high = df.fico_range_high.astype(int)\n\nsns.set(rc={'figure.figsize':(12,8)})\nsns.barplot(x='grade', y='target', data=df)\nplt.show()\nsns.barplot(x='sub_grade', y='target', data=df)\nplt.show()\nsns.barplot(x='home_ownership', y='target', data=df)\nplt.show()\nsns.barplot(x='delinq_2yrs', y='target', data=df)\nplt.show()\nsns.barplot(x='pub_rec', y='target', data=df)\nplt.show()\nsns.barplot(x='pub_rec_bankruptcies', y='target', data=df)\nplt.show()\nsns.set(rc={'figure.figsize':(20,8)})\nsns.barplot(x='open_acc', y='target', data=df)\nplt.show()\n#sns.barplot(x='total_acc', y='target', data=df)\n#plt.show()\nsns.barplot(x='fico_range_high', y='target', data=df)\nplt.show()\nsns.barplot(x='purpose', y='target', data=df)\nplt.show()\nsns.barplot(x='year_issued', y='target', data=df)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:05:13.413298Z","iopub.execute_input":"2022-08-16T14:05:13.413654Z","iopub.status.idle":"2022-08-16T14:06:22.172939Z","shell.execute_reply.started":"2022-08-16T14:05:13.413617Z","shell.execute_reply":"2022-08-16T14:06:22.171934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Unsurprisingly, default rate is strongly positively correlated with loan grade. Low-grade loans are much more likely to default. Similarly, higher FICO ratings lead to lower default probability","metadata":{}},{"cell_type":"code","source":"a = df.dti\nb = df.target\nbins = np.linspace(a.min(), a.max() + 1e-12, 51) \nc = np.digitize(a, bins)\nplt.bar(bins[:-1], [np.mean(b[c == i]) for i in range(1, len(bins))],\n        width=bins[1] - bins[0], align='edge', fc='turquoise', ec='black')\nplt.xticks(bins)\nplt.margins(x=0.02) # smaller margins\nplt.show()\n\na = df.int_rate\nb = df.target\nbins = np.linspace(a.min(), a.max() + 1e-12, 26) \nc = np.digitize(a, bins)\nplt.bar(bins[:-1], [np.mean(b[c == i]) for i in range(1, len(bins))],\n        width=bins[1] - bins[0], align='edge', fc='turquoise', ec='black')\nplt.xticks(bins)\nplt.margins(x=0.02) # smaller margins\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:06:22.174626Z","iopub.execute_input":"2022-08-16T14:06:22.175312Z","iopub.status.idle":"2022-08-16T14:06:23.793884Z","shell.execute_reply.started":"2022-08-16T14:06:22.175272Z","shell.execute_reply":"2022-08-16T14:06:23.792981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"There is strong positive relation between interest rate and default probability. This is to be expected, since otherwise it would mean very bad internal risk modeling at Lending Club.\n\nThere are interesting results regarding debt-to-income ratio. Default probability is predictably rising until DTI reaches 40%. Then it levels off and even to seem to fall after 60%. Due to sall number of loans with such a high DTI, it is hard to say whether this pattern is statistically significant.","metadata":{}},{"cell_type":"markdown","source":"### 5. Train-test split, missing values and feature engineering\n\nIn this section I fill missing values from train set and encode categorical features.\n\nFor categorical features, which will be target-encoded later, I fill missing values with 'MISSING' category.\n\nTo mitigate effect of outliers, I use median to impute values of missing numerical features.","metadata":{}},{"cell_type":"code","source":"#display(df.count())\n\nfeatures_fill_M = ['emp_title', 'title']\nfeatures_fill_med = ['dti', 'delinq_2yrs', 'inq_last_6mths', 'open_acc', 'pub_rec', 'revol_util', \n                     'total_acc', 'mort_acc', 'pub_rec_bankruptcies', 'bc_open_to_buy','tot_cur_bal_inc', 'total_rev_inc',\n                    'total_bal_ex_mort_inc', 'pct_tl_nvr_dlq', 'percent_bc_gt_75', 'avg_cur_bal_inc',\n                    'mo_sin_rcnt_rev_tl_op', 'num_actv_rev_tl', 'num_op_rev_tl', 'tax_liens']\nfeatures_fill_zero = ['open_rv_12m', 'open_il_12m', 'emp_length', 'num_tl_90g_dpd_24m', \n                      'num_tl_30dpd', 'num_tl_120dpd_2m', 'num_accts_ever_120_pd',\n                     'acc_open_past_24mths', 'tot_coll_amt']\n\nfor col in features_fill_zero:\n    df[col] = df[col].fillna(value=0)\n\nfor col in features_fill_M:\n    df[col] = df[col].cat.add_categories(['MISSING']) \n    df[col] = df[col].fillna(value='MISSING')\n    \ntest_size = 0.25\ndf.reset_index(inplace=True, drop=True)\nrandom.seed(2)\ntest_index = random.sample(list(df.index), int(test_size*df.shape[0]))\ntrain = df.iloc[list(set(df.index)-set(test_index))]\ntest = df.iloc[test_index]\ntrain.reset_index(drop=True, inplace=True)\ntest.reset_index(drop=True, inplace=True)\ntest00 = test.copy()\ntrain.drop(columns=['id'],inplace=True)\ntest.drop(columns=['id'],inplace=True)\ndisplay(train.shape, test.shape, train.head(3), test.head(3))\n\nfor col in features_fill_med:\n    train[col] = train[col].fillna(train[col].median())\n    test[col] = test[col].fillna(train[col].median())\n    \ntrain['total_rev_hi_lim'] = train.total_rev_inc.median()*train.annual_inc\ntest['total_rev_hi_lim'] = train.total_rev_inc.median()*test.annual_inc\n\ntrain['total_bal_ex_mort'] = train.total_bal_ex_mort_inc.median()*train.annual_inc\ntest['total_bal_ex_mort'] = train.total_bal_ex_mort_inc.median()*test.annual_inc\n\ntrain['tot_cur_bal'] = train.tot_cur_bal_inc.median()*train.annual_inc\ntest['tot_cur_bal'] = train.tot_cur_bal_inc.median()*test.annual_inc\n\ntrain['avg_cur_bal'] = train.avg_cur_bal_inc.median()*train.annual_inc\ntest['avg_cur_bal'] = train.avg_cur_bal_inc.median()*test.annual_inc\n\ndisplay(train.count())","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:06:23.795427Z","iopub.execute_input":"2022-08-16T14:06:23.795793Z","iopub.status.idle":"2022-08-16T14:06:24.992005Z","shell.execute_reply.started":"2022-08-16T14:06:23.795756Z","shell.execute_reply":"2022-08-16T14:06:24.991007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical features encoding\n\nDepending of feature type, I use three encoding types:\n\n- Target encoding. I do it for feature with more than 5 unique values.\n- One-hot encoding. I use it for features with less than 5 unique values.\n- Frequency encoding. 'Title' feature only.","metadata":{"execution":{"iopub.status.busy":"2022-08-07T21:51:08.677332Z","iopub.execute_input":"2022-08-07T21:51:08.677854Z","iopub.status.idle":"2022-08-07T21:51:08.686502Z","shell.execute_reply.started":"2022-08-07T21:51:08.677811Z","shell.execute_reply":"2022-08-07T21:51:08.685019Z"}}},{"cell_type":"code","source":"# TE for categorical features\n\ntime1 = time.time()\nencoder = CrossFoldEncoder(MEstimateEncoder, m=10)\ntrain_encoded = encoder.fit_transform(train, train.target, cols=cat_features_te)\ntest_encoded = encoder.transform(test)\n\nfreq_enc = (train.groupby('title').size()) / len(train)\ntrain['title_fencoded'] = train['title'].apply(lambda x : freq_enc[x])\ntest['title_fencoded'] = test['title'].apply(lambda x : freq_enc[x])\n\ntrain.drop(columns=cat_features_te, inplace=True)\ntest.drop(columns=cat_features_te,  inplace=True)\ntrain = pd.concat([train, train_encoded], axis = 1)\ntest = pd.concat([test, test_encoded], axis = 1)\n\ndisplay(time.time()-time0, time.time()-time1)\ndisplay(train.shape, train.head(), train.count())\ntrain0 = train.copy()\ntest0 = test.copy()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:06:24.993633Z","iopub.execute_input":"2022-08-16T14:06:24.994021Z","iopub.status.idle":"2022-08-16T14:06:36.635473Z","shell.execute_reply.started":"2022-08-16T14:06:24.993982Z","shell.execute_reply":"2022-08-16T14:06:36.634465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train.copy()\ny_train = X_train.pop('target')\nX_test = test.copy()\ny_test = X_test.pop('target')\ndisplay(X_test.head())\n\n### Do OHE for some features ###\n\nfeature_transformer = ColumnTransformer([\n    (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\", drop='if_binary'), cat_features_ohe)], remainder=\"passthrough\")\n\nprint('Number of features before transaformation: ', X_train.shape)\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nX_train.columns = X_train.columns.str.replace(r'^cat__', '').str.replace(r'^remainder__', '')\nX_test.columns = X_test.columns.str.replace(r'^cat__', '').str.replace(r'^remainder__', '')\n\nprint('time to do feature proprocessing: ', time.time()-time1)\nprint('Number of features after transaformation: ', X_train.shape)\nX_train.head()\nX_test_0 = X_test.copy()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:06:36.637063Z","iopub.execute_input":"2022-08-16T14:06:36.637429Z","iopub.status.idle":"2022-08-16T14:06:38.150315Z","shell.execute_reply.started":"2022-08-16T14:06:36.637394Z","shell.execute_reply":"2022-08-16T14:06:38.149123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6. Modeling\n\nHere I will build 2 models to predict loan bankruptcies:\n\n- Baseline XGBoost model (default hyperparameters).\n- XGBoost model with hyperparameter optimization using Optuna.","metadata":{}},{"cell_type":"code","source":"time1 = time.time()\nxgb = XGBClassifier(tree_method = 'gpu_hist')\nxgb.fit(X_train, y_train)\n\nprecision_t, recall_t, threshold = precision_recall_curve(y_train, xgb.predict_proba(X_train)[:, 1])\nauc_precision_recall_train = auc(recall_t, precision_t)\ntemp = recall_t[(recall_t>0.095)&(recall_t<0.105)]\ntemp = temp[int(len(temp)/2)]\nindexx = ((np.where(recall_t==temp)))[0][0]\nr10prec_train = precision_t[indexx]\nprecision_t, recall_t, threshold = precision_recall_curve(y_test, xgb.predict_proba(X_test)[:, 1])\nauc_precision_recall_test = auc(recall_t, precision_t)\ntemp = recall_t[(recall_t>0.095)&(recall_t<0.105)]\ntemp = temp[int(len(temp)/2)]\nindexx = ((np.where(recall_t==temp)))[0][0]\nr10prec_test = precision_t[indexx]\n\ndisplay('Train Accuracy: ', accuracy_score(y_train,xgb.predict(X_train)))\ndisplay('F1 score: ', f1_score(y_train,xgb.predict(X_train)))\ndisplay('ROCAUC: ', roc_auc_score(y_train,xgb.predict(X_train)))\ndisplay('PRAUC: ', auc_precision_recall_train)\ndisplay('R10P: ', r10prec_train)\n\n# Performance evaluation:\ndisplay('Test Accuracy: ', accuracy_score(y_test,xgb.predict(X_test)))\ndisplay('F1 score: ', f1_score(y_test,xgb.predict(X_test)))\ndisplay('ROCAUC: ', roc_auc_score(y_test,xgb.predict(X_test)))\ndisplay('PRAUC: ', auc_precision_recall_test)\ndisplay('R10P: ', r10prec_test)\ndisplay(time.time()-time1)\n\nfig, ax = plt.subplots()\nax.plot(recall_t, precision_t, color='purple')\nax.set_title('Precision-Recall Curve, test')\nax.set_ylabel('Precision')\nax.set_xlabel('Recall')\nax.set_ylim(bottom=0, top=1.02)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:06:38.15195Z","iopub.execute_input":"2022-08-16T14:06:38.152358Z","iopub.status.idle":"2022-08-16T14:06:47.514479Z","shell.execute_reply.started":"2022-08-16T14:06:38.152319Z","shell.execute_reply":"2022-08-16T14:06:47.513513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In order to save computing time for users of this notebook, I save hyperparameter values, which Optuna converged upon. If you want to run hyperparameter optimization yourself, uncomment the code, performing hyperparameter optimization below. That is, everything between 'def objective' and 'optuna_hyperpars' two cells below. Due to random elements of XGBoost, the results may be somwhat different.","metadata":{"execution":{"iopub.status.busy":"2022-08-16T00:06:39.925492Z","iopub.execute_input":"2022-08-16T00:06:39.926171Z","iopub.status.idle":"2022-08-16T00:06:39.934023Z","shell.execute_reply.started":"2022-08-16T00:06:39.926135Z","shell.execute_reply":"2022-08-16T00:06:39.932393Z"}}},{"cell_type":"code","source":"# here are starting hyperparameters for Optuna optimization\n\nstarting_hyperparameters = {'n_estimators': 1186,\n 'max_depth': 4,\n 'learning_rate': 0.04582452146963614,\n 'colsample_bytree': 0.600410879432218,\n 'subsample': 0.5561312005431341,\n 'alpha': 5.0981168922187665,\n 'lambda': 4.090612170344411,\n 'gamma': 0.0029390495081042726,\n 'min_child_weight': 0.21653462492301606,\n 'scale_pos_weight': 1,\n 'tree_method': 'gpu_hist'}","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:06:47.51601Z","iopub.execute_input":"2022-08-16T14:06:47.516645Z","iopub.status.idle":"2022-08-16T14:06:47.522604Z","shell.execute_reply.started":"2022-08-16T14:06:47.516604Z","shell.execute_reply":"2022-08-16T14:06:47.521632Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Fit XGBoost using Optuna hyperparameter optimization ###\n\ntime1 = time.time()\n\n# def objective(trial, cv_runs=1, n_splits=2, n_jobs=-1):\n\n#     cv_regularizer=0.01\n#     # Usually values between 0.1 and 0.2 work fine.\n\n#     params = {\n#         \"tree_method\": 'gpu_hist',\n#         \"verbosity\": 0,  # 0 (silent) - 3 (debug)\n#         \"n_estimators\": trial.suggest_int(\"n_estimators\", 500, 1500),\n#         \"max_depth\": trial.suggest_int(\"max_depth\", 2, 6),\n#         \"learning_rate\": trial.suggest_uniform(\"learning_rate\", 0.005, 0.2),\n#         \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0.1, 0.95),\n#         \"subsample\": trial.suggest_uniform(\"subsample\", 0.5, 0.95),\n#         \"alpha\": trial.suggest_loguniform(\"alpha\", 0.1, 10.0),\n#         \"lambda\": trial.suggest_loguniform(\"lambda\", 0.1, 150.0),\n#         \"gamma\": trial.suggest_loguniform(\"gamma\", 1e-10, 10.0),\n#         \"min_child_weight\": trial.suggest_loguniform(\"min_child_weight\", 0.1, 10),\n#         \"n_jobs\": n_jobs,\n#         \"scale_pos_weight\": trial.suggest_uniform(\"scale_pos_weight\", 1, 3),\n#     }\n#     # usually it makes sense to resrtict hyperparameter space from some solutions which Optuna will find\n#     # e.g., for tmx-joined data only (downsampled tmx), optuna keeps selecting depths of 2 and 3.\n#     # for my purposes (smooth left side of prc, close to 1), those solutions are no good.\n\n#     temp_out = []\n\n#     for i in range(cv_runs):\n\n#         X = X_train\n#         y = y_train\n\n#         model = XGBClassifier(**params)\n#         rkf = KFold(n_splits=n_splits, shuffle=True)\n#         X_values = X.values\n#         y_values = y.values\n#         y_pred = np.zeros_like(y_values)\n#         y_pred_train = np.zeros_like(y_values)\n#         for train_index, test_index in rkf.split(X_values):\n#             X_A, X_B = X_values[train_index, :], X_values[test_index, :]\n#             y_A, y_B = y_values[train_index], y_values[test_index]\n#             model.fit(X_A, y_A, eval_set=[(X_B, y_B)], verbose = False)\n#             y_pred[test_index] += model.predict_proba(X_B)[:, 1]\n#             #y_pred_train[train_index] += model.predict_proba(X_A)[:, 1]\n                      \n#         #precision_t, recall_t, threshold = precision_recall_curve(y_train, y_pred_train)\n#         #score_train = auc(recall_t, precision_t)\n#         precision_t, recall_t, threshold = precision_recall_curve(y_train, y_pred)\n#         score_test = auc(recall_t, precision_t)\n            \n#         #score_train = roc_auc_score(y_train, y_pred_train)\n#         #score_test = roc_auc_score(y_train, y_pred) \n#         #overfit = score_train-score_test\n#         #temp_out.append(score_test-cv_regularizer*overfit)\n#         temp_out.append(score_test)\n\n#     return (np.mean(temp_out))\n\n# study = optuna.create_study(direction=\"maximize\")\n# study.optimize(objective, n_trials=50)\n\n# # study = optuna.create_study(direction=\"maximize\")\n# # study.enqueue_trial(starting_hyperparameters)\n# # study.optimize(objective, n_trials=50)\n\n# print('Total time for hypermarameter optimization ', time.time()-time1)\n# hp = study.best_params\n# for key, value in hp.items():\n#     print(f\"{key:>20s} : {value}\")\n# print(f\"{'best objective value':>20s} : {study.best_value}\")\n\n# optuna_hyperpars = study.best_params\n# optuna_hyperpars['tree_method']='gpu_hist'\n# optuna_hyperpars['scale_pos_weight']=1\n\noptuna_hyperpars = starting_hyperparameters\n\noptuna_xgb = XGBClassifier(**optuna_hyperpars, seed=8)\noptuna_xgb.fit(X_train, y_train)\n\nprecision_t, recall_t, threshold = precision_recall_curve(y_train, optuna_xgb.predict_proba(X_train)[:, 1])\nauc_precision_recall_train = auc(recall_t, precision_t)\ntemp = recall_t[(recall_t>0.095)&(recall_t<0.105)]\ntemp = temp[int(len(temp)/2)]\nindexx = ((np.where(recall_t==temp)))[0][0]\nr10prec_train = precision_t[indexx]\n\nfig, ax = plt.subplots()\nax.plot(recall_t, precision_t, color='purple')\nax.set_title('Precision-Recall Curve, train')\nax.set_ylabel('Precision')\nax.set_xlabel('Recall')\nax.set_ylim(bottom=0, top=1.02)\nplt.show()\n\nprecision_t, recall_t, threshold = precision_recall_curve(y_test, optuna_xgb.predict_proba(X_test)[:, 1])\nauc_precision_recall_test = auc(recall_t, precision_t)\ntemp = recall_t[(recall_t>0.095)&(recall_t<0.105)]\ntemp = temp[int(len(temp)/2)]\nindexx = ((np.where(recall_t==temp)))[0][0]\nr10prec_test = precision_t[indexx]\n\nfig, ax = plt.subplots()\nax.plot(recall_t, precision_t, color='purple')\nax.set_title('Precision-Recall Curve, test')\nax.set_ylabel('Precision')\nax.set_xlabel('Recall')\nax.set_ylim(bottom=0, top=1.02)\nplt.show()\n\ndisplay('Train Accuracy: ', accuracy_score(y_train,optuna_xgb.predict(X_train)))\ndisplay('F1 score: ', f1_score(y_train,optuna_xgb.predict(X_train)))\ndisplay('ROCAUC: ', roc_auc_score(y_train,optuna_xgb.predict(X_train)))\ndisplay('PRAUC: ', auc_precision_recall_train)\ndisplay('R10P: ', r10prec_train)\n# Performance evaluation:\ndisplay('Test Accuracy: ', accuracy_score(y_test,optuna_xgb.predict(X_test)))\ndisplay('F1 score: ', f1_score(y_test,optuna_xgb.predict(X_test)))\ndisplay('ROCAUC: ', roc_auc_score(y_test,optuna_xgb.predict(X_test)))\ndisplay('PRAUC: ', auc_precision_recall_test)\ndisplay('R10P: ', r10prec_test)\ndisplay('Time to do hyperparameter optimization: ', time.time()-time1)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:06:47.524329Z","iopub.execute_input":"2022-08-16T14:06:47.52479Z","iopub.status.idle":"2022-08-16T14:07:29.775771Z","shell.execute_reply.started":"2022-08-16T14:06:47.524752Z","shell.execute_reply":"2022-08-16T14:07:29.774848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Optuna hyperparameter optimization delivers significant improvement in a performance over baseline XGBoost model. Due to faster convergence and faster runtime I prefer Optuna to GridSearch or RandomizedSearch.","metadata":{}},{"cell_type":"markdown","source":"### 7. Model interpretation\n\nIn this section I use SHAP library in order to provide interpretation of the results.\n\nFeature importances tell us which features give the model its predictive power. SHAP values indicate how certain values of each feature affect model predictions.","metadata":{}},{"cell_type":"code","source":"# template here: https://www.kaggle.com/code/kaanboke/catboost-lightgbm-xgboost-explained-by-shap/notebook\nexplainerxgbc = shap.TreeExplainer(optuna_xgb)\nshap_values_XGBoost_test = explainerxgbc.shap_values(X_test)\nshap_values_XGBoost_train = explainerxgbc.shap_values(X_train)\n\nvals = np.abs(shap_values_XGBoost_test).mean(0)\nfeature_names = X_test.columns\nfeature_importance = pd.DataFrame(list(zip(feature_names, vals)),\n                                 columns=['col_name','feature_importance_vals'])\nfeature_importance.sort_values(by=['feature_importance_vals'],\n                              ascending=False, inplace=True)\n#display(feature_importance)\n\nshap.summary_plot(shap_values_XGBoost_test, X_test, plot_type=\"bar\", plot_size=(8,8), max_display=30)\nshap.summary_plot(shap_values_XGBoost_train, X_train,plot_type=\"dot\", plot_size=(8,8), max_display=20)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:07:29.777273Z","iopub.execute_input":"2022-08-16T14:07:29.777798Z","iopub.status.idle":"2022-08-16T14:08:36.402872Z","shell.execute_reply.started":"2022-08-16T14:07:29.777751Z","shell.execute_reply":"2022-08-16T14:08:36.401932Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that subgrade is the most important feature. SHAP values of encoded subgrade are positively related to loan default risk. This is to beexpected, since by construction target encodings measure average value of a target. So we would expect all target-encoded features to positively predict defaults. \n\nThe other important features are loan term, number of accounts, opened in the past 24 months, LTI (Loan-to_Income ratio) and DTI (Debt-toIncome ratio). Their partial efects make sense: Higher LTI, DTI and higher number of recently opened accounts are positively related to loan defaults.","metadata":{}},{"cell_type":"markdown","source":"The code below explains model predictions for a specific instances. For example, it illustrates why the model predicts no default for a loan at index 1 and default for a loan with index 10.","metadata":{}},{"cell_type":"code","source":"indx = 1\n#fig = plt.subplots(figsize=(2,2),dpi=200)\nax_2= shap.decision_plot(explainerxgbc.expected_value, shap_values_XGBoost_test[indx], X_test.iloc[indx],link= \"logit\")\nshap.initjs()\nshap.force_plot(explainerxgbc.expected_value, shap_values_XGBoost_test[indx], X_test.iloc[[indx]],link= \"logit\")","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:08:36.404439Z","iopub.execute_input":"2022-08-16T14:08:36.40514Z","iopub.status.idle":"2022-08-16T14:08:37.07403Z","shell.execute_reply.started":"2022-08-16T14:08:36.4051Z","shell.execute_reply":"2022-08-16T14:08:37.073102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This loan has high subgrade and low interest rate. Its other features are the features, common for well-performing loans. Thus model confidently predicts no default.","metadata":{"execution":{"iopub.status.busy":"2022-08-16T03:08:16.14954Z","iopub.execute_input":"2022-08-16T03:08:16.150787Z","iopub.status.idle":"2022-08-16T03:08:16.158489Z","shell.execute_reply.started":"2022-08-16T03:08:16.150741Z","shell.execute_reply":"2022-08-16T03:08:16.156901Z"}}},{"cell_type":"code","source":"indx = 10\n#fig = plt.subplots(figsize=(2,2),dpi=200)\nax_2= shap.decision_plot(explainerxgbc.expected_value, shap_values_XGBoost_test[indx], X_test.iloc[indx],link= \"logit\")\nshap.initjs()\nshap.force_plot(explainerxgbc.expected_value, shap_values_XGBoost_test[indx], X_test.iloc[[indx]],link= \"logit\")","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:08:37.075748Z","iopub.execute_input":"2022-08-16T14:08:37.076471Z","iopub.status.idle":"2022-08-16T14:08:37.735533Z","shell.execute_reply.started":"2022-08-16T14:08:37.076422Z","shell.execute_reply":"2022-08-16T14:08:37.734324Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"This loan looks very different. It has very low rating (i.e., subgrade), high LTI, high interest rate and long maturity. These features explain why the model assigns default probability of 55% to this loan.","metadata":{"execution":{"iopub.status.busy":"2022-08-16T00:14:20.502516Z","iopub.execute_input":"2022-08-16T00:14:20.502957Z","iopub.status.idle":"2022-08-16T00:14:20.514042Z","shell.execute_reply.started":"2022-08-16T00:14:20.50292Z","shell.execute_reply":"2022-08-16T00:14:20.512531Z"}}},{"cell_type":"markdown","source":"### 8. Business implications","metadata":{}},{"cell_type":"markdown","source":"#### 8.1 Is this model useful?\n\nThis project is a perfect illustration of how machine learning can create value if we think hard about business problem at hand rather than focusing on a few narrow model evaluation metrics.\n\nIf you view this project as a purely data mining/prediction exercise, then it is arguably a failure. Model accuracy is 81% which barely beats 80% from a trivial model, always predicting no default. F1 score is only 18%, which is not impressive. ROCAUC is 55% which is barely better than a random guess.\n\nHowever, the predictive model above can create value if used properly. Let me show how.\n\nPrecision-recall curve above shows that model is actually very good in predicting defaults for around 20% of defaulted loans. For example, its precision at 10% recall is 60%. This means that if we use the model only to predict default for those loans it is most confident about, we can avoid at least 10% of credit losses while forgoing relatively few profitable lending opportunities. ","metadata":{}},{"cell_type":"markdown","source":"#### 8.2 How much value does it create?\n\nTo quantify value created by the model, I consider a problem faced by all Lending Club investors as a group to maximize their investment profit. They can do so by avoiding x% of the riskiest loans as determined by my model. In order to keep this analysis simple and accessible to wider audience I abstract away from time discounting, compound interest and present value calculations. All the calculations below assume simple interest.\n\nThe decision of an investor to avoid financing x% of the riskiest loans is a tradeoff between a foregone interest income and a credit loss. \n\nIf investors invest in a loan:\n- They receive interest income in the future as long as the borrower repays a loan.\n- They suffer credit losses and receive only recovered amount if the borrower defaults.\n\nSo their total dollar return (TR) from investment is as follows:\n\n$$ TR_{Invest} = \\sum_{j \\in RepaidLoans}LoanAmount_j*(r_j+1) + \\sum_{k \\in DefaultedLoans}Recovery_k.$$\n\nTheir outisde riskless option is to invest in Treasury bills with similar maturity. Average yield of 3-year treasury notes over this sample period [is around 2%](https://fred.stlouisfed.org/series/DGS3). So their TR if they forgo such loans is:\n\n$$ TR_{Forgo} = \\sum_{i \\in AllLoans} LoanAmount_i*(1.02).$$\n\nThus the decision to issue such loans depends on which part dominates. Savings of investors from avoiding top x% riskiest loans are the difference between the returns fom investing in T-notes and LendingClub loans:\n\n$$ \\sum_{i \\in AllLoans} LoanAmount_i*(1.02) - \\sum_{j \\in RepaidLoans}LoanAmount_j*(r_j+1) + \\sum_{k \\in DefaultedLoans}Recovery_k $$\n\nThe expression above is the value created from avoiding the riskiest loans. For example, if I use the model to identify top 10% riskiest loans, then AllLoans will be the loans with 10% of the highest predicted default probabilities. RepaidLoans and DefaultedLoans will be subsets of those loans.\n\nThe code below uses loan_amnt, int_rate and recovery features to calculate this value created. Risky loans are defined based on threshold, necessary to reach 10% recall.","metadata":{"execution":{"iopub.status.busy":"2022-08-13T22:19:40.839948Z","iopub.execute_input":"2022-08-13T22:19:40.840421Z","iopub.status.idle":"2022-08-13T22:19:40.84692Z","shell.execute_reply.started":"2022-08-13T22:19:40.840356Z","shell.execute_reply":"2022-08-13T22:19:40.845476Z"}}},{"cell_type":"code","source":"recoveries['total_recovery'] = recoveries.total_rec_prncp + recoveries.total_rec_int + recoveries.recoveries\nrecoveries['tot_recov_rp'] = recoveries.total_recovery/recoveries.loan_amnt\nrecoveries['tot_recov_rt'] = recoveries.total_recovery/(recoveries.loan_amnt*((recoveries.int_rate/100+1)**3))\n# in a few cases the recoveries seem to exceed total proceeds.\n# this may be the result of ignoring time discounting and compound interest in my calculations\n# ior those may be rarer 60-months loans. so i do not adjust such cases.\ndisplay(recoveries.describe(), recoveries.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:08:37.737489Z","iopub.execute_input":"2022-08-16T14:08:37.73786Z","iopub.status.idle":"2022-08-16T14:08:38.05233Z","shell.execute_reply.started":"2022-08-16T14:08:37.737826Z","shell.execute_reply":"2022-08-16T14:08:38.051225Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = X_test_0.copy()\ntest = X_test[['loan_amnt', 'int_rate']]\ntest.int_rate = test.int_rate/100+1\ntest['y_pred'] = optuna_xgb.predict_proba(X_test)[:,1]\ntest['id'] = test00.id\ntest['y'] = y_test\ntest = pd.merge(test, recoveries[['id', 'loan_amnt', 'int_rate', 'total_recovery']], on='id', how = 'left')\ndisplay(recoveries.head(), test.head())\ndisplay(test.loc[test.y==0].count(), test.loc[test.y==1].count())","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:08:38.054073Z","iopub.execute_input":"2022-08-16T14:08:38.054455Z","iopub.status.idle":"2022-08-16T14:08:40.595983Z","shell.execute_reply.started":"2022-08-16T14:08:38.054418Z","shell.execute_reply":"2022-08-16T14:08:40.595056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# select riskiest loans by calculating decision threshold, giving 10% recall:\n\ndesired_recall = 0.1\n\ntemp = recall_t[(recall_t>(desired_recall-0.001))&(recall_t<(desired_recall+0.001))]\ntemp = temp[int(len(temp)/2)]\nindexx = ((np.where(recall_t==temp)))[0][0]\nr10threshold = threshold[indexx]\np90risk = r10threshold\ntest['p90risk'] = (test.y_pred>=p90risk).astype(int)\ndisplay(test.shape, p90risk)\n\nrisky_loans = test[test.p90risk==1]\nrisky_loans.loc[risky_loans.total_recovery.isnull(),'total_recovery']=\\\nrisky_loans.loan_amnt_x*(risky_loans.int_rate_x**3)\n# when the loan is repaid, I calculate total return and save it into total_recovery column.\ndisplay(risky_loans.head(), risky_loans.shape)\n\nproceeds = risky_loans.total_recovery.sum()\nproceeds_tnotes = (risky_loans.loan_amnt_x.sum())*(1.02**3)\nprint(\"Investors' total returns from investing in risky loans\", int(proceeds))\nprint(\"Investors' total returns from investing in T-notes\", int(proceeds_tnotes))\nprint(\"Investors' savings: $\", int(proceeds_tnotes-proceeds))\nestimated_savings = (proceeds_tnotes-proceeds)*3.3*4\nprint(\"Estimated investors' savings from all LendingClub loans: $\", estimated_savings)\n\n\n# In this particular sample, the savings are maximized at 8.5% recall threshold.\n# Then the saving for investors will be $4.9M in test set or 64.6M for all loans.\n\nprint(time.time() - time0)","metadata":{"execution":{"iopub.status.busy":"2022-08-16T14:08:40.597395Z","iopub.execute_input":"2022-08-16T14:08:40.598421Z","iopub.status.idle":"2022-08-16T14:08:40.633276Z","shell.execute_reply.started":"2022-08-16T14:08:40.598381Z","shell.execute_reply":"2022-08-16T14:08:40.632179Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that using this model allows investors in the test sample to save \\\\$ 4.53M. \nAccounting for 75/25 train-test split and initial 30\\% sample from the original dataset, the estimated saving should be $$4.53M*3.3*4 = 59.7M.$$\n\n**The takeaway is that we do not necessarily need ML model with superior predictive performance over the whole feature space. As long as ML model performs well on a subset of instances, it can create great value if used properly**.\n","metadata":{}}]}
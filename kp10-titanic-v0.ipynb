{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 0. Load libraries #\n\nimport numpy as np\nimport pandas as pd\nimport os, time, warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.inspection import permutation_importance\nfrom xgboost import XGBClassifier\n\npd.set_option('display.max_columns', 20)\npd.set_option('mode.chained_assignment', None)\npd.set_option('display.expand_frame_repr', False)\nwarnings.filterwarnings('ignore')\n\ndef draw_histograms(df, variables, n_rows, n_cols):\n    # stolen from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n    fig.tight_layout()  \n    plt.show()\n\n\ndef fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode'):\n    \"\"\"This function speeds up filling missing values for 3 main datasets using different imputation methods.\n    Later may replace it with some subclass.\n    Example: fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\"\"\"\n    # set df_pred to None if it does not exist\n    if not ((cat_fill=='mode') and (num_fill=='median')):\n        print ('Imputation method not Implemented yet!')\n        return None\n    \n    df_train[num_features] = df_train[num_features].fillna(value=df_train[num_features].median())\n    df_test[num_features] = df_test[num_features].fillna(value=df_train[num_features].median())\n    df_train[cat_features] = df_train[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n    df_test[cat_features] = df_test[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n    if (df_pred is not None):\n        df_pred[num_features] = df_pred[num_features].fillna(value=df_train[num_features].median())\n        df_pred[cat_features] = df_pred[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n    df_train[num_features+cat_features].count\n    \n    all_good = (\n    (np.prod(df_train[num_features+cat_features].shape)==df_train[num_features+cat_features].count().sum()) and \n    (np.prod(df_test[num_features+cat_features].shape) == df_test[num_features+cat_features].count().sum()) and \n    (np.prod(df_pred[num_features+cat_features].shape) == df_pred[num_features+cat_features].count().sum()))\n    if (all_good):\n        print('Missing values imputed successfully')\n    else:\n        print('There are still some missing values...')\n    \ndef add_misDummy_mp_i1(df_train, df_test, df_pred, features):\n    \"\"\"This function creates new dummy columns for missing features.\n    Example: add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\"\"\"\n    # set df_pred to None if it does not exist\n    for feature_name in features:\n        misColName = 'mis'+feature_name\n        df_train.loc[df_train[feature_name].isnull(), misColName]=1\n        df_train.loc[df_train[feature_name].notnull(), misColName]=0\n        df_test.loc[df_test[feature_name].isnull(), misColName]=1\n        df_test.loc[df_test[feature_name].notnull(), misColName]=0\n        if (df_pred is not None):\n            df_pred.loc[df_pred[feature_name].isnull(), misColName]=1\n            df_pred.loc[df_pred[feature_name].notnull(), misColName]=0\n   \n\ndef discretize_mp_i1(df_train, df_test, df_pred, feature, ntiles, delete_feature=False):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: discretize_mp_i1(X_train, X_test, X_pred, 'Age', 15)\"\"\"\n    # set df_pred to None if it does not exist\n    _,bin = pd.qcut(df_train[feature], ntiles, retbins = True, labels = False, duplicates = 'drop')\n    df_train[feature+'Ntile'] = pd.cut(df_train[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    df_test[feature+'Ntile'] = pd.cut(df_test[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (df_pred is not None):\n        df_pred[feature+'Ntile'] = pd.cut(df_pred[feature], labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n    if (delete_feature==True):\n        df_train.drop(columns=[feature], inplace=True)\n        df_test.drop(columns=[feature], inplace=True)\n        df_pred.drop(columns=[feature], inplace=True)\n    print('Discretized ',feature, ' into ', len(bin)-1, ' bins')\n\n\ndef log_transformer_mp_i1(df_train, df_test, df_pred, feature_subset=False, min_skew=3):\n    \"\"\"This function divides a continuous feature into quantile groups.\n    Example: log_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\"\"\"\n    # set df_pred to None if it does not exist\n    if (feature_subset==False):\n        features_totransform = df_train.columns\n    else:\n        features_totransform = feature_subset.copy()\n    skewed_vars = list(df_train.skew()[abs(df_train.skew())>min_skew].index)\n    for col in list(set(skewed_vars)&set(features_totransform)):\n        df_train[col] = np.log1p(df_train[col])\n        df_test[col] = np.log1p(df_test[col])\n        if (df_pred is not None):\n            df_pred[col] = np.log1p(df_pred[col])\n    print('Skewed columns log-transformed: ', list(set(skewed_vars)&set(features_totransform)))\n    \n    \n    \n# 1. Load data #\n\ntime0 = time.time()\n\npath = '../input/titanic/train.csv'\ndf = pd.read_csv(path) \n\ndf.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],inplace=True)\npred=pd.read_csv('../input/titanic/test.csv')\npred0 = pred.copy()\npred.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],inplace=True)\n\nprint(df.shape, pred.shape)\n#df.head()\n\n# 2. pEDA #\n\n#df.Survived.value_counts()\ndf['Age2'] = df['Age']**2\npred['Age2'] = pred['Age']**2\n\n# 3. Train-test split #\n\ntrain_y = df[['Survived']]\ntrain_x = df.drop(columns = ['Survived'])\nX_pred = pred.copy()\n\n#bin_cols = [col for col in train_x.columns if train_x[col].nunique()==2]\ncat_cols = [col for col in train_x.columns if train_x[col].nunique() in range(2,10)]\nnum_cols = list(set(train_x.columns)-set(cat_cols))\n\nprint('categorical features: ', cat_cols, 'numerical features: ', num_cols)\n\nX_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.01, random_state=22)\nprint(X_train.shape, X_test.shape, y_train.shape, X_pred.shape)\n\nX_train.info()\n\n# 4. Misisng values #\n\nadd_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\n\nfillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\n#[X_train.count(), X_test.count(), X_pred.count()]\n\n# extra feature engineering (manual)\n\ndiscretize_mp_i1(X_train, X_test, X_pred, 'Age', 15)\ndiscretize_mp_i1(X_train, X_test, X_pred, 'SibSp', 30)\ndiscretize_mp_i1(X_train, X_test, X_pred, 'Parch', 60)\n\ncat_cols.extend(['misAge', 'AgeNtile', 'SibSpNtile', 'ParchNtile'])\ncat_cols = list(set(cat_cols)-set(['SibSp', 'Parch']))\n\n\n# 5.Feature engineering #\n\nlog_transformer_mp_i1(X_train, X_test, X_pred, feature_subset=num_cols)\n\n# in general, if I plan using raw ols, I should drop one group. o/w, it is beteer to leabe all ohc groups.\n\nfeature_transformer = ColumnTransformer([\n    (\"num\", StandardScaler(), num_cols),\n    (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\"), cat_cols),\n    ])\n\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nX_pred = pd.DataFrame(feature_transformer.transform(X_pred), columns=feature_transformer.get_feature_names_out())\n\nfewfeatures = ['num__Age', 'num__Age2', 'num__Fare', 'cat__Sex_male', 'cat__Pclass_2', 'cat__Pclass_3']\n\nX_train\n\n# 6. Fit models #\n\nlr = LogisticRegression()\nparam_grid = {'C':[0.3, 1, 3, 10, 30]}\nlrm = GridSearchCV(lr, param_grid, cv=8)\nlrm.fit(X_train, y_train)\nprint('Logistic ', lrm.best_params_, accuracy_score(y_train, lrm.predict(X_train)))\n\nsvm = SVC()\nparam_grid = {'C':[0.3, 1, 2, 3, 10]}\nsvmm = GridSearchCV(svm, param_grid, cv=8)\nsvmm.fit(X_train, y_train)\nprint('SVM ', svmm.best_params_, accuracy_score(y_train, svmm.predict(X_train)))\n\nknn = KNeighborsClassifier()\nparam_grid = dict(n_neighbors=range(2,20))\nknnm = GridSearchCV(knn, param_grid, cv=8)\nknnm.fit(X_train[fewfeatures], y_train)\nprint('KNN ', knnm.best_params_, accuracy_score(y_train, knnm.predict(X_train[fewfeatures])))\n\ntime1 = time.time()\nrf = RandomForestClassifier()\nparam_grid = {'n_estimators':[100,200], 'max_depth':[2,4,6,8], 'max_features':[4,5,6]}\nrfm = GridSearchCV(rf, param_grid, cv=4)\nrfm.fit(X_train, y_train)\nprint('RF ', rfm.best_params_, accuracy_score(y_train, rfm.predict(X_train)), time.time()-time1)\n\ntime1 = time.time()\nxgb = XGBClassifier()\n# use 'gpu_hist' for more than 100,000 examples.\nparam_grid = {'n_estimators':[100,200], 'max_depth':[2,3,4], 'eta':[0.03, 0.04, 0.05], 'subsample':[0.4, 0.6],\n             'colsample_bytree':[0.6, 0.8]}\nxgbm = GridSearchCV(xgb, param_grid, cv=4)\nxgbm.fit(X_train, y_train)\nprint('XGB ', xgbm.best_params_, accuracy_score(y_train, xgbm.predict(X_train)), time.time()-time1)\n\n# 7. accuracy #\n\nprint('Out of Sample:')\nprint('Logistic ', accuracy_score(y_test, lrm.predict(X_test)))\nprint('SVM ', accuracy_score(y_test, svmm.predict(X_test)))\nprint('KNN ', accuracy_score(y_test, knnm.predict(X_test[fewfeatures])))\nprint('RF ', accuracy_score(y_test, rfm.predict(X_test)))\nprint('XGB ', accuracy_score(y_test, xgbm.predict(X_test)))\nprint('Total time ', time.time()-time0)\n\n# VotingClassifier:\n\nestimator = []\nestimator.append(('LR', LogisticRegression(C=1)))\nestimator.append(('SVM', SVC(C=1, probability = True)))\n#estimator.append(('KNN', KNeighborsClassifier(n_neighbors=5)))\nestimator.append(('RF', RandomForestClassifier(max_depth=4, max_features=5, n_estimators=200)))\nestimator.append(('XGB', XGBClassifier(eta=0.04, max_depth=3, n_estimators=200, \n                                       subsample=0.6, colsample_bytree=0.6)))\nvot_soft = VotingClassifier(estimators = estimator, voting ='soft')\nvot_soft.fit(X_train, y_train)\nprint('VotingClassifier5 ', accuracy_score(y_train, vot_soft.predict(X_train)))\nprint('VotingClassifier5 ', accuracy_score(y_test, vot_soft.predict(X_test)))\n# to add KNN with different feature sets, \n# see https://stackoverflow.com/questions/45074579/votingclassifier-different-feature-sets","metadata":{"execution":{"iopub.status.busy":"2022-05-22T15:35:16.223271Z","iopub.execute_input":"2022-05-22T15:35:16.223683Z","iopub.status.idle":"2022-05-22T15:36:30.524915Z","shell.execute_reply.started":"2022-05-22T15:35:16.223591Z","shell.execute_reply":"2022-05-22T15:36:30.524278Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"(891, 8) (418, 7)\ncategorical features:  ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked'] numerical features:  ['Age', 'Fare', 'Age2']\n(882, 8) (9, 8) (882, 1) (418, 8)\n<class 'pandas.core.frame.DataFrame'>\nInt64Index: 882 entries, 504 to 885\nData columns (total 8 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   Pclass    882 non-null    int64  \n 1   Sex       882 non-null    object \n 2   Age       707 non-null    float64\n 3   SibSp     882 non-null    int64  \n 4   Parch     882 non-null    int64  \n 5   Fare      882 non-null    float64\n 6   Embarked  880 non-null    object \n 7   Age2      707 non-null    float64\ndtypes: float64(3), int64(3), object(2)\nmemory usage: 62.0+ KB\nMissing values imputed successfully\nDiscretized  Age  into  12  bins\nDiscretized  SibSp  into  4  bins\nDiscretized  Parch  into  4  bins\nSkewed columns log-transformed:  ['Fare']\nLogistic  {'C': 0.3} 0.81859410430839\nSVM  {'C': 1} 0.8435374149659864\nKNN  {'n_neighbors': 4} 0.873015873015873\nRF  {'max_depth': 8, 'max_features': 6, 'n_estimators': 200} 0.8968253968253969 29.902045011520386\nXGB  {'colsample_bytree': 0.8, 'eta': 0.05, 'max_depth': 4, 'n_estimators': 200, 'subsample': 0.4} 0.8922902494331065 36.692791223526\nOut of Sample:\nLogistic  0.5555555555555556\nSVM  0.6666666666666666\nKNN  0.6666666666666666\nRF  0.6666666666666666\nXGB  0.7777777777777778\nTotal time  71.84984016418457\nVotingClassifier5  0.8435374149659864\nVotingClassifier5  0.6666666666666666\n","output_type":"stream"}]},{"cell_type":"code","source":"# 8. feature importance #\n\nresults = permutation_importance(xgbm, X_test, y_test, scoring='accuracy', n_jobs=-1)\nfi_lr = pd.DataFrame({'col':X_test.columns, 'FI':results.importances_mean})\n#fi_lr.sort_values('FI', ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:36:21.582424Z","iopub.execute_input":"2022-05-21T21:36:21.583119Z","iopub.status.idle":"2022-05-21T21:36:24.369168Z","shell.execute_reply.started":"2022-05-21T21:36:21.583082Z","shell.execute_reply":"2022-05-21T21:36:24.368351Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# 9. predictions #\n\nsubmission_df_vc = pd.DataFrame({'PassengerId': pred0.PassengerId, 'Survived': vot_soft.predict(X_pred)}, columns=['PassengerId', 'Survived'])\nsubmission_df_svm = pd.DataFrame({'PassengerId': pred0.PassengerId, 'Survived': svmm.predict(X_pred)}, columns=['PassengerId', 'Survived'])\nsubmission_df_rf = pd.DataFrame({'PassengerId': pred0.PassengerId, 'Survived': rfm.predict(X_pred)}, columns=['PassengerId', 'Survived'])\nsubmission_df_bt = pd.DataFrame({'PassengerId': pred0.PassengerId, 'Survived': xgbm.predict(X_pred)}, columns=['PassengerId', 'Survived'])\n\nsubmission_df_vc.to_csv('KP10_v2_vc.csv',index=False)\nsubmission_df_svm.to_csv('KP10_v2_svm.csv',index=False)\nsubmission_df_rf.to_csv('KP10_v2_rf.csv',index=False)\nsubmission_df_bt.to_csv('KP10_v2_bt.csv',index=False)\n\nos.chdir(r'/kaggle/working')\n\nfrom IPython.display import FileLink\nFileLink(r'KP10_v2_vc.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:07:00.674332Z","iopub.execute_input":"2022-05-21T22:07:00.674621Z","iopub.status.idle":"2022-05-21T22:07:00.849514Z","shell.execute_reply.started":"2022-05-21T22:07:00.674594Z","shell.execute_reply":"2022-05-21T22:07:00.848804Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/KP10_v2_vc.csv","text/html":"<a href='KP10_v2_vc.csv' target='_blank'>KP10_v2_vc.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"np.prod(list(df.shape)) == df.count().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T23:17:24.260134Z","iopub.execute_input":"2022-05-21T23:17:24.260380Z","iopub.status.idle":"2022-05-21T23:17:24.269679Z","shell.execute_reply.started":"2022-05-21T23:17:24.260355Z","shell.execute_reply":"2022-05-21T23:17:24.268968Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]}]}
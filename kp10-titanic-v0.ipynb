{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# 0. Load libraries #\n\nimport numpy as np\nimport pandas as pd\nimport os, time, warnings\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.svm import SVC\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, OneHotEncoder, StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, GridSearchCV, train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier, VotingClassifier\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom sklearn.inspection import permutation_importance\nfrom xgboost import XGBClassifier\n\npd.set_option('display.max_columns', 20)\npd.set_option('mode.chained_assignment', None)\npd.set_option('display.expand_frame_repr', False)\nwarnings.filterwarnings('ignore')\n\ndef draw_histograms(df, variables, n_rows, n_cols):\n    # stolen from https://stackoverflow.com/questions/29530355/plotting-multiple-histograms-in-grid\n    fig=plt.figure()\n    for i, var_name in enumerate(variables):\n        ax=fig.add_subplot(n_rows,n_cols,i+1)\n        df[var_name].hist(bins=10,ax=ax)\n        ax.set_title(var_name+\" Distribution\")\n    fig.tight_layout()  \n    plt.show()\n\n\ndef fillna_mp_i1(df_train, df_test, df_pred, num_features, cat_features, num_fill='median', cat_fill='mode'):\n    \"\"\"This function speeds up filling missing values for 3 main datasets using different imputation methods.\n    Later may replace it with some subclass.\n    Example: fillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\"\"\"\n    # set df_pred to 0 if it does not exist\n    if not ((cat_fill=='mode') and (num_fill=='median')):\n        print ('Imputation method not Implemented yet!')\n        return None\n    \n    df_train[num_features] = df_train[num_features].fillna(value=df_train[num_features].median())\n    df_test[num_features] = df_test[num_features].fillna(value=df_train[num_features].median())\n    df_train[cat_features] = df_train[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n    df_test[cat_features] = df_test[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n    if (type(df_pred)!=int):\n        df_pred[num_features] = df_pred[num_features].fillna(value=df_train[num_features].median())\n        df_pred[cat_features] = df_pred[cat_features].fillna(value=df_train[cat_features].mode().iloc[0])\n    df_train[num_features+cat_features].count\n    \n    all_good = (\n    (np.prod(df_train[num_features+cat_features].shape)==df_train[num_features+cat_features].count().sum()) and \n    (np.prod(df_test[num_features+cat_features].shape) == df_test[num_features+cat_features].count().sum()) and \n    (np.prod(df_pred[num_features+cat_features].shape) == df_pred[num_features+cat_features].count().sum()))\n    if (all_good):\n        print('Missing values imputed successfully')\n    else:\n        print('There are still some missing values...')\n    \ndef add_misDummy_mp_i1(df_train, df_test, df_pred, features):\n    \"\"\"This function creates new dummy columns for missing features.\n    Example: add_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\"\"\"\n    for feature_name in features:\n        misColName = 'mis'+feature_name\n        df_train.loc[df_train[feature_name].isnull(), misColName]=1\n        df_train.loc[df_train[feature_name].notnull(), misColName]=0\n        df_test.loc[df_test[feature_name].isnull(), misColName]=1\n        df_test.loc[df_test[feature_name].notnull(), misColName]=0\n        df_pred.loc[df_pred[feature_name].isnull(), misColName]=1\n        df_pred.loc[df_pred[feature_name].notnull(), misColName]=0\n    \n    \n# 1. Load data #\n\ntime0 = time.time()\n\npath = '../input/titanic/train.csv'\ndf = pd.read_csv(path) \n\ndf.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],inplace=True)\npred=pd.read_csv('../input/titanic/test.csv')\npred0 = pred.copy()\npred.drop(columns=['Name', 'Ticket', 'Cabin', 'PassengerId'],inplace=True)\n\nprint(df.shape, pred.shape)\n#df.head()\n\n# 2. pEDA #\n\n#df.Survived.value_counts()\ndf['Age2'] = df['Age']**2\npred['Age2'] = pred['Age']**2\n\n# 3. Train-test split #\n\ntrain_y = df[['Survived']]\ntrain_x = df.drop(columns = ['Survived'])\nX_pred = pred.copy()\n\n#bin_cols = [col for col in train_x.columns if train_x[col].nunique()==2]\ncat_cols = [col for col in train_x.columns if train_x[col].nunique() in range(2,10)]\nnum_cols = list(set(train_x.columns)-set(cat_cols))\n\nprint('categorical features: ', cat_cols, 'numerical features: ', num_cols)\n\nX_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size = 0.01, random_state=22)\nprint(X_train.shape, X_test.shape, y_train.shape, X_pred.shape)\n\nX_train.info()\n\n# 4. Misisng values #\n\n\nadd_misDummy_mp_i1(X_train, X_test, X_pred, ['Age'])\n\nfillna_mp_i1(X_train, X_test, X_pred, num_cols, cat_cols)\n[X_train.count(), X_test.count(), X_pred.count()]\n\n\n# extra feature engineering (manual)\n\n_,bin = pd.qcut(X_train.Age, 15, retbins = True, labels = False, duplicates = 'drop')\nX_train['AgeDecile'] = pd.cut(X_train.Age, labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\nX_test['AgeDecile'] = pd.cut(X_test.Age, labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\nX_pred['AgeDecile'] = pd.cut(X_pred.Age, labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n\n#X_train.Age_decile.value_counts()\n\n_,bin = pd.qcut(X_train.SibSp, 30, retbins = True, labels = False, duplicates = 'drop')\nX_train['SibspNtile'] = pd.cut(X_train.SibSp, labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\nX_test['SibspNtile'] = pd.cut(X_test.SibSp, labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\nX_pred['SibspNtile'] = pd.cut(X_pred.SibSp, labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n\n_,bin = pd.qcut(X_train.Parch, 60, retbins = True, labels = False, duplicates = 'drop')\nX_train['ParchNtile'] = pd.cut(X_train.Parch, labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\nX_test['ParchNtile'] = pd.cut(X_test.Parch, labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\nX_pred['ParchNtile'] = pd.cut(X_pred.Parch, labels=False, duplicates = 'drop', bins = bin ,include_lowest = True)\n\n\ncat_cols.extend(['misAge', 'AgeDecile', 'SibspNtile', 'ParchNtile'])\ncat_cols = list(set(cat_cols)-set(['SibSp', 'Parch']))\n\n\n# 5.1 Feature engineering, dealing with skew #\n\nskewed_vars = list(X_train.skew()[abs(X_train.skew())>3].index)\n\nfor col in X_train.columns:\n    if (col in skewed_vars) and (col in num_cols):\n        X_train[col] = np.log1p(X_train[col])\n        X_test[col] = np.log1p(X_test[col])\n        X_pred[col] = np.log1p(X_pred[col])\n\n# 5.2 Feature engineering #\n\n# in general, if I plan using raw ols, I should drop one group. o/w, it is beteer to leabe all ohc groups.\n\nfeature_transformer = ColumnTransformer([\n    (\"num\", StandardScaler(), num_cols),\n    (\"cat\", OneHotEncoder(sparse = False, handle_unknown=\"ignore\"), cat_cols),\n    ])\n\nX_train = pd.DataFrame(feature_transformer.fit_transform(X_train), columns=feature_transformer.get_feature_names_out())\nX_test = pd.DataFrame(feature_transformer.transform(X_test), columns=feature_transformer.get_feature_names_out())\nX_pred = pd.DataFrame(feature_transformer.transform(X_pred), columns=feature_transformer.get_feature_names_out())\n\nfewfeatures = ['num__Age', 'num__Age2', 'num__Fare', 'cat__Sex_male', 'cat__Pclass_2', 'cat__Pclass_3']\n\nX_train\n\n# 6. Fit models #\n\nlr = LogisticRegression()\nparam_grid = {'C':[0.3, 1, 3, 10, 30]}\nlrm = GridSearchCV(lr, param_grid, cv=8)\nlrm.fit(X_train, y_train)\nprint('Logistic ', lrm.best_params_, accuracy_score(y_train, lrm.predict(X_train)))\n\nsvm = SVC()\nparam_grid = {'C':[0.3, 1, 2, 3, 10]}\nsvmm = GridSearchCV(svm, param_grid, cv=8)\nsvmm.fit(X_train, y_train)\nprint('SVM ', svmm.best_params_, accuracy_score(y_train, svmm.predict(X_train)))\n\nknn = KNeighborsClassifier()\nparam_grid = dict(n_neighbors=range(2,20))\nknnm = GridSearchCV(knn, param_grid, cv=8)\nknnm.fit(X_train[fewfeatures], y_train)\nprint('KNN ', knnm.best_params_, accuracy_score(y_train, knnm.predict(X_train[fewfeatures])))\n\ntime1 = time.time()\nrf = RandomForestClassifier()\nparam_grid = {'n_estimators':[100,200], 'max_depth':[2,4,6,8], 'max_features':[4,5,6]}\nrfm = GridSearchCV(rf, param_grid, cv=4)\nrfm.fit(X_train, y_train)\nprint('RF ', rfm.best_params_, accuracy_score(y_train, rfm.predict(X_train)), time.time()-time1)\n\ntime1 = time.time()\nxgb = XGBClassifier()\n# use 'gpu_hist' for more than 100,000 examples.\nparam_grid = {'n_estimators':[100,200], 'max_depth':[2,3,4], 'eta':[0.03, 0.04, 0.05], 'subsample':[0.4, 0.6],\n             'colsample_bytree':[0.6, 0.8]}\nxgbm = GridSearchCV(xgb, param_grid, cv=4)\nxgbm.fit(X_train, y_train)\nprint('XGB ', xgbm.best_params_, accuracy_score(y_train, xgbm.predict(X_train)), time.time()-time1)\n\n# 7. accuracy #\n\nprint('Out of Sample:')\nprint('Logistic ', accuracy_score(y_test, lrm.predict(X_test)))\nprint('SVM ', accuracy_score(y_test, svmm.predict(X_test)))\nprint('KNN ', accuracy_score(y_test, knnm.predict(X_test[fewfeatures])))\nprint('RF ', accuracy_score(y_test, rfm.predict(X_test)))\nprint('XGB ', accuracy_score(y_test, xgbm.predict(X_test)))\nprint('Total time ', time.time()-time0)\n\n# VotingClassifier:\n\nestimator = []\nestimator.append(('LR', LogisticRegression(C=1)))\nestimator.append(('SVM', SVC(C=1, probability = True)))\n#estimator.append(('KNN', KNeighborsClassifier(n_neighbors=5)))\nestimator.append(('RF', RandomForestClassifier(max_depth=4, max_features=5, n_estimators=200)))\nestimator.append(('XGB', XGBClassifier(eta=0.04, max_depth=3, n_estimators=200, \n                                       subsample=0.6, colsample_bytree=0.6)))\nvot_soft = VotingClassifier(estimators = estimator, voting ='soft')\nvot_soft.fit(X_train, y_train)\nprint('VotingClassifier5 ', accuracy_score(y_train, vot_soft.predict(X_train)))\nprint('VotingClassifier5 ', accuracy_score(y_test, vot_soft.predict(X_test)))\n# to add KNN with different feature sets, \n# see https://stackoverflow.com/questions/45074579/votingclassifier-different-feature-sets","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:04:59.172791Z","iopub.execute_input":"2022-05-21T22:04:59.173061Z","iopub.status.idle":"2022-05-21T22:06:09.245450Z","shell.execute_reply.started":"2022-05-21T22:04:59.173028Z","shell.execute_reply":"2022-05-21T22:06:09.244725Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# 8. feature importance #\n\nresults = permutation_importance(xgbm, X_test, y_test, scoring='accuracy', n_jobs=-1)\nfi_lr = pd.DataFrame({'col':X_test.columns, 'FI':results.importances_mean})\n#fi_lr.sort_values('FI', ascending = False)","metadata":{"execution":{"iopub.status.busy":"2022-05-21T21:36:21.582424Z","iopub.execute_input":"2022-05-21T21:36:21.583119Z","iopub.status.idle":"2022-05-21T21:36:24.369168Z","shell.execute_reply.started":"2022-05-21T21:36:21.583082Z","shell.execute_reply":"2022-05-21T21:36:24.368351Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# 9. predictions #\n\nsubmission_df_vc = pd.DataFrame({'PassengerId': pred0.PassengerId, 'Survived': vot_soft.predict(X_pred)}, columns=['PassengerId', 'Survived'])\nsubmission_df_svm = pd.DataFrame({'PassengerId': pred0.PassengerId, 'Survived': svmm.predict(X_pred)}, columns=['PassengerId', 'Survived'])\nsubmission_df_rf = pd.DataFrame({'PassengerId': pred0.PassengerId, 'Survived': rfm.predict(X_pred)}, columns=['PassengerId', 'Survived'])\nsubmission_df_bt = pd.DataFrame({'PassengerId': pred0.PassengerId, 'Survived': xgbm.predict(X_pred)}, columns=['PassengerId', 'Survived'])\n\nsubmission_df_vc.to_csv('KP10_v2_vc.csv',index=False)\nsubmission_df_svm.to_csv('KP10_v2_svm.csv',index=False)\nsubmission_df_rf.to_csv('KP10_v2_rf.csv',index=False)\nsubmission_df_bt.to_csv('KP10_v2_bt.csv',index=False)\n\nos.chdir(r'/kaggle/working')\n\nfrom IPython.display import FileLink\nFileLink(r'KP10_v2_vc.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-21T22:07:00.674332Z","iopub.execute_input":"2022-05-21T22:07:00.674621Z","iopub.status.idle":"2022-05-21T22:07:00.849514Z","shell.execute_reply.started":"2022-05-21T22:07:00.674594Z","shell.execute_reply":"2022-05-21T22:07:00.848804Z"},"trusted":true},"execution_count":38,"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"/kaggle/working/KP10_v2_vc.csv","text/html":"<a href='KP10_v2_vc.csv' target='_blank'>KP10_v2_vc.csv</a><br>"},"metadata":{}}]},{"cell_type":"code","source":"np.prod(list(df.shape)) == df.count().sum()","metadata":{"execution":{"iopub.status.busy":"2022-05-21T23:17:24.260134Z","iopub.execute_input":"2022-05-21T23:17:24.260380Z","iopub.status.idle":"2022-05-21T23:17:24.269679Z","shell.execute_reply.started":"2022-05-21T23:17:24.260355Z","shell.execute_reply":"2022-05-21T23:17:24.268968Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"False"},"metadata":{}}]}]}